[{"st": 8, "ed": 10, "text": "recurrent layers"}, {"st": 12, "ed": 15, "text": "visual and textual"}, {"st": 31, "ed": 34, "text": "visual and textual"}, {"st": 80, "ed": 82, "text": "attention mechanisms"}, {"st": 101, "ed": 103, "text": "attention mechanism"}, {"st": 103, "ed": 105, "text": "improves performance"}]
[{"st": 0, "ed": 2, "text": "recent approaches"}, {"st": 4, "ed": 7, "text": "artificial neural networks"}, {"st": 9, "ed": 12, "text": "shown promising results"}, {"st": 13, "ed": 15, "text": "short text"}, {"st": 60, "ed": 63, "text": "recurrent neural networks"}, {"st": 64, "ed": 67, "text": "convolutional neural networks"}]
[{"st": 4, "ed": 7, "text": "recurrent neural network"}, {"st": 16, "ed": 18, "text": "natural language"}, {"st": 36, "ed": 38, "text": "natural language"}, {"st": 77, "ed": 80, "text": "recurrent neural network"}, {"st": 85, "ed": 87, "text": "log likelihood"}, {"st": 95, "ed": 97, "text": "log likelihood"}, {"st": 99, "ed": 101, "text": "natural language"}, {"st": 107, "ed": 109, "text": "log likelihood"}, {"st": 127, "ed": 129, "text": "response generation"}, {"st": 146, "ed": 148, "text": "competing approaches"}, {"st": 161, "ed": 163, "text": "automatic evaluation"}, {"st": 184, "ed": 186, "text": "automatic evaluation"}, {"st": 189, "ed": 191, "text": "experiments demonstrate"}, {"st": 203, "ed": 205, "text": "natural language"}]
[{"st": 0, "ed": 3, "text": "multi task learning"}, {"st": 17, "ed": 19, "text": "related problems"}, {"st": 24, "ed": 27, "text": "deep neural networks"}, {"st": 30, "ed": 32, "text": "related tasks"}, {"st": 45, "ed": 47, "text": "transfer knowledge"}, {"st": 50, "ed": 52, "text": "natural language"}, {"st": 83, "ed": 86, "text": "multi task learning"}, {"st": 87, "ed": 89, "text": "trainable parameters"}, {"st": 160, "ed": 162, "text": "parameter sharing"}]
[{"st": 4, "ed": 7, "text": "deep reinforcement learning"}, {"st": 14, "ed": 16, "text": "learning algorithms"}, {"st": 20, "ed": 22, "text": "alexa prize"}, {"st": 47, "ed": 49, "text": "natural language"}, {"st": 52, "ed": 54, "text": "models including"}, {"st": 57, "ed": 60, "text": "bag of words"}, {"st": 64, "ed": 66, "text": "neural network"}, {"st": 67, "ed": 69, "text": "latent variable"}, {"st": 69, "ed": 71, "text": "neural network"}, {"st": 74, "ed": 76, "text": "reinforcement learning"}, {"st": 80, "ed": 82, "text": "real world"}, {"st": 82, "ed": 84, "text": "user interactions"}, {"st": 106, "ed": 109, "text": "a b testing"}, {"st": 110, "ed": 112, "text": "real world"}, {"st": 125, "ed": 127, "text": "machine learning"}]
[{"st": 4, "ed": 6, "text": "generative model"}, {"st": 58, "ed": 60, "text": "higher quality"}, {"st": 84, "ed": 86, "text": "sentence level"}]
[{"st": 4, "ed": 7, "text": "deep reinforcement learning"}, {"st": 14, "ed": 16, "text": "learning algorithms"}, {"st": 20, "ed": 22, "text": "alexa prize"}, {"st": 47, "ed": 49, "text": "natural language"}, {"st": 52, "ed": 54, "text": "models including"}, {"st": 54, "ed": 56, "text": "neural network"}, {"st": 62, "ed": 64, "text": "reinforcement learning"}, {"st": 68, "ed": 70, "text": "real world"}, {"st": 70, "ed": 72, "text": "user interactions"}, {"st": 94, "ed": 97, "text": "a b testing"}, {"st": 98, "ed": 100, "text": "real world"}, {"st": 119, "ed": 122, "text": "deep reinforcement learning"}, {"st": 128, "ed": 130, "text": "real world"}, {"st": 130, "ed": 132, "text": "open domain"}]
[{"st": 70, "ed": 72, "text": "local binary"}, {"st": 77, "ed": 79, "text": "feature vectors"}, {"st": 88, "ed": 90, "text": "clustering approach"}, {"st": 93, "ed": 95, "text": "feature vector"}, {"st": 131, "ed": 133, "text": "proposed method"}]
[{"st": 9, "ed": 11, "text": "computer vision"}, {"st": 12, "ed": 14, "text": "natural language"}, {"st": 25, "ed": 27, "text": "real world"}, {"st": 37, "ed": 39, "text": "based approach"}, {"st": 40, "ed": 42, "text": "answer questions"}, {"st": 71, "ed": 73, "text": "competitive performance"}, {"st": 114, "ed": 116, "text": "deep learning"}, {"st": 133, "ed": 135, "text": "performance improvement"}]
[{"st": 2, "ed": 5, "text": "graphical user interface"}, {"st": 37, "ed": 39, "text": "deep learning"}, {"st": 47, "ed": 50, "text": "end to end"}, {"st": 51, "ed": 53, "text": "automatically generate"}, {"st": 57, "ed": 59, "text": "input image"}, {"st": 72, "ed": 74, "text": "web based"}]
[{"st": 1, "ed": 3, "text": "feature representations"}, {"st": 8, "ed": 11, "text": "deep neural networks"}, {"st": 18, "ed": 21, "text": "significant performance gains"}, {"st": 56, "ed": 58, "text": "performance improvements"}, {"st": 62, "ed": 64, "text": "domain adaptation"}, {"st": 65, "ed": 67, "text": "speaker recognition"}]
[{"st": 3, "ed": 5, "text": "neural architecture"}, {"st": 23, "ed": 25, "text": "neural network"}, {"st": 42, "ed": 44, "text": "policy gradient"}, {"st": 51, "ed": 53, "text": "expected reward"}, {"st": 76, "ed": 78, "text": "parameter sharing"}, {"st": 108, "ed": 110, "text": "neural architecture"}, {"st": 144, "ed": 146, "text": "cifar 10"}]
[{"st": 0, "ed": 2, "text": "recent progress"}, {"st": 3, "ed": 6, "text": "artificial intelligence ai"}, {"st": 24, "ed": 27, "text": "deep neural networks"}, {"st": 27, "ed": 31, "text": "trained end to end"}, {"st": 35, "ed": 37, "text": "object recognition"}, {"st": 37, "ed": 39, "text": "video games"}, {"st": 40, "ed": 42, "text": "board games"}, {"st": 64, "ed": 66, "text": "human intelligence"}, {"st": 73, "ed": 75, "text": "cognitive science"}, {"st": 82, "ed": 84, "text": "thinking machines"}, {"st": 111, "ed": 113, "text": "causal models"}, {"st": 125, "ed": 127, "text": "pattern recognition"}, {"st": 183, "ed": 185, "text": "neural network"}]
[{"st": 5, "ed": 8, "text": "visual object recognition"}, {"st": 13, "ed": 15, "text": "important role"}, {"st": 16, "ed": 18, "text": "human intelligence"}, {"st": 30, "ed": 32, "text": "higher level"}, {"st": 47, "ed": 49, "text": "deep learning"}, {"st": 51, "ed": 53, "text": "higher level"}, {"st": 55, "ed": 58, "text": "probabilistic graphical models"}, {"st": 85, "ed": 87, "text": "deep learning"}, {"st": 93, "ed": 95, "text": "probabilistic framework"}, {"st": 103, "ed": 105, "text": "unified framework"}, {"st": 112, "ed": 114, "text": "deep learning"}, {"st": 119, "ed": 121, "text": "higher level"}, {"st": 129, "ed": 131, "text": "inference process"}, {"st": 149, "ed": 151, "text": "deep learning"}, {"st": 157, "ed": 159, "text": "recommender systems"}, {"st": 159, "ed": 161, "text": "topic models"}, {"st": 175, "ed": 177, "text": "deep learning"}]
[{"st": 14, "ed": 16, "text": "reinforcement learning"}, {"st": 162, "ed": 164, "text": "decision process"}]
[{"st": 21, "ed": 23, "text": "low level"}, {"st": 53, "ed": 55, "text": "ground truth"}, {"st": 58, "ed": 60, "text": "convolutional network"}, {"st": 92, "ed": 94, "text": "unsupervised learning"}, {"st": 109, "ed": 111, "text": "extensive experiments"}, {"st": 120, "ed": 122, "text": "transfer learning"}, {"st": 123, "ed": 125, "text": "object detection"}, {"st": 127, "ed": 129, "text": "significantly outperforms"}, {"st": 137, "ed": 139, "text": "training data"}]
[{"st": 4, "ed": 6, "text": "neural network"}, {"st": 11, "ed": 13, "text": "domain adaptation"}, {"st": 21, "ed": 25, "text": "maximum mean discrepancy mmd"}, {"st": 31, "ed": 33, "text": "supervised learning"}, {"st": 40, "ed": 44, "text": "source and target domains"}, {"st": 63, "ed": 65, "text": "domain adaptation"}, {"st": 71, "ed": 73, "text": "raw image"}, {"st": 77, "ed": 79, "text": "image data"}, {"st": 91, "ed": 93, "text": "auto encoder"}]
[{"st": 0, "ed": 2, "text": "recent studies"}, {"st": 7, "ed": 10, "text": "recurrent neural networks"}, {"st": 11, "ed": 13, "text": "machine translation"}, {"st": 13, "ed": 15, "text": "image captioning"}, {"st": 32, "ed": 34, "text": "open research"}, {"st": 63, "ed": 65, "text": "gesture recognition"}, {"st": 66, "ed": 68, "text": "temporal information"}, {"st": 79, "ed": 81, "text": "deep architectures"}, {"st": 82, "ed": 84, "text": "gesture recognition"}, {"st": 90, "ed": 93, "text": "end to end"}, {"st": 94, "ed": 96, "text": "neural network"}, {"st": 104, "ed": 106, "text": "main contributions"}, {"st": 137, "ed": 139, "text": "gesture recognition"}]
[{"st": 8, "ed": 11, "text": "optical character recognition"}, {"st": 19, "ed": 22, "text": "end to end"}, {"st": 52, "ed": 54, "text": "challenging task"}, {"st": 101, "ed": 103, "text": "neural networks"}, {"st": 114, "ed": 117, "text": "convolutional neural networks"}, {"st": 132, "ed": 134, "text": "deep learning"}]
[{"st": 4, "ed": 8, "text": "generative adversarial networks gans"}, {"st": 11, "ed": 13, "text": "generative models"}, {"st": 20, "ed": 22, "text": "complex data"}, {"st": 33, "ed": 35, "text": "latent space"}, {"st": 52, "ed": 54, "text": "latent representations"}, {"st": 60, "ed": 62, "text": "feature representations"}, {"st": 93, "ed": 96, "text": "generative adversarial networks"}, {"st": 111, "ed": 113, "text": "feature representation"}]
[{"st": 0, "ed": 3, "text": "supervised machine learning"}, {"st": 144, "ed": 146, "text": "linear models"}, {"st": 150, "ed": 153, "text": "deep neural networks"}]
[{"st": 7, "ed": 9, "text": "representation learning"}, {"st": 10, "ed": 12, "text": "non stationary"}, {"st": 28, "ed": 30, "text": "sparse coding"}, {"st": 38, "ed": 40, "text": "hidden units"}, {"st": 52, "ed": 54, "text": "dentate gyrus"}, {"st": 63, "ed": 65, "text": "cognitive function"}, {"st": 72, "ed": 74, "text": "online learning"}, {"st": 131, "ed": 133, "text": "l2 regularization"}, {"st": 133, "ed": 135, "text": "group sparsity"}, {"st": 140, "ed": 143, "text": "block coordinate descent"}, {"st": 150, "ed": 152, "text": "alternating minimization"}, {"st": 162, "ed": 164, "text": "hidden unit"}, {"st": 175, "ed": 177, "text": "empirical evaluation"}, {"st": 179, "ed": 181, "text": "real life"}, {"st": 189, "ed": 191, "text": "synthetic data"}, {"st": 194, "ed": 196, "text": "proposed approach"}, {"st": 203, "ed": 205, "text": "fixed size"}, {"st": 207, "ed": 209, "text": "sparse coding"}]
[{"st": 0, "ed": 3, "text": "deep neural networks"}, {"st": 8, "ed": 11, "text": "labeled training data"}, {"st": 37, "ed": 39, "text": "fine tuning"}, {"st": 45, "ed": 47, "text": "deep learning"}, {"st": 57, "ed": 59, "text": "learning task"}, {"st": 61, "ed": 63, "text": "training data"}, {"st": 70, "ed": 72, "text": "learning task"}, {"st": 79, "ed": 81, "text": "learning task"}, {"st": 105, "ed": 107, "text": "learning task"}, {"st": 108, "ed": 110, "text": "low level"}, {"st": 118, "ed": 120, "text": "learning task"}, {"st": 122, "ed": 124, "text": "fine tune"}, {"st": 125, "ed": 127, "text": "convolutional layers"}, {"st": 158, "ed": 160, "text": "training samples"}, {"st": 165, "ed": 167, "text": "experiments demonstrate"}, {"st": 171, "ed": 173, "text": "fine tuning"}, {"st": 183, "ed": 185, "text": "classification tasks"}, {"st": 187, "ed": 189, "text": "training data"}, {"st": 210, "ed": 212, "text": "fine tuning"}, {"st": 214, "ed": 216, "text": "source domain"}, {"st": 217, "ed": 219, "text": "proposed method"}, {"st": 222, "ed": 224, "text": "classification accuracy"}]
[{"st": 4, "ed": 6, "text": "computer vision"}, {"st": 12, "ed": 14, "text": "visual representations"}, {"st": 29, "ed": 31, "text": "vision language"}, {"st": 56, "ed": 58, "text": "visual recognition"}, {"st": 64, "ed": 67, "text": "visual question answering"}, {"st": 93, "ed": 95, "text": "visual recognition"}, {"st": 125, "ed": 127, "text": "vision systems"}]
[{"st": 1, "ed": 3, "text": "deep learning"}, {"st": 15, "ed": 18, "text": "vulnerable to adversarial"}, {"st": 70, "ed": 72, "text": "image classification"}, {"st": 78, "ed": 80, "text": "image segmentation"}, {"st": 87, "ed": 89, "text": "adversarial perturbations"}]
[{"st": 2, "ed": 4, "text": "optimization problem"}, {"st": 5, "ed": 8, "text": "deep neural networks"}, {"st": 20, "ed": 22, "text": "deep networks"}, {"st": 41, "ed": 43, "text": "local minima"}, {"st": 60, "ed": 62, "text": "local minima"}, {"st": 63, "ed": 65, "text": "globally optimal"}, {"st": 67, "ed": 69, "text": "fully connected"}, {"st": 71, "ed": 73, "text": "squared loss"}, {"st": 75, "ed": 77, "text": "activation function"}, {"st": 80, "ed": 84, "text": "number of hidden units"}, {"st": 96, "ed": 98, "text": "training points"}, {"st": 100, "ed": 102, "text": "network structure"}]
[{"st": 7, "ed": 10, "text": "generative adversarial networks"}, {"st": 11, "ed": 13, "text": "jointly learns"}, {"st": 72, "ed": 74, "text": "training scheme"}, {"st": 151, "ed": 153, "text": "face verification"}]
[{"st": 38, "ed": 41, "text": "deep neural networks"}, {"st": 45, "ed": 48, "text": "online convex optimization"}, {"st": 50, "ed": 52, "text": "sqrt t"}, {"st": 70, "ed": 72, "text": "regret bounds"}, {"st": 73, "ed": 75, "text": "strongly convex"}, {"st": 92, "ed": 95, "text": "stochastic gradient descent"}, {"st": 99, "ed": 101, "text": "strongly convex"}]
[{"st": 9, "ed": 11, "text": "adversarial training"}, {"st": 12, "ed": 14, "text": "joint distribution"}, {"st": 33, "ed": 35, "text": "joint distributions"}, {"st": 36, "ed": 39, "text": "unsupervised and supervised"}, {"st": 49, "ed": 51, "text": "joint distribution"}, {"st": 69, "ed": 72, "text": "semi supervised learning"}, {"st": 73, "ed": 75, "text": "theoretical results"}, {"st": 78, "ed": 80, "text": "synthetic data"}, {"st": 81, "ed": 83, "text": "real world"}]
[{"st": 9, "ed": 11, "text": "class imbalance"}, {"st": 12, "ed": 14, "text": "classification performance"}, {"st": 15, "ed": 19, "text": "convolutional neural networks cnns"}, {"st": 28, "ed": 30, "text": "class imbalance"}, {"st": 41, "ed": 43, "text": "machine learning"}, {"st": 62, "ed": 64, "text": "benchmark datasets"}, {"st": 65, "ed": 67, "text": "increasing complexity"}, {"st": 67, "ed": 70, "text": "mnist cifar 10"}, {"st": 121, "ed": 123, "text": "multi class"}, {"st": 152, "ed": 154, "text": "class imbalance"}, {"st": 155, "ed": 157, "text": "classification performance"}, {"st": 164, "ed": 166, "text": "class imbalance"}, {"st": 210, "ed": 212, "text": "machine learning"}, {"st": 230, "ed": 232, "text": "class probabilities"}]
[{"st": 8, "ed": 10, "text": "deep learning"}, {"st": 18, "ed": 20, "text": "regularization methods"}, {"st": 46, "ed": 48, "text": "network architectures"}, {"st": 50, "ed": 52, "text": "regularization terms"}]
[{"st": 4, "ed": 6, "text": "machine learning"}, {"st": 21, "ed": 24, "text": "deep neural networks"}, {"st": 45, "ed": 47, "text": "deep learning"}, {"st": 93, "ed": 95, "text": "image datasets"}, {"st": 104, "ed": 106, "text": "clustering quality"}]
[{"st": 17, "ed": 19, "text": "remote sensing"}, {"st": 56, "ed": 58, "text": "fully convolutional"}, {"st": 58, "ed": 60, "text": "neural networks"}, {"st": 100, "ed": 102, "text": "remote sensing"}]
[{"st": 0, "ed": 2, "text": "recent progress"}, {"st": 4, "ed": 8, "text": "recurrent neural networks rnns"}, {"st": 9, "ed": 11, "text": "image description"}, {"st": 43, "ed": 45, "text": "natural language"}, {"st": 55, "ed": 58, "text": "takes into account"}, {"st": 60, "ed": 63, "text": "local and global"}, {"st": 75, "ed": 77, "text": "spatial temporal"}, {"st": 79, "ed": 82, "text": "convolutional neural network"}, {"st": 100, "ed": 102, "text": "action recognition"}, {"st": 122, "ed": 124, "text": "attention mechanism"}, {"st": 181, "ed": 183, "text": "natural language"}]
[{"st": 22, "ed": 24, "text": "handcrafted features"}, {"st": 26, "ed": 29, "text": "bag of words"}, {"st": 35, "ed": 37, "text": "content information"}, {"st": 72, "ed": 74, "text": "collaborative filtering"}, {"st": 79, "ed": 81, "text": "recent advances"}, {"st": 83, "ed": 85, "text": "deep learning"}, {"st": 118, "ed": 121, "text": "hierarchical bayesian model"}, {"st": 155, "ed": 157, "text": "real world"}, {"st": 177, "ed": 179, "text": "content information"}, {"st": 189, "ed": 191, "text": "significantly outperform"}]
[{"st": 15, "ed": 17, "text": "image data"}, {"st": 20, "ed": 22, "text": "sentiment analysis"}, {"st": 44, "ed": 46, "text": "image data"}, {"st": 52, "ed": 54, "text": "text data"}]
[{"st": 0, "ed": 2, "text": "neural networks"}, {"st": 17, "ed": 19, "text": "training data"}, {"st": 43, "ed": 45, "text": "neural networks"}, {"st": 87, "ed": 89, "text": "neural networks"}, {"st": 109, "ed": 111, "text": "exponential family"}, {"st": 192, "ed": 194, "text": "real world"}]
[{"st": 49, "ed": 51, "text": "recent advances"}, {"st": 52, "ed": 54, "text": "artificial intelligence"}, {"st": 65, "ed": 67, "text": "natural language"}, {"st": 70, "ed": 72, "text": "control problems"}, {"st": 134, "ed": 137, "text": "deep reinforcement learning"}, {"st": 169, "ed": 171, "text": "agents learn"}]
[{"st": 14, "ed": 16, "text": "task oriented"}, {"st": 16, "ed": 18, "text": "dialogue systems"}, {"st": 40, "ed": 42, "text": "statistical learning"}, {"st": 52, "ed": 54, "text": "neural network"}, {"st": 59, "ed": 62, "text": "end to end"}, {"st": 63, "ed": 65, "text": "goal oriented"}, {"st": 92, "ed": 94, "text": "dialogue systems"}]
[{"st": 11, "ed": 13, "text": "distributed representations"}]
[{"st": 0, "ed": 2, "text": "existing models"}, {"st": 4, "ed": 7, "text": "artificial neural networks"}, {"st": 9, "ed": 11, "text": "sentence classification"}, {"st": 27, "ed": 29, "text": "sentence classification"}, {"st": 88, "ed": 90, "text": "sentence classification"}]
[{"st": 4, "ed": 7, "text": "electronic health records"}, {"st": 38, "ed": 40, "text": "united states"}, {"st": 41, "ed": 43, "text": "health insurance"}, {"st": 67, "ed": 69, "text": "de identification"}, {"st": 100, "ed": 102, "text": "de identification"}, {"st": 116, "ed": 118, "text": "de identification"}, {"st": 121, "ed": 124, "text": "artificial neural networks"}, {"st": 128, "ed": 130, "text": "handcrafted features"}, {"st": 132, "ed": 134, "text": "unlike existing"}, {"st": 154, "ed": 156, "text": "de identification"}, {"st": 164, "ed": 166, "text": "de identification"}, {"st": 170, "ed": 172, "text": "de identification"}, {"st": 227, "ed": 229, "text": "de identification"}, {"st": 248, "ed": 250, "text": "de identification"}, {"st": 259, "ed": 261, "text": "previously published"}]
[{"st": 54, "ed": 56, "text": "proposed approach"}, {"st": 87, "ed": 89, "text": "previous results"}]
[{"st": 1, "ed": 3, "text": "deep learning"}, {"st": 11, "ed": 13, "text": "natural language"}, {"st": 51, "ed": 54, "text": "short term memory"}, {"st": 94, "ed": 96, "text": "sentiment analysis"}, {"st": 97, "ed": 99, "text": "question answering"}, {"st": 120, "ed": 122, "text": "rule based"}]
[{"st": 4, "ed": 6, "text": "deep learning"}, {"st": 8, "ed": 10, "text": "natural language"}, {"st": 23, "ed": 25, "text": "classification task"}, {"st": 50, "ed": 52, "text": "deep learning"}, {"st": 65, "ed": 69, "text": "convolutional neural networks cnns"}, {"st": 69, "ed": 71, "text": "n gram"}, {"st": 79, "ed": 81, "text": "pre defined"}, {"st": 184, "ed": 186, "text": "deep learning"}, {"st": 196, "ed": 198, "text": "automatically learns"}, {"st": 214, "ed": 216, "text": "domain experts"}, {"st": 222, "ed": 224, "text": "task specific"}, {"st": 244, "ed": 246, "text": "deep learning"}]
[{"st": 31, "ed": 34, "text": "artificial neural networks"}, {"st": 57, "ed": 60, "text": "convolutional neural network"}, {"st": 75, "ed": 77, "text": "relation extraction"}]
[{"st": 0, "ed": 2, "text": "recent approaches"}, {"st": 4, "ed": 7, "text": "artificial neural networks"}, {"st": 9, "ed": 12, "text": "shown promising results"}, {"st": 13, "ed": 16, "text": "named entity recognition"}, {"st": 21, "ed": 23, "text": "achieve high"}, {"st": 60, "ed": 62, "text": "de identification"}, {"st": 76, "ed": 78, "text": "transfer learning"}, {"st": 95, "ed": 97, "text": "labeled dataset"}]
[{"st": 0, "ed": 4, "text": "generative adversarial networks gans"}, {"st": 12, "ed": 14, "text": "computer vision"}, {"st": 16, "ed": 18, "text": "impressive results"}, {"st": 27, "ed": 29, "text": "natural language"}, {"st": 47, "ed": 49, "text": "likelihood based"}, {"st": 59, "ed": 61, "text": "natural language"}, {"st": 75, "ed": 77, "text": "output space"}, {"st": 104, "ed": 106, "text": "quantitative results"}, {"st": 110, "ed": 112, "text": "context free"}, {"st": 114, "ed": 116, "text": "context free"}]
[{"st": 2, "ed": 4, "text": "technique called"}, {"st": 4, "ed": 6, "text": "layer wise"}, {"st": 19, "ed": 21, "text": "input space"}, {"st": 24, "ed": 28, "text": "feed forward neural network"}, {"st": 58, "ed": 60, "text": "recurrent network"}, {"st": 74, "ed": 76, "text": "bi directional"}, {"st": 83, "ed": 85, "text": "prediction task"}, {"st": 92, "ed": 95, "text": "qualitatively and quantitatively"}]
[{"st": 1, "ed": 3, "text": "textual data"}, {"st": 46, "ed": 48, "text": "classification accuracy"}, {"st": 53, "ed": 55, "text": "sentiment analysis"}, {"st": 66, "ed": 68, "text": "sentiment analysis"}, {"st": 87, "ed": 89, "text": "sentiment classification"}, {"st": 108, "ed": 110, "text": "sentiment analysis"}, {"st": 123, "ed": 125, "text": "text data"}, {"st": 128, "ed": 130, "text": "sentiment analysis"}, {"st": 142, "ed": 144, "text": "sentiment analysis"}]
[{"st": 8, "ed": 11, "text": "end to end"}, {"st": 13, "ed": 16, "text": "received increasing attention"}, {"st": 22, "ed": 25, "text": "automatic speech recognition"}, {"st": 30, "ed": 32, "text": "context dependent"}, {"st": 32, "ed": 35, "text": "hidden markov model"}, {"st": 71, "ed": 74, "text": "orders of magnitude"}, {"st": 75, "ed": 77, "text": "training data"}, {"st": 130, "ed": 133, "text": "word error rate"}, {"st": 155, "ed": 157, "text": "training data"}]
[{"st": 5, "ed": 9, "text": "visual question answering vqa"}, {"st": 24, "ed": 26, "text": "recent approaches"}, {"st": 29, "ed": 31, "text": "image captioning"}, {"st": 35, "ed": 37, "text": "recurrent networks"}, {"st": 57, "ed": 59, "text": "spatial memory"}, {"st": 70, "ed": 73, "text": "recurrent neural networks"}, {"st": 76, "ed": 78, "text": "attention mechanism"}, {"st": 89, "ed": 91, "text": "spatial memory"}, {"st": 97, "ed": 99, "text": "spatial regions"}, {"st": 139, "ed": 141, "text": "image patches"}, {"st": 147, "ed": 149, "text": "improved results"}, {"st": 163, "ed": 165, "text": "evidence based"}, {"st": 176, "ed": 178, "text": "inference process"}, {"st": 203, "ed": 206, "text": "visual question answering"}, {"st": 214, "ed": 216, "text": "improved results"}]
[{"st": 0, "ed": 4, "text": "visual question answering vqa"}, {"st": 16, "ed": 19, "text": "visual and textual"}, {"st": 49, "ed": 51, "text": "attention based"}, {"st": 57, "ed": 60, "text": "visual and textual"}, {"st": 76, "ed": 78, "text": "attention based"}, {"st": 118, "ed": 120, "text": "bidirectional lstm"}, {"st": 138, "ed": 141, "text": "visual and textual"}, {"st": 143, "ed": 145, "text": "conduct experiments"}, {"st": 147, "ed": 149, "text": "large scale"}]
[{"st": 16, "ed": 18, "text": "input features"}, {"st": 32, "ed": 34, "text": "context dependent"}, {"st": 34, "ed": 37, "text": "deep neural networks"}, {"st": 40, "ed": 43, "text": "hidden markov models"}, {"st": 98, "ed": 100, "text": "common practice"}]
[{"st": 3, "ed": 6, "text": "short term memory"}, {"st": 6, "ed": 8, "text": "neural network"}, {"st": 20, "ed": 22, "text": "lstm architecture"}, {"st": 51, "ed": 53, "text": "lstm architecture"}, {"st": 87, "ed": 89, "text": "experiment results"}, {"st": 98, "ed": 100, "text": "accurately predict"}, {"st": 109, "ed": 111, "text": "lstm architecture"}]
[{"st": 6, "ed": 8, "text": "neural network"}, {"st": 27, "ed": 29, "text": "higher level"}, {"st": 36, "ed": 38, "text": "neural network"}, {"st": 42, "ed": 44, "text": "neural network"}, {"st": 56, "ed": 58, "text": "higher level"}, {"st": 83, "ed": 85, "text": "significant improvement"}]
[{"st": 35, "ed": 37, "text": "text mining"}, {"st": 42, "ed": 44, "text": "unstructured text"}, {"st": 72, "ed": 74, "text": "labeled dataset"}, {"st": 156, "ed": 158, "text": "word embedding"}, {"st": 171, "ed": 173, "text": "multi layer"}, {"st": 180, "ed": 182, "text": "deep network"}, {"st": 185, "ed": 188, "text": "deep belief networks"}, {"st": 214, "ed": 217, "text": "short term memory"}]
[{"st": 44, "ed": 46, "text": "prediction model"}, {"st": 67, "ed": 69, "text": "machine learning"}, {"st": 75, "ed": 78, "text": "deep neural network"}, {"st": 87, "ed": 89, "text": "prediction model"}, {"st": 97, "ed": 99, "text": "randomly selected"}, {"st": 118, "ed": 120, "text": "prediction model"}, {"st": 148, "ed": 150, "text": "method outperforms"}, {"st": 185, "ed": 187, "text": "analysis shows"}]
[{"st": 21, "ed": 23, "text": "deep learning"}, {"st": 31, "ed": 33, "text": "significant improvements"}, {"st": 78, "ed": 80, "text": "word level"}, {"st": 93, "ed": 95, "text": "deep architecture"}, {"st": 112, "ed": 114, "text": "word level"}, {"st": 161, "ed": 163, "text": "dynamic programming"}]
[{"st": 3, "ed": 5, "text": "variational inference"}, {"st": 16, "ed": 18, "text": "latent variables"}, {"st": 33, "ed": 35, "text": "multi modal"}, {"st": 35, "ed": 37, "text": "latent factors"}, {"st": 38, "ed": 41, "text": "real world data"}, {"st": 43, "ed": 45, "text": "natural language"}, {"st": 55, "ed": 57, "text": "latent variables"}, {"st": 62, "ed": 64, "text": "gaussian distribution"}, {"st": 70, "ed": 72, "text": "latent factors"}, {"st": 115, "ed": 117, "text": "latent distribution"}, {"st": 121, "ed": 123, "text": "substantial improvements"}, {"st": 124, "ed": 126, "text": "natural language"}, {"st": 133, "ed": 135, "text": "natural language"}]
[{"st": 0, "ed": 4, "text": "recurrent neural networks rnns"}, {"st": 6, "ed": 8, "text": "increasingly popular"}, {"st": 76, "ed": 78, "text": "gradient vanishing"}, {"st": 86, "ed": 88, "text": "external memory"}, {"st": 95, "ed": 97, "text": "conducted experiments"}]
[{"st": 4, "ed": 6, "text": "response generation"}, {"st": 10, "ed": 14, "text": "trained end to end"}, {"st": 22, "ed": 24, "text": "neural network"}, {"st": 35, "ed": 37, "text": "contextual information"}, {"st": 39, "ed": 41, "text": "statistical models"}, {"st": 54, "ed": 56, "text": "generative models"}, {"st": 61, "ed": 63, "text": "context sensitive"}, {"st": 65, "ed": 67, "text": "context sensitive"}, {"st": 67, "ed": 69, "text": "machine translation"}]
[{"st": 41, "ed": 44, "text": "neural language models"}, {"st": 66, "ed": 68, "text": "dialog state"}]
[{"st": 6, "ed": 8, "text": "open domain"}, {"st": 9, "ed": 11, "text": "dialogue systems"}, {"st": 19, "ed": 21, "text": "generative models"}, {"st": 47, "ed": 49, "text": "recently proposed"}, {"st": 51, "ed": 53, "text": "encoder decoder"}, {"st": 53, "ed": 55, "text": "neural network"}, {"st": 71, "ed": 74, "text": "neural language models"}, {"st": 77, "ed": 79, "text": "n gram"}, {"st": 104, "ed": 106, "text": "question answer"}]
[{"st": 8, "ed": 10, "text": "large vocabulary"}, {"st": 10, "ed": 13, "text": "continuous speech recognition"}, {"st": 18, "ed": 20, "text": "neural networks"}, {"st": 21, "ed": 24, "text": "hidden markov models"}, {"st": 58, "ed": 62, "text": "recurrent neural network rnn"}, {"st": 74, "ed": 76, "text": "input features"}, {"st": 86, "ed": 88, "text": "attention mechanism"}, {"st": 97, "ed": 99, "text": "attention mechanism"}, {"st": 101, "ed": 103, "text": "input sequence"}, {"st": 143, "ed": 145, "text": "n gram"}, {"st": 159, "ed": 161, "text": "rnn based"}]
[{"st": 7, "ed": 9, "text": "neural network"}, {"st": 12, "ed": 14, "text": "natural language"}, {"st": 54, "ed": 56, "text": "deep architecture"}, {"st": 106, "ed": 109, "text": "end to end"}, {"st": 111, "ed": 113, "text": "empirical studies"}, {"st": 118, "ed": 120, "text": "outperform existing"}]
[{"st": 3, "ed": 6, "text": "end to end"}, {"st": 43, "ed": 46, "text": "recurrent neural network"}, {"st": 50, "ed": 53, "text": "coarse to fine"}, {"st": 94, "ed": 96, "text": "relative improvement"}, {"st": 114, "ed": 117, "text": "k nearest neighbor"}]
[{"st": 12, "ed": 14, "text": "hand engineered"}, {"st": 18, "ed": 20, "text": "natural language"}, {"st": 34, "ed": 36, "text": "word pair"}, {"st": 49, "ed": 52, "text": "end to end"}, {"st": 53, "ed": 55, "text": "neural network"}, {"st": 82, "ed": 85, "text": "short term memory"}, {"st": 95, "ed": 97, "text": "neural attention"}, {"st": 113, "ed": 115, "text": "qualitative analysis"}, {"st": 116, "ed": 118, "text": "attention weights"}, {"st": 143, "ed": 145, "text": "engineered features"}, {"st": 154, "ed": 157, "text": "end to end"}, {"st": 168, "ed": 170, "text": "textual entailment"}]
[{"st": 8, "ed": 11, "text": "short term memory"}, {"st": 12, "ed": 15, "text": "recurrent neural networks"}, {"st": 71, "ed": 73, "text": "efficient algorithms"}, {"st": 92, "ed": 94, "text": "speech recognition"}, {"st": 145, "ed": 147, "text": "relative improvement"}]
[{"st": 6, "ed": 8, "text": "neural network"}, {"st": 12, "ed": 14, "text": "natural language"}, {"st": 28, "ed": 30, "text": "distributed representation"}, {"st": 57, "ed": 60, "text": "end to end"}, {"st": 110, "ed": 112, "text": "multiple layers"}, {"st": 158, "ed": 161, "text": "end to end"}, {"st": 188, "ed": 190, "text": "neural network"}]
[{"st": 27, "ed": 29, "text": "textual entailment"}, {"st": 66, "ed": 68, "text": "attention based"}, {"st": 117, "ed": 120, "text": "open source software"}, {"st": 133, "ed": 135, "text": "multi task"}]
[{"st": 21, "ed": 23, "text": "input sequence"}, {"st": 81, "ed": 83, "text": "neural network"}, {"st": 94, "ed": 96, "text": "encoder decoder"}, {"st": 122, "ed": 124, "text": "input sequence"}, {"st": 135, "ed": 137, "text": "empirical study"}, {"st": 139, "ed": 141, "text": "synthetic data"}, {"st": 143, "ed": 147, "text": "real world data sets"}, {"st": 158, "ed": 160, "text": "rnn based"}, {"st": 165, "ed": 167, "text": "text summarization"}]
[{"st": 1, "ed": 4, "text": "the past decade"}, {"st": 4, "ed": 6, "text": "large scale"}, {"st": 6, "ed": 8, "text": "supervised learning"}, {"st": 11, "ed": 13, "text": "machine learning"}, {"st": 25, "ed": 27, "text": "large scale"}, {"st": 27, "ed": 29, "text": "question answer"}, {"st": 39, "ed": 41, "text": "question answer"}, {"st": 44, "ed": 46, "text": "question answer"}, {"st": 53, "ed": 55, "text": "neural network"}, {"st": 65, "ed": 67, "text": "natural language"}, {"st": 70, "ed": 73, "text": "question answer pairs"}, {"st": 81, "ed": 83, "text": "automatic evaluation"}, {"st": 87, "ed": 89, "text": "machine translation"}, {"st": 95, "ed": 97, "text": "evaluation criteria"}]
[{"st": 2, "ed": 4, "text": "evaluation metrics"}, {"st": 6, "ed": 8, "text": "response generation"}, {"st": 19, "ed": 21, "text": "recent works"}, {"st": 22, "ed": 24, "text": "response generation"}, {"st": 28, "ed": 30, "text": "machine translation"}, {"st": 70, "ed": 73, "text": "quantitative and qualitative"}, {"st": 88, "ed": 90, "text": "automatic evaluation"}]
[{"st": 0, "ed": 2, "text": "sequential data"}, {"st": 5, "ed": 7, "text": "hierarchical structure"}, {"st": 29, "ed": 31, "text": "generative process"}, {"st": 34, "ed": 36, "text": "neural network"}, {"st": 61, "ed": 63, "text": "response generation"}, {"st": 68, "ed": 70, "text": "neural network"}, {"st": 77, "ed": 79, "text": "automatic evaluation"}, {"st": 88, "ed": 90, "text": "experiments demonstrate"}, {"st": 95, "ed": 97, "text": "recently proposed"}, {"st": 101, "ed": 103, "text": "latent variables"}]
[{"st": 16, "ed": 18, "text": "recent advances"}, {"st": 20, "ed": 23, "text": "end to end"}, {"st": 23, "ed": 25, "text": "neural architectures"}, {"st": 27, "ed": 29, "text": "highly successful"}, {"st": 57, "ed": 59, "text": "associative memory"}, {"st": 63, "ed": 66, "text": "recurrent neural networks"}, {"st": 88, "ed": 90, "text": "competitive results"}, {"st": 94, "ed": 96, "text": "qualitative analysis"}, {"st": 102, "ed": 105, "text": "source and target"}, {"st": 142, "ed": 144, "text": "additional supervision"}]
[{"st": 4, "ed": 6, "text": "log linear"}, {"st": 10, "ed": 13, "text": "recurrent neural networks"}, {"st": 17, "ed": 19, "text": "output layer"}, {"st": 21, "ed": 23, "text": "log linear"}, {"st": 23, "ed": 25, "text": "output layer"}, {"st": 34, "ed": 36, "text": "conceptually simple"}, {"st": 48, "ed": 50, "text": "training data"}, {"st": 77, "ed": 79, "text": "training data"}, {"st": 91, "ed": 93, "text": "prior knowledge"}, {"st": 104, "ed": 106, "text": "neural network"}, {"st": 115, "ed": 117, "text": "log linear"}, {"st": 122, "ed": 124, "text": "conduct experiments"}, {"st": 135, "ed": 137, "text": "prior knowledge"}, {"st": 159, "ed": 161, "text": "log linear"}, {"st": 163, "ed": 165, "text": "neural network"}, {"st": 182, "ed": 184, "text": "prior knowledge"}, {"st": 191, "ed": 193, "text": "representation learning"}]
[{"st": 7, "ed": 9, "text": "natural language"}]
[{"st": 0, "ed": 3, "text": "recurrent neural networks"}, {"st": 5, "ed": 7, "text": "powerful tool"}, {"st": 9, "ed": 11, "text": "sequential data"}, {"st": 38, "ed": 41, "text": "recurrent neural networks"}, {"st": 50, "ed": 52, "text": "convolutional layers"}, {"st": 73, "ed": 75, "text": "recurrent layers"}, {"st": 79, "ed": 81, "text": "predictive accuracy"}, {"st": 99, "ed": 101, "text": "times faster"}, {"st": 110, "ed": 112, "text": "sentiment classification"}, {"st": 113, "ed": 115, "text": "character level"}, {"st": 115, "ed": 118, "text": "neural machine translation"}, {"st": 130, "ed": 132, "text": "building block"}]
[{"st": 9, "ed": 11, "text": "neural network"}, {"st": 20, "ed": 22, "text": "recurrent architecture"}, {"st": 39, "ed": 41, "text": "input dependent"}, {"st": 79, "ed": 81, "text": "input output"}, {"st": 83, "ed": 85, "text": "hidden unit"}, {"st": 89, "ed": 91, "text": "reverse engineer"}, {"st": 120, "ed": 122, "text": "computational efficiency"}]
[{"st": 0, "ed": 3, "text": "neural language models"}, {"st": 9, "ed": 11, "text": "latent representation"}, {"st": 21, "ed": 24, "text": "neural language models"}, {"st": 26, "ed": 28, "text": "attention mechanism"}, {"st": 62, "ed": 64, "text": "attention mechanisms"}, {"st": 68, "ed": 71, "text": "neural language models"}, {"st": 118, "ed": 120, "text": "attention mechanism"}, {"st": 143, "ed": 145, "text": "model outperforms"}, {"st": 148, "ed": 151, "text": "neural language models"}]
[{"st": 50, "ed": 52, "text": "attention mechanism"}, {"st": 55, "ed": 57, "text": "regularization term"}, {"st": 94, "ed": 96, "text": "sentiment classification"}, {"st": 107, "ed": 109, "text": "performance gain"}, {"st": 113, "ed": 115, "text": "embedding methods"}]
[{"st": 3, "ed": 5, "text": "attention based"}, {"st": 24, "ed": 26, "text": "word order"}, {"st": 34, "ed": 36, "text": "sampling scheme"}, {"st": 38, "ed": 40, "text": "conceptually simple"}]
[{"st": 46, "ed": 49, "text": "recurrent neural networks"}, {"st": 76, "ed": 78, "text": "mid level"}, {"st": 136, "ed": 138, "text": "natural language"}, {"st": 145, "ed": 147, "text": "empirical results"}]
[{"st": 13, "ed": 15, "text": "answer questions"}, {"st": 34, "ed": 36, "text": "question answer"}, {"st": 41, "ed": 43, "text": "significant improvement"}, {"st": 61, "ed": 63, "text": "jointly learning"}, {"st": 82, "ed": 84, "text": "architectural engineering"}]
[{"st": 9, "ed": 13, "text": "recurrent neural networks rnns"}, {"st": 27, "ed": 29, "text": "large scale"}, {"st": 39, "ed": 42, "text": "short term memory"}, {"st": 51, "ed": 53, "text": "lstm units"}, {"st": 57, "ed": 59, "text": "hidden states"}, {"st": 124, "ed": 126, "text": "lstm units"}, {"st": 139, "ed": 141, "text": "group lasso"}, {"st": 143, "ed": 145, "text": "method achieves"}, {"st": 174, "ed": 176, "text": "question answering"}, {"st": 186, "ed": 188, "text": "lstm rnns"}, {"st": 195, "ed": 197, "text": "source code"}, {"st": 199, "ed": 203, "text": "available at https github.com"}]
[{"st": 62, "ed": 64, "text": "empirical success"}, {"st": 65, "ed": 67, "text": "remains unclear"}, {"st": 86, "ed": 88, "text": "theoretical analysis"}, {"st": 99, "ed": 101, "text": "ell 2"}, {"st": 112, "ed": 114, "text": "word embeddings"}, {"st": 143, "ed": 145, "text": "word embedding"}]
[{"st": 2, "ed": 4, "text": "object oriented"}, {"st": 27, "ed": 29, "text": "object oriented"}, {"st": 41, "ed": 43, "text": "domain specific"}, {"st": 55, "ed": 57, "text": "decision process"}, {"st": 58, "ed": 60, "text": "neural net"}, {"st": 134, "ed": 136, "text": "supervised learning"}, {"st": 137, "ed": 139, "text": "reinforcement learning"}, {"st": 149, "ed": 153, "text": "synthetic and real world"}, {"st": 168, "ed": 170, "text": "training data"}]
[{"st": 6, "ed": 8, "text": "machine reading"}, {"st": 10, "ed": 12, "text": "open domain"}, {"st": 12, "ed": 14, "text": "question answering"}, {"st": 20, "ed": 22, "text": "typically assume"}, {"st": 57, "ed": 59, "text": "large scale"}, {"st": 59, "ed": 61, "text": "open domain"}, {"st": 61, "ed": 63, "text": "question answering"}, {"st": 111, "ed": 113, "text": "open domain"}]
[{"st": 0, "ed": 2, "text": "speech recognition"}, {"st": 4, "ed": 7, "text": "taking advantage of"}, {"st": 7, "ed": 9, "text": "deep learning"}, {"st": 18, "ed": 22, "text": "recurrent neural networks rnns"}, {"st": 29, "ed": 32, "text": "short term memory"}, {"st": 47, "ed": 50, "text": "ability to learn"}, {"st": 84, "ed": 86, "text": "gated recurrent"}, {"st": 181, "ed": 183, "text": "recognition performance"}, {"st": 186, "ed": 188, "text": "input features"}]
[{"st": 10, "ed": 12, "text": "reinforcement learning"}, {"st": 15, "ed": 17, "text": "prohibitively expensive"}, {"st": 38, "ed": 41, "text": "simulated and real"}, {"st": 59, "ed": 61, "text": "dialogue policy"}, {"st": 71, "ed": 73, "text": "sample efficient"}, {"st": 92, "ed": 94, "text": "user experience"}, {"st": 107, "ed": 109, "text": "user experience"}]
[{"st": 0, "ed": 3, "text": "deep neural networks"}, {"st": 16, "ed": 18, "text": "speech recognition"}, {"st": 20, "ed": 22, "text": "neural network"}, {"st": 29, "ed": 31, "text": "network architecture"}, {"st": 33, "ed": 35, "text": "training loss"}, {"st": 54, "ed": 56, "text": "speech recognition"}, {"st": 61, "ed": 63, "text": "classifier performance"}, {"st": 68, "ed": 70, "text": "error rates"}, {"st": 109, "ed": 111, "text": "convolutional networks"}, {"st": 120, "ed": 122, "text": "neural networks"}, {"st": 137, "ed": 139, "text": "training data"}, {"st": 172, "ed": 174, "text": "speech recognition"}, {"st": 185, "ed": 187, "text": "optimization technique"}, {"st": 207, "ed": 209, "text": "speech recognition"}, {"st": 211, "ed": 213, "text": "maximum likelihood"}, {"st": 223, "ed": 225, "text": "case study"}, {"st": 230, "ed": 232, "text": "loss functions"}]
[{"st": 4, "ed": 8, "text": "deep recurrent neural network"}, {"st": 8, "ed": 10, "text": "rnn model"}, {"st": 14, "ed": 17, "text": "automatic speech recognition"}, {"st": 34, "ed": 37, "text": "deep neural network"}, {"st": 47, "ed": 50, "text": "short term memory"}, {"st": 87, "ed": 89, "text": "posterior probabilities"}, {"st": 100, "ed": 103, "text": "wall street journal"}, {"st": 110, "ed": 112, "text": "relative improvement"}]
[{"st": 5, "ed": 9, "text": "recurrent neural networks rnns"}, {"st": 16, "ed": 18, "text": "hidden states"}, {"st": 20, "ed": 22, "text": "penalty term"}, {"st": 35, "ed": 37, "text": "character level"}, {"st": 49, "ed": 51, "text": "achieve competitive"}, {"st": 57, "ed": 59, "text": "phoneme recognition"}, {"st": 72, "ed": 74, "text": "penalty term"}, {"st": 77, "ed": 79, "text": "similar performance"}, {"st": 87, "ed": 89, "text": "penalty term"}, {"st": 97, "ed": 99, "text": "penalty term"}, {"st": 102, "ed": 104, "text": "exponential growth"}]
[{"st": 4, "ed": 6, "text": "neural network"}, {"st": 87, "ed": 89, "text": "computational efficiency"}, {"st": 98, "ed": 101, "text": "mixture of experts"}, {"st": 109, "ed": 111, "text": "feed forward"}, {"st": 140, "ed": 142, "text": "machine translation"}, {"st": 185, "ed": 187, "text": "machine translation"}]
[{"st": 5, "ed": 7, "text": "objective function"}, {"st": 9, "ed": 11, "text": "unsupervised training"}, {"st": 12, "ed": 14, "text": "neural network"}, {"st": 42, "ed": 45, "text": "times faster than"}, {"st": 48, "ed": 50, "text": "prior methods"}]
[{"st": 0, "ed": 3, "text": "visual question answering"}, {"st": 5, "ed": 7, "text": "recently proposed"}, {"st": 7, "ed": 9, "text": "artificial intelligence"}, {"st": 21, "ed": 23, "text": "deep learning"}, {"st": 28, "ed": 31, "text": "convolutional neural networks"}, {"st": 50, "ed": 52, "text": "computer vision"}, {"st": 55, "ed": 57, "text": "object recognition"}, {"st": 58, "ed": 60, "text": "image classification"}, {"st": 60, "ed": 63, "text": "visual question answering"}, {"st": 74, "ed": 76, "text": "natural language"}, {"st": 84, "ed": 86, "text": "detailed analysis"}, {"st": 87, "ed": 89, "text": "natural language"}, {"st": 103, "ed": 106, "text": "convolutional neural networks"}, {"st": 116, "ed": 119, "text": "convolutional neural networks"}, {"st": 121, "ed": 123, "text": "text data"}, {"st": 149, "ed": 152, "text": "visual question answering"}, {"st": 162, "ed": 165, "text": "visual question answering"}, {"st": 174, "ed": 176, "text": "natural language"}, {"st": 195, "ed": 197, "text": "comparable results"}, {"st": 198, "ed": 200, "text": "deep learning"}, {"st": 204, "ed": 206, "text": "text classification"}]
[{"st": 7, "ed": 9, "text": "neural network"}, {"st": 12, "ed": 14, "text": "encoder decoder"}, {"st": 18, "ed": 21, "text": "recurrent neural networks"}, {"st": 32, "ed": 34, "text": "fixed length"}, {"st": 48, "ed": 51, "text": "encoder and decoder"}, {"st": 56, "ed": 58, "text": "jointly trained"}, {"st": 61, "ed": 63, "text": "conditional probability"}, {"st": 75, "ed": 78, "text": "statistical machine translation"}, {"st": 87, "ed": 89, "text": "conditional probabilities"}, {"st": 96, "ed": 98, "text": "encoder decoder"}, {"st": 105, "ed": 107, "text": "log linear"}]
[{"st": 0, "ed": 4, "text": "recurrent neural networks rnns"}, {"st": 6, "ed": 9, "text": "short term memory"}, {"st": 15, "ed": 18, "text": "automatic speech recognition"}, {"st": 37, "ed": 39, "text": "recent research"}, {"st": 43, "ed": 45, "text": "trained model"}, {"st": 68, "ed": 70, "text": "knowledge transfer"}, {"st": 77, "ed": 79, "text": "neural nets"}, {"st": 86, "ed": 88, "text": "final performance"}, {"st": 90, "ed": 92, "text": "a level"}, {"st": 106, "ed": 108, "text": "knowledge transfer"}, {"st": 117, "ed": 121, "text": "deep neural network dnn"}, {"st": 135, "ed": 137, "text": "knowledge transfer"}, {"st": 170, "ed": 172, "text": "learning scheme"}]
[{"st": 1, "ed": 4, "text": "short term memory"}, {"st": 7, "ed": 11, "text": "recurrent neural network rnn"}, {"st": 19, "ed": 22, "text": "vanishing and exploding"}, {"st": 28, "ed": 31, "text": "feedforward neural networks"}, {"st": 50, "ed": 52, "text": "sequence prediction"}, {"st": 55, "ed": 57, "text": "handwriting recognition"}, {"st": 69, "ed": 72, "text": "deep neural networks"}, {"st": 77, "ed": 79, "text": "speech recognition"}, {"st": 86, "ed": 88, "text": "small scale"}, {"st": 97, "ed": 99, "text": "rnn architectures"}, {"st": 112, "ed": 114, "text": "large vocabulary"}, {"st": 120, "ed": 122, "text": "lstm rnn"}, {"st": 135, "ed": 137, "text": "lstm models"}, {"st": 145, "ed": 147, "text": "speech recognition"}]
[{"st": 52, "ed": 55, "text": "vector space model"}, {"st": 63, "ed": 65, "text": "self organizing"}, {"st": 163, "ed": 165, "text": "statistically significant"}, {"st": 169, "ed": 171, "text": "highly scalable"}]
[{"st": 1, "ed": 3, "text": "recently proposed"}, {"st": 10, "ed": 12, "text": "complex data"}, {"st": 18, "ed": 21, "text": "automatic speech recognition"}, {"st": 25, "ed": 27, "text": "neural network"}, {"st": 30, "ed": 33, "text": "end to end"}, {"st": 40, "ed": 42, "text": "attention based"}, {"st": 43, "ed": 45, "text": "speech recognition"}, {"st": 84, "ed": 86, "text": "error rates"}, {"st": 88, "ed": 91, "text": "wall street journal"}]
[{"st": 0, "ed": 3, "text": "neural machine translation"}, {"st": 5, "ed": 7, "text": "recently proposed"}, {"st": 14, "ed": 17, "text": "statistical machine translation"}, {"st": 18, "ed": 21, "text": "neural machine translation"}, {"st": 26, "ed": 28, "text": "neural network"}, {"st": 43, "ed": 46, "text": "neural machine translation"}, {"st": 62, "ed": 64, "text": "source sentence"}, {"st": 66, "ed": 68, "text": "fixed length"}, {"st": 86, "ed": 88, "text": "fixed length"}, {"st": 99, "ed": 102, "text": "encoder decoder architecture"}, {"st": 119, "ed": 121, "text": "source sentence"}, {"st": 157, "ed": 159, "text": "phrase based"}, {"st": 169, "ed": 171, "text": "qualitative analysis"}]
[{"st": 11, "ed": 13, "text": "recently introduced"}, {"st": 13, "ed": 15, "text": "neural network"}, {"st": 29, "ed": 31, "text": "unlike existing"}, {"st": 31, "ed": 33, "text": "phrase based"}, {"st": 61, "ed": 63, "text": "neural network"}, {"st": 74, "ed": 77, "text": "neural machine translation"}, {"st": 88, "ed": 90, "text": "empirical results"}, {"st": 92, "ed": 94, "text": "significant improvement"}]
[{"st": 0, "ed": 3, "text": "deep neural network"}, {"st": 15, "ed": 18, "text": "automatic speech recognition"}, {"st": 22, "ed": 26, "text": "recurrent neural network rnn"}, {"st": 41, "ed": 43, "text": "rnn models"}, {"st": 50, "ed": 52, "text": "embedded systems"}, {"st": 79, "ed": 81, "text": "output distribution"}, {"st": 97, "ed": 99, "text": "transfer knowledge"}, {"st": 105, "ed": 107, "text": "rnn model"}, {"st": 114, "ed": 117, "text": "kullback leibler divergence"}, {"st": 136, "ed": 139, "text": "wall street journal"}]
[{"st": 1, "ed": 3, "text": "representation learning"}, {"st": 30, "ed": 34, "text": "canonical correlation analysis cca"}, {"st": 34, "ed": 36, "text": "based approaches"}, {"st": 42, "ed": 44, "text": "based approaches"}, {"st": 46, "ed": 48, "text": "joint representation"}, {"st": 61, "ed": 63, "text": "based methods"}, {"st": 90, "ed": 92, "text": "based approaches"}, {"st": 94, "ed": 96, "text": "based approaches"}, {"st": 100, "ed": 102, "text": "transfer learning"}, {"st": 117, "ed": 119, "text": "based approach"}, {"st": 121, "ed": 123, "text": "neural network"}, {"st": 159, "ed": 162, "text": "ability to learn"}, {"st": 171, "ed": 173, "text": "cross language"}]
[{"st": 5, "ed": 7, "text": "input data"}, {"st": 9, "ed": 11, "text": "attention mechanism"}, {"st": 12, "ed": 14, "text": "recently shown"}, {"st": 24, "ed": 26, "text": "machine translation"}, {"st": 29, "ed": 31, "text": "image caption"}, {"st": 36, "ed": 38, "text": "attention mechanism"}, {"st": 55, "ed": 57, "text": "machine translation"}, {"st": 63, "ed": 65, "text": "error rate"}, {"st": 69, "ed": 71, "text": "phoneme recognition"}, {"st": 112, "ed": 114, "text": "attention mechanism"}, {"st": 120, "ed": 122, "text": "method yields"}]
[{"st": 2, "ed": 4, "text": "recently shown"}, {"st": 7, "ed": 10, "text": "short term memory"}, {"st": 11, "ed": 15, "text": "recurrent neural networks rnns"}, {"st": 16, "ed": 18, "text": "feed forward"}, {"st": 18, "ed": 21, "text": "deep neural networks"}, {"st": 39, "ed": 41, "text": "context dependent"}, {"st": 42, "ed": 45, "text": "hidden markov model"}, {"st": 50, "ed": 52, "text": "lstm rnns"}, {"st": 75, "ed": 77, "text": "improve performance"}, {"st": 78, "ed": 80, "text": "lstm rnn"}, {"st": 83, "ed": 85, "text": "large vocabulary"}, {"st": 94, "ed": 96, "text": "frame rate"}, {"st": 114, "ed": 116, "text": "initial results"}, {"st": 117, "ed": 119, "text": "lstm rnn"}]
[{"st": 8, "ed": 10, "text": "neural network"}, {"st": 18, "ed": 20, "text": "unlike traditional"}, {"st": 20, "ed": 22, "text": "dnn hmm"}, {"st": 49, "ed": 51, "text": "recurrent network"}, {"st": 63, "ed": 65, "text": "attention based"}, {"st": 65, "ed": 67, "text": "recurrent network"}, {"st": 95, "ed": 98, "text": "end to end"}, {"st": 107, "ed": 109, "text": "search task"}, {"st": 112, "ed": 116, "text": "word error rate wer"}]
[{"st": 4, "ed": 6, "text": "approximation algorithm"}, {"st": 10, "ed": 14, "text": "recurrent neural network language"}, {"st": 33, "ed": 35, "text": "sampling strategy"}, {"st": 36, "ed": 38, "text": "significantly reduces"}, {"st": 42, "ed": 44, "text": "sample efficiency"}, {"st": 66, "ed": 68, "text": "output layer"}, {"st": 73, "ed": 75, "text": "training loss"}, {"st": 87, "ed": 89, "text": "importance sampling"}, {"st": 135, "ed": 137, "text": "typically require"}]
[{"st": 0, "ed": 3, "text": "neural machine translation"}, {"st": 15, "ed": 17, "text": "main challenges"}, {"st": 68, "ed": 70, "text": "word embeddings"}, {"st": 83, "ed": 85, "text": "attention based"}, {"st": 94, "ed": 96, "text": "improved results"}, {"st": 99, "ed": 101, "text": "source language"}, {"st": 109, "ed": 111, "text": "bleu points"}]
[{"st": 5, "ed": 7, "text": "latent variable"}, {"st": 7, "ed": 11, "text": "recurrent neural network architecture"}, {"st": 26, "ed": 29, "text": "recurrent neural network"}, {"st": 30, "ed": 32, "text": "individual words"}, {"st": 48, "ed": 50, "text": "latent variable"}, {"st": 67, "ed": 69, "text": "training objective"}, {"st": 74, "ed": 76, "text": "relation classification"}, {"st": 95, "ed": 97, "text": "relation classification"}]
[{"st": 1, "ed": 3, "text": "highly correlated"}, {"st": 62, "ed": 64, "text": "neural network"}, {"st": 81, "ed": 83, "text": "multi task"}, {"st": 93, "ed": 95, "text": "task specific"}]
[{"st": 3, "ed": 5, "text": "neural networks"}, {"st": 7, "ed": 9, "text": "explicit memory"}, {"st": 35, "ed": 38, "text": "end to end"}, {"st": 57, "ed": 59, "text": "extremely large"}, {"st": 70, "ed": 72, "text": "attention mechanisms"}, {"st": 74, "ed": 76, "text": "reinforcement learning"}, {"st": 102, "ed": 104, "text": "soft attention"}, {"st": 112, "ed": 114, "text": "hierarchical structure"}, {"st": 125, "ed": 127, "text": "soft attention"}, {"st": 150, "ed": 152, "text": "inner product"}, {"st": 156, "ed": 159, "text": "training and inference"}, {"st": 185, "ed": 187, "text": "large scale"}, {"st": 188, "ed": 190, "text": "question answering"}]
[{"st": 20, "ed": 22, "text": "text generation"}, {"st": 29, "ed": 31, "text": "deep neural"}, {"st": 54, "ed": 56, "text": "training scheme"}, {"st": 87, "ed": 89, "text": "training loss"}, {"st": 116, "ed": 118, "text": "attention based"}]
[{"st": 7, "ed": 10, "text": "recurrent neural network"}, {"st": 12, "ed": 16, "text": "recurrent neural network architecture"}, {"st": 17, "ed": 19, "text": "multi label"}, {"st": 30, "ed": 32, "text": "hidden state"}, {"st": 81, "ed": 83, "text": "intensive care"}, {"st": 99, "ed": 101, "text": "evaluation shows"}]
[{"st": 14, "ed": 16, "text": "natural language"}, {"st": 24, "ed": 26, "text": "machine learning"}, {"st": 27, "ed": 29, "text": "traditional approaches"}, {"st": 41, "ed": 43, "text": "reinforcement learning"}, {"st": 77, "ed": 79, "text": "latent variable"}, {"st": 93, "ed": 95, "text": "goal oriented"}, {"st": 145, "ed": 147, "text": "latent variable"}, {"st": 150, "ed": 152, "text": "goal oriented"}]
[{"st": 0, "ed": 3, "text": "end to end"}, {"st": 6, "ed": 9, "text": "speech recognition asr"}, {"st": 11, "ed": 13, "text": "massive data"}, {"st": 18, "ed": 20, "text": "transfer learning"}, {"st": 47, "ed": 50, "text": "convolutional neural network"}, {"st": 65, "ed": 67, "text": "faster training"}, {"st": 74, "ed": 76, "text": "training data"}]
[{"st": 10, "ed": 12, "text": "de facto"}, {"st": 18, "ed": 20, "text": "speech recognition"}, {"st": 36, "ed": 40, "text": "word error rate wer"}, {"st": 45, "ed": 47, "text": "cross entropy"}, {"st": 91, "ed": 93, "text": "previous approaches"}, {"st": 146, "ed": 148, "text": "monte carlo"}, {"st": 161, "ed": 163, "text": "relative improvement"}, {"st": 176, "ed": 178, "text": "recognition task"}]
[{"st": 15, "ed": 19, "text": "recurrent neural networks rnns"}, {"st": 38, "ed": 40, "text": "space complexity"}, {"st": 74, "ed": 76, "text": "low rank"}, {"st": 81, "ed": 83, "text": "lstm networks"}]
[{"st": 1, "ed": 4, "text": "deep neural networks"}, {"st": 5, "ed": 8, "text": "massive amounts of"}, {"st": 8, "ed": 10, "text": "training data"}, {"st": 16, "ed": 18, "text": "labeled data"}, {"st": 22, "ed": 24, "text": "weak supervision"}, {"st": 45, "ed": 47, "text": "semi supervised"}, {"st": 62, "ed": 64, "text": "neural network"}, {"st": 66, "ed": 68, "text": "fine tune"}, {"st": 116, "ed": 119, "text": "semi supervised learning"}, {"st": 124, "ed": 126, "text": "neural networks"}, {"st": 128, "ed": 130, "text": "multi task"}, {"st": 157, "ed": 159, "text": "unlabeled data"}, {"st": 198, "ed": 200, "text": "weight updates"}, {"st": 202, "ed": 204, "text": "noisy labels"}, {"st": 215, "ed": 217, "text": "learning strategy"}, {"st": 246, "ed": 248, "text": "learning process"}]
[{"st": 23, "ed": 25, "text": "efficient exploration"}, {"st": 32, "ed": 35, "text": "deep reinforcement learning"}, {"st": 42, "ed": 44, "text": "epsilon greedy"}, {"st": 61, "ed": 63, "text": "gaussian process"}, {"st": 69, "ed": 71, "text": "sample efficient"}, {"st": 74, "ed": 76, "text": "user experience"}, {"st": 91, "ed": 93, "text": "uncertainty estimates"}, {"st": 94, "ed": 97, "text": "deep q networks"}, {"st": 115, "ed": 117, "text": "uncertainty estimates"}]
[{"st": 9, "ed": 12, "text": "recurrent neural networks"}, {"st": 40, "ed": 43, "text": "recurrent neural network"}, {"st": 56, "ed": 59, "text": "recurrent neural network"}, {"st": 62, "ed": 65, "text": "short term memory"}, {"st": 68, "ed": 72, "text": "gated recurrent unit gru"}, {"st": 96, "ed": 99, "text": "short term memory"}, {"st": 107, "ed": 110, "text": "recurrent neural network"}, {"st": 127, "ed": 129, "text": "extensive experiments"}]
[{"st": 8, "ed": 12, "text": "convolutional neural network cnn"}, {"st": 15, "ed": 17, "text": "question answering"}, {"st": 24, "ed": 27, "text": "end to end"}, {"st": 29, "ed": 31, "text": "convolutional architectures"}, {"st": 64, "ed": 66, "text": "image content"}, {"st": 84, "ed": 86, "text": "joint representation"}, {"st": 114, "ed": 116, "text": "benchmark datasets"}]
[{"st": 11, "ed": 13, "text": "natural language"}, {"st": 43, "ed": 45, "text": "question answering"}, {"st": 65, "ed": 67, "text": "multiple times"}, {"st": 72, "ed": 74, "text": "experiments conducted"}, {"st": 85, "ed": 87, "text": "significantly outperform"}]
[{"st": 0, "ed": 3, "text": "visual question answering"}, {"st": 40, "ed": 42, "text": "deep networks"}, {"st": 64, "ed": 66, "text": "jointly trained"}, {"st": 69, "ed": 71, "text": "deep networks"}, {"st": 117, "ed": 120, "text": "visual question answering"}, {"st": 130, "ed": 132, "text": "natural image"}, {"st": 138, "ed": 140, "text": "complex questions"}]
[{"st": 50, "ed": 52, "text": "abstract concepts"}, {"st": 64, "ed": 66, "text": "artificial intelligence"}, {"st": 106, "ed": 108, "text": "learning rule"}, {"st": 118, "ed": 122, "text": "dynamic time warping dtw"}]
[{"st": 10, "ed": 12, "text": "deep learning"}, {"st": 32, "ed": 34, "text": "large datasets"}, {"st": 135, "ed": 138, "text": "deep neural network"}, {"st": 142, "ed": 144, "text": "natural language"}, {"st": 160, "ed": 163, "text": "significantly outperforms existing"}]
[{"st": 1, "ed": 4, "text": "deep neural networks"}, {"st": 9, "ed": 11, "text": "classification tasks"}, {"st": 19, "ed": 21, "text": "structured output"}, {"st": 30, "ed": 32, "text": "random variables"}, {"st": 36, "ed": 38, "text": "joint distribution"}, {"st": 59, "ed": 62, "text": "input and output"}, {"st": 90, "ed": 92, "text": "machine translation"}, {"st": 92, "ed": 94, "text": "image caption"}, {"st": 111, "ed": 113, "text": "building blocks"}, {"st": 113, "ed": 115, "text": "gated recurrent"}, {"st": 115, "ed": 117, "text": "neural networks"}, {"st": 118, "ed": 121, "text": "convolutional neural networks"}]
[{"st": 10, "ed": 12, "text": "image description"}, {"st": 16, "ed": 19, "text": "neural machine translation"}, {"st": 33, "ed": 35, "text": "target language"}, {"st": 41, "ed": 43, "text": "feature vectors"}, {"st": 50, "ed": 52, "text": "source language"}, {"st": 69, "ed": 71, "text": "image description"}, {"st": 89, "ed": 91, "text": "substantial improvements"}, {"st": 100, "ed": 102, "text": "multiple languages"}]
[{"st": 20, "ed": 22, "text": "deep neural"}, {"st": 49, "ed": 51, "text": "multilayer perceptron"}, {"st": 58, "ed": 63, "text": "long short term memory lstm"}, {"st": 79, "ed": 81, "text": "background knowledge"}, {"st": 89, "ed": 91, "text": "spatial relations"}, {"st": 111, "ed": 113, "text": "fine tuning"}]
[{"st": 4, "ed": 6, "text": "encoder decoder"}, {"st": 17, "ed": 20, "text": "coarse to fine"}, {"st": 41, "ed": 43, "text": "real world"}, {"st": 61, "ed": 63, "text": "attention based"}, {"st": 72, "ed": 74, "text": "approach outperforms"}, {"st": 80, "ed": 82, "text": "large margin"}, {"st": 107, "ed": 109, "text": "attention based"}, {"st": 114, "ed": 117, "text": "coarse to fine"}]
[{"st": 3, "ed": 7, "text": "deep recurrent neural network"}, {"st": 10, "ed": 12, "text": "visual attention"}, {"st": 19, "ed": 21, "text": "real world"}, {"st": 40, "ed": 42, "text": "machine translation"}]
[{"st": 5, "ed": 7, "text": "highly efficient"}, {"st": 7, "ed": 10, "text": "deep neural networks"}, {"st": 19, "ed": 21, "text": "biological evolution"}, {"st": 39, "ed": 41, "text": "encoding scheme"}, {"st": 69, "ed": 72, "text": "deep neural networks"}, {"st": 140, "ed": 143, "text": "deep neural network"}, {"st": 152, "ed": 155, "text": "deep neural network"}, {"st": 167, "ed": 169, "text": "object categorization"}, {"st": 170, "ed": 172, "text": "object detection"}, {"st": 182, "ed": 184, "text": "encoding scheme"}, {"st": 195, "ed": 197, "text": "significantly smaller"}, {"st": 251, "ed": 253, "text": "parallel computing"}, {"st": 261, "ed": 264, "text": "deep neural network"}]
[{"st": 5, "ed": 7, "text": "cognitive neuroscience"}, {"st": 38, "ed": 40, "text": "machine learning"}, {"st": 128, "ed": 130, "text": "least squares"}, {"st": 153, "ed": 155, "text": "neural networks"}, {"st": 155, "ed": 158, "text": "k nearest neighbor"}, {"st": 176, "ed": 180, "text": "functional magnetic resonance imaging"}]
[{"st": 10, "ed": 13, "text": "deep neural networks"}, {"st": 41, "ed": 43, "text": "convolutional layer"}, {"st": 47, "ed": 49, "text": "biologically inspired"}, {"st": 52, "ed": 54, "text": "log normal"}, {"st": 67, "ed": 70, "text": "deep neural network"}]
[{"st": 16, "ed": 19, "text": "particle swarm optimization"}, {"st": 38, "ed": 40, "text": "search space"}]
[{"st": 0, "ed": 2, "text": "unsupervised learning"}, {"st": 3, "ed": 5, "text": "probabilistic models"}, {"st": 9, "ed": 11, "text": "challenging problem"}, {"st": 38, "ed": 40, "text": "real valued"}, {"st": 57, "ed": 59, "text": "unsupervised learning"}, {"st": 62, "ed": 64, "text": "log likelihood"}, {"st": 67, "ed": 69, "text": "exact inference"}, {"st": 70, "ed": 72, "text": "latent variables"}, {"st": 83, "ed": 85, "text": "natural images"}, {"st": 90, "ed": 92, "text": "log likelihood"}, {"st": 94, "ed": 96, "text": "latent variable"}]
[{"st": 11, "ed": 14, "text": "black box optimization"}, {"st": 97, "ed": 99, "text": "competitive results"}, {"st": 101, "ed": 103, "text": "atari games"}, {"st": 118, "ed": 121, "text": "black box optimization"}]
[{"st": 7, "ed": 9, "text": "neural network"}, {"st": 33, "ed": 35, "text": "policy network"}, {"st": 74, "ed": 76, "text": "fully differentiable"}, {"st": 79, "ed": 82, "text": "end to end"}, {"st": 114, "ed": 116, "text": "preliminary experiments"}, {"st": 119, "ed": 121, "text": "strong performance"}, {"st": 148, "ed": 151, "text": "end to end"}]
[{"st": 1, "ed": 3, "text": "deep model"}, {"st": 4, "ed": 6, "text": "reinforcement learning"}, {"st": 26, "ed": 28, "text": "look ahead"}, {"st": 43, "ed": 45, "text": "complex environments"}, {"st": 57, "ed": 59, "text": "learned models"}, {"st": 91, "ed": 93, "text": "deep rl"}, {"st": 105, "ed": 107, "text": "transition model"}, {"st": 153, "ed": 157, "text": "trained end to end"}]
[{"st": 112, "ed": 114, "text": "attention mechanism"}, {"st": 116, "ed": 118, "text": "hidden states"}]
[{"st": 7, "ed": 10, "text": "deep neural networks"}, {"st": 23, "ed": 25, "text": "dynamic range"}, {"st": 60, "ed": 62, "text": "dynamic range"}, {"st": 81, "ed": 84, "text": "loss in accuracy"}, {"st": 86, "ed": 88, "text": "floating point"}, {"st": 151, "ed": 153, "text": "deep learning"}]
[{"st": 0, "ed": 2, "text": "multi step"}, {"st": 3, "ed": 5, "text": "time series"}, {"st": 11, "ed": 13, "text": "challenging research"}, {"st": 18, "ed": 20, "text": "time series"}, {"st": 46, "ed": 48, "text": "multi step"}, {"st": 49, "ed": 51, "text": "time series"}, {"st": 126, "ed": 128, "text": "neural networks"}, {"st": 158, "ed": 161, "text": "simulated and real"}]
[{"st": 14, "ed": 16, "text": "feed forward"}]
[{"st": 7, "ed": 9, "text": "neural network"}, {"st": 64, "ed": 66, "text": "expressive power"}, {"st": 82, "ed": 84, "text": "neural network"}]
[{"st": 4, "ed": 6, "text": "transfer learning"}, {"st": 7, "ed": 9, "text": "representation learning"}, {"st": 54, "ed": 56, "text": "representation learning"}, {"st": 91, "ed": 95, "text": "maximum mean discrepancy mmd"}, {"st": 123, "ed": 125, "text": "multiple domains"}, {"st": 161, "ed": 163, "text": "domain adaptation"}, {"st": 168, "ed": 170, "text": "invariant representations"}]
[{"st": 5, "ed": 8, "text": "deep reinforcement learning"}, {"st": 27, "ed": 29, "text": "temporal difference"}, {"st": 29, "ed": 31, "text": "based method"}, {"st": 51, "ed": 53, "text": "neural networks"}, {"st": 72, "ed": 74, "text": "challenging tasks"}, {"st": 75, "ed": 77, "text": "contextual bandit"}, {"st": 80, "ed": 82, "text": "nonparametric regression"}, {"st": 91, "ed": 93, "text": "reinforcement learning"}, {"st": 95, "ed": 97, "text": "accurately estimate"}, {"st": 104, "ed": 106, "text": "reinforcement learning"}, {"st": 111, "ed": 113, "text": "fully supervised"}]
[{"st": 23, "ed": 25, "text": "multi dimensional"}, {"st": 40, "ed": 42, "text": "efficient inference"}, {"st": 61, "ed": 63, "text": "biological neural"}, {"st": 79, "ed": 81, "text": "spike timing"}, {"st": 90, "ed": 92, "text": "learning rule"}, {"st": 108, "ed": 110, "text": "time series"}, {"st": 124, "ed": 126, "text": "learning rule"}, {"st": 144, "ed": 146, "text": "limited memory"}]
[{"st": 0, "ed": 3, "text": "graph structured data"}, {"st": 6, "ed": 8, "text": "domains including"}, {"st": 9, "ed": 11, "text": "natural language"}, {"st": 12, "ed": 14, "text": "social networks"}, {"st": 22, "ed": 24, "text": "feature learning"}, {"st": 30, "ed": 32, "text": "starting point"}, {"st": 37, "ed": 39, "text": "neural networks"}, {"st": 48, "ed": 50, "text": "gated recurrent"}, {"st": 53, "ed": 55, "text": "optimization techniques"}, {"st": 71, "ed": 73, "text": "neural network"}, {"st": 77, "ed": 79, "text": "inductive biases"}]
[{"st": 18, "ed": 20, "text": "reinforcement learning"}, {"st": 26, "ed": 28, "text": "recommender systems"}, {"st": 39, "ed": 41, "text": "real world"}, {"st": 41, "ed": 43, "text": "tasks involving"}, {"st": 50, "ed": 52, "text": "current methods"}, {"st": 73, "ed": 75, "text": "linear complexity"}, {"st": 89, "ed": 91, "text": "current approaches"}, {"st": 107, "ed": 109, "text": "proposed approach"}, {"st": 110, "ed": 112, "text": "prior information"}, {"st": 120, "ed": 122, "text": "continuous space"}, {"st": 128, "ed": 131, "text": "approximate nearest neighbor"}, {"st": 156, "ed": 158, "text": "reinforcement learning"}, {"st": 163, "ed": 165, "text": "large scale"}, {"st": 165, "ed": 167, "text": "learning problems"}]
[{"st": 3, "ed": 5, "text": "value iteration"}, {"st": 8, "ed": 10, "text": "fully differentiable"}, {"st": 10, "ed": 12, "text": "neural network"}, {"st": 51, "ed": 53, "text": "value iteration"}, {"st": 60, "ed": 63, "text": "convolutional neural network"}, {"st": 64, "ed": 68, "text": "trained end to end"}, {"st": 77, "ed": 80, "text": "discrete and continuous"}, {"st": 80, "ed": 82, "text": "path planning"}, {"st": 86, "ed": 88, "text": "natural language"}]
[{"st": 7, "ed": 9, "text": "powerful tools"}, {"st": 11, "ed": 13, "text": "sequential data"}, {"st": 41, "ed": 43, "text": "synthetic datasets"}, {"st": 105, "ed": 107, "text": "recent methods"}]
[{"st": 1, "ed": 3, "text": "learning algorithms"}, {"st": 32, "ed": 34, "text": "reinforcement learning"}, {"st": 62, "ed": 64, "text": "atari games"}, {"st": 85, "ed": 87, "text": "learning algorithm"}, {"st": 90, "ed": 92, "text": "reward function"}, {"st": 106, "ed": 108, "text": "domain specific"}]
[{"st": 1, "ed": 3, "text": "human genome"}, {"st": 7, "ed": 9, "text": "base pair"}, {"st": 17, "ed": 19, "text": "deep learning"}, {"st": 62, "ed": 64, "text": "search algorithm"}, {"st": 73, "ed": 75, "text": "simultaneously learns"}]
[{"st": 18, "ed": 21, "text": "deep reinforcement learning"}, {"st": 79, "ed": 81, "text": "inner product"}, {"st": 100, "ed": 103, "text": "end to end"}, {"st": 103, "ed": 106, "text": "deep reinforcement learning"}]
[{"st": 0, "ed": 3, "text": "deep reinforcement learning"}, {"st": 3, "ed": 5, "text": "deep rl"}, {"st": 15, "ed": 17, "text": "learning process"}, {"st": 38, "ed": 40, "text": "prior knowledge"}, {"st": 55, "ed": 57, "text": "reinforcement learning"}, {"st": 65, "ed": 69, "text": "recurrent neural network rnn"}, {"st": 76, "ed": 78, "text": "proposed method"}, {"st": 129, "ed": 132, "text": "markov decision process"}, {"st": 150, "ed": 152, "text": "previously unseen"}, {"st": 160, "ed": 162, "text": "small scale"}, {"st": 163, "ed": 165, "text": "large scale"}, {"st": 168, "ed": 170, "text": "small scale"}, {"st": 176, "ed": 178, "text": "randomly generated"}, {"st": 180, "ed": 182, "text": "bandit problems"}, {"st": 206, "ed": 208, "text": "large scale"}, {"st": 215, "ed": 217, "text": "vision based"}]
[{"st": 7, "ed": 11, "text": "recurrent neural networks rnns"}, {"st": 40, "ed": 42, "text": "rnn architectures"}, {"st": 93, "ed": 95, "text": "real number"}, {"st": 125, "ed": 127, "text": "previous results"}, {"st": 128, "ed": 130, "text": "rnn architectures"}, {"st": 161, "ed": 164, "text": "difficult to train"}, {"st": 174, "ed": 176, "text": "rnn architectures"}]
[{"st": 1, "ed": 3, "text": "application domains"}, {"st": 9, "ed": 11, "text": "predictive models"}, {"st": 28, "ed": 30, "text": "predictive models"}, {"st": 41, "ed": 43, "text": "large scale"}, {"st": 45, "ed": 48, "text": "electronic health records"}, {"st": 55, "ed": 57, "text": "l1 regularized"}, {"st": 78, "ed": 80, "text": "neural network"}, {"st": 95, "ed": 97, "text": "representation learning"}, {"st": 105, "ed": 107, "text": "multilayer perceptron"}]
[{"st": 0, "ed": 3, "text": "deep neural networks"}, {"st": 4, "ed": 6, "text": "representation learning"}, {"st": 34, "ed": 36, "text": "trained cnn"}, {"st": 41, "ed": 43, "text": "image data"}, {"st": 85, "ed": 87, "text": "feature representation"}, {"st": 89, "ed": 91, "text": "transfer learning"}, {"st": 145, "ed": 147, "text": "discriminative power"}, {"st": 154, "ed": 156, "text": "deep cnn"}, {"st": 181, "ed": 183, "text": "convolutional layers"}, {"st": 203, "ed": 205, "text": "level features"}, {"st": 209, "ed": 211, "text": "high level"}]
[{"st": 3, "ed": 5, "text": "probabilistic models"}, {"st": 19, "ed": 22, "text": "generative adversarial networks"}, {"st": 46, "ed": 48, "text": "quantitative evaluations"}, {"st": 58, "ed": 61, "text": "generative adversarial network"}, {"st": 74, "ed": 76, "text": "maximum likelihood"}, {"st": 90, "ed": 92, "text": "log likelihood"}, {"st": 97, "ed": 99, "text": "mixture model"}, {"st": 101, "ed": 103, "text": "training data"}, {"st": 114, "ed": 118, "text": "mnist and cifar 10"}]
[{"st": 7, "ed": 9, "text": "maximum likelihood"}, {"st": 11, "ed": 13, "text": "latent variable"}, {"st": 15, "ed": 18, "text": "evidence lower bound"}, {"st": 50, "ed": 52, "text": "marginal likelihood"}, {"st": 120, "ed": 122, "text": "substantial improvements"}]
[{"st": 0, "ed": 2, "text": "recent progress"}, {"st": 3, "ed": 5, "text": "variational inference"}, {"st": 33, "ed": 35, "text": "existing methods"}, {"st": 67, "ed": 69, "text": "variational inference"}, {"st": 83, "ed": 85, "text": "variational inference"}, {"st": 86, "ed": 88, "text": "successfully applied"}, {"st": 90, "ed": 92, "text": "neural networks"}, {"st": 94, "ed": 96, "text": "promising results"}]
[{"st": 0, "ed": 2, "text": "partially observable"}, {"st": 29, "ed": 31, "text": "reinforcement learning"}, {"st": 54, "ed": 56, "text": "recently proposed"}, {"st": 56, "ed": 58, "text": "attention based"}, {"st": 61, "ed": 64, "text": "end to end"}, {"st": 76, "ed": 79, "text": "end to end"}, {"st": 96, "ed": 98, "text": "partially observed"}, {"st": 108, "ed": 111, "text": "end to end"}, {"st": 121, "ed": 123, "text": "attention mechanism"}, {"st": 136, "ed": 138, "text": "attention mechanism"}, {"st": 147, "ed": 149, "text": "encouraging results"}, {"st": 155, "ed": 157, "text": "attention based"}, {"st": 163, "ed": 165, "text": "continuous state"}, {"st": 165, "ed": 167, "text": "non stationary"}, {"st": 167, "ed": 169, "text": "control problem"}, {"st": 196, "ed": 198, "text": "decision process"}]
[{"st": 15, "ed": 17, "text": "supervised learning"}, {"st": 22, "ed": 24, "text": "deep learning"}, {"st": 58, "ed": 60, "text": "machine learning"}, {"st": 65, "ed": 68, "text": "deep neural networks"}, {"st": 101, "ed": 104, "text": "cross entropy loss"}, {"st": 149, "ed": 151, "text": "evolutionary algorithm"}, {"st": 157, "ed": 160, "text": "deep neural network"}, {"st": 178, "ed": 180, "text": "classification regression"}, {"st": 181, "ed": 183, "text": "sentiment analysis"}, {"st": 228, "ed": 230, "text": "classification problems"}, {"st": 240, "ed": 242, "text": "loss function"}, {"st": 248, "ed": 250, "text": "neural network"}, {"st": 258, "ed": 260, "text": "fully automated"}]
[{"st": 0, "ed": 3, "text": "deep neural networks"}, {"st": 36, "ed": 38, "text": "meta learning"}, {"st": 57, "ed": 59, "text": "related tasks"}, {"st": 80, "ed": 82, "text": "meta learning"}, {"st": 85, "ed": 87, "text": "hand designed"}, {"st": 129, "ed": 131, "text": "soft attention"}, {"st": 154, "ed": 156, "text": "meta learning"}, {"st": 181, "ed": 183, "text": "reinforcement learning"}]
[{"st": 0, "ed": 2, "text": "neural networks"}, {"st": 27, "ed": 29, "text": "activation functions"}, {"st": 65, "ed": 67, "text": "activation functions"}, {"st": 83, "ed": 85, "text": "kernel based"}, {"st": 96, "ed": 98, "text": "activation functions"}, {"st": 127, "ed": 129, "text": "real line"}, {"st": 158, "ed": 160, "text": "ell 1"}, {"st": 193, "ed": 195, "text": "activation functions"}]
[{"st": 0, "ed": 2, "text": "conventional wisdom"}, {"st": 54, "ed": 57, "text": "sequential decision making"}, {"st": 163, "ed": 165, "text": "computational costs"}, {"st": 181, "ed": 183, "text": "continuous control"}]
[{"st": 18, "ed": 20, "text": "hierarchical latent"}, {"st": 33, "ed": 35, "text": "wide variety"}, {"st": 60, "ed": 62, "text": "competitive results"}, {"st": 71, "ed": 73, "text": "higher order"}, {"st": 85, "ed": 87, "text": "fully supervised"}, {"st": 87, "ed": 89, "text": "semi supervised"}, {"st": 102, "ed": 104, "text": "powerful tools"}]
[{"st": 5, "ed": 7, "text": "neural networks"}, {"st": 19, "ed": 21, "text": "open problem"}, {"st": 27, "ed": 29, "text": "unlike previous"}, {"st": 58, "ed": 60, "text": "deep learning"}, {"st": 78, "ed": 80, "text": "open question"}]
[{"st": 59, "ed": 61, "text": "higher level"}, {"st": 79, "ed": 82, "text": "convolutional neural network"}, {"st": 84, "ed": 86, "text": "latent space"}]
[{"st": 1, "ed": 4, "text": "short term memory"}, {"st": 14, "ed": 17, "text": "recurrent neural networks"}, {"st": 27, "ed": 29, "text": "lstm network"}, {"st": 61, "ed": 63, "text": "hidden states"}, {"st": 127, "ed": 129, "text": "experiments conducted"}]
[{"st": 5, "ed": 7, "text": "neural network"}, {"st": 12, "ed": 14, "text": "handwritten digit"}, {"st": 29, "ed": 31, "text": "feature extraction"}, {"st": 50, "ed": 52, "text": "feature extraction"}, {"st": 75, "ed": 77, "text": "recently developed"}, {"st": 82, "ed": 84, "text": "supervised learning"}, {"st": 86, "ed": 89, "text": "spiking neural networks"}, {"st": 101, "ed": 103, "text": "handwritten digits"}, {"st": 112, "ed": 114, "text": "training set"}, {"st": 123, "ed": 125, "text": "fewer parameters"}, {"st": 143, "ed": 145, "text": "user interface"}]
[{"st": 0, "ed": 2, "text": "catastrophic forgetting"}, {"st": 5, "ed": 7, "text": "neural network"}, {"st": 26, "ed": 28, "text": "artificial intelligence"}, {"st": 42, "ed": 44, "text": "attention mechanism"}, {"st": 46, "ed": 48, "text": "previous tasks"}, {"st": 67, "ed": 70, "text": "stochastic gradient descent"}, {"st": 89, "ed": 91, "text": "catastrophic forgetting"}, {"st": 143, "ed": 145, "text": "online learning"}]
[{"st": 5, "ed": 8, "text": "training and test"}, {"st": 134, "ed": 136, "text": "experiments demonstrate"}, {"st": 136, "ed": 138, "text": "accurate estimates"}]
[{"st": 7, "ed": 9, "text": "learning theory"}, {"st": 18, "ed": 20, "text": "learning theory"}, {"st": 42, "ed": 44, "text": "representation learning"}, {"st": 61, "ed": 63, "text": "rademacher complexity"}, {"st": 68, "ed": 70, "text": "learning algorithms"}, {"st": 83, "ed": 85, "text": "generalization bounds"}, {"st": 110, "ed": 112, "text": "deep learning"}, {"st": 119, "ed": 121, "text": "statistical learning"}, {"st": 124, "ed": 126, "text": "learning theory"}, {"st": 132, "ed": 134, "text": "measure theory"}, {"st": 139, "ed": 141, "text": "problem instances"}, {"st": 155, "ed": 157, "text": "learning theory"}, {"st": 164, "ed": 166, "text": "learning theory"}]
[{"st": 10, "ed": 12, "text": "neural networks"}, {"st": 48, "ed": 50, "text": "extensive empirical"}, {"st": 71, "ed": 73, "text": "fully connected"}, {"st": 77, "ed": 79, "text": "hyper parameters"}, {"st": 84, "ed": 86, "text": "image classification"}, {"st": 91, "ed": 93, "text": "neural networks"}, {"st": 104, "ed": 106, "text": "training data"}, {"st": 114, "ed": 116, "text": "input output"}, {"st": 161, "ed": 163, "text": "non linearities"}, {"st": 174, "ed": 176, "text": "input output"}]
[{"st": 5, "ed": 7, "text": "large datasets"}, {"st": 7, "ed": 10, "text": "deep neural networks"}, {"st": 50, "ed": 52, "text": "linear combination"}, {"st": 92, "ed": 94, "text": "generalization performance"}, {"st": 134, "ed": 136, "text": "batch normalization"}]
[{"st": 20, "ed": 22, "text": "image pixels"}, {"st": 35, "ed": 37, "text": "machine learning"}, {"st": 83, "ed": 85, "text": "machine learning"}, {"st": 173, "ed": 175, "text": "learning algorithm"}]
[{"st": 11, "ed": 14, "text": "visual object recognition"}]
[{"st": 2, "ed": 4, "text": "mnist handwritten"}, {"st": 4, "ed": 6, "text": "digit recognition"}, {"st": 19, "ed": 21, "text": "substantial improvement"}, {"st": 27, "ed": 29, "text": "error rate"}, {"st": 36, "ed": 38, "text": "significantly improve"}, {"st": 41, "ed": 43, "text": "graphics cards"}, {"st": 67, "ed": 69, "text": "substantial improvement"}]
[{"st": 0, "ed": 3, "text": "artificial neural network"}, {"st": 13, "ed": 15, "text": "neural networks"}, {"st": 130, "ed": 132, "text": "method called"}, {"st": 137, "ed": 139, "text": "decision tree"}, {"st": 145, "ed": 147, "text": "neural network"}, {"st": 154, "ed": 156, "text": "theoretical analysis"}]
[{"st": 28, "ed": 30, "text": "image generation"}, {"st": 30, "ed": 32, "text": "message passing"}, {"st": 32, "ed": 34, "text": "multi agent"}, {"st": 34, "ed": 37, "text": "generative adversarial networks"}, {"st": 51, "ed": 53, "text": "image generation"}, {"st": 73, "ed": 75, "text": "multi agent"}, {"st": 79, "ed": 81, "text": "message passing"}]
[{"st": 1, "ed": 4, "text": "generative adversarial networks"}, {"st": 48, "ed": 50, "text": "high dimensional"}, {"st": 113, "ed": 116, "text": "data generating distribution"}]
[{"st": 1, "ed": 3, "text": "increasing complexity"}, {"st": 4, "ed": 6, "text": "deep learning"}, {"st": 25, "ed": 27, "text": "vanishing gradients"}, {"st": 33, "ed": 35, "text": "back propagation"}, {"st": 36, "ed": 38, "text": "extremely large"}, {"st": 46, "ed": 48, "text": "output layer"}, {"st": 77, "ed": 79, "text": "convex problems"}, {"st": 81, "ed": 84, "text": "deep neural networks"}, {"st": 93, "ed": 95, "text": "saddle points"}, {"st": 114, "ed": 116, "text": "optimization method"}, {"st": 118, "ed": 121, "text": "deep neural networks"}, {"st": 123, "ed": 125, "text": "learning rates"}, {"st": 145, "ed": 147, "text": "learning rate"}, {"st": 180, "ed": 182, "text": "image classification"}]
[{"st": 3, "ed": 5, "text": "machine learning"}, {"st": 23, "ed": 25, "text": "practical scenarios"}, {"st": 31, "ed": 33, "text": "machine learning"}, {"st": 35, "ed": 37, "text": "domain adaptation"}, {"st": 78, "ed": 81, "text": "effective and efficient"}, {"st": 83, "ed": 86, "text": "unsupervised domain adaptation"}, {"st": 93, "ed": 95, "text": "domain shift"}, {"st": 102, "ed": 105, "text": "source and target"}]
[{"st": 5, "ed": 7, "text": "computer vision"}, {"st": 16, "ed": 20, "text": "deep convolutional neural networks"}, {"st": 27, "ed": 29, "text": "object recognition"}, {"st": 35, "ed": 37, "text": "optical flow"}, {"st": 39, "ed": 41, "text": "hardware acceleration"}, {"st": 54, "ed": 56, "text": "computer vision"}]
[{"st": 9, "ed": 11, "text": "reinforcement learning"}, {"st": 46, "ed": 48, "text": "reinforcement learning"}, {"st": 58, "ed": 60, "text": "multiple tasks"}, {"st": 69, "ed": 71, "text": "dynamical systems"}, {"st": 86, "ed": 88, "text": "spectral clustering"}, {"st": 138, "ed": 140, "text": "learning task"}, {"st": 188, "ed": 190, "text": "complex tasks"}]
[{"st": 9, "ed": 11, "text": "residual networks"}, {"st": 27, "ed": 29, "text": "residual networks"}, {"st": 33, "ed": 35, "text": "deep networks"}, {"st": 49, "ed": 51, "text": "residual networks"}, {"st": 57, "ed": 59, "text": "unlike traditional"}, {"st": 62, "ed": 64, "text": "residual networks"}, {"st": 128, "ed": 130, "text": "residual network"}, {"st": 159, "ed": 161, "text": "deep networks"}, {"st": 161, "ed": 163, "text": "residual networks"}, {"st": 165, "ed": 168, "text": "vanishing gradient problem"}]
[{"st": 0, "ed": 3, "text": "deep neural networks"}, {"st": 13, "ed": 15, "text": "pattern recognition"}, {"st": 46, "ed": 48, "text": "human brain"}, {"st": 62, "ed": 64, "text": "neural network"}, {"st": 119, "ed": 121, "text": "generator network"}, {"st": 161, "ed": 163, "text": "network architectures"}]
[{"st": 23, "ed": 26, "text": "deep convolutional networks"}, {"st": 45, "ed": 47, "text": "classification accuracy"}, {"st": 50, "ed": 52, "text": "image recognition"}, {"st": 63, "ed": 65, "text": "convolutional networks"}, {"st": 72, "ed": 75, "text": "rectified linear units"}]
[{"st": 0, "ed": 3, "text": "deep neural networks"}, {"st": 14, "ed": 16, "text": "input data"}, {"st": 27, "ed": 29, "text": "domain adaptation"}, {"st": 65, "ed": 68, "text": "unsupervised domain adaptation"}, {"st": 77, "ed": 80, "text": "source and target"}, {"st": 101, "ed": 104, "text": "deep neural networks"}, {"st": 109, "ed": 112, "text": "standard benchmark datasets"}]
[{"st": 32, "ed": 34, "text": "rnn based"}, {"st": 53, "ed": 55, "text": "spatio temporal"}, {"st": 67, "ed": 69, "text": "input data"}, {"st": 80, "ed": 82, "text": "human skeleton"}, {"st": 117, "ed": 119, "text": "input data"}, {"st": 129, "ed": 131, "text": "context information"}, {"st": 137, "ed": 139, "text": "method achieves"}, {"st": 147, "ed": 149, "text": "benchmark datasets"}]
[{"st": 0, "ed": 3, "text": "deep neural networks"}, {"st": 20, "ed": 22, "text": "deep learning"}, {"st": 35, "ed": 37, "text": "approximate bayesian"}, {"st": 83, "ed": 85, "text": "special case"}, {"st": 97, "ed": 99, "text": "neural network"}, {"st": 108, "ed": 110, "text": "generalization performance"}]
[{"st": 36, "ed": 38, "text": "training procedure"}, {"st": 135, "ed": 137, "text": "mobile computing"}]
[{"st": 5, "ed": 7, "text": "meta learning"}, {"st": 32, "ed": 34, "text": "learning problems"}, {"st": 35, "ed": 37, "text": "classification regression"}, {"st": 43, "ed": 45, "text": "meta learning"}, {"st": 96, "ed": 98, "text": "training data"}, {"st": 105, "ed": 107, "text": "generalization performance"}, {"st": 137, "ed": 139, "text": "few shot"}, {"st": 139, "ed": 141, "text": "image classification"}, {"st": 146, "ed": 148, "text": "few shot"}, {"st": 151, "ed": 153, "text": "fine tuning"}, {"st": 154, "ed": 156, "text": "policy gradient"}, {"st": 156, "ed": 158, "text": "reinforcement learning"}, {"st": 159, "ed": 161, "text": "neural network"}]
[{"st": 1, "ed": 3, "text": "computer vision"}, {"st": 4, "ed": 6, "text": "prior works"}, {"st": 18, "ed": 20, "text": "network weights"}, {"st": 21, "ed": 24, "text": "deep neural networks"}, {"st": 108, "ed": 110, "text": "significantly reduce"}, {"st": 122, "ed": 125, "text": "training and inference"}, {"st": 154, "ed": 156, "text": "previously reported"}, {"st": 168, "ed": 170, "text": "previously reported"}]
[{"st": 0, "ed": 3, "text": "deep neural networks"}, {"st": 11, "ed": 13, "text": "impressive results"}, {"st": 14, "ed": 16, "text": "image classification"}, {"st": 25, "ed": 27, "text": "time consuming"}, {"st": 50, "ed": 53, "text": "deep neural networks"}, {"st": 58, "ed": 60, "text": "training data"}, {"st": 62, "ed": 64, "text": "true labels"}, {"st": 143, "ed": 145, "text": "dataset size"}]
[{"st": 32, "ed": 35, "text": "2d and 3d"}, {"st": 52, "ed": 54, "text": "regularization term"}, {"st": 105, "ed": 107, "text": "regularization term"}]
[{"st": 2, "ed": 4, "text": "activity recognition"}, {"st": 16, "ed": 18, "text": "low level"}, {"st": 21, "ed": 23, "text": "pattern recognition"}, {"st": 40, "ed": 42, "text": "hand crafted"}, {"st": 42, "ed": 44, "text": "feature extraction"}, {"st": 51, "ed": 53, "text": "existing methods"}, {"st": 58, "ed": 60, "text": "incremental learning"}, {"st": 66, "ed": 68, "text": "deep learning"}, {"st": 76, "ed": 78, "text": "feature extraction"}, {"st": 80, "ed": 82, "text": "promising performance"}, {"st": 87, "ed": 90, "text": "deep learning based"}, {"st": 93, "ed": 95, "text": "widely adopted"}, {"st": 99, "ed": 101, "text": "activity recognition"}, {"st": 109, "ed": 111, "text": "deep learning"}, {"st": 118, "ed": 120, "text": "existing literature"}, {"st": 125, "ed": 127, "text": "deep model"}]
[{"st": 7, "ed": 10, "text": "deep neural networks"}, {"st": 33, "ed": 35, "text": "training speed"}, {"st": 101, "ed": 103, "text": "linear algebra"}, {"st": 123, "ed": 125, "text": "neural network"}]
[{"st": 4, "ed": 6, "text": "fully automatic"}, {"st": 32, "ed": 34, "text": "highly accurate"}, {"st": 35, "ed": 37, "text": "semi automatic"}, {"st": 52, "ed": 54, "text": "image processing"}, {"st": 91, "ed": 93, "text": "fully convolutional"}, {"st": 93, "ed": 96, "text": "artificial neural network"}]
[{"st": 7, "ed": 9, "text": "neural network"}, {"st": 36, "ed": 38, "text": "forward pass"}, {"st": 40, "ed": 42, "text": "classification accuracies"}, {"st": 53, "ed": 55, "text": "error reduction"}, {"st": 82, "ed": 85, "text": "credit card fraud"}, {"st": 86, "ed": 88, "text": "credit card"}, {"st": 97, "ed": 99, "text": "hidden layers"}]
[{"st": 41, "ed": 43, "text": "spherical harmonics"}, {"st": 64, "ed": 66, "text": "higher order"}, {"st": 93, "ed": 95, "text": "3d shapes"}]
[{"st": 6, "ed": 8, "text": "prior information"}, {"st": 13, "ed": 15, "text": "neural networks"}, {"st": 17, "ed": 19, "text": "learning task"}, {"st": 27, "ed": 29, "text": "machine learning"}, {"st": 144, "ed": 146, "text": "pre trained"}, {"st": 204, "ed": 207, "text": "unsupervised pre training"}, {"st": 213, "ed": 215, "text": "decision trees"}, {"st": 233, "ed": 235, "text": "pre training"}, {"st": 265, "ed": 267, "text": "optimization problems"}, {"st": 268, "ed": 270, "text": "deep learning"}]
[{"st": 8, "ed": 10, "text": "hidden unit"}, {"st": 29, "ed": 31, "text": "hidden layer"}, {"st": 38, "ed": 40, "text": "input data"}, {"st": 65, "ed": 67, "text": "intrinsic dimensionality"}, {"st": 74, "ed": 76, "text": "activation function"}, {"st": 83, "ed": 85, "text": "hidden layer"}, {"st": 97, "ed": 99, "text": "intrinsic dimensionality"}, {"st": 107, "ed": 109, "text": "activation function"}, {"st": 125, "ed": 127, "text": "training data"}]
[{"st": 0, "ed": 5, "text": "deep convolutional neural networks cnns"}, {"st": 7, "ed": 9, "text": "great potential"}, {"st": 11, "ed": 13, "text": "real world"}, {"st": 13, "ed": 15, "text": "machine learning"}, {"st": 40, "ed": 43, "text": "spiking neural networks"}, {"st": 46, "ed": 48, "text": "similar accuracy"}, {"st": 96, "ed": 98, "text": "deep networks"}, {"st": 131, "ed": 133, "text": "max pooling"}, {"st": 135, "ed": 137, "text": "batch normalization"}, {"st": 151, "ed": 153, "text": "empirical evaluation"}, {"st": 155, "ed": 157, "text": "network architectures"}, {"st": 159, "ed": 162, "text": "mnist and cifar10"}]
[{"st": 7, "ed": 9, "text": "generative model"}, {"st": 11, "ed": 14, "text": "generative adversarial networks"}, {"st": 21, "ed": 23, "text": "hierarchical representations"}, {"st": 48, "ed": 50, "text": "higher level"}, {"st": 114, "ed": 117, "text": "variational lower bound"}, {"st": 159, "ed": 161, "text": "multiple levels"}, {"st": 179, "ed": 181, "text": "turing test"}, {"st": 192, "ed": 194, "text": "higher quality"}]
[{"st": 5, "ed": 7, "text": "large scale"}, {"st": 7, "ed": 9, "text": "multi label"}, {"st": 9, "ed": 11, "text": "visual recognition"}, {"st": 25, "ed": 28, "text": "trained neural network"}, {"st": 37, "ed": 39, "text": "significantly improve"}, {"st": 86, "ed": 88, "text": "hidden layer"}, {"st": 101, "ed": 103, "text": "significant improvement"}, {"st": 109, "ed": 111, "text": "large scale"}, {"st": 111, "ed": 113, "text": "object recognition"}]
[{"st": 0, "ed": 3, "text": "artificial neural networks"}, {"st": 8, "ed": 10, "text": "linear activation"}, {"st": 22, "ed": 24, "text": "linear activation"}, {"st": 38, "ed": 40, "text": "activation function"}, {"st": 46, "ed": 49, "text": "deep neural network"}, {"st": 53, "ed": 56, "text": "rectified linear units"}, {"st": 63, "ed": 65, "text": "cifar 10"}, {"st": 66, "ed": 68, "text": "cifar 100"}, {"st": 73, "ed": 76, "text": "high energy physics"}, {"st": 77, "ed": 79, "text": "higgs boson"}]
[{"st": 1, "ed": 3, "text": "lateral connections"}, {"st": 4, "ed": 7, "text": "encoder and decoder"}, {"st": 11, "ed": 13, "text": "higher layers"}, {"st": 15, "ed": 17, "text": "denoising autoencoder"}, {"st": 37, "ed": 39, "text": "lateral connections"}, {"st": 51, "ed": 53, "text": "invariant features"}, {"st": 60, "ed": 62, "text": "invariant features"}, {"st": 79, "ed": 81, "text": "lateral connections"}, {"st": 83, "ed": 85, "text": "lateral connections"}, {"st": 90, "ed": 92, "text": "real world"}, {"st": 99, "ed": 101, "text": "lateral connections"}, {"st": 131, "ed": 133, "text": "higher layers"}]
[{"st": 4, "ed": 6, "text": "machine learning"}, {"st": 19, "ed": 21, "text": "inference tasks"}, {"st": 29, "ed": 32, "text": "visual object recognition"}, {"st": 41, "ed": 43, "text": "object recognition"}, {"st": 44, "ed": 46, "text": "speech recognition"}, {"st": 59, "ed": 61, "text": "deep learning"}, {"st": 67, "ed": 69, "text": "inference tasks"}, {"st": 72, "ed": 74, "text": "pattern recognition"}, {"st": 101, "ed": 103, "text": "deep learning"}, {"st": 115, "ed": 117, "text": "probabilistic framework"}, {"st": 118, "ed": 120, "text": "deep learning"}, {"st": 128, "ed": 130, "text": "probabilistic model"}, {"st": 139, "ed": 141, "text": "generative model"}, {"st": 153, "ed": 155, "text": "deep learning"}, {"st": 156, "ed": 160, "text": "deep convolutional neural networks"}]
[{"st": 1, "ed": 3, "text": "pattern recognition"}, {"st": 8, "ed": 10, "text": "computer vision"}, {"st": 20, "ed": 23, "text": "taking into account"}, {"st": 23, "ed": 25, "text": "task specific"}, {"st": 31, "ed": 33, "text": "structured prediction"}, {"st": 34, "ed": 36, "text": "internal structure"}, {"st": 51, "ed": 54, "text": "support vector machines"}, {"st": 62, "ed": 64, "text": "input output"}, {"st": 66, "ed": 68, "text": "margin based"}, {"st": 82, "ed": 84, "text": "training phase"}, {"st": 95, "ed": 97, "text": "linear combinations"}, {"st": 98, "ed": 100, "text": "input features"}, {"st": 106, "ed": 108, "text": "prediction accuracy"}, {"st": 112, "ed": 114, "text": "joint inference"}, {"st": 119, "ed": 121, "text": "back propagation"}, {"st": 128, "ed": 130, "text": "descent ii"}, {"st": 134, "ed": 136, "text": "neural networks"}, {"st": 138, "ed": 140, "text": "highly nonlinear"}, {"st": 144, "ed": 146, "text": "image segmentation"}, {"st": 163, "ed": 166, "text": "end to end"}]
[{"st": 5, "ed": 7, "text": "central role"}, {"st": 8, "ed": 10, "text": "human perception"}, {"st": 22, "ed": 24, "text": "deep networks"}, {"st": 26, "ed": 30, "text": "convolutional neural networks cnns"}, {"st": 78, "ed": 80, "text": "affine transformation"}, {"st": 138, "ed": 140, "text": "previously unseen"}, {"st": 154, "ed": 157, "text": "zero shot learning"}, {"st": 208, "ed": 210, "text": "classification performance"}]
[{"st": 1, "ed": 3, "text": "computer vision"}, {"st": 4, "ed": 6, "text": "typically require"}, {"st": 20, "ed": 22, "text": "recent progress"}, {"st": 23, "ed": 25, "text": "computer graphics"}, {"st": 31, "ed": 33, "text": "photo realistic"}, {"st": 42, "ed": 44, "text": "virtual world"}, {"st": 67, "ed": 69, "text": "computer vision"}, {"st": 70, "ed": 72, "text": "virtual worlds"}, {"st": 76, "ed": 78, "text": "ground truth"}, {"st": 79, "ed": 81, "text": "object detection"}, {"st": 84, "ed": 86, "text": "instance segmentation"}, {"st": 99, "ed": 101, "text": "deep learning"}, {"st": 102, "ed": 104, "text": "pre trained"}, {"st": 105, "ed": 107, "text": "real data"}, {"st": 112, "ed": 114, "text": "virtual worlds"}, {"st": 116, "ed": 118, "text": "pre training"}, {"st": 129, "ed": 131, "text": "virtual worlds"}, {"st": 133, "ed": 135, "text": "virtual worlds"}, {"st": 146, "ed": 148, "text": "recognition performance"}, {"st": 163, "ed": 165, "text": "deep models"}]
[{"st": 23, "ed": 25, "text": "non stationary"}, {"st": 35, "ed": 37, "text": "spatial temporal"}, {"st": 66, "ed": 68, "text": "spatial temporal"}, {"st": 72, "ed": 74, "text": "multiple layers"}, {"st": 75, "ed": 77, "text": "spatial temporal"}, {"st": 80, "ed": 82, "text": "spatial temporal"}, {"st": 101, "ed": 103, "text": "learning algorithm"}, {"st": 144, "ed": 146, "text": "learning algorithm"}]
[{"st": 3, "ed": 5, "text": "biological evolution"}, {"st": 11, "ed": 14, "text": "deep neural networks"}, {"st": 20, "ed": 22, "text": "highly efficient"}, {"st": 22, "ed": 25, "text": "deep neural networks"}, {"st": 32, "ed": 34, "text": "highly efficient"}, {"st": 36, "ed": 39, "text": "deep neural networks"}, {"st": 56, "ed": 59, "text": "deep neural networks"}, {"st": 80, "ed": 82, "text": "network architectures"}, {"st": 106, "ed": 108, "text": "natural selection"}, {"st": 133, "ed": 135, "text": "network architectures"}, {"st": 168, "ed": 170, "text": "network architectures"}]
[{"st": 5, "ed": 7, "text": "back propagation"}, {"st": 11, "ed": 13, "text": "generator network"}, {"st": 32, "ed": 34, "text": "latent factors"}, {"st": 47, "ed": 49, "text": "back propagation"}, {"st": 57, "ed": 59, "text": "back propagation"}, {"st": 62, "ed": 64, "text": "latent factors"}, {"st": 65, "ed": 67, "text": "langevin dynamics"}, {"st": 72, "ed": 74, "text": "back propagation"}, {"st": 81, "ed": 83, "text": "latent factors"}, {"st": 95, "ed": 97, "text": "back propagation"}, {"st": 111, "ed": 113, "text": "back propagation"}, {"st": 120, "ed": 122, "text": "natural images"}]
[{"st": 2, "ed": 4, "text": "optimization methods"}, {"st": 6, "ed": 8, "text": "successfully applied"}, {"st": 10, "ed": 12, "text": "hyperparameter optimization"}, {"st": 13, "ed": 16, "text": "deep neural networks"}, {"st": 24, "ed": 26, "text": "joint distribution"}, {"st": 86, "ed": 88, "text": "comparable performance"}, {"st": 97, "ed": 99, "text": "transfer learning"}, {"st": 101, "ed": 103, "text": "proposed method"}, {"st": 106, "ed": 108, "text": "hand designed"}, {"st": 126, "ed": 128, "text": "neural network"}, {"st": 133, "ed": 135, "text": "extensive experiments"}, {"st": 138, "ed": 140, "text": "benchmark datasets"}]
[{"st": 5, "ed": 8, "text": "visual object recognition"}, {"st": 13, "ed": 15, "text": "important role"}, {"st": 16, "ed": 18, "text": "human intelligence"}, {"st": 30, "ed": 32, "text": "higher level"}, {"st": 47, "ed": 49, "text": "deep learning"}, {"st": 51, "ed": 53, "text": "higher level"}, {"st": 55, "ed": 58, "text": "probabilistic graphical models"}, {"st": 85, "ed": 87, "text": "deep learning"}, {"st": 93, "ed": 95, "text": "probabilistic framework"}, {"st": 103, "ed": 105, "text": "unified framework"}, {"st": 112, "ed": 114, "text": "deep learning"}, {"st": 119, "ed": 121, "text": "higher level"}, {"st": 129, "ed": 131, "text": "inference process"}, {"st": 149, "ed": 151, "text": "deep learning"}, {"st": 157, "ed": 159, "text": "recommender systems"}, {"st": 159, "ed": 161, "text": "topic models"}, {"st": 175, "ed": 177, "text": "deep learning"}]
[{"st": 11, "ed": 14, "text": "artificial neural networks"}, {"st": 21, "ed": 23, "text": "input signals"}, {"st": 67, "ed": 69, "text": "computational budget"}]
[{"st": 10, "ed": 12, "text": "deep models"}, {"st": 16, "ed": 18, "text": "encoder decoder"}, {"st": 23, "ed": 26, "text": "deep generative models"}, {"st": 120, "ed": 122, "text": "without compromising"}, {"st": 158, "ed": 160, "text": "spatial features"}, {"st": 177, "ed": 179, "text": "image generation"}]
[{"st": 15, "ed": 17, "text": "prototypical networks"}, {"st": 28, "ed": 30, "text": "prototypical networks"}, {"st": 36, "ed": 38, "text": "embedding vectors"}, {"st": 80, "ed": 82, "text": "distance metric"}, {"st": 84, "ed": 86, "text": "embedding space"}, {"st": 98, "ed": 100, "text": "prototypical networks"}, {"st": 106, "ed": 108, "text": "prototypical networks"}, {"st": 165, "ed": 167, "text": "training set"}, {"st": 178, "ed": 180, "text": "prototypical networks"}, {"st": 192, "ed": 194, "text": "real world"}]
[{"st": 13, "ed": 15, "text": "residual networks"}, {"st": 42, "ed": 44, "text": "deep networks"}, {"st": 58, "ed": 60, "text": "learning rates"}, {"st": 74, "ed": 76, "text": "learning rates"}, {"st": 76, "ed": 78, "text": "improves performance"}, {"st": 103, "ed": 106, "text": "labeled training data"}, {"st": 116, "ed": 118, "text": "free optimization"}]
[{"st": 1, "ed": 4, "text": "taking into account"}, {"st": 17, "ed": 20, "text": "deep neural networks"}, {"st": 26, "ed": 28, "text": "conditional distribution"}, {"st": 40, "ed": 42, "text": "joint probability"}, {"st": 57, "ed": 60, "text": "and vice versa"}, {"st": 83, "ed": 85, "text": "approximate bayesian"}, {"st": 90, "ed": 92, "text": "multi layer"}, {"st": 101, "ed": 103, "text": "probabilistic model"}, {"st": 108, "ed": 110, "text": "hidden units"}, {"st": 141, "ed": 143, "text": "generator network"}]
[{"st": 3, "ed": 5, "text": "neural architecture"}, {"st": 14, "ed": 16, "text": "evolutionary algorithm"}, {"st": 46, "ed": 48, "text": "search space"}, {"st": 67, "ed": 69, "text": "image classification"}, {"st": 76, "ed": 78, "text": "cifar 10"}, {"st": 91, "ed": 93, "text": "neural architecture"}, {"st": 100, "ed": 102, "text": "random search"}, {"st": 109, "ed": 111, "text": "cifar 10"}]
[{"st": 3, "ed": 5, "text": "neural networks"}, {"st": 58, "ed": 60, "text": "generative model"}, {"st": 69, "ed": 73, "text": "conditional generative adversarial networks"}, {"st": 77, "ed": 79, "text": "source domain"}, {"st": 98, "ed": 100, "text": "generative process"}, {"st": 113, "ed": 115, "text": "unseen classes"}, {"st": 123, "ed": 126, "text": "generative adversarial network"}, {"st": 139, "ed": 142, "text": "few shot learning"}]
[{"st": 5, "ed": 8, "text": "deep neural network"}, {"st": 13, "ed": 15, "text": "jigsaw puzzle"}, {"st": 22, "ed": 24, "text": "neural network"}, {"st": 52, "ed": 54, "text": "extremely high"}, {"st": 59, "ed": 61, "text": "feature extraction"}]
[{"st": 17, "ed": 19, "text": "deep convolutional"}, {"st": 23, "ed": 25, "text": "previous approaches"}, {"st": 27, "ed": 29, "text": "image processing"}, {"st": 31, "ed": 33, "text": "feature extraction"}, {"st": 54, "ed": 56, "text": "deep convolutional"}, {"st": 70, "ed": 73, "text": "convolutional neural network"}, {"st": 78, "ed": 80, "text": "proposed approach"}, {"st": 81, "ed": 83, "text": "outperforms previous"}, {"st": 95, "ed": 97, "text": "classification problem"}]
[{"st": 5, "ed": 8, "text": "deep learning based"}, {"st": 22, "ed": 24, "text": "deep convolutional"}, {"st": 24, "ed": 26, "text": "denoising autoencoder"}, {"st": 31, "ed": 33, "text": "compact representation"}, {"st": 41, "ed": 43, "text": "existing methods"}, {"st": 54, "ed": 56, "text": "highly complex"}, {"st": 77, "ed": 79, "text": "gene ontology"}, {"st": 85, "ed": 87, "text": "highly accurate"}, {"st": 93, "ed": 95, "text": "method outperforms"}, {"st": 122, "ed": 124, "text": "input images"}]
[{"st": 6, "ed": 8, "text": "generative models"}, {"st": 10, "ed": 12, "text": "adversarial examples"}, {"st": 16, "ed": 18, "text": "natural images"}, {"st": 23, "ed": 25, "text": "pre trained"}, {"st": 29, "ed": 32, "text": "deep neural networks"}, {"st": 64, "ed": 66, "text": "impressive results"}, {"st": 85, "ed": 87, "text": "extensive experiments"}, {"st": 102, "ed": 104, "text": "achieve high"}]
[{"st": 14, "ed": 16, "text": "neural network"}]
[{"st": 15, "ed": 17, "text": "deep learning"}, {"st": 23, "ed": 25, "text": "network design"}, {"st": 98, "ed": 101, "text": "taking advantage of"}, {"st": 102, "ed": 104, "text": "recurrent network"}, {"st": 106, "ed": 108, "text": "expressive power"}, {"st": 120, "ed": 122, "text": "empirical studies"}, {"st": 128, "ed": 130, "text": "accurate predictions"}]
[{"st": 0, "ed": 4, "text": "convolutional neural networks cnns"}, {"st": 8, "ed": 10, "text": "neural networks"}, {"st": 19, "ed": 21, "text": "hidden layers"}, {"st": 33, "ed": 35, "text": "dot product"}, {"st": 50, "ed": 52, "text": "raw image"}, {"st": 98, "ed": 101, "text": "support vector machine"}, {"st": 104, "ed": 107, "text": "artificial neural network"}, {"st": 144, "ed": 146, "text": "mnist dataset"}, {"st": 179, "ed": 181, "text": "recently published"}, {"st": 182, "ed": 184, "text": "mnist dataset"}, {"st": 197, "ed": 199, "text": "image classification"}]
[{"st": 1, "ed": 3, "text": "neural networks"}, {"st": 8, "ed": 10, "text": "learning systems"}, {"st": 12, "ed": 14, "text": "biological neural"}, {"st": 36, "ed": 39, "text": "artificial neural networks"}, {"st": 79, "ed": 81, "text": "feed forward"}, {"st": 106, "ed": 108, "text": "large scale"}, {"st": 108, "ed": 110, "text": "machine learning"}, {"st": 119, "ed": 121, "text": "learning process"}, {"st": 156, "ed": 159, "text": "speed and accuracy"}, {"st": 171, "ed": 173, "text": "multilayer perceptron"}]
[{"st": 0, "ed": 2, "text": "image segmentation"}, {"st": 51, "ed": 53, "text": "prior information"}, {"st": 74, "ed": 77, "text": "takes into account"}, {"st": 79, "ed": 81, "text": "spatial information"}, {"st": 109, "ed": 111, "text": "weakly supervised"}]
[{"st": 8, "ed": 11, "text": "deep neural networks"}, {"st": 12, "ed": 14, "text": "resource constrained"}, {"st": 30, "ed": 32, "text": "error rates"}, {"st": 45, "ed": 47, "text": "error rates"}, {"st": 48, "ed": 50, "text": "multiple datasets"}, {"st": 51, "ed": 55, "text": "deep convolutional neural networks"}, {"st": 63, "ed": 65, "text": "residual networks"}, {"st": 72, "ed": 74, "text": "existing methods"}, {"st": 80, "ed": 82, "text": "sign function"}, {"st": 106, "ed": 110, "text": "cifar 10 cifar 100"}, {"st": 129, "ed": 131, "text": "error rates"}, {"st": 139, "ed": 141, "text": "top 5"}, {"st": 145, "ed": 147, "text": "mnist svhn"}, {"st": 166, "ed": 168, "text": "error rates"}, {"st": 169, "ed": 171, "text": "previously reported"}, {"st": 179, "ed": 181, "text": "error rates"}, {"st": 196, "ed": 198, "text": "significant improvements"}, {"st": 199, "ed": 201, "text": "error rate"}, {"st": 204, "ed": 206, "text": "batch normalization"}, {"st": 226, "ed": 228, "text": "learning rate"}, {"st": 263, "ed": 265, "text": "training epochs"}, {"st": 266, "ed": 268, "text": "cifar 10"}, {"st": 274, "ed": 276, "text": "trained models"}, {"st": 282, "ed": 284, "text": "https github.com"}]
[{"st": 5, "ed": 8, "text": "rectified linear units"}, {"st": 15, "ed": 18, "text": "deep neural network"}, {"st": 26, "ed": 28, "text": "activation function"}, {"st": 75, "ed": 77, "text": "neural network"}, {"st": 81, "ed": 83, "text": "weight parameters"}]
[{"st": 52, "ed": 54, "text": "alternating minimization"}, {"st": 86, "ed": 88, "text": "unsupervised methods"}, {"st": 91, "ed": 93, "text": "factor analysis"}, {"st": 100, "ed": 102, "text": "sparse coding"}, {"st": 118, "ed": 120, "text": "significantly smaller"}, {"st": 129, "ed": 131, "text": "deep networks"}, {"st": 149, "ed": 151, "text": "pharmaceutical drug"}]
[{"st": 19, "ed": 21, "text": "recently proposed"}, {"st": 50, "ed": 53, "text": "deep network architecture"}, {"st": 108, "ed": 110, "text": "continuous functions"}, {"st": 116, "ed": 118, "text": "expressive power"}, {"st": 133, "ed": 135, "text": "image classification"}, {"st": 144, "ed": 146, "text": "cifar 100"}, {"st": 155, "ed": 157, "text": "image classification"}]
[{"st": 8, "ed": 10, "text": "neural networks"}, {"st": 25, "ed": 27, "text": "pre trained"}, {"st": 29, "ed": 31, "text": "proposed method"}, {"st": 45, "ed": 47, "text": "internal representation"}]
[{"st": 9, "ed": 11, "text": "classification error"}, {"st": 14, "ed": 16, "text": "learning process"}, {"st": 17, "ed": 19, "text": "hidden layers"}, {"st": 29, "ed": 31, "text": "classification performance"}, {"st": 42, "ed": 46, "text": "convolutional neural networks cnn"}, {"st": 67, "ed": 69, "text": "learned features"}, {"st": 95, "ed": 97, "text": "hidden layers"}, {"st": 105, "ed": 107, "text": "output layer"}, {"st": 111, "ed": 113, "text": "layer wise"}, {"st": 113, "ed": 115, "text": "pre training"}, {"st": 139, "ed": 141, "text": "benchmark datasets"}, {"st": 141, "ed": 143, "text": "shows significant"}, {"st": 143, "ed": 145, "text": "performance gain"}, {"st": 146, "ed": 148, "text": "existing methods"}, {"st": 156, "ed": 159, "text": "mnist cifar 10"}, {"st": 159, "ed": 161, "text": "cifar 100"}]
[{"st": 8, "ed": 11, "text": "deep neural networks"}]
[{"st": 14, "ed": 18, "text": "feed forward neural networks"}, {"st": 31, "ed": 34, "text": "deep neural networks"}, {"st": 69, "ed": 72, "text": "deep neural networks"}, {"st": 82, "ed": 84, "text": "mnist dataset"}, {"st": 96, "ed": 98, "text": "learning speed"}]
[{"st": 0, "ed": 2, "text": "autism spectrum"}, {"st": 63, "ed": 65, "text": "real world"}, {"st": 98, "ed": 100, "text": "machine learning"}, {"st": 104, "ed": 106, "text": "handcrafted features"}, {"st": 118, "ed": 120, "text": "deep learning"}, {"st": 138, "ed": 140, "text": "feature learning"}, {"st": 141, "ed": 143, "text": "transfer learning"}, {"st": 146, "ed": 148, "text": "deep architecture"}]
[{"st": 0, "ed": 2, "text": "residual networks"}, {"st": 4, "ed": 6, "text": "recently achieved"}, {"st": 12, "ed": 14, "text": "computer vision"}, {"st": 42, "ed": 44, "text": "improves performance"}, {"st": 54, "ed": 56, "text": "cifar 10"}]
[{"st": 8, "ed": 10, "text": "highly efficient"}, {"st": 10, "ed": 13, "text": "deep neural network"}, {"st": 31, "ed": 33, "text": "biological evolution"}, {"st": 36, "ed": 38, "text": "highly efficient"}, {"st": 38, "ed": 41, "text": "deep neural networks"}, {"st": 54, "ed": 56, "text": "encoding scheme"}, {"st": 87, "ed": 89, "text": "encoding scheme"}, {"st": 111, "ed": 113, "text": "image classification"}, {"st": 125, "ed": 127, "text": "encoding scheme"}, {"st": 136, "ed": 138, "text": "network architectures"}, {"st": 166, "ed": 168, "text": "machine learning"}]
[{"st": 31, "ed": 33, "text": "neural networks"}, {"st": 90, "ed": 92, "text": "generalization performance"}, {"st": 108, "ed": 110, "text": "cifar 100"}]
[{"st": 7, "ed": 10, "text": "deep neural networks"}, {"st": 61, "ed": 63, "text": "deep network"}, {"st": 72, "ed": 74, "text": "correctly classified"}, {"st": 132, "ed": 134, "text": "correctly classified"}, {"st": 179, "ed": 182, "text": "layer by layer"}, {"st": 183, "ed": 185, "text": "binary classification"}, {"st": 193, "ed": 195, "text": "computational cost"}, {"st": 210, "ed": 212, "text": "image recognition"}]
[{"st": 1, "ed": 3, "text": "key idea"}, {"st": 4, "ed": 7, "text": "variational auto encoders"}, {"st": 12, "ed": 14, "text": "auto encoder"}, {"st": 17, "ed": 19, "text": "spatial information"}, {"st": 31, "ed": 33, "text": "latent variables"}, {"st": 43, "ed": 45, "text": "feature maps"}, {"st": 53, "ed": 55, "text": "spatial information"}, {"st": 70, "ed": 72, "text": "latent variables"}, {"st": 73, "ed": 75, "text": "feature maps"}, {"st": 89, "ed": 91, "text": "latent variables"}, {"st": 95, "ed": 97, "text": "matrix variate"}, {"st": 114, "ed": 116, "text": "latent feature"}, {"st": 129, "ed": 131, "text": "low rank"}]
[{"st": 1, "ed": 3, "text": "key idea"}, {"st": 5, "ed": 7, "text": "deep learning"}, {"st": 42, "ed": 44, "text": "network architecture"}, {"st": 75, "ed": 78, "text": "encoder decoder architecture"}, {"st": 91, "ed": 94, "text": "encoder and decoder"}, {"st": 149, "ed": 151, "text": "image segmentation"}]
[{"st": 10, "ed": 13, "text": "artificial neural network"}, {"st": 22, "ed": 24, "text": "linear functions"}, {"st": 28, "ed": 30, "text": "linear unit"}, {"st": 43, "ed": 45, "text": "network size"}, {"st": 87, "ed": 89, "text": "convex optimization"}, {"st": 105, "ed": 107, "text": "weight matrices"}, {"st": 145, "ed": 147, "text": "extensive experiments"}, {"st": 148, "ed": 151, "text": "classification and regression"}]
[{"st": 11, "ed": 15, "text": "generative adversarial networks gans"}, {"st": 34, "ed": 36, "text": "current approaches"}, {"st": 150, "ed": 153, "text": "orders of magnitude"}]
[{"st": 2, "ed": 4, "text": "visual reasoning"}, {"st": 14, "ed": 16, "text": "multi step"}, {"st": 28, "ed": 30, "text": "multi modal"}, {"st": 44, "ed": 46, "text": "deep learning"}, {"st": 58, "ed": 60, "text": "underlying structure"}, {"st": 70, "ed": 72, "text": "hand crafted"}, {"st": 81, "ed": 83, "text": "batch normalization"}, {"st": 83, "ed": 85, "text": "approach achieves"}, {"st": 93, "ed": 95, "text": "visual reasoning"}, {"st": 106, "ed": 109, "text": "end to end"}, {"st": 138, "ed": 140, "text": "multi step"}, {"st": 149, "ed": 151, "text": "visual reasoning"}]
[{"st": 8, "ed": 10, "text": "visual recognition"}, {"st": 19, "ed": 21, "text": "structured prediction"}, {"st": 37, "ed": 39, "text": "visual input"}, {"st": 60, "ed": 62, "text": "object classification"}, {"st": 63, "ed": 65, "text": "visual semantic"}, {"st": 107, "ed": 109, "text": "training set"}, {"st": 111, "ed": 113, "text": "trained model"}, {"st": 132, "ed": 134, "text": "structured prediction"}, {"st": 152, "ed": 154, "text": "performance loss"}, {"st": 157, "ed": 159, "text": "recognition task"}, {"st": 174, "ed": 176, "text": "visual semantic"}]
[{"st": 9, "ed": 11, "text": "real world"}, {"st": 102, "ed": 104, "text": "common sense"}, {"st": 128, "ed": 130, "text": "question answering"}, {"st": 193, "ed": 195, "text": "similar performance"}, {"st": 238, "ed": 240, "text": "word embeddings"}, {"st": 254, "ed": 256, "text": "common sense"}]
[{"st": 8, "ed": 10, "text": "neural networks"}, {"st": 19, "ed": 21, "text": "neural network"}, {"st": 27, "ed": 29, "text": "affine transformation"}, {"st": 39, "ed": 41, "text": "highly effective"}, {"st": 42, "ed": 44, "text": "visual reasoning"}, {"st": 51, "ed": 53, "text": "multi step"}, {"st": 64, "ed": 66, "text": "deep learning"}, {"st": 70, "ed": 72, "text": "explicitly model"}, {"st": 77, "ed": 79, "text": "visual reasoning"}]
[{"st": 100, "ed": 102, "text": "previous approaches"}]
[{"st": 4, "ed": 6, "text": "machine learning"}, {"st": 22, "ed": 24, "text": "word embedding"}, {"st": 29, "ed": 31, "text": "text data"}, {"st": 39, "ed": 41, "text": "machine learning"}, {"st": 42, "ed": 44, "text": "natural language"}, {"st": 50, "ed": 52, "text": "word embeddings"}, {"st": 54, "ed": 56, "text": "google news"}, {"st": 106, "ed": 108, "text": "linearly separable"}, {"st": 181, "ed": 183, "text": "standard benchmarks"}, {"st": 184, "ed": 186, "text": "empirically demonstrate"}, {"st": 189, "ed": 191, "text": "significantly reduce"}]
[{"st": 7, "ed": 11, "text": "recurrent neural network rnn"}, {"st": 70, "ed": 72, "text": "latent topic"}, {"st": 102, "ed": 104, "text": "latent topic"}, {"st": 120, "ed": 122, "text": "unlike previous"}, {"st": 135, "ed": 137, "text": "empirical results"}, {"st": 143, "ed": 145, "text": "outperforms existing"}, {"st": 157, "ed": 159, "text": "feature extractor"}, {"st": 165, "ed": 167, "text": "sentiment analysis"}, {"st": 176, "ed": 178, "text": "error rate"}, {"st": 194, "ed": 196, "text": "semi supervised"}]
[{"st": 4, "ed": 6, "text": "attention model"}, {"st": 7, "ed": 9, "text": "content based"}, {"st": 15, "ed": 17, "text": "attention model"}, {"st": 18, "ed": 20, "text": "neural network"}, {"st": 54, "ed": 56, "text": "latent space"}, {"st": 65, "ed": 67, "text": "attention model"}, {"st": 69, "ed": 71, "text": "scoring function"}, {"st": 90, "ed": 92, "text": "question answering"}, {"st": 101, "ed": 103, "text": "attention model"}, {"st": 136, "ed": 139, "text": "fifa world cup"}]
[{"st": 0, "ed": 4, "text": "recurrent neural networks rnns"}, {"st": 28, "ed": 30, "text": "recurrent units"}, {"st": 36, "ed": 38, "text": "capture complex"}, {"st": 54, "ed": 56, "text": "sequential data"}, {"st": 69, "ed": 71, "text": "recurrent models"}, {"st": 73, "ed": 75, "text": "input features"}, {"st": 108, "ed": 110, "text": "recurrent units"}, {"st": 127, "ed": 129, "text": "prior knowledge"}]
[{"st": 9, "ed": 11, "text": "neural networks"}, {"st": 38, "ed": 40, "text": "neural networks"}, {"st": 70, "ed": 72, "text": "unlabeled data"}, {"st": 114, "ed": 116, "text": "weight updates"}, {"st": 118, "ed": 120, "text": "noisy labels"}]
[{"st": 26, "ed": 28, "text": "structural information"}, {"st": 47, "ed": 49, "text": "automatically learns"}, {"st": 54, "ed": 56, "text": "chemical properties"}, {"st": 61, "ed": 63, "text": "explicit feature"}, {"st": 65, "ed": 67, "text": "bayesian optimization"}, {"st": 71, "ed": 73, "text": "network architecture"}, {"st": 86, "ed": 88, "text": "neural network"}, {"st": 91, "ed": 93, "text": "chemical properties"}, {"st": 105, "ed": 107, "text": "neural networks"}, {"st": 114, "ed": 117, "text": "proof of concept"}, {"st": 167, "ed": 169, "text": "neural networks"}, {"st": 184, "ed": 187, "text": "deep neural networks"}]
[{"st": 2, "ed": 4, "text": "dialogue systems"}, {"st": 8, "ed": 10, "text": "artificial intelligence"}, {"st": 67, "ed": 70, "text": "deep reinforcement learning"}, {"st": 83, "ed": 85, "text": "off policy"}, {"st": 85, "ed": 87, "text": "reinforcement learning"}, {"st": 88, "ed": 90, "text": "experience replay"}, {"st": 97, "ed": 100, "text": "bias and variance"}, {"st": 109, "ed": 111, "text": "previously proposed"}, {"st": 115, "ed": 117, "text": "competitive results"}, {"st": 144, "ed": 146, "text": "dialogue policy"}, {"st": 160, "ed": 162, "text": "deep learning"}, {"st": 174, "ed": 176, "text": "sample efficient"}, {"st": 209, "ed": 212, "text": "orders of magnitude"}, {"st": 222, "ed": 224, "text": "significantly faster"}]
[{"st": 32, "ed": 34, "text": "random vectors"}]
[{"st": 2, "ed": 4, "text": "common sense"}, {"st": 35, "ed": 37, "text": "distributed representations"}, {"st": 44, "ed": 46, "text": "distributed representations"}]
[{"st": 109, "ed": 111, "text": "natural language"}, {"st": 113, "ed": 115, "text": "text data"}, {"st": 160, "ed": 162, "text": "clustering approaches"}, {"st": 166, "ed": 168, "text": "clustering algorithms"}, {"st": 242, "ed": 244, "text": "real world"}, {"st": 258, "ed": 260, "text": "large scale"}]
[{"st": 8, "ed": 10, "text": "speech signals"}, {"st": 22, "ed": 24, "text": "machine learning"}, {"st": 24, "ed": 26, "text": "method called"}, {"st": 26, "ed": 28, "text": "nonparametric bayesian"}, {"st": 43, "ed": 45, "text": "continuous speech"}, {"st": 53, "ed": 55, "text": "generative model"}, {"st": 67, "ed": 69, "text": "generative model"}, {"st": 71, "ed": 74, "text": "hierarchical dirichlet process"}, {"st": 88, "ed": 91, "text": "hierarchical dirichlet process"}, {"st": 103, "ed": 105, "text": "inference procedure"}, {"st": 114, "ed": 116, "text": "gibbs sampler"}, {"st": 136, "ed": 138, "text": "continuous speech"}, {"st": 146, "ed": 148, "text": "inference procedure"}, {"st": 161, "ed": 163, "text": "generative model"}, {"st": 165, "ed": 167, "text": "time series"}, {"st": 171, "ed": 173, "text": "latent variables"}, {"st": 229, "ed": 231, "text": "synthetic data"}, {"st": 234, "ed": 236, "text": "continuous speech"}, {"st": 261, "ed": 264, "text": "automatic speech recognition"}]
[{"st": 1, "ed": 4, "text": "deep neural networks"}, {"st": 31, "ed": 33, "text": "neural networks"}, {"st": 39, "ed": 42, "text": "first order logic"}, {"st": 72, "ed": 74, "text": "sentiment analysis"}, {"st": 89, "ed": 91, "text": "substantial improvements"}, {"st": 98, "ed": 100, "text": "comparable results"}]
[{"st": 15, "ed": 17, "text": "deep generative"}, {"st": 27, "ed": 29, "text": "natural language"}, {"st": 38, "ed": 40, "text": "latent representations"}, {"st": 48, "ed": 50, "text": "generative model"}, {"st": 52, "ed": 55, "text": "variational auto encoders"}, {"st": 80, "ed": 82, "text": "collaborative learning"}, {"st": 104, "ed": 106, "text": "quantitative evaluation"}]
[{"st": 2, "ed": 4, "text": "relation classification"}, {"st": 48, "ed": 50, "text": "neural network"}, {"st": 60, "ed": 62, "text": "salient features"}]
[{"st": 103, "ed": 105, "text": "task specific"}]
[{"st": 0, "ed": 2, "text": "word embeddings"}, {"st": 4, "ed": 7, "text": "representations of words"}, {"st": 38, "ed": 40, "text": "max margin"}, {"st": 64, "ed": 66, "text": "benchmark datasets"}]
[{"st": 9, "ed": 11, "text": "natural language"}, {"st": 13, "ed": 15, "text": "reinforcement learning"}, {"st": 22, "ed": 25, "text": "neural machine translation"}, {"st": 29, "ed": 31, "text": "encoder decoder"}, {"st": 36, "ed": 38, "text": "natural language"}, {"st": 56, "ed": 58, "text": "modified version"}, {"st": 78, "ed": 80, "text": "arcade game"}, {"st": 88, "ed": 90, "text": "evaluation shows"}, {"st": 100, "ed": 102, "text": "learning agent"}]
[{"st": 5, "ed": 7, "text": "deep learning"}, {"st": 8, "ed": 10, "text": "multi task"}, {"st": 11, "ed": 14, "text": "few shot learning"}, {"st": 36, "ed": 38, "text": "deep learning"}, {"st": 60, "ed": 62, "text": "clustering methods"}, {"st": 79, "ed": 81, "text": "clustering algorithms"}, {"st": 96, "ed": 98, "text": "clustering algorithm"}, {"st": 101, "ed": 103, "text": "matrix completion"}, {"st": 105, "ed": 107, "text": "proposed algorithm"}, {"st": 109, "ed": 111, "text": "partially observed"}, {"st": 111, "ed": 113, "text": "similarity matrix"}, {"st": 128, "ed": 130, "text": "matrix completion"}, {"st": 137, "ed": 139, "text": "theoretical analysis"}, {"st": 145, "ed": 147, "text": "proposed algorithm"}, {"st": 153, "ed": 155, "text": "similarity matrix"}, {"st": 166, "ed": 168, "text": "clustering method"}, {"st": 177, "ed": 179, "text": "neural network"}, {"st": 182, "ed": 185, "text": "multi task learning"}, {"st": 187, "ed": 189, "text": "sentiment classification"}, {"st": 196, "ed": 198, "text": "clustering approach"}, {"st": 202, "ed": 205, "text": "few shot learning"}]
[{"st": 2, "ed": 4, "text": "multi task"}, {"st": 46, "ed": 48, "text": "representation space"}, {"st": 57, "ed": 59, "text": "representation space"}]
[{"st": 14, "ed": 16, "text": "sentiment analysis"}, {"st": 17, "ed": 20, "text": "received increasing attention"}, {"st": 26, "ed": 28, "text": "previous works"}, {"st": 30, "ed": 32, "text": "sentiment analysis"}, {"st": 42, "ed": 45, "text": "bag of words"}, {"st": 48, "ed": 50, "text": "facial expression"}, {"st": 55, "ed": 57, "text": "deep architecture"}, {"st": 59, "ed": 61, "text": "sentiment analysis"}, {"st": 112, "ed": 114, "text": "word level"}, {"st": 183, "ed": 185, "text": "sentiment classification"}, {"st": 188, "ed": 190, "text": "qualitative analysis"}, {"st": 241, "ed": 243, "text": "sentiment analysis"}]
[{"st": 14, "ed": 16, "text": "recent research"}, {"st": 34, "ed": 36, "text": "large datasets"}, {"st": 50, "ed": 52, "text": "challenging research"}, {"st": 70, "ed": 72, "text": "computer science"}, {"st": 119, "ed": 122, "text": "local and global"}]
[{"st": 1, "ed": 3, "text": "recent approaches"}, {"st": 29, "ed": 33, "text": "convolutional neural network cnn"}, {"st": 39, "ed": 41, "text": "maximum entropy"}, {"st": 68, "ed": 72, "text": "recurrent neural network rnn"}, {"st": 139, "ed": 141, "text": "previously published"}]
[{"st": 10, "ed": 12, "text": "question answering"}, {"st": 25, "ed": 27, "text": "neural networks"}, {"st": 28, "ed": 30, "text": "visual semantic"}, {"st": 36, "ed": 38, "text": "object detection"}, {"st": 39, "ed": 41, "text": "image segmentation"}, {"st": 58, "ed": 60, "text": "published results"}]
[{"st": 5, "ed": 8, "text": "vision and language"}, {"st": 14, "ed": 16, "text": "challenging research"}, {"st": 53, "ed": 55, "text": "visual information"}, {"st": 74, "ed": 78, "text": "visual question answering vqa"}, {"st": 168, "ed": 171, "text": "visual question answering"}, {"st": 186, "ed": 188, "text": "vqa models"}, {"st": 218, "ed": 220, "text": "empirical evidence"}]
[{"st": 7, "ed": 10, "text": "questions about images"}, {"st": 13, "ed": 15, "text": "recent advances"}, {"st": 16, "ed": 18, "text": "natural language"}, {"st": 65, "ed": 67, "text": "object classes"}]
[{"st": 4, "ed": 6, "text": "image understanding"}, {"st": 13, "ed": 16, "text": "the research community"}, {"st": 18, "ed": 20, "text": "open ended"}, {"st": 46, "ed": 48, "text": "question answering"}, {"st": 72, "ed": 74, "text": "turing test"}]
[{"st": 4, "ed": 6, "text": "deep learning"}, {"st": 15, "ed": 19, "text": "visual question answering vqa"}, {"st": 52, "ed": 55, "text": "strengths and weaknesses"}, {"st": 74, "ed": 76, "text": "vqa models"}, {"st": 105, "ed": 107, "text": "behavior analysis"}, {"st": 110, "ed": 112, "text": "recent progress"}, {"st": 114, "ed": 116, "text": "vqa models"}]
[{"st": 1, "ed": 3, "text": "common sense"}, {"st": 12, "ed": 14, "text": "document summarization"}, {"st": 30, "ed": 32, "text": "image caption"}, {"st": 47, "ed": 49, "text": "output sequence"}, {"st": 66, "ed": 68, "text": "ensemble based"}, {"st": 78, "ed": 80, "text": "text based"}]
[{"st": 7, "ed": 9, "text": "visual representation"}, {"st": 52, "ed": 54, "text": "objective function"}, {"st": 71, "ed": 73, "text": "internal representation"}, {"st": 75, "ed": 77, "text": "deep learning"}, {"st": 83, "ed": 85, "text": "approach achieves"}, {"st": 86, "ed": 88, "text": "significant improvement"}]
[{"st": 0, "ed": 3, "text": "recurrent neural networks"}, {"st": 103, "ed": 105, "text": "ms coco"}, {"st": 111, "ed": 113, "text": "recurrent network"}, {"st": 124, "ed": 126, "text": "structured prediction"}, {"st": 180, "ed": 182, "text": "image captioning"}]
[{"st": 55, "ed": 57, "text": "case study"}, {"st": 63, "ed": 65, "text": "image captioning"}, {"st": 82, "ed": 85, "text": "visual question answering"}]
[{"st": 0, "ed": 3, "text": "deep neural networks"}, {"st": 46, "ed": 50, "text": "visual question answering vqa"}, {"st": 85, "ed": 87, "text": "visualization techniques"}, {"st": 107, "ed": 110, "text": "qualitative and quantitative"}, {"st": 121, "ed": 123, "text": "attention mechanisms"}, {"st": 123, "ed": 125, "text": "vqa models"}]
[{"st": 5, "ed": 7, "text": "visual dialog"}, {"st": 57, "ed": 59, "text": "visual dialog"}, {"st": 107, "ed": 109, "text": "large scale"}, {"st": 109, "ed": 111, "text": "visual dialog"}, {"st": 125, "ed": 128, "text": "question answer pairs"}, {"st": 148, "ed": 150, "text": "encoder decoder"}, {"st": 152, "ed": 154, "text": "visual dialog"}, {"st": 168, "ed": 171, "text": "generative and discriminative"}, {"st": 186, "ed": 188, "text": "visual dialog"}, {"st": 223, "ed": 225, "text": "visual dialog"}, {"st": 242, "ed": 244, "text": "trained models"}]
[{"st": 0, "ed": 3, "text": "multi task learning"}, {"st": 12, "ed": 14, "text": "related tasks"}, {"st": 24, "ed": 26, "text": "audio visual"}, {"st": 26, "ed": 29, "text": "automatic speech recognition"}, {"st": 41, "ed": 43, "text": "audio visual"}, {"st": 63, "ed": 65, "text": "visual features"}, {"st": 97, "ed": 99, "text": "dnn hmm"}, {"st": 111, "ed": 113, "text": "higher level"}, {"st": 121, "ed": 123, "text": "relative improvement"}]
[{"st": 8, "ed": 11, "text": "visual question answering"}, {"st": 31, "ed": 33, "text": "natural language"}, {"st": 49, "ed": 53, "text": "deep reinforcement learning rl"}, {"st": 60, "ed": 63, "text": "end to end"}, {"st": 66, "ed": 68, "text": "multi agent"}, {"st": 152, "ed": 154, "text": "visual dialog"}, {"st": 162, "ed": 164, "text": "large scale"}, {"st": 183, "ed": 185, "text": "fine tuned"}, {"st": 186, "ed": 188, "text": "significantly outperform"}]
[{"st": 0, "ed": 3, "text": "visual question answering"}, {"st": 18, "ed": 20, "text": "turing test"}, {"st": 21, "ed": 23, "text": "artificial intelligence"}, {"st": 53, "ed": 55, "text": "multiple choice"}, {"st": 138, "ed": 140, "text": "visual information"}, {"st": 202, "ed": 204, "text": "extensive empirical"}]
[{"st": 0, "ed": 4, "text": "visual question answering vqa"}, {"st": 19, "ed": 21, "text": "deep learning"}, {"st": 44, "ed": 46, "text": "training data"}, {"st": 52, "ed": 54, "text": "answer questions"}, {"st": 77, "ed": 80, "text": "visual question answering"}, {"st": 83, "ed": 86, "text": "question answer pairs"}, {"st": 139, "ed": 141, "text": "vqa models"}]
[{"st": 12, "ed": 15, "text": "visual question answering"}, {"st": 63, "ed": 65, "text": "deep learning"}]
[{"st": 3, "ed": 6, "text": "simple yet effective"}, {"st": 8, "ed": 10, "text": "neural network"}, {"st": 19, "ed": 21, "text": "back propagation"}, {"st": 71, "ed": 73, "text": "weight matrix"}, {"st": 106, "ed": 108, "text": "back propagation"}, {"st": 136, "ed": 138, "text": "detailed analysis"}, {"st": 143, "ed": 147, "text": "available at https github.com"}]
[{"st": 0, "ed": 2, "text": "adversarial samples"}, {"st": 23, "ed": 25, "text": "adversarial samples"}, {"st": 62, "ed": 64, "text": "prior works"}, {"st": 69, "ed": 71, "text": "adversarial samples"}, {"st": 143, "ed": 145, "text": "adversarial samples"}, {"st": 174, "ed": 176, "text": "sentiment analysis"}]
[{"st": 18, "ed": 20, "text": "word level"}, {"st": 20, "ed": 23, "text": "cross entropy loss"}, {"st": 27, "ed": 29, "text": "policy gradient"}, {"st": 34, "ed": 36, "text": "reinforcement learning"}, {"st": 37, "ed": 39, "text": "directly optimize"}, {"st": 39, "ed": 41, "text": "sentence level"}, {"st": 47, "ed": 49, "text": "significant improvements"}, {"st": 94, "ed": 96, "text": "significant improvements"}]
[{"st": 5, "ed": 8, "text": "end to end"}, {"st": 27, "ed": 29, "text": "natural language"}, {"st": 52, "ed": 54, "text": "recurrent neural"}]
[{"st": 14, "ed": 16, "text": "machine learning"}, {"st": 27, "ed": 29, "text": "adversarial examples"}, {"st": 66, "ed": 68, "text": "semantically meaningful"}, {"st": 89, "ed": 91, "text": "adversarial examples"}, {"st": 105, "ed": 107, "text": "continuous data"}, {"st": 110, "ed": 112, "text": "recent advances"}, {"st": 126, "ed": 128, "text": "proposed approach"}, {"st": 140, "ed": 142, "text": "image classification"}, {"st": 142, "ed": 144, "text": "textual entailment"}]
[{"st": 3, "ed": 6, "text": "simple yet effective"}, {"st": 19, "ed": 21, "text": "back propagation"}, {"st": 71, "ed": 73, "text": "weight matrix"}, {"st": 108, "ed": 110, "text": "computational cost"}, {"st": 121, "ed": 123, "text": "real world"}, {"st": 145, "ed": 147, "text": "back propagation"}, {"st": 164, "ed": 166, "text": "detailed analysis"}]
[{"st": 7, "ed": 9, "text": "question answering"}, {"st": 62, "ed": 64, "text": "challenging task"}, {"st": 77, "ed": 79, "text": "commonsense reasoning"}, {"st": 92, "ed": 95, "text": "end to end"}, {"st": 96, "ed": 98, "text": "reinforcement learning"}]
[{"st": 9, "ed": 13, "text": "visual question answering vqa"}, {"st": 22, "ed": 24, "text": "training data"}, {"st": 56, "ed": 58, "text": "prior distributions"}, {"st": 76, "ed": 79, "text": "visual question answering"}, {"st": 96, "ed": 98, "text": "vqa models"}, {"st": 121, "ed": 124, "text": "visual question answering"}, {"st": 128, "ed": 130, "text": "inductive biases"}, {"st": 135, "ed": 137, "text": "specifically designed"}, {"st": 159, "ed": 161, "text": "visual concepts"}, {"st": 202, "ed": 204, "text": "experiments demonstrate"}, {"st": 206, "ed": 208, "text": "significantly outperforms"}, {"st": 225, "ed": 227, "text": "vqa models"}]
[{"st": 12, "ed": 14, "text": "vision language"}, {"st": 18, "ed": 20, "text": "virtual environment"}, {"st": 43, "ed": 45, "text": "virtual world"}, {"st": 48, "ed": 50, "text": "clip art"}, {"st": 70, "ed": 72, "text": "semantically meaningful"}, {"st": 120, "ed": 123, "text": "amazon mechanical turk"}, {"st": 140, "ed": 142, "text": "attention model"}, {"st": 153, "ed": 155, "text": "attention models"}, {"st": 166, "ed": 168, "text": "conventional approaches"}]
[{"st": 0, "ed": 2, "text": "goal oriented"}, {"st": 9, "ed": 11, "text": "numerous applications"}, {"st": 18, "ed": 20, "text": "deep learning"}, {"st": 21, "ed": 23, "text": "reinforcement learning"}, {"st": 35, "ed": 37, "text": "recurrent neural"}, {"st": 65, "ed": 67, "text": "goal oriented"}, {"st": 79, "ed": 81, "text": "probabilistic model"}, {"st": 119, "ed": 121, "text": "goal oriented"}, {"st": 154, "ed": 157, "text": "deep reinforcement learning"}, {"st": 165, "ed": 167, "text": "goal oriented"}]
[{"st": 5, "ed": 7, "text": "structured prediction"}, {"st": 23, "ed": 25, "text": "structured prediction"}, {"st": 27, "ed": 29, "text": "computer vision"}, {"st": 30, "ed": 32, "text": "natural language"}, {"st": 50, "ed": 52, "text": "computational cost"}, {"st": 74, "ed": 76, "text": "structured learning"}, {"st": 83, "ed": 85, "text": "structured learning"}, {"st": 89, "ed": 91, "text": "theoretical justification"}, {"st": 107, "ed": 109, "text": "structured prediction"}, {"st": 110, "ed": 113, "text": "optical character recognition"}, {"st": 119, "ed": 121, "text": "strong performance"}]
[{"st": 24, "ed": 26, "text": "encoder decoder"}, {"st": 29, "ed": 32, "text": "short term memory"}, {"st": 32, "ed": 35, "text": "recurrent neural networks"}, {"st": 35, "ed": 37, "text": "lstm rnn"}, {"st": 38, "ed": 40, "text": "natural language"}, {"st": 56, "ed": 58, "text": "multi level"}, {"st": 85, "ed": 87, "text": "existing methods"}, {"st": 97, "ed": 99, "text": "task specific"}, {"st": 124, "ed": 126, "text": "competitive results"}]
[{"st": 1, "ed": 3, "text": "neural networks"}, {"st": 11, "ed": 13, "text": "natural language"}, {"st": 26, "ed": 28, "text": "typically requires"}, {"st": 39, "ed": 41, "text": "previous studies"}, {"st": 62, "ed": 65, "text": "end to end"}, {"st": 87, "ed": 90, "text": "difficult to train"}, {"st": 106, "ed": 108, "text": "natural language"}, {"st": 133, "ed": 136, "text": "approach significantly outperforms"}]
[{"st": 29, "ed": 31, "text": "authorship attribution"}, {"st": 38, "ed": 40, "text": "authorship attribution"}, {"st": 48, "ed": 50, "text": "great importance"}, {"st": 64, "ed": 66, "text": "multi agent"}, {"st": 67, "ed": 69, "text": "machine learning"}, {"st": 116, "ed": 118, "text": "neural networks"}, {"st": 131, "ed": 133, "text": "proposed approach"}]
[{"st": 84, "ed": 86, "text": "existing models"}, {"st": 158, "ed": 160, "text": "humanoid robot"}, {"st": 212, "ed": 214, "text": "data set"}]
[{"st": 0, "ed": 5, "text": "deep convolutional neural networks cnns"}, {"st": 9, "ed": 12, "text": "deep neural networks"}, {"st": 37, "ed": 41, "text": "word error rate wer"}, {"st": 74, "ed": 76, "text": "weight sharing"}, {"st": 78, "ed": 80, "text": "weight sharing"}, {"st": 97, "ed": 99, "text": "computer vision"}, {"st": 154, "ed": 156, "text": "relative improvement"}, {"st": 184, "ed": 186, "text": "relative improvement"}]
[{"st": 0, "ed": 2, "text": "collaborative filtering"}, {"st": 15, "ed": 17, "text": "based methods"}, {"st": 48, "ed": 50, "text": "based methods"}, {"st": 67, "ed": 69, "text": "content information"}, {"st": 100, "ed": 102, "text": "latent representation"}, {"st": 123, "ed": 125, "text": "recent advances"}, {"st": 126, "ed": 128, "text": "deep learning"}, {"st": 143, "ed": 146, "text": "hierarchical bayesian model"}, {"st": 148, "ed": 150, "text": "deep learning"}, {"st": 155, "ed": 157, "text": "representation learning"}, {"st": 159, "ed": 161, "text": "content information"}, {"st": 162, "ed": 164, "text": "collaborative filtering"}, {"st": 169, "ed": 171, "text": "extensive experiments"}, {"st": 173, "ed": 175, "text": "real world"}]
[{"st": 0, "ed": 2, "text": "layer wise"}, {"st": 7, "ed": 9, "text": "recently proposed"}, {"st": 16, "ed": 18, "text": "linear classifiers"}, {"st": 34, "ed": 36, "text": "natural language"}, {"st": 50, "ed": 54, "text": "convolutional neural network cnn"}, {"st": 80, "ed": 82, "text": "sensitivity analysis"}, {"st": 83, "ed": 86, "text": "qualitatively and quantitatively"}, {"st": 120, "ed": 122, "text": "image classification"}]
[{"st": 4, "ed": 6, "text": "statistical model"}, {"st": 8, "ed": 10, "text": "machine learning"}, {"st": 25, "ed": 28, "text": "directed acyclic graph"}, {"st": 80, "ed": 82, "text": "geometric analysis"}, {"st": 85, "ed": 87, "text": "moduli space"}]
[{"st": 3, "ed": 5, "text": "statistical model"}, {"st": 6, "ed": 8, "text": "natural language"}, {"st": 38, "ed": 40, "text": "born rule"}]
[{"st": 11, "ed": 13, "text": "optimization technique"}, {"st": 14, "ed": 17, "text": "deep neural network"}, {"st": 87, "ed": 89, "text": "fixed point"}, {"st": 105, "ed": 107, "text": "convergence guarantees"}, {"st": 116, "ed": 118, "text": "sampling algorithm"}]
[{"st": 16, "ed": 18, "text": "visual cues"}, {"st": 34, "ed": 36, "text": "data mining"}, {"st": 37, "ed": 39, "text": "machine learning"}, {"st": 67, "ed": 69, "text": "deep learning"}, {"st": 115, "ed": 117, "text": "proposed framework"}, {"st": 192, "ed": 195, "text": "qualitative and quantitative"}, {"st": 199, "ed": 201, "text": "proposed framework"}]
[{"st": 1, "ed": 4, "text": "principal component analysis"}, {"st": 38, "ed": 40, "text": "cross validation"}, {"st": 81, "ed": 83, "text": "missing data"}, {"st": 107, "ed": 109, "text": "missing values"}]
[{"st": 8, "ed": 10, "text": "similarity matrix"}, {"st": 12, "ed": 14, "text": "spectral clustering"}, {"st": 20, "ed": 22, "text": "pairwise similarities"}, {"st": 37, "ed": 39, "text": "theoretical analysis"}, {"st": 47, "ed": 49, "text": "previous results"}, {"st": 52, "ed": 54, "text": "spectral clustering"}, {"st": 75, "ed": 77, "text": "previous methods"}]
[{"st": 4, "ed": 7, "text": "problem of classifying"}]
[{"st": 10, "ed": 12, "text": "image data"}, {"st": 93, "ed": 96, "text": "principal component analysis"}, {"st": 122, "ed": 124, "text": "irrelevant features"}, {"st": 142, "ed": 144, "text": "irrelevant features"}, {"st": 146, "ed": 148, "text": "clustering algorithms"}, {"st": 198, "ed": 200, "text": "big data"}, {"st": 205, "ed": 207, "text": "randomized algorithm"}, {"st": 209, "ed": 211, "text": "feature selection"}, {"st": 216, "ed": 218, "text": "proposed method"}, {"st": 223, "ed": 225, "text": "kidney cancer"}, {"st": 226, "ed": 228, "text": "image data"}, {"st": 238, "ed": 240, "text": "feature selection"}, {"st": 244, "ed": 247, "text": "principal component analysis"}, {"st": 252, "ed": 254, "text": "clustering algorithms"}]
[{"st": 68, "ed": 70, "text": "jointly learns"}, {"st": 86, "ed": 88, "text": "multi view"}, {"st": 95, "ed": 97, "text": "proposed method"}, {"st": 100, "ed": 102, "text": "generalization error"}, {"st": 132, "ed": 134, "text": "input features"}, {"st": 135, "ed": 137, "text": "class labels"}, {"st": 140, "ed": 143, "text": "compares favorably to"}, {"st": 143, "ed": 145, "text": "existing approaches"}, {"st": 146, "ed": 148, "text": "multi task"}, {"st": 148, "ed": 150, "text": "metric learning"}]
[{"st": 1, "ed": 3, "text": "gaussian process"}, {"st": 3, "ed": 5, "text": "latent variable"}, {"st": 14, "ed": 17, "text": "linear dimensionality reduction"}, {"st": 33, "ed": 35, "text": "maximum likelihood"}, {"st": 64, "ed": 66, "text": "variational inference"}, {"st": 74, "ed": 76, "text": "latent variables"}, {"st": 110, "ed": 112, "text": "dynamical systems"}, {"st": 125, "ed": 127, "text": "variational bayesian"}, {"st": 161, "ed": 163, "text": "gaussian process"}, {"st": 168, "ed": 170, "text": "semi supervised"}, {"st": 177, "ed": 179, "text": "synthetic data"}, {"st": 181, "ed": 183, "text": "machine learning"}, {"st": 188, "ed": 190, "text": "real world"}]
[{"st": 0, "ed": 3, "text": "generative adversarial nets"}, {"st": 5, "ed": 7, "text": "recently introduced"}, {"st": 24, "ed": 27, "text": "generative adversarial nets"}, {"st": 73, "ed": 75, "text": "multi modal"}]
[{"st": 66, "ed": 68, "text": "observational data"}, {"st": 82, "ed": 84, "text": "machine learning"}, {"st": 104, "ed": 106, "text": "active learning"}, {"st": 120, "ed": 122, "text": "automatically identify"}, {"st": 132, "ed": 135, "text": "inference and learning"}]
[{"st": 13, "ed": 15, "text": "network size"}, {"st": 17, "ed": 19, "text": "central role"}, {"st": 22, "ed": 24, "text": "feed forward"}, {"st": 31, "ed": 33, "text": "matrix factorization"}, {"st": 37, "ed": 39, "text": "inductive bias"}]
[{"st": 17, "ed": 19, "text": "training data"}, {"st": 28, "ed": 30, "text": "previously unseen"}, {"st": 35, "ed": 37, "text": "feature learning"}, {"st": 38, "ed": 40, "text": "multi task"}, {"st": 45, "ed": 47, "text": "generalization performance"}, {"st": 48, "ed": 50, "text": "cross domain"}, {"st": 57, "ed": 59, "text": "denoising autoencoder"}, {"st": 127, "ed": 129, "text": "image recognition"}, {"st": 138, "ed": 140, "text": "multiple datasets"}]
[{"st": 2, "ed": 4, "text": "reinforcement learning"}, {"st": 6, "ed": 8, "text": "continuous state"}, {"st": 17, "ed": 19, "text": "key challenge"}, {"st": 40, "ed": 42, "text": "rl agent"}, {"st": 60, "ed": 62, "text": "reinforcement learning"}, {"st": 85, "ed": 87, "text": "low dimensional"}, {"st": 87, "ed": 89, "text": "feature embedding"}, {"st": 94, "ed": 96, "text": "predictive model"}, {"st": 98, "ed": 100, "text": "low dimensional"}, {"st": 102, "ed": 104, "text": "joint learning"}, {"st": 140, "ed": 143, "text": "states and actions"}, {"st": 162, "ed": 165, "text": "end to end"}]
[{"st": 3, "ed": 5, "text": "classification tasks"}, {"st": 12, "ed": 15, "text": "labeled training data"}, {"st": 29, "ed": 31, "text": "closely related"}, {"st": 32, "ed": 34, "text": "domain adaptation"}, {"st": 57, "ed": 59, "text": "domain adaptation"}, {"st": 76, "ed": 78, "text": "representation learning"}, {"st": 85, "ed": 87, "text": "domain adaptation"}, {"st": 138, "ed": 140, "text": "optimization problem"}, {"st": 162, "ed": 164, "text": "cross domain"}, {"st": 164, "ed": 166, "text": "object recognition"}, {"st": 187, "ed": 189, "text": "classification accuracy"}, {"st": 191, "ed": 193, "text": "domain adaptation"}, {"st": 208, "ed": 210, "text": "generalization bound"}]
[{"st": 1, "ed": 3, "text": "rank minimization"}, {"st": 10, "ed": 12, "text": "nuclear norm"}, {"st": 25, "ed": 27, "text": "nuclear norm"}, {"st": 30, "ed": 32, "text": "singular values"}, {"st": 35, "ed": 37, "text": "approximation error"}, {"st": 79, "ed": 81, "text": "subspace clustering"}, {"st": 85, "ed": 87, "text": "minimization problem"}, {"st": 91, "ed": 93, "text": "optimization procedure"}, {"st": 103, "ed": 105, "text": "extensive experiments"}, {"st": 109, "ed": 111, "text": "motion segmentation"}, {"st": 114, "ed": 116, "text": "proposed method"}]
[{"st": 106, "ed": 108, "text": "hyper parameters"}, {"st": 109, "ed": 112, "text": "deep neural networks"}, {"st": 163, "ed": 165, "text": "low level"}, {"st": 175, "ed": 177, "text": "low level"}, {"st": 192, "ed": 194, "text": "deep network"}, {"st": 204, "ed": 206, "text": "real world"}]
[{"st": 7, "ed": 10, "text": "unsupervised domain adaptation"}, {"st": 13, "ed": 15, "text": "deep learning"}, {"st": 32, "ed": 34, "text": "jointly learns"}, {"st": 42, "ed": 44, "text": "supervised classification"}, {"st": 46, "ed": 48, "text": "source data"}, {"st": 100, "ed": 102, "text": "cross domain"}, {"st": 102, "ed": 104, "text": "object recognition"}, {"st": 138, "ed": 140, "text": "source domain"}, {"st": 191, "ed": 193, "text": "domain adaptation"}]
[{"st": 12, "ed": 15, "text": "a long standing"}, {"st": 52, "ed": 54, "text": "latent factor"}, {"st": 56, "ed": 58, "text": "promising results"}, {"st": 123, "ed": 125, "text": "multi view"}, {"st": 125, "ed": 127, "text": "convolutional autoencoder"}]
[{"st": 83, "ed": 85, "text": "regularization parameter"}, {"st": 171, "ed": 173, "text": "100 000"}]
[{"st": 13, "ed": 15, "text": "computational efficiency"}, {"st": 96, "ed": 98, "text": "optimization problem"}, {"st": 109, "ed": 111, "text": "optimization problem"}, {"st": 133, "ed": 135, "text": "optimization problem"}, {"st": 144, "ed": 147, "text": "sheds light on"}]
[{"st": 0, "ed": 3, "text": "zero shot learning"}, {"st": 35, "ed": 37, "text": "zero shot"}, {"st": 37, "ed": 39, "text": "classification problems"}, {"st": 44, "ed": 46, "text": "metric learning"}, {"st": 50, "ed": 52, "text": "objective function"}, {"st": 91, "ed": 93, "text": "significant improvement"}]
[{"st": 20, "ed": 22, "text": "face recognition"}, {"st": 107, "ed": 109, "text": "k means"}, {"st": 185, "ed": 187, "text": "face verification"}, {"st": 191, "ed": 193, "text": "comparable performance"}, {"st": 199, "ed": 201, "text": "deep features"}, {"st": 205, "ed": 207, "text": "vision tasks"}]
[{"st": 15, "ed": 17, "text": "real data"}, {"st": 25, "ed": 27, "text": "supervised learning"}, {"st": 36, "ed": 38, "text": "machine learning"}, {"st": 133, "ed": 135, "text": "activity recognition"}, {"st": 142, "ed": 144, "text": "proposed method"}, {"st": 152, "ed": 154, "text": "class classification"}]
[{"st": 4, "ed": 6, "text": "generalization error"}, {"st": 18, "ed": 20, "text": "classification task"}, {"st": 48, "ed": 50, "text": "input space"}, {"st": 67, "ed": 69, "text": "generalization error"}, {"st": 81, "ed": 83, "text": "input space"}, {"st": 84, "ed": 86, "text": "generalization error"}, {"st": 105, "ed": 107, "text": "sufficient conditions"}, {"st": 164, "ed": 168, "text": "mnist and cifar 10"}]
[{"st": 6, "ed": 9, "text": "deep neural network"}, {"st": 26, "ed": 28, "text": "natural images"}, {"st": 50, "ed": 53, "text": "deep neural networks"}, {"st": 100, "ed": 102, "text": "decision boundary"}, {"st": 118, "ed": 120, "text": "input space"}]
[{"st": 1, "ed": 3, "text": "annotated data"}, {"st": 8, "ed": 10, "text": "facial expression"}, {"st": 17, "ed": 19, "text": "deep networks"}, {"st": 27, "ed": 29, "text": "linear model"}, {"st": 49, "ed": 51, "text": "linear model"}, {"st": 75, "ed": 77, "text": "training data"}, {"st": 94, "ed": 96, "text": "linear models"}, {"st": 98, "ed": 100, "text": "sparse representation"}, {"st": 103, "ed": 105, "text": "previous attempts"}, {"st": 119, "ed": 121, "text": "low rank"}, {"st": 135, "ed": 137, "text": "sparse representation"}, {"st": 142, "ed": 144, "text": "group sparsity"}, {"st": 192, "ed": 194, "text": "challenging task"}, {"st": 195, "ed": 197, "text": "facial action"}]
[{"st": 0, "ed": 2, "text": "machine learning"}, {"st": 3, "ed": 5, "text": "deep learning"}, {"st": 21, "ed": 23, "text": "adversarial perturbations"}, {"st": 49, "ed": 52, "text": "deep neural networks"}, {"st": 62, "ed": 64, "text": "binary classification"}, {"st": 83, "ed": 85, "text": "adversarial perturbations"}, {"st": 101, "ed": 103, "text": "adversarial perturbations"}, {"st": 141, "ed": 143, "text": "adversarial attack"}, {"st": 154, "ed": 156, "text": "training procedure"}]
[{"st": 0, "ed": 2, "text": "class labels"}, {"st": 13, "ed": 16, "text": "generative adversarial nets"}, {"st": 36, "ed": 38, "text": "class label"}, {"st": 44, "ed": 46, "text": "cross entropy"}, {"st": 50, "ed": 52, "text": "class labels"}, {"st": 66, "ed": 69, "text": "generative adversarial networks"}, {"st": 96, "ed": 98, "text": "strong baselines"}, {"st": 104, "ed": 106, "text": "inception score"}, {"st": 120, "ed": 122, "text": "inception score"}, {"st": 168, "ed": 170, "text": "baseline methods"}]
[{"st": 1, "ed": 3, "text": "machine learning"}, {"st": 19, "ed": 21, "text": "autonomous driving"}, {"st": 36, "ed": 38, "text": "recent years"}, {"st": 50, "ed": 52, "text": "highly complex"}, {"st": 52, "ed": 54, "text": "neural networks"}, {"st": 81, "ed": 83, "text": "main contributions"}, {"st": 118, "ed": 120, "text": "unlike previous"}]
[{"st": 2, "ed": 4, "text": "deep learning"}, {"st": 7, "ed": 9, "text": "machine learning"}, {"st": 16, "ed": 18, "text": "machine learning"}, {"st": 30, "ed": 32, "text": "prior knowledge"}, {"st": 43, "ed": 45, "text": "transfer function"}, {"st": 46, "ed": 48, "text": "cost function"}, {"st": 61, "ed": 63, "text": "theoretical framework"}, {"st": 69, "ed": 71, "text": "prior knowledge"}, {"st": 106, "ed": 108, "text": "transfer function"}, {"st": 109, "ed": 111, "text": "cost function"}, {"st": 141, "ed": 143, "text": "monte carlo"}, {"st": 147, "ed": 149, "text": "input output"}, {"st": 154, "ed": 156, "text": "input output"}, {"st": 173, "ed": 175, "text": "pattern recognition"}, {"st": 185, "ed": 187, "text": "learning machines"}]
[{"st": 6, "ed": 10, "text": "convolutional neural networks cnns"}, {"st": 11, "ed": 13, "text": "low dimensional"}, {"st": 30, "ed": 32, "text": "random walk"}, {"st": 63, "ed": 66, "text": "efficient and scalable"}, {"st": 88, "ed": 90, "text": "classification problems"}, {"st": 97, "ed": 99, "text": "empirically demonstrate"}]
[{"st": 27, "ed": 29, "text": "correctly classified"}, {"st": 78, "ed": 80, "text": "instance specific"}, {"st": 115, "ed": 117, "text": "kernel methods"}, {"st": 118, "ed": 120, "text": "neural networks"}]
[{"st": 9, "ed": 11, "text": "geometric properties"}, {"st": 12, "ed": 15, "text": "deep neural network"}, {"st": 30, "ed": 32, "text": "deep networks"}, {"st": 60, "ed": 62, "text": "decision boundary"}, {"st": 84, "ed": 86, "text": "deep networks"}, {"st": 105, "ed": 107, "text": "decision boundary"}, {"st": 134, "ed": 136, "text": "decision boundary"}, {"st": 167, "ed": 169, "text": "adversarial perturbations"}]
[{"st": 0, "ed": 2, "text": "deep networks"}, {"st": 22, "ed": 24, "text": "natural images"}, {"st": 37, "ed": 39, "text": "quantitative analysis"}, {"st": 68, "ed": 70, "text": "theoretical bounds"}, {"st": 77, "ed": 79, "text": "decision boundary"}, {"st": 93, "ed": 95, "text": "deep networks"}, {"st": 114, "ed": 116, "text": "decision boundary"}, {"st": 117, "ed": 119, "text": "deep networks"}]
[{"st": 0, "ed": 4, "text": "generative adversarial networks gans"}, {"st": 30, "ed": 34, "text": "unsupervised and semi supervised"}, {"st": 45, "ed": 47, "text": "monte carlo"}, {"st": 93, "ed": 95, "text": "mode collapse"}, {"st": 107, "ed": 109, "text": "quantitative results"}, {"st": 110, "ed": 113, "text": "semi supervised learning"}, {"st": 119, "ed": 121, "text": "cifar 10"}, {"st": 123, "ed": 125, "text": "wasserstein gans"}]
[{"st": 9, "ed": 11, "text": "image representations"}, {"st": 17, "ed": 19, "text": "temporal coherence"}, {"st": 24, "ed": 26, "text": "adversarial loss"}, {"st": 78, "ed": 81, "text": "synthetic and real"}]
[{"st": 0, "ed": 3, "text": "generative adversarial nets"}, {"st": 20, "ed": 22, "text": "gan training"}, {"st": 64, "ed": 66, "text": "saddle point"}, {"st": 128, "ed": 130, "text": "gan training"}]
[{"st": 2, "ed": 4, "text": "iterative reconstruction"}, {"st": 7, "ed": 9, "text": "low dose"}, {"st": 9, "ed": 11, "text": "x ray"}, {"st": 20, "ed": 22, "text": "recently proposed"}, {"st": 25, "ed": 30, "text": "deep convolutional neural network cnn"}, {"st": 31, "ed": 33, "text": "low dose"}, {"st": 33, "ed": 35, "text": "x ray"}, {"st": 44, "ed": 46, "text": "low dose"}, {"st": 67, "ed": 69, "text": "deep residual"}, {"st": 76, "ed": 78, "text": "proposed method"}, {"st": 85, "ed": 89, "text": "deep convolutional neural network"}, {"st": 116, "ed": 118, "text": "deep convolutional"}, {"st": 132, "ed": 135, "text": "extensive experimental results"}, {"st": 141, "ed": 143, "text": "significantly improved"}]
[{"st": 4, "ed": 6, "text": "applications including"}, {"st": 31, "ed": 33, "text": "human perception"}, {"st": 34, "ed": 36, "text": "3d shapes"}, {"st": 66, "ed": 69, "text": "recurrent neural network"}, {"st": 81, "ed": 83, "text": "generative model"}, {"st": 118, "ed": 120, "text": "large scale"}, {"st": 145, "ed": 147, "text": "nearest neighbor"}, {"st": 158, "ed": 160, "text": "generative models"}, {"st": 163, "ed": 165, "text": "significantly reduced"}]
[{"st": 11, "ed": 13, "text": "inception score"}, {"st": 33, "ed": 36, "text": "generative adversarial nets"}, {"st": 44, "ed": 46, "text": "detailed analysis"}, {"st": 47, "ed": 49, "text": "inception score"}]
[{"st": 0, "ed": 3, "text": "deep reinforcement learning"}, {"st": 17, "ed": 19, "text": "autonomous systems"}, {"st": 21, "ed": 23, "text": "higher level"}, {"st": 29, "ed": 31, "text": "deep learning"}, {"st": 33, "ed": 35, "text": "reinforcement learning"}, {"st": 48, "ed": 50, "text": "video games"}, {"st": 53, "ed": 56, "text": "deep reinforcement learning"}, {"st": 63, "ed": 65, "text": "control policies"}, {"st": 91, "ed": 93, "text": "reinforcement learning"}, {"st": 113, "ed": 116, "text": "deep reinforcement learning"}, {"st": 118, "ed": 121, "text": "deep q network"}, {"st": 121, "ed": 123, "text": "trust region"}, {"st": 138, "ed": 141, "text": "deep neural networks"}]
[{"st": 0, "ed": 2, "text": "large scale"}, {"st": 2, "ed": 5, "text": "deep neural networks"}, {"st": 48, "ed": 50, "text": "weight pruning"}, {"st": 62, "ed": 64, "text": "network structure"}, {"st": 79, "ed": 81, "text": "compression ratio"}, {"st": 93, "ed": 95, "text": "principled approach"}, {"st": 100, "ed": 102, "text": "neural networks"}, {"st": 109, "ed": 112, "text": "fast fourier transform"}, {"st": 119, "ed": 121, "text": "computational complexity"}, {"st": 175, "ed": 177, "text": "inference engine"}]
[{"st": 4, "ed": 6, "text": "deep learning"}, {"st": 10, "ed": 12, "text": "cross modal"}, {"st": 16, "ed": 18, "text": "feature extractors"}, {"st": 48, "ed": 50, "text": "visual data"}, {"st": 66, "ed": 68, "text": "deep learning"}, {"st": 94, "ed": 96, "text": "previously proposed"}, {"st": 109, "ed": 111, "text": "cross modal"}]
[{"st": 0, "ed": 2, "text": "low dimensional"}, {"st": 38, "ed": 40, "text": "multi dimensional"}, {"st": 47, "ed": 49, "text": "existing models"}]
[{"st": 4, "ed": 6, "text": "deep learning"}, {"st": 8, "ed": 10, "text": "computer vision"}, {"st": 17, "ed": 19, "text": "image recognition"}, {"st": 27, "ed": 29, "text": "representation learning"}, {"st": 37, "ed": 39, "text": "recent advances"}, {"st": 57, "ed": 59, "text": "chemical properties"}, {"st": 77, "ed": 79, "text": "domain specific"}, {"st": 190, "ed": 192, "text": "deep learning"}, {"st": 194, "ed": 196, "text": "accurately predict"}]
[{"st": 0, "ed": 2, "text": "disentangled representations"}, {"st": 4, "ed": 6, "text": "higher level"}, {"st": 23, "ed": 25, "text": "invariant representations"}, {"st": 36, "ed": 38, "text": "unsupervised learning"}, {"st": 39, "ed": 41, "text": "disentangled representations"}, {"st": 50, "ed": 52, "text": "variational inference"}, {"st": 52, "ed": 54, "text": "based approach"}, {"st": 68, "ed": 70, "text": "approximate posterior"}, {"st": 71, "ed": 73, "text": "observed data"}, {"st": 80, "ed": 82, "text": "proposed approach"}, {"st": 89, "ed": 91, "text": "significant gains"}, {"st": 92, "ed": 94, "text": "existing methods"}]
[{"st": 8, "ed": 11, "text": "stochastic gradient descent"}, {"st": 18, "ed": 21, "text": "stochastic differential equation"}, {"st": 49, "ed": 51, "text": "learning rate"}, {"st": 51, "ed": 53, "text": "batch size"}, {"st": 66, "ed": 69, "text": "depth and width"}, {"st": 84, "ed": 86, "text": "learning rate"}, {"st": 95, "ed": 97, "text": "learning rate"}, {"st": 98, "ed": 100, "text": "batch size"}, {"st": 123, "ed": 125, "text": "learning rate"}, {"st": 126, "ed": 128, "text": "batch size"}, {"st": 141, "ed": 143, "text": "learning rate"}, {"st": 144, "ed": 146, "text": "batch size"}, {"st": 155, "ed": 157, "text": "learning rate"}, {"st": 158, "ed": 160, "text": "batch size"}, {"st": 193, "ed": 195, "text": "batch size"}, {"st": 196, "ed": 198, "text": "learning rate"}, {"st": 232, "ed": 234, "text": "learning rate"}, {"st": 235, "ed": 237, "text": "batch size"}, {"st": 245, "ed": 247, "text": "batch size"}, {"st": 248, "ed": 250, "text": "learning rate"}, {"st": 252, "ed": 254, "text": "learning rate"}, {"st": 265, "ed": 267, "text": "noise levels"}, {"st": 282, "ed": 284, "text": "learning rate"}, {"st": 285, "ed": 287, "text": "batch size"}, {"st": 291, "ed": 293, "text": "learning rate"}, {"st": 298, "ed": 300, "text": "batch size"}]
[{"st": 10, "ed": 12, "text": "convolutional neural"}, {"st": 114, "ed": 116, "text": "prediction accuracy"}, {"st": 120, "ed": 122, "text": "empirical evaluation"}, {"st": 135, "ed": 137, "text": "prediction accuracy"}, {"st": 162, "ed": 164, "text": "open source"}]
[{"st": 1, "ed": 4, "text": "artificial intelligence ai"}, {"st": 14, "ed": 16, "text": "deep learning"}, {"st": 39, "ed": 41, "text": "deep learning"}, {"st": 43, "ed": 45, "text": "inverse problems"}, {"st": 46, "ed": 48, "text": "deep network"}, {"st": 95, "ed": 98, "text": "deep neural network"}]
[{"st": 3, "ed": 5, "text": "large datasets"}, {"st": 5, "ed": 8, "text": "deep neural networks"}, {"st": 17, "ed": 19, "text": "speech recognition"}, {"st": 38, "ed": 40, "text": "rule based"}, {"st": 48, "ed": 51, "text": "deep neural network"}, {"st": 60, "ed": 62, "text": "supervised manner"}, {"st": 70, "ed": 72, "text": "transfer learning"}, {"st": 79, "ed": 81, "text": "chemical properties"}, {"st": 111, "ed": 113, "text": "pre training"}, {"st": 130, "ed": 132, "text": "network architecture"}, {"st": 144, "ed": 146, "text": "pre trained"}, {"st": 150, "ed": 152, "text": "domain knowledge"}, {"st": 157, "ed": 159, "text": "neural networks"}, {"st": 161, "ed": 163, "text": "accurate prediction"}]
[{"st": 32, "ed": 34, "text": "scan line"}, {"st": 75, "ed": 77, "text": "compressed sensing"}, {"st": 101, "ed": 103, "text": "computationally expensive"}, {"st": 113, "ed": 115, "text": "deep learning"}, {"st": 134, "ed": 136, "text": "network design"}, {"st": 144, "ed": 147, "text": "deep neural network"}, {"st": 163, "ed": 166, "text": "extensive experimental results"}, {"st": 179, "ed": 181, "text": "proposed method"}, {"st": 187, "ed": 189, "text": "without sacrificing"}]
[{"st": 18, "ed": 20, "text": "x ray"}, {"st": 64, "ed": 66, "text": "deep learning"}, {"st": 68, "ed": 70, "text": "low dose"}, {"st": 78, "ed": 80, "text": "deep learning"}, {"st": 96, "ed": 98, "text": "proposed method"}, {"st": 111, "ed": 113, "text": "existing methods"}, {"st": 116, "ed": 118, "text": "significantly reduced"}]
[{"st": 7, "ed": 9, "text": "x ray"}, {"st": 34, "ed": 36, "text": "computed tomography"}, {"st": 52, "ed": 54, "text": "x ray"}, {"st": 78, "ed": 80, "text": "deep learning"}, {"st": 95, "ed": 97, "text": "deep learning"}, {"st": 113, "ed": 115, "text": "real data"}]
[{"st": 0, "ed": 2, "text": "deep learning"}, {"st": 3, "ed": 6, "text": "shown promising results"}, {"st": 8, "ed": 10, "text": "machine learning"}, {"st": 37, "ed": 39, "text": "deep model"}, {"st": 101, "ed": 103, "text": "building blocks"}, {"st": 104, "ed": 108, "text": "convolutional neural networks cnn"}, {"st": 152, "ed": 154, "text": "search space"}, {"st": 170, "ed": 172, "text": "building blocks"}, {"st": 196, "ed": 198, "text": "image datasets"}, {"st": 227, "ed": 229, "text": "learned models"}]
[{"st": 1, "ed": 3, "text": "metric learning"}, {"st": 8, "ed": 10, "text": "highly effective"}, {"st": 38, "ed": 40, "text": "variational autoencoder"}, {"st": 46, "ed": 48, "text": "approximate inference"}, {"st": 73, "ed": 75, "text": "representation learning"}, {"st": 96, "ed": 98, "text": "latent embedding"}, {"st": 124, "ed": 127, "text": "evidence lower bound"}, {"st": 137, "ed": 139, "text": "variational autoencoder"}, {"st": 145, "ed": 147, "text": "fine grained"}]
[{"st": 32, "ed": 34, "text": "neural network"}, {"st": 63, "ed": 65, "text": "world model"}, {"st": 98, "ed": 100, "text": "world model"}, {"st": 151, "ed": 153, "text": "world model"}, {"st": 155, "ed": 157, "text": "agent learns"}, {"st": 158, "ed": 160, "text": "improved performance"}]
[{"st": 33, "ed": 35, "text": "neural network"}, {"st": 62, "ed": 64, "text": "agent learns"}, {"st": 65, "ed": 67, "text": "world model"}, {"st": 76, "ed": 78, "text": "agent learns"}, {"st": 86, "ed": 88, "text": "world model"}, {"st": 128, "ed": 130, "text": "world model"}, {"st": 132, "ed": 134, "text": "agent learns"}, {"st": 135, "ed": 137, "text": "improved performance"}, {"st": 152, "ed": 154, "text": "computational models"}, {"st": 155, "ed": 157, "text": "intrinsic motivation"}]
[{"st": 4, "ed": 6, "text": "accurately predict"}, {"st": 11, "ed": 13, "text": "existing approaches"}, {"st": 76, "ed": 80, "text": "trained end to end"}, {"st": 99, "ed": 101, "text": "compare favorably"}]
[{"st": 1, "ed": 3, "text": "object detection"}, {"st": 16, "ed": 18, "text": "image level"}, {"st": 24, "ed": 26, "text": "weakly supervised"}, {"st": 36, "ed": 38, "text": "weakly supervised"}, {"st": 41, "ed": 43, "text": "significantly lower"}, {"st": 45, "ed": 47, "text": "fully supervised"}, {"st": 55, "ed": 57, "text": "weakly supervised"}, {"st": 61, "ed": 63, "text": "multi label"}, {"st": 63, "ed": 65, "text": "object recognition"}, {"st": 76, "ed": 78, "text": "object localization"}, {"st": 93, "ed": 95, "text": "task specific"}, {"st": 95, "ed": 97, "text": "deep networks"}, {"st": 99, "ed": 101, "text": "fully supervised"}, {"st": 110, "ed": 112, "text": "object localization"}, {"st": 119, "ed": 121, "text": "object instances"}, {"st": 128, "ed": 130, "text": "task specific"}, {"st": 135, "ed": 137, "text": "object instances"}, {"st": 151, "ed": 153, "text": "object instances"}, {"st": 164, "ed": 166, "text": "metric learning"}, {"st": 167, "ed": 169, "text": "density based"}, {"st": 179, "ed": 181, "text": "weakly supervised"}, {"st": 189, "ed": 191, "text": "multi label"}, {"st": 191, "ed": 193, "text": "image classification"}, {"st": 196, "ed": 198, "text": "weakly supervised"}, {"st": 198, "ed": 200, "text": "object detection"}, {"st": 202, "ed": 204, "text": "competitive results"}, {"st": 205, "ed": 207, "text": "weakly supervised"}, {"st": 210, "ed": 212, "text": "ms coco"}, {"st": 212, "ed": 215, "text": "pascal voc 2007"}]
[{"st": 2, "ed": 4, "text": "recent literature"}, {"st": 5, "ed": 7, "text": "important role"}, {"st": 10, "ed": 12, "text": "deep learning"}, {"st": 43, "ed": 45, "text": "neural network"}, {"st": 55, "ed": 57, "text": "activation functions"}, {"st": 60, "ed": 62, "text": "neural networks"}, {"st": 72, "ed": 74, "text": "hidden units"}, {"st": 108, "ed": 110, "text": "neural networks"}]
[{"st": 11, "ed": 14, "text": "convolutional neural networks"}, {"st": 24, "ed": 26, "text": "sensitivity analysis"}, {"st": 29, "ed": 31, "text": "image segmentation"}, {"st": 67, "ed": 69, "text": "sensitivity analysis"}, {"st": 69, "ed": 71, "text": "based approach"}, {"st": 76, "ed": 78, "text": "cerebral cortex"}]
[{"st": 0, "ed": 3, "text": "deep neural networks"}, {"st": 9, "ed": 11, "text": "loss function"}, {"st": 20, "ed": 22, "text": "learning rate"}, {"st": 42, "ed": 44, "text": "learning rate"}, {"st": 101, "ed": 103, "text": "residual networks"}, {"st": 110, "ed": 114, "text": "cifar 10 cifar 100"}, {"st": 121, "ed": 124, "text": "easy to implement"}]
[{"st": 7, "ed": 10, "text": "artificial neural networks"}, {"st": 14, "ed": 16, "text": "sparse representations"}, {"st": 25, "ed": 27, "text": "neural networks"}, {"st": 31, "ed": 33, "text": "feature extraction"}, {"st": 35, "ed": 37, "text": "proposed method"}, {"st": 48, "ed": 50, "text": "output space"}, {"st": 51, "ed": 53, "text": "pairwise distances"}, {"st": 100, "ed": 102, "text": "proposed method"}, {"st": 107, "ed": 109, "text": "objective function"}, {"st": 116, "ed": 118, "text": "output space"}, {"st": 130, "ed": 132, "text": "pair wise"}, {"st": 152, "ed": 154, "text": "pair wise"}, {"st": 166, "ed": 168, "text": "weight decay"}, {"st": 176, "ed": 178, "text": "objective function"}, {"st": 189, "ed": 191, "text": "chain rule"}, {"st": 239, "ed": 241, "text": "back propagation"}, {"st": 253, "ed": 255, "text": "standard backpropagation"}, {"st": 292, "ed": 294, "text": "machine learning"}]
[{"st": 14, "ed": 16, "text": "probability distributions"}, {"st": 28, "ed": 32, "text": "maximum mean discrepancy mmd"}, {"st": 42, "ed": 44, "text": "unsupervised learning"}, {"st": 45, "ed": 48, "text": "generative adversarial networks"}, {"st": 55, "ed": 57, "text": "generate realistic"}, {"st": 107, "ed": 109, "text": "generative model"}]
[{"st": 1, "ed": 3, "text": "real world"}, {"st": 4, "ed": 6, "text": "control problems"}, {"st": 9, "ed": 12, "text": "curse of dimensionality"}, {"st": 20, "ed": 22, "text": "deep learning"}, {"st": 29, "ed": 31, "text": "control problems"}, {"st": 33, "ed": 35, "text": "monte carlo"}, {"st": 43, "ed": 46, "text": "feedforward neural networks"}, {"st": 55, "ed": 57, "text": "objective function"}, {"st": 59, "ed": 61, "text": "control problem"}, {"st": 66, "ed": 68, "text": "loss function"}]
[{"st": 3, "ed": 5, "text": "drug design"}, {"st": 27, "ed": 30, "text": "recurrent neural networks"}, {"st": 34, "ed": 36, "text": "generative models"}, {"st": 45, "ed": 47, "text": "natural language"}, {"st": 82, "ed": 84, "text": "biological target"}, {"st": 87, "ed": 89, "text": "fine tune"}, {"st": 138, "ed": 140, "text": "scoring function"}, {"st": 148, "ed": 150, "text": "drug design"}]
[{"st": 0, "ed": 4, "text": "deep reinforcement learning rl"}]
[{"st": 4, "ed": 6, "text": "neural networks"}, {"st": 7, "ed": 9, "text": "machine learning"}, {"st": 14, "ed": 16, "text": "critical applications"}, {"st": 26, "ed": 28, "text": "neural networks"}, {"st": 53, "ed": 55, "text": "neural network"}, {"st": 77, "ed": 79, "text": "neural networks"}, {"st": 125, "ed": 127, "text": "expected error"}, {"st": 142, "ed": 144, "text": "network parameters"}, {"st": 168, "ed": 170, "text": "activation function"}, {"st": 188, "ed": 190, "text": "theoretical results"}, {"st": 204, "ed": 206, "text": "network parameters"}, {"st": 215, "ed": 217, "text": "neural networks"}, {"st": 239, "ed": 241, "text": "training set"}]
[{"st": 8, "ed": 10, "text": "probabilistic programming"}, {"st": 13, "ed": 15, "text": "deep learning"}, {"st": 31, "ed": 33, "text": "unlike existing"}, {"st": 33, "ed": 35, "text": "deep learning"}, {"st": 42, "ed": 44, "text": "neural networks"}, {"st": 45, "ed": 47, "text": "supervised tasks"}, {"st": 62, "ed": 64, "text": "probabilistic models"}, {"st": 68, "ed": 70, "text": "hierarchical bayesian"}, {"st": 83, "ed": 85, "text": "probabilistic programming"}, {"st": 89, "ed": 91, "text": "logistic regression"}, {"st": 91, "ed": 94, "text": "variational auto encoders"}, {"st": 96, "ed": 98, "text": "belief networks"}]
[{"st": 3, "ed": 5, "text": "efficient algorithms"}, {"st": 6, "ed": 8, "text": "reinforcement learning"}, {"st": 13, "ed": 15, "text": "policy search"}, {"st": 47, "ed": 49, "text": "recently introduced"}, {"st": 56, "ed": 58, "text": "optimization algorithm"}, {"st": 78, "ed": 80, "text": "policy search"}, {"st": 181, "ed": 183, "text": "policy search"}]
[{"st": 11, "ed": 13, "text": "policy search"}, {"st": 24, "ed": 26, "text": "previous tasks"}, {"st": 67, "ed": 69, "text": "acquisition function"}, {"st": 90, "ed": 92, "text": "acquisition function"}, {"st": 94, "ed": 96, "text": "transfer learning"}]
[{"st": 6, "ed": 8, "text": "representational power"}, {"st": 9, "ed": 12, "text": "deep neural networks"}, {"st": 66, "ed": 68, "text": "linear functions"}, {"st": 136, "ed": 138, "text": "mixed integer"}, {"st": 143, "ed": 145, "text": "input space"}]
[{"st": 10, "ed": 12, "text": "deep networks"}, {"st": 26, "ed": 28, "text": "deep learning"}, {"st": 42, "ed": 44, "text": "neural network"}, {"st": 75, "ed": 77, "text": "supervised training"}, {"st": 92, "ed": 94, "text": "total number"}, {"st": 114, "ed": 117, "text": "recurrent neural networks"}, {"st": 118, "ed": 120, "text": "standard benchmark"}, {"st": 135, "ed": 137, "text": "theoretical foundation"}]
[{"st": 10, "ed": 13, "text": "artificial neural network"}, {"st": 33, "ed": 35, "text": "activation functions"}, {"st": 52, "ed": 54, "text": "activation functions"}, {"st": 60, "ed": 62, "text": "cosine similarity"}, {"st": 90, "ed": 92, "text": "user interface"}]
[{"st": 24, "ed": 26, "text": "active learning"}, {"st": 83, "ed": 85, "text": "low level"}, {"st": 153, "ed": 155, "text": "inverse kinematics"}, {"st": 159, "ed": 161, "text": "robotic arm"}, {"st": 230, "ed": 232, "text": "increasing complexity"}]
[{"st": 7, "ed": 10, "text": "end to end"}, {"st": 24, "ed": 26, "text": "partially observable"}, {"st": 26, "ed": 28, "text": "real world"}, {"st": 33, "ed": 36, "text": "recurrent neural network"}, {"st": 75, "ed": 77, "text": "recently proposed"}, {"st": 77, "ed": 79, "text": "theoretical framework"}, {"st": 80, "ed": 83, "text": "end to end"}, {"st": 105, "ed": 107, "text": "internal representation"}, {"st": 143, "ed": 145, "text": "labelled data"}, {"st": 160, "ed": 164, "text": "recurrent neural network architecture"}, {"st": 172, "ed": 174, "text": "real world"}, {"st": 181, "ed": 183, "text": "classification performance"}, {"st": 187, "ed": 190, "text": "real world data"}, {"st": 197, "ed": 199, "text": "evaluation shows"}, {"st": 202, "ed": 205, "text": "end to end"}, {"st": 226, "ed": 228, "text": "training scheme"}]
[{"st": 11, "ed": 14, "text": "end to end"}, {"st": 14, "ed": 16, "text": "object tracking"}, {"st": 33, "ed": 35, "text": "feature engineering"}, {"st": 87, "ed": 89, "text": "deep learning"}, {"st": 98, "ed": 101, "text": "recurrent neural networks"}, {"st": 130, "ed": 132, "text": "unsupervised manner"}, {"st": 142, "ed": 144, "text": "ground truth"}]
[{"st": 8, "ed": 10, "text": "deep learning"}, {"st": 13, "ed": 15, "text": "supervised learning"}, {"st": 19, "ed": 21, "text": "unsupervised learning"}, {"st": 22, "ed": 24, "text": "unlabeled examples"}, {"st": 42, "ed": 44, "text": "future frames"}, {"st": 50, "ed": 52, "text": "unsupervised learning"}, {"st": 66, "ed": 68, "text": "neural network"}, {"st": 87, "ed": 89, "text": "predict future"}, {"st": 140, "ed": 142, "text": "internal representations"}, {"st": 154, "ed": 156, "text": "object recognition"}, {"st": 170, "ed": 172, "text": "natural image"}, {"st": 191, "ed": 193, "text": "visual scene"}, {"st": 218, "ed": 220, "text": "unsupervised learning"}]
[{"st": 4, "ed": 6, "text": "computationally efficient"}, {"st": 13, "ed": 15, "text": "point clouds"}, {"st": 16, "ed": 20, "text": "convolutional neural networks cnns"}, {"st": 36, "ed": 38, "text": "convolutional layers"}, {"st": 56, "ed": 59, "text": "accuracy and speed"}, {"st": 96, "ed": 98, "text": "convolutional layers"}, {"st": 103, "ed": 105, "text": "large scale"}, {"st": 119, "ed": 121, "text": "object detection"}, {"st": 145, "ed": 147, "text": "vision based"}, {"st": 156, "ed": 158, "text": "highly competitive"}]
[{"st": 3, "ed": 5, "text": "gan training"}, {"st": 35, "ed": 37, "text": "gan training"}, {"st": 40, "ed": 43, "text": "point of view"}, {"st": 46, "ed": 48, "text": "mode collapse"}, {"st": 84, "ed": 86, "text": "real data"}, {"st": 109, "ed": 111, "text": "faster training"}]
[{"st": 0, "ed": 2, "text": "imitation learning"}, {"st": 7, "ed": 9, "text": "autonomous systems"}, {"st": 11, "ed": 13, "text": "control policies"}, {"st": 16, "ed": 18, "text": "reward function"}, {"st": 34, "ed": 36, "text": "imitation learning"}, {"st": 55, "ed": 57, "text": "supervised learning"}, {"st": 98, "ed": 100, "text": "imitation learning"}, {"st": 107, "ed": 109, "text": "imitation learning"}, {"st": 126, "ed": 128, "text": "imitation learning"}, {"st": 147, "ed": 149, "text": "applications including"}, {"st": 180, "ed": 182, "text": "real world"}]
[{"st": 22, "ed": 24, "text": "convergence rates"}, {"st": 26, "ed": 28, "text": "parameter estimates"}, {"st": 64, "ed": 66, "text": "network structure"}, {"st": 82, "ed": 84, "text": "network structure"}, {"st": 106, "ed": 108, "text": "convergence rates"}, {"st": 126, "ed": 128, "text": "denoising autoencoder"}, {"st": 134, "ed": 136, "text": "convergence rates"}, {"st": 150, "ed": 152, "text": "learning parameters"}]
[{"st": 0, "ed": 5, "text": "deep convolutional neural networks cnn"}, {"st": 11, "ed": 13, "text": "output layer"}, {"st": 14, "ed": 16, "text": "hidden layers"}, {"st": 22, "ed": 24, "text": "prediction error"}, {"st": 26, "ed": 28, "text": "output layer"}, {"st": 35, "ed": 37, "text": "feature learning"}, {"st": 40, "ed": 42, "text": "neural network"}, {"st": 56, "ed": 58, "text": "hidden layer"}, {"st": 61, "ed": 63, "text": "class label"}, {"st": 70, "ed": 72, "text": "input signals"}, {"st": 91, "ed": 93, "text": "hidden layers"}, {"st": 97, "ed": 99, "text": "classification error"}, {"st": 120, "ed": 122, "text": "faster convergence"}, {"st": 130, "ed": 132, "text": "hidden layers"}, {"st": 140, "ed": 142, "text": "k nn"}, {"st": 144, "ed": 146, "text": "input signals"}, {"st": 160, "ed": 162, "text": "approach achieves"}, {"st": 174, "ed": 176, "text": "object category"}]
[{"st": 13, "ed": 15, "text": "learning algorithm"}, {"st": 18, "ed": 20, "text": "temporal information"}, {"st": 41, "ed": 43, "text": "noise free"}, {"st": 43, "ed": 45, "text": "training data"}, {"st": 49, "ed": 51, "text": "proposed framework"}, {"st": 65, "ed": 67, "text": "spatio temporal"}, {"st": 124, "ed": 126, "text": "image data"}, {"st": 154, "ed": 156, "text": "proposed algorithm"}]
[{"st": 0, "ed": 2, "text": "machine learning"}, {"st": 6, "ed": 9, "text": "deep neural networks"}, {"st": 15, "ed": 18, "text": "vulnerable to adversarial"}, {"st": 40, "ed": 42, "text": "adversarial perturbations"}, {"st": 69, "ed": 71, "text": "adversarial perturbations"}, {"st": 74, "ed": 76, "text": "deep network"}]
[{"st": 1, "ed": 3, "text": "machine learning"}, {"st": 21, "ed": 23, "text": "adversarial perturbations"}, {"st": 28, "ed": 30, "text": "real world"}, {"st": 30, "ed": 32, "text": "machine learning"}, {"st": 47, "ed": 49, "text": "gradient based"}, {"st": 56, "ed": 58, "text": "class probabilities"}, {"st": 58, "ed": 60, "text": "score based"}, {"st": 68, "ed": 70, "text": "real world"}, {"st": 82, "ed": 84, "text": "based attacks"}, {"st": 94, "ed": 96, "text": "training data"}, {"st": 118, "ed": 120, "text": "based attacks"}, {"st": 124, "ed": 126, "text": "real world"}, {"st": 132, "ed": 134, "text": "cars 2"}, {"st": 144, "ed": 146, "text": "based attacks"}, {"st": 157, "ed": 159, "text": "score based"}, {"st": 188, "ed": 190, "text": "adversarial perturbation"}, {"st": 203, "ed": 205, "text": "conceptually simple"}, {"st": 209, "ed": 211, "text": "hyperparameter tuning"}, {"st": 223, "ed": 225, "text": "gradient based"}, {"st": 228, "ed": 230, "text": "computer vision"}, {"st": 254, "ed": 256, "text": "based attacks"}, {"st": 266, "ed": 268, "text": "machine learning"}, {"st": 278, "ed": 280, "text": "machine learning"}, {"st": 293, "ed": 295, "text": "https github.com"}]
[{"st": 0, "ed": 3, "text": "effective and efficient"}, {"st": 13, "ed": 15, "text": "information security"}, {"st": 50, "ed": 52, "text": "deep learning"}, {"st": 120, "ed": 122, "text": "models 1"}, {"st": 138, "ed": 140, "text": "empirical evidence"}, {"st": 154, "ed": 156, "text": "predictive accuracy"}]
[{"st": 0, "ed": 2, "text": "feature extraction"}, {"st": 3, "ed": 5, "text": "gained increasing"}, {"st": 10, "ed": 12, "text": "machine learning"}, {"st": 21, "ed": 23, "text": "predict future"}, {"st": 25, "ed": 27, "text": "big data"}, {"st": 43, "ed": 45, "text": "dimensionality reduction"}, {"st": 56, "ed": 58, "text": "high dimensional"}, {"st": 60, "ed": 62, "text": "higher level"}, {"st": 67, "ed": 69, "text": "neural networks"}, {"st": 74, "ed": 77, "text": "latent dirichlet allocation"}, {"st": 79, "ed": 81, "text": "higher level"}, {"st": 86, "ed": 88, "text": "neural networks"}, {"st": 101, "ed": 103, "text": "paragraph vectors"}, {"st": 113, "ed": 116, "text": "latent dirichlet allocation"}, {"st": 149, "ed": 152, "text": "k nearest neighbors"}, {"st": 163, "ed": 165, "text": "distributed stochastic"}, {"st": 201, "ed": 203, "text": "collaborative filtering"}]
[{"st": 1, "ed": 4, "text": "the past decade"}, {"st": 26, "ed": 28, "text": "dialogue systems"}, {"st": 48, "ed": 50, "text": "recent results"}, {"st": 74, "ed": 77, "text": "publicly available datasets"}, {"st": 112, "ed": 114, "text": "transfer learning"}, {"st": 128, "ed": 130, "text": "evaluation metrics"}]
[{"st": 0, "ed": 2, "text": "word embedding"}, {"st": 6, "ed": 8, "text": "low dimensional"}, {"st": 9, "ed": 11, "text": "embedding space"}, {"st": 27, "ed": 29, "text": "topic modeling"}, {"st": 33, "ed": 35, "text": "low dimensional"}, {"st": 63, "ed": 65, "text": "embedding model"}, {"st": 79, "ed": 81, "text": "embedding vectors"}, {"st": 102, "ed": 104, "text": "variational inference"}, {"st": 104, "ed": 106, "text": "method yields"}, {"st": 126, "ed": 128, "text": "low dimensional"}, {"st": 132, "ed": 134, "text": "document classification"}, {"st": 141, "ed": 143, "text": "existing methods"}]
[{"st": 8, "ed": 10, "text": "fine grained"}, {"st": 25, "ed": 27, "text": "fine grained"}, {"st": 85, "ed": 87, "text": "fine grained"}]
[{"st": 25, "ed": 27, "text": "joint learning"}, {"st": 61, "ed": 63, "text": "turing test"}, {"st": 140, "ed": 142, "text": "question answering"}, {"st": 145, "ed": 147, "text": "real world"}, {"st": 163, "ed": 165, "text": "ground truth"}, {"st": 197, "ed": 199, "text": "output space"}]
[{"st": 6, "ed": 8, "text": "artificial intelligence"}, {"st": 134, "ed": 136, "text": "initial results"}, {"st": 148, "ed": 150, "text": "humanoid robot"}]
[{"st": 65, "ed": 67, "text": "dnn based"}, {"st": 67, "ed": 69, "text": "speech recognition"}, {"st": 177, "ed": 179, "text": "speech processing"}, {"st": 181, "ed": 183, "text": "context information"}, {"st": 196, "ed": 198, "text": "audio visual"}]
[{"st": 10, "ed": 12, "text": "visual dialog"}, {"st": 56, "ed": 58, "text": "visual dialog"}, {"st": 101, "ed": 103, "text": "visual dialog"}]
[{"st": 28, "ed": 30, "text": "visual features"}, {"st": 35, "ed": 37, "text": "real world"}, {"st": 61, "ed": 63, "text": "north american"}, {"st": 81, "ed": 83, "text": "character level"}, {"st": 84, "ed": 86, "text": "rnn model"}]
[{"st": 1, "ed": 5, "text": "automatic speech recognition asr"}, {"st": 32, "ed": 34, "text": "neural network"}, {"st": 35, "ed": 37, "text": "asr systems"}, {"st": 57, "ed": 59, "text": "neural network"}, {"st": 63, "ed": 65, "text": "invariant feature"}, {"st": 70, "ed": 72, "text": "recent research"}, {"st": 73, "ed": 75, "text": "image generation"}, {"st": 76, "ed": 79, "text": "generative adversarial networks"}, {"st": 80, "ed": 82, "text": "domain adaptation"}, {"st": 98, "ed": 100, "text": "adversarial training"}, {"st": 102, "ed": 104, "text": "domain adaptation"}, {"st": 119, "ed": 121, "text": "classifier performance"}, {"st": 131, "ed": 133, "text": "neural architectures"}, {"st": 138, "ed": 140, "text": "noise conditions"}]
[{"st": 57, "ed": 59, "text": "visual information"}]
[{"st": 15, "ed": 17, "text": "e commerce"}, {"st": 38, "ed": 40, "text": "customer experience"}, {"st": 102, "ed": 104, "text": "e commerce"}]
[{"st": 143, "ed": 145, "text": "low level"}, {"st": 187, "ed": 189, "text": "self organizing"}, {"st": 224, "ed": 226, "text": "self organization"}, {"st": 229, "ed": 231, "text": "natural selection"}]
[{"st": 1, "ed": 3, "text": "f measure"}, {"st": 19, "ed": 21, "text": "natural language"}, {"st": 23, "ed": 25, "text": "machine learning"}]
[{"st": 3, "ed": 5, "text": "open source"}, {"st": 29, "ed": 32, "text": "statistical machine learning"}, {"st": 77, "ed": 79, "text": "starting point"}, {"st": 101, "ed": 103, "text": "machine learning"}, {"st": 106, "ed": 109, "text": "recurrent neural networks"}, {"st": 125, "ed": 128, "text": "machine learning tasks"}]
[{"st": 9, "ed": 11, "text": "machine learning"}, {"st": 26, "ed": 28, "text": "frank wolfe"}, {"st": 37, "ed": 39, "text": "successfully applied"}, {"st": 41, "ed": 43, "text": "large scale"}, {"st": 47, "ed": 50, "text": "support vector machines"}, {"st": 61, "ed": 63, "text": "efficient algorithms"}, {"st": 66, "ed": 68, "text": "theoretical results"}, {"st": 69, "ed": 71, "text": "convergence analysis"}, {"st": 148, "ed": 150, "text": "computational geometry"}, {"st": 171, "ed": 173, "text": "convergence rate"}, {"st": 174, "ed": 177, "text": "number of iterations"}, {"st": 190, "ed": 193, "text": "rate of convergence"}, {"st": 259, "ed": 261, "text": "without sacrificing"}, {"st": 262, "ed": 264, "text": "predictive accuracy"}]
[{"st": 1, "ed": 3, "text": "significant progress"}, {"st": 4, "ed": 6, "text": "object categorization"}, {"st": 7, "ed": 9, "text": "recent years"}, {"st": 16, "ed": 19, "text": "ability to learn"}, {"st": 21, "ed": 23, "text": "labeled data"}, {"st": 27, "ed": 29, "text": "object classes"}, {"st": 32, "ed": 34, "text": "open set"}, {"st": 36, "ed": 39, "text": "zero shot learning"}, {"st": 60, "ed": 62, "text": "typically requires"}, {"st": 64, "ed": 67, "text": "supervised and unsupervised"}, {"st": 83, "ed": 85, "text": "semi supervised"}, {"st": 99, "ed": 101, "text": "zero shot"}, {"st": 102, "ed": 104, "text": "open set"}, {"st": 113, "ed": 115, "text": "maximum margin"}, {"st": 127, "ed": 130, "text": "supervised and unsupervised"}, {"st": 134, "ed": 136, "text": "labeled samples"}, {"st": 145, "ed": 147, "text": "embedding space"}, {"st": 159, "ed": 161, "text": "zero shot"}, {"st": 163, "ed": 165, "text": "open set"}]
[{"st": 13, "ed": 15, "text": "computer vision"}, {"st": 16, "ed": 18, "text": "natural language"}, {"st": 20, "ed": 22, "text": "prediction methods"}, {"st": 36, "ed": 38, "text": "structured prediction"}, {"st": 58, "ed": 60, "text": "greedy algorithms"}, {"st": 68, "ed": 70, "text": "structured output"}, {"st": 76, "ed": 78, "text": "submodular functions"}]
[{"st": 3, "ed": 5, "text": "image processing"}, {"st": 6, "ed": 8, "text": "computer vision"}, {"st": 10, "ed": 12, "text": "style transfer"}, {"st": 18, "ed": 20, "text": "input image"}, {"st": 32, "ed": 34, "text": "holy grail"}, {"st": 46, "ed": 48, "text": "input image"}, {"st": 66, "ed": 68, "text": "existing methods"}, {"st": 101, "ed": 103, "text": "zero shot"}, {"st": 113, "ed": 115, "text": "input image"}, {"st": 141, "ed": 143, "text": "zero shot"}, {"st": 148, "ed": 150, "text": "fully differentiable"}, {"st": 185, "ed": 187, "text": "zero shot"}, {"st": 206, "ed": 208, "text": "extensive experiments"}, {"st": 245, "ed": 247, "text": "large scale"}]
[{"st": 7, "ed": 11, "text": "generative adversarial networks gans"}, {"st": 20, "ed": 22, "text": "mode collapse"}, {"st": 27, "ed": 29, "text": "multi agent"}, {"st": 52, "ed": 54, "text": "objective function"}, {"st": 120, "ed": 122, "text": "extensive experiments"}, {"st": 123, "ed": 126, "text": "synthetic and real"}, {"st": 145, "ed": 147, "text": "challenging tasks"}, {"st": 149, "ed": 152, "text": "image to image"}, {"st": 180, "ed": 182, "text": "multi view"}, {"st": 201, "ed": 203, "text": "feature representation"}, {"st": 210, "ed": 212, "text": "similarity based"}, {"st": 226, "ed": 228, "text": "user defined"}, {"st": 240, "ed": 243, "text": "mixture of gaussians"}]
[{"st": 0, "ed": 3, "text": "generative adversarial nets"}, {"st": 10, "ed": 12, "text": "generative models"}, {"st": 25, "ed": 27, "text": "main contributions"}, {"st": 35, "ed": 37, "text": "geometric structure"}, {"st": 48, "ed": 50, "text": "generative model"}, {"st": 90, "ed": 92, "text": "existing approaches"}, {"st": 112, "ed": 114, "text": "theoretical analysis"}, {"st": 122, "ed": 124, "text": "nash equilibrium"}, {"st": 131, "ed": 133, "text": "extensive numerical"}]
[{"st": 1, "ed": 3, "text": "deep networks"}, {"st": 6, "ed": 8, "text": "time consuming"}, {"st": 29, "ed": 31, "text": "distributed training"}, {"st": 32, "ed": 34, "text": "deep networks"}, {"st": 57, "ed": 59, "text": "deep networks"}, {"st": 62, "ed": 64, "text": "training data"}, {"st": 94, "ed": 96, "text": "significant speedup"}, {"st": 121, "ed": 123, "text": "deep networks"}, {"st": 153, "ed": 155, "text": "fully connected"}, {"st": 155, "ed": 157, "text": "feedforward networks"}, {"st": 157, "ed": 160, "text": "convolutional neural networks"}, {"st": 160, "ed": 163, "text": "recurrent neural networks"}, {"st": 165, "ed": 168, "text": "short term memory"}, {"st": 174, "ed": 176, "text": "extensive simulations"}, {"st": 207, "ed": 209, "text": "proposed framework"}, {"st": 211, "ed": 213, "text": "significant speedup"}]
[{"st": 1, "ed": 4, "text": "deep neural networks"}, {"st": 69, "ed": 71, "text": "neural network"}, {"st": 106, "ed": 108, "text": "layer wise"}]
[{"st": 5, "ed": 8, "text": "stochastic gradient descent"}, {"st": 15, "ed": 19, "text": "rectified linear unit relu"}, {"st": 46, "ed": 48, "text": "previous works"}, {"st": 58, "ed": 61, "text": "stochastic gradient descent"}, {"st": 62, "ed": 64, "text": "random initialization"}, {"st": 74, "ed": 76, "text": "convergence rate"}, {"st": 120, "ed": 122, "text": "learning rate"}]
[{"st": 0, "ed": 2, "text": "sparsity inducing"}, {"st": 17, "ed": 19, "text": "ell 1"}, {"st": 46, "ed": 48, "text": "norm regularization"}, {"st": 98, "ed": 100, "text": "neural networks"}, {"st": 103, "ed": 105, "text": "ell 1"}, {"st": 124, "ed": 126, "text": "global convergence"}, {"st": 142, "ed": 144, "text": "convergence analysis"}]
[{"st": 9, "ed": 11, "text": "deep learning"}, {"st": 19, "ed": 21, "text": "performance improvement"}, {"st": 22, "ed": 24, "text": "recent studies"}, {"st": 30, "ed": 32, "text": "neural networks"}, {"st": 33, "ed": 35, "text": "adversarial examples"}, {"st": 75, "ed": 77, "text": "experimentally demonstrate"}, {"st": 79, "ed": 82, "text": "accuracy and robustness"}, {"st": 99, "ed": 101, "text": "adversarial examples"}]
[{"st": 8, "ed": 10, "text": "hidden layer"}, {"st": 10, "ed": 12, "text": "neural network"}, {"st": 15, "ed": 17, "text": "convolutional layer"}, {"st": 19, "ed": 21, "text": "activation function"}, {"st": 71, "ed": 73, "text": "local minimum"}, {"st": 84, "ed": 86, "text": "local minimum"}, {"st": 136, "ed": 138, "text": "local minimum"}, {"st": 141, "ed": 143, "text": "local minimum"}, {"st": 145, "ed": 147, "text": "non trivial"}, {"st": 156, "ed": 158, "text": "quantitative analysis"}]
[{"st": 2, "ed": 4, "text": "real world"}, {"st": 25, "ed": 27, "text": "reward signal"}, {"st": 68, "ed": 70, "text": "feature space"}, {"st": 84, "ed": 86, "text": "continuous state"}, {"st": 110, "ed": 112, "text": "proposed approach"}, {"st": 119, "ed": 121, "text": "super mario"}]
[{"st": 0, "ed": 2, "text": "generating adversarial"}, {"st": 19, "ed": 21, "text": "existing methods"}, {"st": 33, "ed": 35, "text": "performance measure"}, {"st": 49, "ed": 51, "text": "generating adversarial"}, {"st": 56, "ed": 58, "text": "final performance"}, {"st": 80, "ed": 82, "text": "speech recognition"}, {"st": 82, "ed": 84, "text": "pose estimation"}, {"st": 97, "ed": 99, "text": "success rate"}]
[{"st": 7, "ed": 9, "text": "neural networks"}, {"st": 17, "ed": 19, "text": "recognition tasks"}, {"st": 24, "ed": 26, "text": "training data"}, {"st": 46, "ed": 48, "text": "training samples"}, {"st": 73, "ed": 75, "text": "training instances"}, {"st": 76, "ed": 78, "text": "zero shot"}, {"st": 88, "ed": 90, "text": "zero shot"}, {"st": 110, "ed": 112, "text": "recognition tasks"}, {"st": 116, "ed": 118, "text": "open set"}, {"st": 127, "ed": 129, "text": "zero shot"}, {"st": 140, "ed": 142, "text": "zero shot"}, {"st": 147, "ed": 149, "text": "real world"}, {"st": 156, "ed": 158, "text": "existing approaches"}, {"st": 161, "ed": 164, "text": "future research directions"}]
[{"st": 9, "ed": 14, "text": "deep convolutional neural networks cnns"}, {"st": 18, "ed": 20, "text": "max pooling"}, {"st": 27, "ed": 29, "text": "linearly independent"}, {"st": 61, "ed": 63, "text": "sufficient conditions"}, {"st": 81, "ed": 83, "text": "fully connected"}, {"st": 89, "ed": 91, "text": "critical point"}, {"st": 93, "ed": 95, "text": "empirical loss"}, {"st": 97, "ed": 99, "text": "global minimum"}, {"st": 108, "ed": 111, "text": "depth and width"}, {"st": 121, "ed": 123, "text": "representational power"}, {"st": 129, "ed": 131, "text": "high level"}, {"st": 139, "ed": 141, "text": "loss function"}]
[{"st": 8, "ed": 10, "text": "scientific knowledge"}, {"st": 15, "ed": 17, "text": "neural networks"}, {"st": 27, "ed": 29, "text": "neural network"}, {"st": 47, "ed": 49, "text": "neural network"}, {"st": 61, "ed": 63, "text": "loss functions"}, {"st": 65, "ed": 67, "text": "learning objective"}, {"st": 68, "ed": 70, "text": "neural networks"}, {"st": 83, "ed": 85, "text": "training set"}, {"st": 133, "ed": 135, "text": "scientific knowledge"}, {"st": 142, "ed": 144, "text": "neural networks"}, {"st": 151, "ed": 153, "text": "proposed framework"}]
[{"st": 0, "ed": 2, "text": "spectral clustering"}, {"st": 11, "ed": 13, "text": "spectral clustering"}, {"st": 19, "ed": 21, "text": "similarity graph"}, {"st": 34, "ed": 36, "text": "common practice"}, {"st": 52, "ed": 54, "text": "similarity graph"}, {"st": 66, "ed": 68, "text": "similarity graph"}, {"st": 79, "ed": 81, "text": "automatically learn"}, {"st": 92, "ed": 94, "text": "similarity matrix"}, {"st": 115, "ed": 117, "text": "k means"}, {"st": 155, "ed": 157, "text": "unified framework"}, {"st": 182, "ed": 184, "text": "kernel method"}, {"st": 217, "ed": 220, "text": "multiple kernel learning"}, {"st": 221, "ed": 223, "text": "extensive experiments"}, {"st": 228, "ed": 230, "text": "proposed method"}]
[{"st": 10, "ed": 12, "text": "neural networks"}, {"st": 17, "ed": 19, "text": "theoretical understanding"}, {"st": 56, "ed": 58, "text": "linear classification"}, {"st": 68, "ed": 70, "text": "logistic loss"}, {"st": 72, "ed": 74, "text": "logistic regression"}, {"st": 91, "ed": 93, "text": "dropout regularization"}, {"st": 108, "ed": 110, "text": "dropout regularization"}, {"st": 124, "ed": 126, "text": "dropout regularization"}, {"st": 141, "ed": 143, "text": "dropout regularization"}, {"st": 145, "ed": 147, "text": "convex loss"}, {"st": 157, "ed": 159, "text": "dropout regularization"}, {"st": 160, "ed": 162, "text": "l 2"}, {"st": 187, "ed": 189, "text": "dropout regularization"}, {"st": 190, "ed": 192, "text": "l 2"}, {"st": 204, "ed": 206, "text": "inductive biases"}, {"st": 209, "ed": 211, "text": "l 2"}]
[{"st": 4, "ed": 6, "text": "deep networks"}, {"st": 7, "ed": 10, "text": "rectified linear units"}, {"st": 85, "ed": 87, "text": "weight decay"}, {"st": 102, "ed": 104, "text": "input features"}, {"st": 117, "ed": 119, "text": "local minima"}]
[{"st": 2, "ed": 4, "text": "neural networks"}, {"st": 5, "ed": 8, "text": "spiking neural networks"}, {"st": 50, "ed": 53, "text": "generalized linear model"}, {"st": 63, "ed": 65, "text": "computational neuroscience"}, {"st": 67, "ed": 69, "text": "classification rules"}, {"st": 107, "ed": 109, "text": "classification decision"}, {"st": 118, "ed": 120, "text": "numerical results"}]
[{"st": 32, "ed": 34, "text": "based clustering"}, {"st": 136, "ed": 138, "text": "clustering algorithms"}]
[{"st": 6, "ed": 8, "text": "deep learning"}, {"st": 10, "ed": 12, "text": "theoretical understanding"}, {"st": 18, "ed": 21, "text": "deep neural networks"}, {"st": 32, "ed": 35, "text": "theory and practice"}, {"st": 36, "ed": 38, "text": "deep learning"}, {"st": 41, "ed": 43, "text": "learning dynamics"}, {"st": 48, "ed": 50, "text": "deep linear"}, {"st": 57, "ed": 59, "text": "input output"}, {"st": 82, "ed": 84, "text": "deep linear"}, {"st": 110, "ed": 112, "text": "faster convergence"}, {"st": 116, "ed": 118, "text": "initial conditions"}, {"st": 144, "ed": 146, "text": "theoretical analysis"}, {"st": 160, "ed": 162, "text": "learning speed"}, {"st": 171, "ed": 173, "text": "initial conditions"}, {"st": 177, "ed": 179, "text": "deep networks"}, {"st": 187, "ed": 189, "text": "learning speed"}, {"st": 201, "ed": 203, "text": "training data"}, {"st": 211, "ed": 213, "text": "initial conditions"}, {"st": 228, "ed": 230, "text": "initial conditions"}, {"st": 234, "ed": 237, "text": "unsupervised pre training"}, {"st": 247, "ed": 249, "text": "initial conditions"}]
[{"st": 5, "ed": 7, "text": "linear approximation"}, {"st": 28, "ed": 30, "text": "sparse representations"}, {"st": 70, "ed": 72, "text": "kernel function"}, {"st": 74, "ed": 76, "text": "gaussian kernel"}, {"st": 86, "ed": 88, "text": "neural networks"}, {"st": 88, "ed": 90, "text": "gaussian processes"}, {"st": 91, "ed": 93, "text": "online learning"}, {"st": 183, "ed": 185, "text": "input space"}]
[{"st": 20, "ed": 24, "text": "sloan digital sky survey"}, {"st": 61, "ed": 63, "text": "time consuming"}, {"st": 66, "ed": 69, "text": "scale to large"}, {"st": 100, "ed": 102, "text": "successfully applied"}, {"st": 139, "ed": 142, "text": "deep neural network"}, {"st": 213, "ed": 215, "text": "highly accurate"}, {"st": 254, "ed": 256, "text": "training data"}]
[{"st": 1, "ed": 5, "text": "nonnegative matrix factorization nmf"}, {"st": 11, "ed": 13, "text": "image processing"}, {"st": 16, "ed": 19, "text": "blind source separation"}, {"st": 21, "ed": 23, "text": "image analysis"}, {"st": 43, "ed": 45, "text": "kernel machines"}, {"st": 78, "ed": 80, "text": "kernel based"}, {"st": 108, "ed": 110, "text": "kernel functions"}, {"st": 115, "ed": 117, "text": "iterative algorithms"}, {"st": 119, "ed": 121, "text": "update rule"}, {"st": 130, "ed": 132, "text": "update rule"}, {"st": 145, "ed": 147, "text": "proposed framework"}, {"st": 162, "ed": 164, "text": "total variation"}, {"st": 170, "ed": 172, "text": "proposed method"}, {"st": 184, "ed": 186, "text": "real images"}]
[{"st": 1, "ed": 3, "text": "machine learning"}, {"st": 9, "ed": 11, "text": "kernel based"}, {"st": 12, "ed": 14, "text": "gaussian processes"}, {"st": 15, "ed": 18, "text": "radial basis function"}, {"st": 28, "ed": 30, "text": "online learning"}, {"st": 57, "ed": 59, "text": "linear approximation"}, {"st": 81, "ed": 83, "text": "computationally efficient"}, {"st": 121, "ed": 123, "text": "theoretical bounds"}, {"st": 142, "ed": 144, "text": "approximation error"}]
[{"st": 7, "ed": 11, "text": "deep convolutional neural networks"}, {"st": 12, "ed": 14, "text": "feature extraction"}, {"st": 38, "ed": 41, "text": "convolutional neural network"}, {"st": 59, "ed": 62, "text": "local and global"}, {"st": 68, "ed": 70, "text": "structural properties"}, {"st": 72, "ed": 74, "text": "input signal"}, {"st": 89, "ed": 91, "text": "lipschitz continuous"}, {"st": 91, "ed": 93, "text": "non linearities"}, {"st": 98, "ed": 100, "text": "handwritten digit"}, {"st": 102, "ed": 104, "text": "facial landmark"}, {"st": 106, "ed": 108, "text": "feature importance"}]
[{"st": 7, "ed": 9, "text": "neural network"}, {"st": 20, "ed": 22, "text": "encoder decoder"}, {"st": 36, "ed": 38, "text": "latent representation"}, {"st": 50, "ed": 53, "text": "recurrent neural networks"}, {"st": 73, "ed": 75, "text": "empirical study"}]
[{"st": 4, "ed": 7, "text": "end to end"}, {"st": 12, "ed": 14, "text": "open domain"}, {"st": 26, "ed": 28, "text": "supervised learning"}, {"st": 44, "ed": 46, "text": "hand crafted"}, {"st": 46, "ed": 48, "text": "reward functions"}, {"st": 55, "ed": 57, "text": "interactive learning"}, {"st": 65, "ed": 67, "text": "response generation"}]
[{"st": 4, "ed": 6, "text": "natural language"}, {"st": 11, "ed": 13, "text": "machine reading"}, {"st": 21, "ed": 23, "text": "answer questions"}, {"st": 36, "ed": 38, "text": "large scale"}, {"st": 38, "ed": 41, "text": "training and test"}, {"st": 64, "ed": 66, "text": "large scale"}, {"st": 78, "ed": 80, "text": "attention based"}, {"st": 80, "ed": 83, "text": "deep neural networks"}, {"st": 91, "ed": 93, "text": "complex questions"}, {"st": 95, "ed": 97, "text": "prior knowledge"}]
[{"st": 8, "ed": 11, "text": "representations of words"}, {"st": 32, "ed": 34, "text": "compositional distributional"}, {"st": 41, "ed": 43, "text": "word embeddings"}, {"st": 106, "ed": 109, "text": "qualitatively and quantitatively"}, {"st": 114, "ed": 116, "text": "sentence level"}]
[{"st": 1, "ed": 3, "text": "natural language"}, {"st": 17, "ed": 19, "text": "deep models"}, {"st": 39, "ed": 41, "text": "local information"}, {"st": 53, "ed": 55, "text": "deep architecture"}, {"st": 81, "ed": 84, "text": "short term memory"}, {"st": 100, "ed": 102, "text": "sentence representations"}, {"st": 104, "ed": 106, "text": "max pooling"}, {"st": 133, "ed": 135, "text": "local information"}, {"st": 146, "ed": 148, "text": "sentence representations"}, {"st": 156, "ed": 158, "text": "local information"}, {"st": 172, "ed": 174, "text": "question answering"}]
[{"st": 0, "ed": 3, "text": "artificial neural networks"}, {"st": 9, "ed": 11, "text": "widely applied"}, {"st": 15, "ed": 17, "text": "machine translation"}, {"st": 57, "ed": 59, "text": "word pairs"}, {"st": 64, "ed": 67, "text": "recurrent neural networks"}, {"st": 101, "ed": 103, "text": "significant improvements"}]
[{"st": 0, "ed": 2, "text": "natural language"}, {"st": 10, "ed": 12, "text": "natural language"}, {"st": 21, "ed": 23, "text": "natural language"}, {"st": 39, "ed": 42, "text": "deep neural networks"}, {"st": 43, "ed": 45, "text": "natural language"}, {"st": 55, "ed": 60, "text": "long short term memory lstm"}, {"st": 70, "ed": 72, "text": "recently proposed"}, {"st": 72, "ed": 74, "text": "neural attention"}, {"st": 129, "ed": 131, "text": "word level"}]
[{"st": 1, "ed": 3, "text": "neural networks"}, {"st": 6, "ed": 8, "text": "recently proposed"}, {"st": 11, "ed": 14, "text": "short term memory"}, {"st": 26, "ed": 28, "text": "word embeddings"}, {"st": 39, "ed": 41, "text": "recurrent networks"}, {"st": 46, "ed": 48, "text": "hierarchical structure"}, {"st": 84, "ed": 86, "text": "learning task"}, {"st": 149, "ed": 151, "text": "sentiment analysis"}, {"st": 158, "ed": 160, "text": "hierarchical structure"}]
[{"st": 9, "ed": 11, "text": "challenging task"}, {"st": 20, "ed": 22, "text": "previous research"}, {"st": 39, "ed": 41, "text": "classification performance"}, {"st": 70, "ed": 72, "text": "classification tasks"}, {"st": 80, "ed": 83, "text": "convolutional neural network"}, {"st": 84, "ed": 87, "text": "multi task learning"}, {"st": 110, "ed": 112, "text": "relation classification"}, {"st": 118, "ed": 120, "text": "significant gains"}]
[{"st": 0, "ed": 3, "text": "neural network based"}, {"st": 8, "ed": 10, "text": "automatically generate"}, {"st": 22, "ed": 24, "text": "feature representation"}, {"st": 46, "ed": 50, "text": "deep neural network architecture"}, {"st": 53, "ed": 55, "text": "pre trained"}, {"st": 55, "ed": 57, "text": "word embedding"}, {"st": 75, "ed": 77, "text": "multi layer"}, {"st": 77, "ed": 79, "text": "bidirectional lstm"}, {"st": 109, "ed": 111, "text": "consistently outperforms"}, {"st": 112, "ed": 114, "text": "existing methods"}]
[{"st": 0, "ed": 2, "text": "previous studies"}, {"st": 19, "ed": 21, "text": "automatically learn"}, {"st": 59, "ed": 64, "text": "long short term memory lstm"}, {"st": 67, "ed": 69, "text": "higher level"}, {"st": 83, "ed": 85, "text": "level features"}, {"st": 86, "ed": 88, "text": "lstm networks"}, {"st": 92, "ed": 94, "text": "contextual information"}, {"st": 105, "ed": 107, "text": "lstm networks"}, {"st": 120, "ed": 122, "text": "negative samples"}, {"st": 139, "ed": 141, "text": "approach produces"}, {"st": 167, "ed": 169, "text": "deep learning"}]
[{"st": 24, "ed": 26, "text": "question answering"}, {"st": 43, "ed": 45, "text": "neural network"}, {"st": 47, "ed": 49, "text": "based methods"}, {"st": 56, "ed": 58, "text": "achieved impressive"}, {"st": 101, "ed": 103, "text": "neural attention"}, {"st": 157, "ed": 159, "text": "attention model"}]
[{"st": 5, "ed": 7, "text": "natural language"}, {"st": 19, "ed": 21, "text": "machine translation"}, {"st": 36, "ed": 38, "text": "textual entailment"}, {"st": 59, "ed": 61, "text": "sentence pairs"}, {"st": 91, "ed": 93, "text": "natural language"}, {"st": 103, "ed": 105, "text": "manually annotated"}, {"st": 125, "ed": 127, "text": "qualitative analysis"}, {"st": 156, "ed": 158, "text": "input output"}, {"st": 161, "ed": 163, "text": "natural language"}, {"st": 181, "ed": 184, "text": "source and target"}, {"st": 197, "ed": 199, "text": "additional information"}]
[{"st": 0, "ed": 3, "text": "recurrent neural networks"}, {"st": 13, "ed": 15, "text": "natural language"}, {"st": 70, "ed": 72, "text": "recurrent units"}, {"st": 100, "ed": 102, "text": "experiments demonstrate"}, {"st": 111, "ed": 113, "text": "tasks including"}, {"st": 116, "ed": 118, "text": "propositional logic"}]
[{"st": 0, "ed": 3, "text": "recurrent neural networks"}, {"st": 6, "ed": 11, "text": "long short term memory lstm"}, {"st": 44, "ed": 46, "text": "hidden state"}, {"st": 69, "ed": 72, "text": "recurrent neural networks"}, {"st": 78, "ed": 80, "text": "hidden state"}, {"st": 107, "ed": 109, "text": "large data"}, {"st": 132, "ed": 134, "text": "hidden state"}]
[{"st": 0, "ed": 4, "text": "neural machine translation nmt"}, {"st": 7, "ed": 9, "text": "deep learning"}, {"st": 66, "ed": 68, "text": "weight pruning"}, {"st": 99, "ed": 101, "text": "performance loss"}, {"st": 112, "ed": 115, "text": "sheds light on"}, {"st": 124, "ed": 126, "text": "main result"}]
[{"st": 0, "ed": 2, "text": "natural language"}, {"st": 8, "ed": 10, "text": "natural language"}, {"st": 30, "ed": 32, "text": "neural networks"}, {"st": 41, "ed": 43, "text": "natural language"}, {"st": 70, "ed": 72, "text": "generative model"}]
[{"st": 1, "ed": 3, "text": "question answering"}, {"st": 5, "ed": 7, "text": "neural network"}, {"st": 12, "ed": 14, "text": "promising results"}, {"st": 15, "ed": 17, "text": "recent years"}, {"st": 19, "ed": 21, "text": "large scale"}, {"st": 43, "ed": 45, "text": "large scale"}, {"st": 47, "ed": 49, "text": "real world"}, {"st": 102, "ed": 105, "text": "problem and propose"}, {"st": 106, "ed": 109, "text": "end to end"}]
[{"st": 20, "ed": 22, "text": "character level"}, {"st": 33, "ed": 35, "text": "randomly selected"}, {"st": 35, "ed": 37, "text": "english language"}, {"st": 109, "ed": 111, "text": "english language"}]
[{"st": 81, "ed": 83, "text": "input sequence"}, {"st": 85, "ed": 87, "text": "fixed length"}, {"st": 87, "ed": 89, "text": "hidden states"}, {"st": 106, "ed": 108, "text": "attention weights"}, {"st": 117, "ed": 119, "text": "attention weights"}, {"st": 122, "ed": 124, "text": "latent variable"}, {"st": 142, "ed": 145, "text": "significant performance gains"}]
[{"st": 4, "ed": 6, "text": "semi supervised"}, {"st": 23, "ed": 25, "text": "generative model"}, {"st": 53, "ed": 55, "text": "training data"}]
[{"st": 4, "ed": 6, "text": "deep learning"}, {"st": 54, "ed": 56, "text": "spoken language"}, {"st": 77, "ed": 79, "text": "domain specific"}, {"st": 122, "ed": 124, "text": "representation learning"}, {"st": 136, "ed": 139, "text": "convolutional neural network"}, {"st": 146, "ed": 149, "text": "short term memory"}, {"st": 176, "ed": 178, "text": "significantly higher"}, {"st": 178, "ed": 182, "text": "word error rate wer"}]
[{"st": 14, "ed": 17, "text": "recurrent neural networks"}, {"st": 57, "ed": 59, "text": "training samples"}, {"st": 70, "ed": 72, "text": "latent variable"}, {"st": 111, "ed": 113, "text": "machine translation"}]
[{"st": 5, "ed": 8, "text": "deep neural networks"}, {"st": 20, "ed": 23, "text": "shown promising results"}, {"st": 28, "ed": 30, "text": "word level"}, {"st": 44, "ed": 47, "text": "massive amounts of"}, {"st": 51, "ed": 53, "text": "natural language"}, {"st": 55, "ed": 57, "text": "response generation"}, {"st": 64, "ed": 66, "text": "domain knowledge"}, {"st": 95, "ed": 97, "text": "recently proposed"}, {"st": 101, "ed": 103, "text": "encoder decoder"}, {"st": 103, "ed": 105, "text": "neural network"}]
[{"st": 48, "ed": 50, "text": "dynamic programming"}, {"st": 110, "ed": 112, "text": "large scale"}, {"st": 130, "ed": 133, "text": "neural language models"}, {"st": 209, "ed": 211, "text": "accurate prediction"}, {"st": 215, "ed": 217, "text": "qualitative analysis"}]
[{"st": 3, "ed": 5, "text": "open source"}, {"st": 7, "ed": 11, "text": "neural machine translation nmt"}, {"st": 29, "ed": 31, "text": "feature representations"}, {"st": 36, "ed": 38, "text": "competitive performance"}]
[{"st": 3, "ed": 5, "text": "large scale"}, {"st": 5, "ed": 7, "text": "question answering"}, {"st": 16, "ed": 19, "text": "end to end"}, {"st": 19, "ed": 21, "text": "neural architectures"}, {"st": 24, "ed": 26, "text": "complex systems"}, {"st": 97, "ed": 100, "text": "bag of words"}, {"st": 121, "ed": 123, "text": "competitive performance"}]
[{"st": 10, "ed": 12, "text": "natural language"}, {"st": 44, "ed": 47, "text": "the past decade"}, {"st": 102, "ed": 104, "text": "recent research"}, {"st": 121, "ed": 123, "text": "artificial intelligence"}, {"st": 142, "ed": 144, "text": "natural language"}]
[{"st": 16, "ed": 18, "text": "sentence level"}, {"st": 19, "ed": 21, "text": "word level"}, {"st": 23, "ed": 25, "text": "promising results"}, {"st": 31, "ed": 33, "text": "sentence level"}, {"st": 46, "ed": 48, "text": "word level"}, {"st": 82, "ed": 84, "text": "word level"}, {"st": 86, "ed": 88, "text": "sentence level"}, {"st": 116, "ed": 118, "text": "final results"}, {"st": 128, "ed": 130, "text": "method yields"}]
[{"st": 8, "ed": 10, "text": "nlp applications"}, {"st": 13, "ed": 15, "text": "question answering"}, {"st": 24, "ed": 27, "text": "recurrent neural network"}, {"st": 29, "ed": 31, "text": "residual learning"}, {"st": 42, "ed": 44, "text": "deep residual"}, {"st": 83, "ed": 85, "text": "approach achieves"}, {"st": 89, "ed": 91, "text": "detection performance"}]
[{"st": 6, "ed": 9, "text": "automatic speech recognition"}, {"st": 10, "ed": 12, "text": "error detection"}, {"st": 17, "ed": 19, "text": "spoken language"}, {"st": 66, "ed": 68, "text": "recently proposed"}, {"st": 72, "ed": 74, "text": "word embeddings"}, {"st": 76, "ed": 78, "text": "well calibrated"}, {"st": 96, "ed": 98, "text": "error rate"}, {"st": 106, "ed": 108, "text": "previously published"}, {"st": 125, "ed": 128, "text": "conditional random fields"}, {"st": 131, "ed": 133, "text": "encoder decoder"}, {"st": 133, "ed": 135, "text": "attention based"}]
[{"st": 0, "ed": 2, "text": "common sense"}, {"st": 3, "ed": 5, "text": "background knowledge"}, {"st": 9, "ed": 11, "text": "natural language"}, {"st": 15, "ed": 17, "text": "natural language"}, {"st": 22, "ed": 24, "text": "background knowledge"}, {"st": 42, "ed": 44, "text": "background knowledge"}, {"st": 55, "ed": 57, "text": "word representations"}, {"st": 59, "ed": 61, "text": "task specific"}, {"st": 65, "ed": 67, "text": "background knowledge"}, {"st": 77, "ed": 79, "text": "task specific"}, {"st": 80, "ed": 82, "text": "strong performance"}, {"st": 87, "ed": 89, "text": "question answering"}, {"st": 92, "ed": 94, "text": "textual entailment"}, {"st": 103, "ed": 105, "text": "analysis shows"}]
[{"st": 3, "ed": 5, "text": "skip thought"}, {"st": 17, "ed": 19, "text": "skip thought"}, {"st": 32, "ed": 34, "text": "skip thought"}, {"st": 47, "ed": 49, "text": "trained model"}, {"st": 57, "ed": 60, "text": "detection and classification"}, {"st": 74, "ed": 76, "text": "skip thought"}, {"st": 83, "ed": 85, "text": "skip thought"}, {"st": 116, "ed": 118, "text": "skip thought"}]
[{"st": 1, "ed": 3, "text": "question answering"}, {"st": 11, "ed": 13, "text": "deep learning"}, {"st": 15, "ed": 17, "text": "neural network"}, {"st": 18, "ed": 20, "text": "outperform traditional"}, {"st": 24, "ed": 26, "text": "large datasets"}, {"st": 31, "ed": 33, "text": "100 000"}, {"st": 101, "ed": 103, "text": "open domain"}, {"st": 115, "ed": 117, "text": "transfer learning"}, {"st": 119, "ed": 121, "text": "network architecture"}, {"st": 134, "ed": 136, "text": "word embeddings"}, {"st": 149, "ed": 151, "text": "qa systems"}, {"st": 157, "ed": 159, "text": "domain specific"}, {"st": 184, "ed": 186, "text": "competitive results"}]
[{"st": 35, "ed": 37, "text": "question answering"}, {"st": 75, "ed": 77, "text": "significantly outperforms"}, {"st": 84, "ed": 86, "text": "rule based"}, {"st": 96, "ed": 98, "text": "natural language"}, {"st": 113, "ed": 116, "text": "question answer pairs"}]
[{"st": 22, "ed": 24, "text": "question answering"}, {"st": 74, "ed": 76, "text": "word embeddings"}, {"st": 94, "ed": 96, "text": "pre trained"}, {"st": 100, "ed": 102, "text": "large scale"}, {"st": 102, "ed": 104, "text": "open domain"}, {"st": 109, "ed": 111, "text": "fine tuned"}, {"st": 132, "ed": 134, "text": "competitive results"}]
[{"st": 20, "ed": 22, "text": "deep network"}, {"st": 26, "ed": 28, "text": "competitive accuracy"}, {"st": 29, "ed": 31, "text": "text classification"}, {"st": 40, "ed": 42, "text": "hierarchical representations"}, {"st": 61, "ed": 63, "text": "attention weights"}, {"st": 69, "ed": 71, "text": "higher layers"}]
[{"st": 55, "ed": 57, "text": "word representations"}]
[{"st": 3, "ed": 5, "text": "natural language"}, {"st": 52, "ed": 54, "text": "computational models"}, {"st": 108, "ed": 110, "text": "skip thought"}, {"st": 112, "ed": 114, "text": "f measure"}]
[{"st": 0, "ed": 2, "text": "neural network"}, {"st": 12, "ed": 15, "text": "words and phrases"}, {"st": 17, "ed": 19, "text": "answer questions"}, {"st": 32, "ed": 34, "text": "partially observable"}, {"st": 70, "ed": 72, "text": "prior knowledge"}, {"st": 112, "ed": 114, "text": "future research"}, {"st": 131, "ed": 133, "text": "neural network"}, {"st": 135, "ed": 137, "text": "learning agent"}, {"st": 139, "ed": 141, "text": "policy gradient"}, {"st": 157, "ed": 159, "text": "developmental psychology"}, {"st": 189, "ed": 191, "text": "learning agents"}]
[{"st": 7, "ed": 9, "text": "machine learning"}, {"st": 18, "ed": 20, "text": "large scale"}, {"st": 20, "ed": 22, "text": "spoken language"}, {"st": 79, "ed": 81, "text": "inductive biases"}, {"st": 108, "ed": 110, "text": "dialogue systems"}]
[{"st": 23, "ed": 25, "text": "question answering"}, {"st": 103, "ed": 105, "text": "answer questions"}, {"st": 134, "ed": 136, "text": "pattern matching"}]
[{"st": 9, "ed": 11, "text": "artificial intelligence"}, {"st": 27, "ed": 29, "text": "structured data"}, {"st": 32, "ed": 34, "text": "unstructured text"}, {"st": 43, "ed": 45, "text": "neural network"}, {"st": 48, "ed": 50, "text": "natural language"}, {"st": 52, "ed": 54, "text": "technique called"}, {"st": 89, "ed": 91, "text": "word embedding"}, {"st": 129, "ed": 131, "text": "inductive reasoning"}, {"st": 167, "ed": 169, "text": "inductive reasoning"}, {"st": 173, "ed": 175, "text": "multi modal"}, {"st": 194, "ed": 196, "text": "relational databases"}]
[{"st": 0, "ed": 2, "text": "reinforcement learning"}, {"st": 9, "ed": 11, "text": "dialogue policy"}, {"st": 18, "ed": 21, "text": "scale to large"}, {"st": 74, "ed": 76, "text": "structural information"}, {"st": 127, "ed": 130, "text": "deep q networks"}, {"st": 130, "ed": 132, "text": "significantly outperforms"}]
[{"st": 21, "ed": 23, "text": "word level"}, {"st": 40, "ed": 42, "text": "character level"}, {"st": 56, "ed": 58, "text": "character level"}, {"st": 62, "ed": 64, "text": "word level"}]
[{"st": 7, "ed": 11, "text": "deep neural network dnn"}, {"st": 12, "ed": 15, "text": "automatic speech recognition"}, {"st": 17, "ed": 19, "text": "recognition accuracy"}, {"st": 73, "ed": 75, "text": "dnn based"}, {"st": 81, "ed": 84, "text": "word error rate"}, {"st": 93, "ed": 95, "text": "features extracted"}]
[{"st": 11, "ed": 13, "text": "character level"}, {"st": 26, "ed": 30, "text": "convolutional neural network cnn"}, {"st": 43, "ed": 46, "text": "short term memory"}, {"st": 47, "ed": 51, "text": "recurrent neural network language"}, {"st": 91, "ed": 93, "text": "word level"}, {"st": 117, "ed": 119, "text": "word representations"}]
[{"st": 3, "ed": 5, "text": "machine translation"}, {"st": 13, "ed": 15, "text": "machine translation"}, {"st": 38, "ed": 40, "text": "recently proposed"}, {"st": 56, "ed": 58, "text": "neural network"}, {"st": 71, "ed": 73, "text": "recently proposed"}, {"st": 73, "ed": 76, "text": "neural machine translation"}, {"st": 81, "ed": 83, "text": "encoder decoder"}, {"st": 87, "ed": 89, "text": "source sentence"}, {"st": 93, "ed": 95, "text": "fixed length"}, {"st": 119, "ed": 121, "text": "machine translation"}, {"st": 131, "ed": 133, "text": "text corpus"}, {"st": 150, "ed": 152, "text": "machine translation"}, {"st": 152, "ed": 154, "text": "evaluation metrics"}]
[{"st": 40, "ed": 43, "text": "end to end"}, {"st": 48, "ed": 50, "text": "method called"}, {"st": 66, "ed": 68, "text": "cross entropy"}, {"st": 68, "ed": 70, "text": "objective function"}, {"st": 104, "ed": 106, "text": "discriminative power"}, {"st": 128, "ed": 130, "text": "performance improvements"}]
[{"st": 2, "ed": 5, "text": "end to end"}, {"st": 10, "ed": 12, "text": "dialog state"}, {"st": 15, "ed": 17, "text": "accurately estimate"}, {"st": 18, "ed": 20, "text": "compact representation"}, {"st": 29, "ed": 31, "text": "noisy observations"}, {"st": 34, "ed": 36, "text": "speech recognition"}, {"st": 38, "ed": 40, "text": "natural language"}, {"st": 49, "ed": 51, "text": "dialog state"}, {"st": 58, "ed": 60, "text": "machine reading"}, {"st": 67, "ed": 70, "text": "end to end"}, {"st": 76, "ed": 78, "text": "neural network"}, {"st": 82, "ed": 84, "text": "proposed approach"}, {"st": 87, "ed": 89, "text": "dialog state"}, {"st": 107, "ed": 109, "text": "hidden state"}, {"st": 113, "ed": 115, "text": "question answering"}, {"st": 155, "ed": 157, "text": "question answering"}, {"st": 164, "ed": 166, "text": "encouraging results"}]
[{"st": 17, "ed": 19, "text": "statistical models"}, {"st": 49, "ed": 51, "text": "machine learning"}, {"st": 56, "ed": 58, "text": "extremely large"}, {"st": 59, "ed": 61, "text": "self organizing"}, {"st": 87, "ed": 89, "text": "newtonian mechanics"}]
[{"st": 3, "ed": 6, "text": "artificial neural networks"}, {"st": 16, "ed": 18, "text": "natural language"}, {"st": 26, "ed": 28, "text": "engineered features"}, {"st": 54, "ed": 56, "text": "random search"}, {"st": 65, "ed": 67, "text": "recent approaches"}, {"st": 72, "ed": 75, "text": "gaussian processes gps"}, {"st": 85, "ed": 87, "text": "near optimal"}, {"st": 87, "ed": 89, "text": "machine learning"}, {"st": 92, "ed": 94, "text": "previously published"}, {"st": 148, "ed": 150, "text": "natural language"}]
[{"st": 46, "ed": 48, "text": "recent advances"}, {"st": 66, "ed": 68, "text": "processing steps"}, {"st": 81, "ed": 83, "text": "natural language"}, {"st": 97, "ed": 99, "text": "source language"}, {"st": 101, "ed": 103, "text": "target language"}, {"st": 109, "ed": 111, "text": "target language"}, {"st": 113, "ed": 115, "text": "target language"}, {"st": 164, "ed": 167, "text": "hidden markov models"}, {"st": 183, "ed": 185, "text": "hmm based"}, {"st": 185, "ed": 187, "text": "speech synthesis"}]
[{"st": 6, "ed": 8, "text": "machine learning"}, {"st": 16, "ed": 18, "text": "textual entailment"}]
[{"st": 49, "ed": 51, "text": "de identification"}, {"st": 62, "ed": 64, "text": "neural network"}, {"st": 65, "ed": 67, "text": "de identification"}, {"st": 86, "ed": 88, "text": "engineered features"}, {"st": 105, "ed": 108, "text": "electronic health records"}, {"st": 120, "ed": 122, "text": "engineered features"}, {"st": 131, "ed": 133, "text": "neural network"}, {"st": 134, "ed": 136, "text": "de identification"}, {"st": 160, "ed": 162, "text": "de identification"}, {"st": 178, "ed": 180, "text": "real life"}, {"st": 191, "ed": 193, "text": "de identification"}]
[{"st": 3, "ed": 6, "text": "end to end"}, {"st": 6, "ed": 9, "text": "automatic speech recognition"}, {"st": 52, "ed": 55, "text": "end to end"}, {"st": 89, "ed": 91, "text": "benchmark tasks"}, {"st": 123, "ed": 125, "text": "output units"}, {"st": 129, "ed": 132, "text": "orders of magnitude"}, {"st": 155, "ed": 158, "text": "word error rate"}, {"st": 183, "ed": 185, "text": "a 4"}]
[{"st": 17, "ed": 22, "text": "long short term memory lstm"}, {"st": 27, "ed": 29, "text": "matrix factorization"}, {"st": 65, "ed": 67, "text": "lstm networks"}, {"st": 67, "ed": 69, "text": "significantly faster"}]
[{"st": 0, "ed": 3, "text": "named entity recognition"}, {"st": 13, "ed": 16, "text": "artificial neural networks"}, {"st": 22, "ed": 24, "text": "outperform existing"}, {"st": 46, "ed": 49, "text": "named entity recognition"}, {"st": 60, "ed": 62, "text": "web based"}, {"st": 62, "ed": 64, "text": "user interface"}]
[{"st": 6, "ed": 8, "text": "word level"}, {"st": 37, "ed": 39, "text": "fewer parameters"}]
[{"st": 33, "ed": 36, "text": "markov decision process"}, {"st": 40, "ed": 42, "text": "reinforcement learning"}, {"st": 109, "ed": 112, "text": "deep reinforcement learning"}]
[{"st": 57, "ed": 59, "text": "multi layer"}, {"st": 60, "ed": 62, "text": "embedding model"}, {"st": 85, "ed": 87, "text": "word level"}, {"st": 90, "ed": 92, "text": "large margin"}, {"st": 93, "ed": 95, "text": "multiple languages"}]
[{"st": 2, "ed": 5, "text": "multi task learning"}, {"st": 6, "ed": 9, "text": "semi supervised learning"}, {"st": 13, "ed": 15, "text": "embedding space"}, {"st": 31, "ed": 33, "text": "unlabelled data"}, {"st": 46, "ed": 48, "text": "classification tasks"}, {"st": 57, "ed": 59, "text": "multi task"}]
[{"st": 0, "ed": 2, "text": "neural network"}, {"st": 6, "ed": 8, "text": "attention mechanisms"}, {"st": 45, "ed": 47, "text": "question answering"}, {"st": 118, "ed": 121, "text": "visual question answering"}, {"st": 127, "ed": 129, "text": "question answering"}]
[{"st": 16, "ed": 18, "text": "textual description"}, {"st": 41, "ed": 43, "text": "feature space"}, {"st": 58, "ed": 60, "text": "feature space"}, {"st": 90, "ed": 92, "text": "neural network"}, {"st": 95, "ed": 97, "text": "visual representation"}, {"st": 100, "ed": 102, "text": "feature space"}, {"st": 117, "ed": 119, "text": "loss functions"}, {"st": 138, "ed": 140, "text": "feature mapping"}, {"st": 150, "ed": 152, "text": "higher level"}, {"st": 172, "ed": 174, "text": "preliminary results"}, {"st": 176, "ed": 178, "text": "ms coco"}]
[{"st": 2, "ed": 6, "text": "recurrent neural network language"}, {"st": 10, "ed": 12, "text": "caption generation"}, {"st": 20, "ed": 22, "text": "neural network"}, {"st": 36, "ed": 38, "text": "image features"}, {"st": 107, "ed": 109, "text": "hidden state"}, {"st": 129, "ed": 131, "text": "caption generation"}]
[{"st": 0, "ed": 2, "text": "visual question"}, {"st": 3, "ed": 5, "text": "answering vqa"}, {"st": 21, "ed": 23, "text": "computer vision"}, {"st": 40, "ed": 42, "text": "natural language"}, {"st": 54, "ed": 56, "text": "visual content"}, {"st": 79, "ed": 81, "text": "fine grained"}, {"st": 99, "ed": 101, "text": "image recognition"}, {"st": 113, "ed": 115, "text": "irrelevant features"}, {"st": 134, "ed": 136, "text": "image content"}, {"st": 198, "ed": 200, "text": "large scale"}, {"st": 200, "ed": 202, "text": "benchmark dataset"}]
[{"st": 3, "ed": 5, "text": "textual description"}, {"st": 14, "ed": 16, "text": "computer vision"}, {"st": 17, "ed": 19, "text": "natural language"}, {"st": 63, "ed": 65, "text": "image representation"}, {"st": 70, "ed": 73, "text": "convolutional neural network"}, {"st": 131, "ed": 133, "text": "achieves comparable"}]
[{"st": 6, "ed": 9, "text": "convolutional neural networks"}, {"st": 21, "ed": 24, "text": "end to end"}, {"st": 26, "ed": 28, "text": "convolutional architectures"}, {"st": 30, "ed": 32, "text": "image representation"}, {"st": 52, "ed": 54, "text": "image content"}, {"st": 60, "ed": 62, "text": "joint representation"}, {"st": 107, "ed": 110, "text": "image and sentence"}, {"st": 124, "ed": 127, "text": "image and sentence"}, {"st": 135, "ed": 138, "text": "image and sentence"}]
[{"st": 3, "ed": 5, "text": "question answering"}, {"st": 18, "ed": 20, "text": "natural language"}, {"st": 24, "ed": 26, "text": "neural networks"}, {"st": 37, "ed": 39, "text": "learned jointly"}, {"st": 44, "ed": 46, "text": "reinforcement learning"}, {"st": 49, "ed": 51, "text": "question answer"}, {"st": 71, "ed": 73, "text": "benchmark datasets"}]
[{"st": 11, "ed": 14, "text": "american sign language"}, {"st": 38, "ed": 40, "text": "proper nouns"}, {"st": 114, "ed": 117, "text": "conditional random fields"}, {"st": 122, "ed": 126, "text": "deep neural network dnn"}, {"st": 177, "ed": 179, "text": "frame level"}]
[{"st": 7, "ed": 9, "text": "image annotation"}, {"st": 10, "ed": 12, "text": "image retrieval"}, {"st": 16, "ed": 19, "text": "deep neural networks"}, {"st": 22, "ed": 24, "text": "image representation"}, {"st": 44, "ed": 46, "text": "network embedding"}, {"st": 52, "ed": 54, "text": "image representation"}, {"st": 74, "ed": 76, "text": "network embedding"}, {"st": 78, "ed": 80, "text": "multi scale"}, {"st": 95, "ed": 97, "text": "network embedding"}, {"st": 135, "ed": 137, "text": "image annotation"}, {"st": 138, "ed": 140, "text": "image retrieval"}, {"st": 145, "ed": 147, "text": "network embedding"}, {"st": 163, "ed": 165, "text": "network embedding"}]
[{"st": 2, "ed": 4, "text": "image captioning"}, {"st": 6, "ed": 9, "text": "recurrent neural network"}, {"st": 23, "ed": 25, "text": "image features"}, {"st": 80, "ed": 82, "text": "image features"}]
[{"st": 7, "ed": 9, "text": "fixed size"}, {"st": 27, "ed": 29, "text": "fixed size"}, {"st": 34, "ed": 36, "text": "word order"}, {"st": 59, "ed": 62, "text": "feedforward neural network"}, {"st": 82, "ed": 84, "text": "significantly outperform"}]
[{"st": 5, "ed": 7, "text": "learning representations"}, {"st": 27, "ed": 29, "text": "neural networks"}, {"st": 66, "ed": 68, "text": "continuous space"}, {"st": 92, "ed": 94, "text": "look ahead"}, {"st": 127, "ed": 129, "text": "standard backpropagation"}]
[{"st": 16, "ed": 19, "text": "labeled and unlabeled"}, {"st": 30, "ed": 32, "text": "neural network"}, {"st": 44, "ed": 46, "text": "modified version"}, {"st": 48, "ed": 50, "text": "recently proposed"}, {"st": 50, "ed": 52, "text": "ladder network"}, {"st": 53, "ed": 55, "text": "training procedure"}, {"st": 73, "ed": 75, "text": "successfully applied"}, {"st": 86, "ed": 89, "text": "labeled and unlabeled"}]
[{"st": 8, "ed": 10, "text": "large vocabulary"}, {"st": 10, "ed": 13, "text": "continuous speech recognition"}, {"st": 16, "ed": 18, "text": "neural network"}, {"st": 21, "ed": 24, "text": "deep neural network"}, {"st": 30, "ed": 32, "text": "hmm based"}, {"st": 32, "ed": 34, "text": "speech recognition"}, {"st": 42, "ed": 44, "text": "domain specific"}, {"st": 78, "ed": 82, "text": "recurrent neural network architecture"}, {"st": 91, "ed": 94, "text": "propose and evaluate"}, {"st": 107, "ed": 109, "text": "speech recognition"}, {"st": 120, "ed": 122, "text": "hmm based"}, {"st": 126, "ed": 129, "text": "wall street journal"}, {"st": 134, "ed": 136, "text": "error rates"}, {"st": 140, "ed": 142, "text": "bi directional"}]
[{"st": 12, "ed": 14, "text": "becoming increasingly"}, {"st": 33, "ed": 35, "text": "deep learning"}, {"st": 66, "ed": 68, "text": "text corpora"}, {"st": 150, "ed": 152, "text": "parameter settings"}, {"st": 166, "ed": 168, "text": "pre processing"}, {"st": 193, "ed": 195, "text": "efficient implementation"}, {"st": 207, "ed": 209, "text": "textual data"}, {"st": 243, "ed": 245, "text": "starting point"}]
[{"st": 3, "ed": 5, "text": "natural language"}, {"st": 8, "ed": 10, "text": "machine translation"}, {"st": 11, "ed": 13, "text": "question answering"}, {"st": 76, "ed": 79, "text": "deep neural network"}, {"st": 91, "ed": 93, "text": "learning algorithm"}, {"st": 116, "ed": 118, "text": "social media"}, {"st": 120, "ed": 122, "text": "matching problem"}, {"st": 139, "ed": 141, "text": "models including"}, {"st": 150, "ed": 152, "text": "word embedding"}]
[{"st": 0, "ed": 2, "text": "sentiment analysis"}, {"st": 7, "ed": 9, "text": "natural language"}, {"st": 32, "ed": 35, "text": "positive and negative"}, {"st": 42, "ed": 44, "text": "binary classification"}]
[{"st": 4, "ed": 6, "text": "neural network"}, {"st": 26, "ed": 28, "text": "low dimensional"}, {"st": 34, "ed": 36, "text": "higher dimensional"}, {"st": 41, "ed": 45, "text": "feed forward neural network"}]
[{"st": 0, "ed": 3, "text": "automatic speech recognition"}, {"st": 21, "ed": 23, "text": "prior knowledge"}, {"st": 25, "ed": 27, "text": "speech perception"}, {"st": 32, "ed": 35, "text": "convolutional neural networks"}, {"st": 44, "ed": 46, "text": "conditional probabilities"}, {"st": 57, "ed": 59, "text": "speech signal"}, {"st": 77, "ed": 79, "text": "phoneme recognition"}, {"st": 82, "ed": 84, "text": "large scale"}, {"st": 84, "ed": 87, "text": "continuous speech recognition"}, {"st": 101, "ed": 103, "text": "linear classifier"}, {"st": 112, "ed": 114, "text": "linearly separable"}]
[{"st": 7, "ed": 11, "text": "deep recurrent neural networks"}, {"st": 12, "ed": 14, "text": "natural language"}, {"st": 22, "ed": 24, "text": "representational power"}, {"st": 55, "ed": 57, "text": "recurrent networks"}]
[{"st": 7, "ed": 9, "text": "neural networks"}, {"st": 31, "ed": 34, "text": "feedforward neural networks"}, {"st": 83, "ed": 85, "text": "significantly outperform"}, {"st": 87, "ed": 90, "text": "feedforward neural network"}, {"st": 97, "ed": 101, "text": "recurrent neural network rnn"}]
[{"st": 7, "ed": 10, "text": "deep neural network"}, {"st": 16, "ed": 18, "text": "deep learning"}, {"st": 58, "ed": 61, "text": "automatic speech recognition"}, {"st": 65, "ed": 67, "text": "special case"}, {"st": 70, "ed": 72, "text": "structured learning"}, {"st": 109, "ed": 112, "text": "support vector machine"}, {"st": 120, "ed": 122, "text": "structured learning"}, {"st": 147, "ed": 149, "text": "deep learning"}, {"st": 159, "ed": 161, "text": "preliminary experiments"}]
[{"st": 3, "ed": 5, "text": "speech recognition"}, {"st": 16, "ed": 18, "text": "character level"}, {"st": 19, "ed": 21, "text": "speech recognition"}, {"st": 48, "ed": 52, "text": "recurrent neural network rnn"}, {"st": 54, "ed": 57, "text": "end to end"}, {"st": 65, "ed": 67, "text": "rnn based"}, {"st": 67, "ed": 69, "text": "character level"}, {"st": 82, "ed": 84, "text": "character level"}, {"st": 153, "ed": 157, "text": "word error rate wer"}, {"st": 161, "ed": 164, "text": "wall street journal"}]
[{"st": 7, "ed": 9, "text": "neural network"}, {"st": 16, "ed": 20, "text": "part of speech tagging"}, {"st": 31, "ed": 35, "text": "feed forward neural network"}, {"st": 39, "ed": 41, "text": "task specific"}, {"st": 44, "ed": 46, "text": "achieves comparable"}, {"st": 64, "ed": 66, "text": "key insight"}]
[{"st": 12, "ed": 15, "text": "deep neural network"}, {"st": 50, "ed": 53, "text": "convolutional neural network"}, {"st": 61, "ed": 63, "text": "multilayer perceptron"}, {"st": 69, "ed": 71, "text": "empirical evaluation"}, {"st": 74, "ed": 77, "text": "approach significantly outperforms"}]
[{"st": 16, "ed": 18, "text": "hidden units"}, {"st": 27, "ed": 29, "text": "random noise"}, {"st": 42, "ed": 44, "text": "hidden units"}, {"st": 76, "ed": 78, "text": "performance improvements"}, {"st": 81, "ed": 84, "text": "achieve competitive results"}, {"st": 91, "ed": 93, "text": "word level"}, {"st": 106, "ed": 108, "text": "batch normalization"}]
[{"st": 4, "ed": 7, "text": "task of classifying"}, {"st": 18, "ed": 20, "text": "hillary clinton"}, {"st": 42, "ed": 44, "text": "training data"}, {"st": 67, "ed": 69, "text": "training data"}, {"st": 153, "ed": 155, "text": "weak supervision"}, {"st": 158, "ed": 160, "text": "approach achieves"}]
[{"st": 43, "ed": 46, "text": "latent dirichlet allocation"}, {"st": 50, "ed": 52, "text": "data model"}, {"st": 57, "ed": 59, "text": "denoising autoencoder"}, {"st": 130, "ed": 132, "text": "detection algorithms"}, {"st": 134, "ed": 136, "text": "proposed approach"}]
[{"st": 11, "ed": 13, "text": "unstructured text"}, {"st": 14, "ed": 17, "text": "electronic health record"}, {"st": 30, "ed": 32, "text": "important applications"}, {"st": 45, "ed": 48, "text": "supervised machine learning"}, {"st": 55, "ed": 58, "text": "conditional random fields"}, {"st": 72, "ed": 75, "text": "recurrent neural network"}, {"st": 80, "ed": 82, "text": "significantly outperformed"}]
[{"st": 1, "ed": 4, "text": "deep neural network"}, {"st": 11, "ed": 14, "text": "feedforward neural network"}, {"st": 24, "ed": 26, "text": "hidden layers"}, {"st": 34, "ed": 37, "text": "deep neural networks"}, {"st": 47, "ed": 49, "text": "speech recognition"}, {"st": 60, "ed": 62, "text": "hidden layers"}, {"st": 72, "ed": 74, "text": "without sacrificing"}, {"st": 101, "ed": 103, "text": "speech recognition"}, {"st": 111, "ed": 113, "text": "speech recognition"}, {"st": 122, "ed": 124, "text": "cross entropy"}, {"st": 139, "ed": 141, "text": "hidden layers"}]
[{"st": 5, "ed": 7, "text": "complex nonlinear"}, {"st": 16, "ed": 19, "text": "recurrent neural networks"}, {"st": 24, "ed": 27, "text": "difficult to train"}, {"st": 30, "ed": 35, "text": "long short term memory lstm"}, {"st": 40, "ed": 42, "text": "theoretical analysis"}, {"st": 43, "ed": 45, "text": "recurrent networks"}, {"st": 78, "ed": 80, "text": "lstm architecture"}, {"st": 93, "ed": 95, "text": "experiments demonstrate"}, {"st": 120, "ed": 122, "text": "word level"}, {"st": 147, "ed": 149, "text": "previous results"}]
[{"st": 0, "ed": 2, "text": "current approaches"}, {"st": 73, "ed": 75, "text": "word vectors"}, {"st": 78, "ed": 80, "text": "source language"}, {"st": 85, "ed": 87, "text": "word vectors"}, {"st": 90, "ed": 92, "text": "target language"}, {"st": 99, "ed": 101, "text": "qualitative results"}, {"st": 113, "ed": 115, "text": "cross lingual"}]
[{"st": 0, "ed": 4, "text": "recurrent neural networks rnns"}, {"st": 20, "ed": 23, "text": "short term memory"}, {"st": 25, "ed": 29, "text": "gated recurrent unit gru"}, {"st": 44, "ed": 47, "text": "automatic speech recognition"}, {"st": 52, "ed": 54, "text": "visualization techniques"}, {"st": 64, "ed": 66, "text": "speech recognition"}, {"st": 83, "ed": 86, "text": "simple yet effective"}]
[{"st": 14, "ed": 16, "text": "large vocabulary"}, {"st": 16, "ed": 19, "text": "continuous speech recognition"}, {"st": 33, "ed": 35, "text": "100 000"}, {"st": 39, "ed": 41, "text": "bi directional"}, {"st": 41, "ed": 43, "text": "lstm rnns"}, {"st": 55, "ed": 57, "text": "semi supervised"}, {"st": 58, "ed": 60, "text": "training data"}, {"st": 84, "ed": 87, "text": "end to end"}, {"st": 89, "ed": 91, "text": "speech recognition"}, {"st": 97, "ed": 99, "text": "context dependent"}]
[{"st": 5, "ed": 7, "text": "unsupervised learning"}, {"st": 25, "ed": 28, "text": "encoder and decoder"}, {"st": 44, "ed": 46, "text": "fine tuned"}, {"st": 57, "ed": 59, "text": "machine translation"}, {"st": 66, "ed": 68, "text": "significantly improves"}, {"st": 73, "ed": 75, "text": "main result"}, {"st": 105, "ed": 107, "text": "phrase based"}, {"st": 107, "ed": 109, "text": "machine translation"}, {"st": 114, "ed": 116, "text": "method achieves"}, {"st": 117, "ed": 119, "text": "significant improvement"}, {"st": 149, "ed": 151, "text": "method outperforms"}, {"st": 153, "ed": 155, "text": "supervised learning"}, {"st": 158, "ed": 160, "text": "statistically significant"}]
[{"st": 32, "ed": 35, "text": "end to end"}, {"st": 103, "ed": 106, "text": "conditional random field"}, {"st": 122, "ed": 124, "text": "neural network"}, {"st": 142, "ed": 144, "text": "attention models"}, {"st": 148, "ed": 151, "text": "synthetic and real"}, {"st": 154, "ed": 157, "text": "neural machine translation"}, {"st": 157, "ed": 159, "text": "question answering"}, {"st": 160, "ed": 162, "text": "natural language"}]
[{"st": 3, "ed": 5, "text": "multi view"}, {"st": 24, "ed": 26, "text": "soft attention"}, {"st": 41, "ed": 44, "text": "bag of words"}, {"st": 86, "ed": 89, "text": "depth and width"}, {"st": 96, "ed": 98, "text": "multi view"}]
[{"st": 4, "ed": 6, "text": "continuous relaxation"}, {"st": 35, "ed": 37, "text": "training procedure"}, {"st": 53, "ed": 55, "text": "training objective"}, {"st": 97, "ed": 99, "text": "approach outperforms"}, {"st": 99, "ed": 101, "text": "cross entropy"}, {"st": 108, "ed": 110, "text": "sequence prediction"}, {"st": 111, "ed": 114, "text": "named entity recognition"}]
[{"st": 6, "ed": 8, "text": "lstm rnn"}, {"st": 11, "ed": 13, "text": "great potential"}, {"st": 60, "ed": 62, "text": "deep lstm"}, {"st": 70, "ed": 72, "text": "rnn based"}, {"st": 108, "ed": 110, "text": "experiments conducted"}]
[{"st": 10, "ed": 12, "text": "recent years"}, {"st": 29, "ed": 32, "text": "taking into account"}, {"st": 54, "ed": 56, "text": "previous research"}, {"st": 67, "ed": 69, "text": "neural architecture"}]
[{"st": 9, "ed": 13, "text": "rectified linear unit relu"}, {"st": 16, "ed": 19, "text": "rectified linear unit"}, {"st": 28, "ed": 31, "text": "positive and negative"}, {"st": 43, "ed": 45, "text": "activation function"}, {"st": 51, "ed": 54, "text": "recurrent neural networks"}, {"st": 69, "ed": 72, "text": "vanishing gradient problem"}, {"st": 106, "ed": 109, "text": "short term memory"}, {"st": 112, "ed": 114, "text": "sentiment classification"}, {"st": 115, "ed": 117, "text": "word level"}, {"st": 123, "ed": 125, "text": "character level"}, {"st": 154, "ed": 156, "text": "character level"}]
[{"st": 1, "ed": 4, "text": "deep neural networks"}, {"st": 6, "ed": 8, "text": "training samples"}, {"st": 41, "ed": 43, "text": "weak supervision"}, {"st": 77, "ed": 79, "text": "weakly labeled"}, {"st": 119, "ed": 121, "text": "semi supervised"}, {"st": 126, "ed": 129, "text": "deep neural networks"}, {"st": 130, "ed": 132, "text": "weakly labeled"}, {"st": 195, "ed": 197, "text": "natural language"}, {"st": 206, "ed": 208, "text": "semi supervised"}]
[{"st": 0, "ed": 2, "text": "recent studies"}, {"st": 5, "ed": 8, "text": "deep neural networks"}, {"st": 16, "ed": 19, "text": "gaussian mixture models"}, {"st": 21, "ed": 23, "text": "large vocabulary"}, {"st": 23, "ed": 25, "text": "speech recognition"}, {"st": 33, "ed": 35, "text": "improved accuracy"}, {"st": 48, "ed": 50, "text": "internal representations"}, {"st": 71, "ed": 73, "text": "small perturbations"}, {"st": 78, "ed": 80, "text": "network depth"}, {"st": 84, "ed": 86, "text": "speech recognition"}, {"st": 110, "ed": 112, "text": "training data"}, {"st": 137, "ed": 139, "text": "dnn based"}]
[{"st": 2, "ed": 5, "text": "hidden markov model"}, {"st": 5, "ed": 8, "text": "artificial neural networks"}, {"st": 10, "ed": 13, "text": "automatic speech recognition"}, {"st": 17, "ed": 19, "text": "class conditional"}, {"st": 29, "ed": 31, "text": "speech signal"}, {"st": 33, "ed": 35, "text": "prior knowledge"}, {"st": 37, "ed": 39, "text": "speech perception"}, {"st": 41, "ed": 43, "text": "speech production"}, {"st": 53, "ed": 55, "text": "recent advances"}, {"st": 56, "ed": 58, "text": "machine learning"}, {"st": 65, "ed": 67, "text": "image processing"}, {"st": 68, "ed": 70, "text": "text processing"}, {"st": 74, "ed": 77, "text": "divide and conquer"}, {"st": 80, "ed": 82, "text": "feature extraction"}, {"st": 97, "ed": 101, "text": "convolutional neural networks cnns"}, {"st": 115, "ed": 117, "text": "speech signal"}, {"st": 122, "ed": 124, "text": "class conditional"}, {"st": 128, "ed": 130, "text": "phoneme recognition"}, {"st": 145, "ed": 147, "text": "proposed approach"}, {"st": 168, "ed": 170, "text": "proposed approach"}, {"st": 175, "ed": 177, "text": "phoneme recognition"}]
[{"st": 3, "ed": 5, "text": "neural networks"}, {"st": 19, "ed": 21, "text": "open question"}, {"st": 23, "ed": 25, "text": "fixed length"}, {"st": 92, "ed": 95, "text": "ability to learn"}, {"st": 132, "ed": 134, "text": "simulated data"}]
[{"st": 27, "ed": 30, "text": "convolutional neural network"}, {"st": 52, "ed": 54, "text": "neural network"}, {"st": 59, "ed": 61, "text": "pooling layers"}, {"st": 70, "ed": 72, "text": "feature maps"}, {"st": 88, "ed": 90, "text": "discriminative model"}]
[{"st": 0, "ed": 2, "text": "recent research"}, {"st": 4, "ed": 7, "text": "deep neural networks"}]
[{"st": 1, "ed": 3, "text": "deep cnns"}, {"st": 14, "ed": 16, "text": "strong performance"}, {"st": 23, "ed": 25, "text": "speech recognition"}, {"st": 102, "ed": 104, "text": "batch normalization"}, {"st": 114, "ed": 116, "text": "batch normalization"}, {"st": 143, "ed": 145, "text": "larger scale"}, {"st": 154, "ed": 156, "text": "deep cnn"}, {"st": 166, "ed": 169, "text": "word error rate"}]
[{"st": 0, "ed": 4, "text": "recurrent neural networks rnns"}, {"st": 5, "ed": 10, "text": "long short term memory lstm"}, {"st": 22, "ed": 24, "text": "speech recognition"}, {"st": 57, "ed": 59, "text": "low rank"}, {"st": 61, "ed": 63, "text": "parameter sharing"}, {"st": 94, "ed": 96, "text": "low rank"}]
[{"st": 8, "ed": 11, "text": "recurrent neural networks"}, {"st": 12, "ed": 17, "text": "long short term memory lstm"}, {"st": 21, "ed": 24, "text": "feedforward neural network"}]
[{"st": 10, "ed": 12, "text": "hot topic"}, {"st": 14, "ed": 16, "text": "natural language"}, {"st": 19, "ed": 22, "text": "recurrent neural networks"}, {"st": 24, "ed": 27, "text": "short term memory"}, {"st": 32, "ed": 35, "text": "ability to capture"}, {"st": 35, "ed": 38, "text": "long term memory"}, {"st": 39, "ed": 41, "text": "lstm rnn"}, {"st": 59, "ed": 61, "text": "hidden layer"}, {"st": 76, "ed": 78, "text": "lstm rnn"}, {"st": 82, "ed": 84, "text": "weakly supervised"}, {"st": 140, "ed": 142, "text": "lstm rnn"}, {"st": 181, "ed": 183, "text": "lstm rnn"}, {"st": 213, "ed": 215, "text": "embedding vectors"}, {"st": 222, "ed": 224, "text": "web search"}, {"st": 226, "ed": 228, "text": "lstm rnn"}, {"st": 232, "ed": 234, "text": "significantly outperform"}, {"st": 249, "ed": 251, "text": "embedding vectors"}, {"st": 280, "ed": 282, "text": "proposed method"}, {"st": 285, "ed": 287, "text": "significantly outperforms"}]
[{"st": 1, "ed": 3, "text": "recently proposed"}, {"st": 3, "ed": 5, "text": "neural network"}, {"st": 14, "ed": 16, "text": "n gram"}, {"st": 16, "ed": 18, "text": "target language"}, {"st": 51, "ed": 53, "text": "convolutional architecture"}, {"st": 65, "ed": 67, "text": "specifically designed"}, {"st": 76, "ed": 78, "text": "source sentence"}, {"st": 94, "ed": 96, "text": "source sentence"}, {"st": 105, "ed": 107, "text": "target language"}, {"st": 112, "ed": 115, "text": "deep neural network"}, {"st": 136, "ed": 138, "text": "significant improvements"}, {"st": 146, "ed": 148, "text": "bleu points"}]
[{"st": 12, "ed": 14, "text": "context dependent"}, {"st": 14, "ed": 17, "text": "deep neural network"}, {"st": 17, "ed": 20, "text": "hidden markov model"}, {"st": 21, "ed": 23, "text": "dnn hmm"}, {"st": 25, "ed": 28, "text": "automatic speech recognition"}, {"st": 57, "ed": 60, "text": "a posteriori map"}, {"st": 68, "ed": 70, "text": "dnn hmm"}, {"st": 88, "ed": 90, "text": "feature space"}, {"st": 91, "ed": 93, "text": "linear regression"}, {"st": 104, "ed": 107, "text": "wall street journal"}, {"st": 127, "ed": 129, "text": "relative error"}, {"st": 131, "ed": 133, "text": "consistently outperforms"}]
[{"st": 9, "ed": 12, "text": "statistical machine translation"}, {"st": 15, "ed": 18, "text": "convolutional neural network"}, {"st": 32, "ed": 34, "text": "specifically designed"}, {"st": 34, "ed": 36, "text": "convolutional architecture"}, {"st": 64, "ed": 66, "text": "context dependent"}, {"st": 74, "ed": 76, "text": "curriculum learning"}, {"st": 84, "ed": 86, "text": "training examples"}, {"st": 101, "ed": 103, "text": "sentence level"}, {"st": 106, "ed": 108, "text": "training examples"}, {"st": 117, "ed": 120, "text": "approach significantly outperforms"}]
[{"st": 8, "ed": 10, "text": "natural language"}, {"st": 42, "ed": 45, "text": "convolutional neural network"}, {"st": 73, "ed": 76, "text": "layer by layer"}, {"st": 96, "ed": 98, "text": "prior knowledge"}, {"st": 116, "ed": 118, "text": "empirical study"}]
[{"st": 4, "ed": 7, "text": "short term memory"}, {"st": 21, "ed": 23, "text": "speech recognition"}, {"st": 100, "ed": 102, "text": "natural language"}]
[{"st": 4, "ed": 6, "text": "attention mechanism"}, {"st": 7, "ed": 10, "text": "neural machine translation"}, {"st": 16, "ed": 18, "text": "machine translation"}, {"st": 52, "ed": 54, "text": "tamil language"}, {"st": 79, "ed": 81, "text": "bleu points"}, {"st": 133, "ed": 136, "text": "neural machine translation"}, {"st": 138, "ed": 140, "text": "bleu points"}, {"st": 183, "ed": 185, "text": "human evaluation"}]
[{"st": 5, "ed": 7, "text": "gated recurrent"}, {"st": 7, "ed": 9, "text": "neural network"}, {"st": 26, "ed": 28, "text": "word level"}, {"st": 55, "ed": 57, "text": "dynamical systems"}]
[{"st": 12, "ed": 14, "text": "neural network"}, {"st": 26, "ed": 28, "text": "recent advances"}, {"st": 29, "ed": 31, "text": "deep learning"}, {"st": 66, "ed": 68, "text": "deep learning"}, {"st": 95, "ed": 98, "text": "convolutional neural network"}, {"st": 104, "ed": 106, "text": "convolutional architectures"}, {"st": 110, "ed": 112, "text": "computer vision"}, {"st": 113, "ed": 115, "text": "text processing"}, {"st": 125, "ed": 127, "text": "recent years"}, {"st": 129, "ed": 131, "text": "speech processing"}, {"st": 141, "ed": 144, "text": "end to end"}, {"st": 153, "ed": 155, "text": "similar performance"}, {"st": 172, "ed": 174, "text": "hand crafted"}]
[{"st": 13, "ed": 15, "text": "vocal tract"}, {"st": 31, "ed": 33, "text": "speech synthesis"}, {"st": 58, "ed": 60, "text": "deep learning"}, {"st": 81, "ed": 83, "text": "low dimensional"}, {"st": 96, "ed": 98, "text": "denoising autoencoder"}, {"st": 112, "ed": 116, "text": "multi layer perceptron mlp"}, {"st": 120, "ed": 122, "text": "fine tuned"}, {"st": 171, "ed": 173, "text": "experiments conducted"}]
[{"st": 0, "ed": 4, "text": "neural machine translation nmt"}, {"st": 9, "ed": 11, "text": "machine translation"}, {"st": 13, "ed": 16, "text": "shown promising results"}, {"st": 38, "ed": 41, "text": "end to end"}, {"st": 125, "ed": 127, "text": "post processing"}, {"st": 152, "ed": 154, "text": "substantial improvement"}, {"st": 158, "ed": 160, "text": "bleu points"}, {"st": 173, "ed": 175, "text": "bleu points"}]
[{"st": 11, "ed": 13, "text": "neural network"}, {"st": 33, "ed": 35, "text": "word vectors"}]
[{"st": 7, "ed": 9, "text": "speech recognition"}, {"st": 12, "ed": 15, "text": "end to end"}, {"st": 39, "ed": 41, "text": "perform poorly"}, {"st": 53, "ed": 55, "text": "hand designed"}, {"st": 135, "ed": 137, "text": "previously published"}, {"st": 140, "ed": 142, "text": "widely studied"}]
[{"st": 5, "ed": 7, "text": "neural network"}, {"st": 16, "ed": 18, "text": "speech recognition"}, {"st": 42, "ed": 44, "text": "neural network"}, {"st": 105, "ed": 107, "text": "significant improvements"}]
[{"st": 8, "ed": 10, "text": "neural networks"}, {"st": 26, "ed": 28, "text": "jointly trained"}]
[{"st": 7, "ed": 10, "text": "deep neural networks"}, {"st": 24, "ed": 27, "text": "neural language models"}, {"st": 31, "ed": 33, "text": "word embeddings"}, {"st": 60, "ed": 62, "text": "d dimensional"}, {"st": 67, "ed": 69, "text": "hidden layer"}, {"st": 80, "ed": 82, "text": "computational cost"}, {"st": 93, "ed": 95, "text": "weight matrix"}, {"st": 138, "ed": 140, "text": "sampling based"}, {"st": 157, "ed": 159, "text": "loss functions"}, {"st": 161, "ed": 163, "text": "squared error"}, {"st": 197, "ed": 199, "text": "d dimensional"}, {"st": 201, "ed": 203, "text": "proposed algorithm"}, {"st": 211, "ed": 214, "text": "orders of magnitude"}]
[{"st": 6, "ed": 9, "text": "convolutional neural network"}, {"st": 50, "ed": 52, "text": "output layer"}, {"st": 60, "ed": 62, "text": "feature learning"}, {"st": 71, "ed": 73, "text": "sentiment analysis"}, {"st": 80, "ed": 82, "text": "outperforms previous"}, {"st": 89, "ed": 91, "text": "neural networks"}]
[{"st": 16, "ed": 18, "text": "central role"}, {"st": 19, "ed": 21, "text": "natural language"}, {"st": 83, "ed": 85, "text": "learning task"}, {"st": 100, "ed": 103, "text": "qualitative and quantitative"}, {"st": 103, "ed": 105, "text": "analysis shows"}, {"st": 124, "ed": 126, "text": "classification performance"}]
[{"st": 0, "ed": 2, "text": "relation classification"}, {"st": 26, "ed": 28, "text": "relation classification"}, {"st": 31, "ed": 34, "text": "convolutional neural network"}, {"st": 47, "ed": 49, "text": "ranking loss"}, {"st": 77, "ed": 80, "text": "task of classifying"}, {"st": 143, "ed": 146, "text": "precision and recall"}, {"st": 150, "ed": 152, "text": "word embeddings"}, {"st": 153, "ed": 155, "text": "input features"}]
[{"st": 12, "ed": 14, "text": "source sentence"}, {"st": 24, "ed": 27, "text": "deep neural networks"}, {"st": 31, "ed": 33, "text": "maximum entropy"}, {"st": 94, "ed": 96, "text": "bleu points"}]
[{"st": 0, "ed": 4, "text": "recurrent neural networks rnns"}, {"st": 10, "ed": 13, "text": "short term memory"}, {"st": 22, "ed": 24, "text": "successful applications"}, {"st": 29, "ed": 31, "text": "machine learning"}, {"st": 57, "ed": 59, "text": "character level"}, {"st": 111, "ed": 113, "text": "finite horizon"}, {"st": 113, "ed": 115, "text": "n gram"}]
[{"st": 39, "ed": 41, "text": "multi layered"}, {"st": 116, "ed": 118, "text": "low level"}, {"st": 121, "ed": 123, "text": "features extracted"}, {"st": 143, "ed": 145, "text": "multi layered"}, {"st": 147, "ed": 150, "text": "deep neural network"}]
[{"st": 0, "ed": 4, "text": "recurrent neural networks rnns"}, {"st": 42, "ed": 44, "text": "output layer"}, {"st": 46, "ed": 48, "text": "character level"}]
[{"st": 5, "ed": 7, "text": "deep architecture"}, {"st": 27, "ed": 29, "text": "input sequence"}, {"st": 35, "ed": 37, "text": "final output"}, {"st": 46, "ed": 48, "text": "recently proposed"}, {"st": 97, "ed": 100, "text": "layer by layer"}, {"st": 113, "ed": 115, "text": "machine translation"}, {"st": 125, "ed": 127, "text": "back propagation"}, {"st": 164, "ed": 166, "text": "special case"}, {"st": 180, "ed": 182, "text": "neural network"}, {"st": 190, "ed": 192, "text": "phrase based"}, {"st": 192, "ed": 194, "text": "machine translation"}]
[{"st": 3, "ed": 5, "text": "natural language"}, {"st": 10, "ed": 12, "text": "question answering"}, {"st": 25, "ed": 27, "text": "neural network"}, {"st": 83, "ed": 87, "text": "trained end to end"}, {"st": 101, "ed": 103, "text": "question answering"}, {"st": 107, "ed": 109, "text": "text classification"}, {"st": 110, "ed": 112, "text": "sentiment analysis"}, {"st": 119, "ed": 123, "text": "part of speech tagging"}, {"st": 141, "ed": 143, "text": "question answer"}]
[{"st": 1, "ed": 3, "text": "deep learning"}, {"st": 16, "ed": 19, "text": "deep neural network"}, {"st": 29, "ed": 31, "text": "speaker recognition"}, {"st": 35, "ed": 37, "text": "performance gains"}, {"st": 68, "ed": 70, "text": "deep learning"}, {"st": 86, "ed": 90, "text": "dynamic time warping dtw"}, {"st": 96, "ed": 98, "text": "speaker recognition"}, {"st": 107, "ed": 109, "text": "performance improvement"}]
[{"st": 5, "ed": 8, "text": "short term memory"}, {"st": 26, "ed": 28, "text": "higher dimensional"}, {"st": 37, "ed": 39, "text": "deep lstm"}, {"st": 47, "ed": 49, "text": "network layers"}, {"st": 95, "ed": 97, "text": "significantly outperform"}, {"st": 157, "ed": 159, "text": "phrase based"}]
[{"st": 0, "ed": 2, "text": "previous research"}, {"st": 3, "ed": 5, "text": "relation classification"}, {"st": 77, "ed": 79, "text": "neural networks"}, {"st": 82, "ed": 84, "text": "neural network"}, {"st": 91, "ed": 94, "text": "convolutional neural network"}, {"st": 98, "ed": 100, "text": "important features"}, {"st": 113, "ed": 115, "text": "proposed method"}]
[{"st": 1, "ed": 3, "text": "text embedding"}, {"st": 6, "ed": 8, "text": "skip gram"}, {"st": 14, "ed": 16, "text": "increasing attention"}, {"st": 27, "ed": 29, "text": "deep learning"}, {"st": 32, "ed": 35, "text": "convolutional neural networks"}, {"st": 45, "ed": 47, "text": "machine learning"}, {"st": 54, "ed": 56, "text": "text embedding"}, {"st": 78, "ed": 80, "text": "low dimensional"}, {"st": 106, "ed": 108, "text": "semi supervised"}, {"st": 108, "ed": 110, "text": "representation learning"}, {"st": 112, "ed": 114, "text": "text data"}, {"st": 119, "ed": 121, "text": "predictive text"}, {"st": 124, "ed": 126, "text": "predictive text"}, {"st": 129, "ed": 132, "text": "labeled and unlabeled"}, {"st": 146, "ed": 149, "text": "word co occurrence"}, {"st": 155, "ed": 157, "text": "large scale"}, {"st": 166, "ed": 168, "text": "low dimensional"}, {"st": 176, "ed": 178, "text": "low dimensional"}, {"st": 194, "ed": 196, "text": "predictive power"}, {"st": 203, "ed": 205, "text": "supervised approaches"}, {"st": 207, "ed": 210, "text": "convolutional neural networks"}, {"st": 210, "ed": 212, "text": "predictive text"}, {"st": 223, "ed": 225, "text": "fewer parameters"}]
[{"st": 0, "ed": 2, "text": "deep learning"}, {"st": 7, "ed": 9, "text": "sentence level"}, {"st": 13, "ed": 17, "text": "convolutional neural networks cnn"}, {"st": 19, "ed": 21, "text": "competitive performance"}, {"st": 25, "ed": 27, "text": "feature engineering"}, {"st": 57, "ed": 59, "text": "cnn based"}, {"st": 86, "ed": 89, "text": "recurrent neural networks"}, {"st": 130, "ed": 132, "text": "rnn based"}, {"st": 138, "ed": 140, "text": "relation classification"}, {"st": 156, "ed": 158, "text": "real world"}]
[{"st": 0, "ed": 3, "text": "latent dirichlet allocation"}, {"st": 8, "ed": 11, "text": "hierarchical bayesian model"}, {"st": 18, "ed": 20, "text": "great success"}, {"st": 22, "ed": 24, "text": "latent topic"}, {"st": 33, "ed": 35, "text": "transfer learning"}, {"st": 54, "ed": 57, "text": "deep neural network"}, {"st": 75, "ed": 77, "text": "document classification"}]
[{"st": 6, "ed": 8, "text": "online learning"}, {"st": 9, "ed": 13, "text": "recurrent neural network language"}, {"st": 40, "ed": 42, "text": "promising results"}]
[{"st": 0, "ed": 4, "text": "convolutional neural networks cnns"}, {"st": 5, "ed": 7, "text": "recently achieved"}, {"st": 8, "ed": 10, "text": "strong performance"}, {"st": 16, "ed": 18, "text": "sentence classification"}, {"st": 74, "ed": 76, "text": "sensitivity analysis"}, {"st": 116, "ed": 118, "text": "complex models"}, {"st": 125, "ed": 127, "text": "empirical performance"}, {"st": 133, "ed": 135, "text": "baseline method"}, {"st": 137, "ed": 140, "text": "support vector machine"}, {"st": 150, "ed": 152, "text": "extensive empirical"}, {"st": 164, "ed": 166, "text": "sentence classification"}, {"st": 167, "ed": 169, "text": "real world"}]
[{"st": 11, "ed": 14, "text": "recurrent neural networks"}, {"st": 30, "ed": 32, "text": "neural networks"}, {"st": 78, "ed": 80, "text": "neural networks"}, {"st": 87, "ed": 89, "text": "transfer learning"}, {"st": 97, "ed": 99, "text": "target language"}, {"st": 101, "ed": 103, "text": "improve performance"}]
[{"st": 12, "ed": 16, "text": "deep q network dqn"}, {"st": 19, "ed": 21, "text": "output sequence"}, {"st": 51, "ed": 53, "text": "encoder decoder"}, {"st": 53, "ed": 58, "text": "long short term memory lstm"}, {"st": 64, "ed": 66, "text": "input sequence"}, {"st": 127, "ed": 129, "text": "output sequence"}, {"st": 150, "ed": 152, "text": "reinforcement learning"}, {"st": 195, "ed": 197, "text": "proposed method"}, {"st": 205, "ed": 207, "text": "training set"}, {"st": 208, "ed": 210, "text": "significantly outperformed"}]
[{"st": 6, "ed": 8, "text": "neural network"}, {"st": 33, "ed": 35, "text": "regularization methods"}, {"st": 41, "ed": 44, "text": "deep neural networks"}, {"st": 62, "ed": 64, "text": "activation functions"}, {"st": 69, "ed": 72, "text": "recurrent neural networks"}]
[{"st": 5, "ed": 7, "text": "achieved impressive"}, {"st": 44, "ed": 46, "text": "output sequence"}, {"st": 90, "ed": 92, "text": "partially observed"}, {"st": 92, "ed": 94, "text": "input sequence"}, {"st": 159, "ed": 161, "text": "dynamic programming"}, {"st": 204, "ed": 206, "text": "attention mechanisms"}]
[{"st": 0, "ed": 2, "text": "question answering"}, {"st": 6, "ed": 8, "text": "natural language"}, {"st": 25, "ed": 27, "text": "weakly supervised"}, {"st": 97, "ed": 100, "text": "end to end"}, {"st": 115, "ed": 117, "text": "skip thought"}, {"st": 127, "ed": 129, "text": "skip thought"}, {"st": 141, "ed": 143, "text": "skip thought"}]
[{"st": 0, "ed": 3, "text": "named entity recognition"}, {"st": 5, "ed": 7, "text": "challenging task"}, {"st": 19, "ed": 21, "text": "feature engineering"}, {"st": 24, "ed": 26, "text": "achieve high"}, {"st": 34, "ed": 36, "text": "neural network"}, {"st": 42, "ed": 44, "text": "character level"}, {"st": 48, "ed": 50, "text": "bidirectional lstm"}, {"st": 72, "ed": 74, "text": "neural networks"}, {"st": 81, "ed": 83, "text": "evaluation shows"}, {"st": 91, "ed": 93, "text": "word embeddings"}, {"st": 105, "ed": 107, "text": "previously reported"}, {"st": 156, "ed": 158, "text": "feature engineering"}]
[{"st": 6, "ed": 8, "text": "encoder decoder"}, {"st": 8, "ed": 11, "text": "recurrent neural network"}, {"st": 12, "ed": 14, "text": "lstm units"}, {"st": 43, "ed": 45, "text": "neural network"}, {"st": 71, "ed": 73, "text": "attention mechanism"}, {"st": 79, "ed": 81, "text": "attention mechanism"}]
[{"st": 4, "ed": 6, "text": "distributional semantics"}, {"st": 9, "ed": 11, "text": "word vectors"}, {"st": 21, "ed": 23, "text": "benchmark tasks"}, {"st": 25, "ed": 27, "text": "sentiment classification"}, {"st": 91, "ed": 93, "text": "tf idf"}, {"st": 99, "ed": 101, "text": "significant improvement"}, {"st": 115, "ed": 117, "text": "ensemble based"}, {"st": 123, "ed": 125, "text": "rnn model"}, {"st": 129, "ed": 131, "text": "performance improvement"}, {"st": 158, "ed": 160, "text": "unsupervised manner"}]
[{"st": 1, "ed": 3, "text": "speech recognition"}, {"st": 3, "ed": 6, "text": "deep neural networks"}, {"st": 8, "ed": 10, "text": "significantly improved"}, {"st": 11, "ed": 13, "text": "recognition accuracy"}, {"st": 16, "ed": 18, "text": "benchmark datasets"}, {"st": 26, "ed": 29, "text": "gaussian mixture models"}, {"st": 29, "ed": 31, "text": "dnn based"}, {"st": 37, "ed": 41, "text": "number of model parameters"}, {"st": 48, "ed": 50, "text": "resource constrained"}, {"st": 63, "ed": 65, "text": "recently proposed"}, {"st": 81, "ed": 83, "text": "significantly smaller"}, {"st": 83, "ed": 87, "text": "number of model parameters"}, {"st": 112, "ed": 114, "text": "neural networks"}, {"st": 122, "ed": 126, "text": "number of model parameters"}, {"st": 130, "ed": 132, "text": "without sacrificing"}]
[{"st": 7, "ed": 11, "text": "recurrent neural networks rnns"}, {"st": 16, "ed": 18, "text": "natural language"}, {"st": 26, "ed": 28, "text": "applications including"}, {"st": 28, "ed": 30, "text": "machine translation"}, {"st": 31, "ed": 33, "text": "question answering"}, {"st": 36, "ed": 38, "text": "existing methods"}, {"st": 41, "ed": 43, "text": "joint probability"}, {"st": 47, "ed": 49, "text": "additional information"}, {"st": 85, "ed": 87, "text": "existing approaches"}]
[{"st": 6, "ed": 8, "text": "context aware"}, {"st": 8, "ed": 10, "text": "keyword spotting"}, {"st": 13, "ed": 15, "text": "character level"}, {"st": 15, "ed": 19, "text": "recurrent neural network rnn"}, {"st": 29, "ed": 32, "text": "end to end"}, {"st": 54, "ed": 56, "text": "phonetic transcription"}, {"st": 76, "ed": 78, "text": "text based"}, {"st": 103, "ed": 105, "text": "low latency"}, {"st": 118, "ed": 120, "text": "significantly outperforms"}, {"st": 121, "ed": 124, "text": "deep neural network"}, {"st": 126, "ed": 130, "text": "hidden markov model hmm"}]
[{"st": 0, "ed": 2, "text": "authorship attribution"}, {"st": 56, "ed": 58, "text": "neural networks"}, {"st": 60, "ed": 63, "text": "curse of dimensionality"}, {"st": 67, "ed": 69, "text": "n gram"}, {"st": 92, "ed": 94, "text": "neural network"}, {"st": 121, "ed": 123, "text": "set size"}, {"st": 126, "ed": 129, "text": "training and test"}, {"st": 171, "ed": 173, "text": "classification accuracy"}, {"st": 180, "ed": 182, "text": "n gram"}, {"st": 187, "ed": 189, "text": "open source"}, {"st": 195, "ed": 199, "text": "available at https github.com"}, {"st": 200, "ed": 202, "text": "authorship attribution"}]
[{"st": 4, "ed": 7, "text": "recurrent neural network"}, {"st": 8, "ed": 11, "text": "end to end"}, {"st": 18, "ed": 21, "text": "conditional random field"}, {"st": 24, "ed": 28, "text": "recurrent neural network rnn"}, {"st": 127, "ed": 129, "text": "error rate"}]
[{"st": 0, "ed": 2, "text": "transfer learning"}, {"st": 12, "ed": 14, "text": "source domain"}, {"st": 27, "ed": 29, "text": "neural networks"}, {"st": 40, "ed": 42, "text": "image processing"}, {"st": 49, "ed": 51, "text": "neural network"}, {"st": 64, "ed": 66, "text": "transfer learning"}, {"st": 76, "ed": 78, "text": "case studies"}, {"st": 87, "ed": 89, "text": "neural networks"}]
[{"st": 4, "ed": 7, "text": "recurrent neural network"}, {"st": 12, "ed": 14, "text": "question answering"}, {"st": 46, "ed": 48, "text": "neural attention"}, {"st": 73, "ed": 75, "text": "transfer learning"}, {"st": 98, "ed": 100, "text": "based approach"}, {"st": 101, "ed": 103, "text": "achieve comparable"}, {"st": 108, "ed": 110, "text": "feature based"}]
[{"st": 6, "ed": 8, "text": "distributed representations"}, {"st": 43, "ed": 45, "text": "representation learning"}, {"st": 53, "ed": 55, "text": "promising performance"}, {"st": 59, "ed": 61, "text": "feature representations"}, {"st": 114, "ed": 116, "text": "significant improvement"}]
[{"st": 6, "ed": 10, "text": "recurrent neural networks rnns"}, {"st": 33, "ed": 35, "text": "speech recognition"}, {"st": 83, "ed": 86, "text": "short term memory"}]
[{"st": 9, "ed": 11, "text": "important issue"}, {"st": 29, "ed": 31, "text": "deep learning"}, {"st": 47, "ed": 49, "text": "neural network"}, {"st": 78, "ed": 80, "text": "source sentence"}, {"st": 153, "ed": 156, "text": "neural machine translation"}, {"st": 162, "ed": 164, "text": "parallel corpora"}, {"st": 165, "ed": 167, "text": "text summarization"}]
[{"st": 0, "ed": 2, "text": "deep learning"}, {"st": 8, "ed": 10, "text": "speech recognition"}, {"st": 24, "ed": 27, "text": "end to end"}, {"st": 59, "ed": 61, "text": "convolutional filters"}, {"st": 79, "ed": 81, "text": "computational cost"}, {"st": 85, "ed": 87, "text": "temporal resolution"}, {"st": 111, "ed": 113, "text": "multiple scales"}, {"st": 119, "ed": 122, "text": "word error rate"}]
[{"st": 6, "ed": 8, "text": "textual entailment"}, {"st": 28, "ed": 31, "text": "pieces of evidence"}, {"st": 40, "ed": 42, "text": "natural language"}, {"st": 48, "ed": 50, "text": "neural networks"}, {"st": 57, "ed": 59, "text": "decision making"}, {"st": 77, "ed": 79, "text": "joint training"}, {"st": 160, "ed": 162, "text": "multiple choice"}]
[{"st": 27, "ed": 29, "text": "natural language"}, {"st": 39, "ed": 41, "text": "existing approaches"}, {"st": 58, "ed": 61, "text": "bag of words"}, {"st": 84, "ed": 87, "text": "deep learning based"}, {"st": 96, "ed": 98, "text": "natural language"}, {"st": 140, "ed": 142, "text": "fixed length"}, {"st": 159, "ed": 161, "text": "encoder decoder"}, {"st": 169, "ed": 171, "text": "empirically evaluate"}]
[{"st": 14, "ed": 16, "text": "machine translation"}, {"st": 23, "ed": 25, "text": "phrase based"}, {"st": 28, "ed": 31, "text": "recurrent neural networks"}, {"st": 51, "ed": 53, "text": "multimodal data"}, {"st": 55, "ed": 57, "text": "machine translation"}, {"st": 58, "ed": 60, "text": "image description"}, {"st": 73, "ed": 75, "text": "automatic evaluation"}]
[{"st": 8, "ed": 11, "text": "recurrent neural networks"}, {"st": 28, "ed": 31, "text": "deep convolutional networks"}, {"st": 49, "ed": 51, "text": "text processing"}, {"st": 56, "ed": 58, "text": "character level"}, {"st": 84, "ed": 86, "text": "convolutional layers"}, {"st": 98, "ed": 100, "text": "text classification"}, {"st": 114, "ed": 116, "text": "deep convolutional"}]
[{"st": 8, "ed": 11, "text": "recurrent neural networks"}, {"st": 57, "ed": 59, "text": "spoken language"}]
[{"st": 6, "ed": 8, "text": "sentence level"}, {"st": 38, "ed": 40, "text": "sentence level"}, {"st": 49, "ed": 51, "text": "pre trained"}, {"st": 51, "ed": 53, "text": "word embeddings"}, {"st": 55, "ed": 57, "text": "specific task"}, {"st": 59, "ed": 61, "text": "higher accuracy"}]
[{"st": 0, "ed": 4, "text": "convolutional neural networks cnns"}, {"st": 85, "ed": 87, "text": "feature maps"}]
[{"st": 6, "ed": 8, "text": "cost effective"}, {"st": 41, "ed": 43, "text": "word representations"}, {"st": 59, "ed": 62, "text": "short term memory"}, {"st": 73, "ed": 75, "text": "fully automated"}, {"st": 80, "ed": 82, "text": "excellent results"}, {"st": 97, "ed": 99, "text": "recent advances"}, {"st": 101, "ed": 103, "text": "neural networks"}]
[{"st": 9, "ed": 12, "text": "short term memory"}, {"st": 13, "ed": 17, "text": "recurrent neural network rnn"}, {"st": 21, "ed": 24, "text": "automatic speech recognition"}, {"st": 52, "ed": 54, "text": "optimization methods"}, {"st": 58, "ed": 60, "text": "regularization techniques"}, {"st": 64, "ed": 66, "text": "l 2"}, {"st": 100, "ed": 102, "text": "relative improvement"}, {"st": 103, "ed": 106, "text": "word error rate"}, {"st": 113, "ed": 117, "text": "feed forward neural network"}, {"st": 135, "ed": 137, "text": "bidirectional lstm"}, {"st": 145, "ed": 147, "text": "layer wise"}, {"st": 184, "ed": 187, "text": "recurrent neural networks"}]
[{"st": 0, "ed": 4, "text": "neural machine translation nmt"}, {"st": 21, "ed": 23, "text": "competitive performance"}, {"st": 74, "ed": 76, "text": "word level"}, {"st": 95, "ed": 97, "text": "improve performance"}, {"st": 123, "ed": 126, "text": "times faster than"}, {"st": 160, "ed": 162, "text": "weight pruning"}, {"st": 176, "ed": 178, "text": "fewer parameters"}]
[{"st": 11, "ed": 13, "text": "sentence classification"}, {"st": 23, "ed": 25, "text": "desirable properties"}, {"st": 29, "ed": 31, "text": "attention mechanism"}, {"st": 90, "ed": 92, "text": "domain knowledge"}, {"st": 101, "ed": 103, "text": "attention mechanism"}, {"st": 126, "ed": 128, "text": "efficient learning"}, {"st": 133, "ed": 135, "text": "domain specific"}, {"st": 159, "ed": 161, "text": "efficient learning"}]
[{"st": 10, "ed": 12, "text": "neural network"}, {"st": 30, "ed": 33, "text": "recurrent neural network"}, {"st": 80, "ed": 85, "text": "long short term memory lstm"}]
[{"st": 0, "ed": 4, "text": "recurrent neural network rnn"}, {"st": 5, "ed": 7, "text": "character level"}, {"st": 30, "ed": 32, "text": "word level"}, {"st": 57, "ed": 59, "text": "rnn architectures"}, {"st": 73, "ed": 76, "text": "input and output"}, {"st": 80, "ed": 82, "text": "character level"}, {"st": 127, "ed": 129, "text": "character level"}, {"st": 129, "ed": 132, "text": "end to end"}, {"st": 132, "ed": 134, "text": "speech recognition"}, {"st": 137, "ed": 140, "text": "wall street journal"}]
[{"st": 3, "ed": 5, "text": "speech recognition"}, {"st": 9, "ed": 11, "text": "recent studies"}, {"st": 18, "ed": 20, "text": "multi task"}, {"st": 34, "ed": 36, "text": "improve performance"}, {"st": 60, "ed": 62, "text": "multiple languages"}, {"st": 67, "ed": 69, "text": "significantly reduced"}, {"st": 83, "ed": 85, "text": "multi task"}, {"st": 91, "ed": 93, "text": "speech recognition"}, {"st": 138, "ed": 140, "text": "multi task"}, {"st": 143, "ed": 145, "text": "improve performance"}]
[{"st": 0, "ed": 2, "text": "sentiment analysis"}, {"st": 5, "ed": 7, "text": "action research"}, {"st": 19, "ed": 21, "text": "social media"}, {"st": 29, "ed": 31, "text": "textual data"}, {"st": 57, "ed": 59, "text": "natural language"}, {"st": 72, "ed": 74, "text": "spoken language"}, {"st": 90, "ed": 92, "text": "recent research"}, {"st": 167, "ed": 170, "text": "short term memory"}, {"st": 175, "ed": 177, "text": "loss functions"}, {"st": 187, "ed": 189, "text": "pre training"}, {"st": 196, "ed": 198, "text": "pre train"}]
[{"st": 4, "ed": 6, "text": "word embeddings"}, {"st": 13, "ed": 15, "text": "previously unseen"}, {"st": 20, "ed": 22, "text": "character level"}, {"st": 39, "ed": 41, "text": "attention mechanism"}, {"st": 57, "ed": 59, "text": "character level"}, {"st": 72, "ed": 74, "text": "character level"}, {"st": 78, "ed": 80, "text": "improve performance"}, {"st": 87, "ed": 89, "text": "attention based"}]
[{"st": 6, "ed": 8, "text": "training instances"}, {"st": 25, "ed": 27, "text": "building blocks"}, {"st": 42, "ed": 44, "text": "internal representation"}, {"st": 62, "ed": 67, "text": "long short term memory lstm"}, {"st": 75, "ed": 77, "text": "natural language"}, {"st": 83, "ed": 85, "text": "sentiment analysis"}, {"st": 92, "ed": 94, "text": "sequence prediction"}, {"st": 121, "ed": 123, "text": "internal representation"}, {"st": 130, "ed": 132, "text": "building blocks"}, {"st": 142, "ed": 144, "text": "significantly improves"}, {"st": 163, "ed": 165, "text": "training data"}]
[{"st": 1, "ed": 3, "text": "computer vision"}, {"st": 19, "ed": 22, "text": "convolutional neural networks"}, {"st": 77, "ed": 79, "text": "computational efficiency"}, {"st": 135, "ed": 137, "text": "batch normalization"}, {"st": 147, "ed": 149, "text": "n gram"}]
[{"st": 0, "ed": 3, "text": "end to end"}, {"st": 7, "ed": 9, "text": "competitive results"}, {"st": 13, "ed": 16, "text": "hidden markov model"}, {"st": 17, "ed": 20, "text": "deep neural network"}, {"st": 21, "ed": 25, "text": "automatic speech recognition asr"}, {"st": 60, "ed": 63, "text": "end to end"}, {"st": 92, "ed": 96, "text": "recurrent neural network rnn"}, {"st": 98, "ed": 100, "text": "auto encoder"}, {"st": 107, "ed": 109, "text": "finite dimensional"}, {"st": 116, "ed": 118, "text": "character level"}, {"st": 146, "ed": 150, "text": "feed forward neural network"}]
[{"st": 2, "ed": 4, "text": "back propagation"}, {"st": 14, "ed": 17, "text": "recurrent neural networks"}, {"st": 49, "ed": 51, "text": "extensive experiments"}]
[{"st": 7, "ed": 10, "text": "text to speech"}, {"st": 24, "ed": 27, "text": "end to end"}, {"st": 35, "ed": 37, "text": "building blocks"}, {"st": 53, "ed": 55, "text": "prediction model"}, {"st": 56, "ed": 58, "text": "fundamental frequency"}, {"st": 58, "ed": 60, "text": "prediction model"}, {"st": 80, "ed": 83, "text": "deep neural networks"}, {"st": 102, "ed": 104, "text": "fewer parameters"}, {"st": 113, "ed": 115, "text": "neural network"}, {"st": 127, "ed": 130, "text": "text to speech"}, {"st": 136, "ed": 138, "text": "feature engineering"}]
[{"st": 3, "ed": 5, "text": "generative modeling"}, {"st": 10, "ed": 13, "text": "variational auto encoders"}, {"st": 35, "ed": 37, "text": "poorly understood"}, {"st": 141, "ed": 143, "text": "generative modeling"}, {"st": 163, "ed": 165, "text": "semi supervised"}]
[{"st": 45, "ed": 47, "text": "speech recognition"}, {"st": 86, "ed": 88, "text": "loss function"}, {"st": 99, "ed": 101, "text": "automatically learns"}, {"st": 168, "ed": 170, "text": "large vocabulary"}, {"st": 170, "ed": 172, "text": "speech recognition"}, {"st": 174, "ed": 176, "text": "multiple scales"}]
[{"st": 8, "ed": 10, "text": "question answering"}, {"st": 13, "ed": 15, "text": "real world"}, {"st": 17, "ed": 19, "text": "training examples"}, {"st": 23, "ed": 25, "text": "weakly supervised"}, {"st": 47, "ed": 49, "text": "attention mechanism"}, {"st": 53, "ed": 55, "text": "neural architecture"}, {"st": 96, "ed": 99, "text": "end to end"}]
[{"st": 2, "ed": 4, "text": "speech recognition"}, {"st": 9, "ed": 11, "text": "mutual information"}, {"st": 15, "ed": 18, "text": "end to end"}, {"st": 54, "ed": 57, "text": "method compares favorably"}, {"st": 81, "ed": 83, "text": "ensemble method"}, {"st": 91, "ed": 93, "text": "neural network"}, {"st": 102, "ed": 104, "text": "ensemble method"}]
[{"st": 16, "ed": 18, "text": "training data"}, {"st": 50, "ed": 52, "text": "unsupervised manner"}, {"st": 78, "ed": 80, "text": "labeled examples"}, {"st": 86, "ed": 88, "text": "strong baselines"}, {"st": 104, "ed": 106, "text": "generative process"}, {"st": 115, "ed": 118, "text": "positive or negative"}, {"st": 123, "ed": 126, "text": "positive or negative"}]
[{"st": 9, "ed": 11, "text": "training objective"}, {"st": 64, "ed": 66, "text": "error detection"}, {"st": 69, "ed": 72, "text": "named entity recognition"}, {"st": 83, "ed": 85, "text": "performance improvements"}]
[{"st": 0, "ed": 3, "text": "recurrent neural network"}, {"st": 6, "ed": 8, "text": "widely applied"}, {"st": 14, "ed": 16, "text": "hidden states"}, {"st": 48, "ed": 50, "text": "technique called"}, {"st": 59, "ed": 63, "text": "recurrent neural network rnn"}]
[{"st": 0, "ed": 2, "text": "deep neural"}, {"st": 5, "ed": 7, "text": "lstm rnn"}, {"st": 10, "ed": 12, "text": "great potential"}, {"st": 58, "ed": 60, "text": "lstm rnn"}, {"st": 102, "ed": 104, "text": "frame level"}, {"st": 112, "ed": 114, "text": "experiments conducted"}, {"st": 134, "ed": 136, "text": "significantly outperforms"}]
[{"st": 13, "ed": 15, "text": "word embedding"}, {"st": 17, "ed": 20, "text": "attracted much attention"}, {"st": 21, "ed": 23, "text": "natural language"}, {"st": 25, "ed": 27, "text": "information retrieval"}, {"st": 29, "ed": 31, "text": "embedding vectors"}, {"st": 50, "ed": 52, "text": "word embedding"}, {"st": 57, "ed": 59, "text": "accurately predict"}, {"st": 113, "ed": 115, "text": "word embedding"}, {"st": 118, "ed": 120, "text": "word representations"}, {"st": 136, "ed": 138, "text": "objective functions"}, {"st": 239, "ed": 241, "text": "word embedding"}, {"st": 242, "ed": 244, "text": "significantly outperform"}, {"st": 250, "ed": 252, "text": "embedding models"}]
[{"st": 3, "ed": 5, "text": "neural architectures"}, {"st": 45, "ed": 47, "text": "recurrent neural"}, {"st": 86, "ed": 89, "text": "end to end"}, {"st": 91, "ed": 93, "text": "empirically evaluate"}, {"st": 97, "ed": 99, "text": "neural architectures"}]
[{"st": 4, "ed": 6, "text": "recently proposed"}, {"st": 13, "ed": 15, "text": "joint training"}, {"st": 16, "ed": 18, "text": "multiple languages"}, {"st": 28, "ed": 31, "text": "deep neural network"}, {"st": 136, "ed": 138, "text": "recently proposed"}]
[{"st": 11, "ed": 14, "text": "semi supervised learning"}, {"st": 22, "ed": 24, "text": "image recognition"}, {"st": 37, "ed": 39, "text": "ladder network"}, {"st": 44, "ed": 46, "text": "ladder network"}, {"st": 47, "ed": 50, "text": "semi supervised learning"}, {"st": 51, "ed": 54, "text": "recurrent neural networks"}, {"st": 59, "ed": 61, "text": "phoneme recognition"}, {"st": 75, "ed": 77, "text": "consistently outperform"}, {"st": 81, "ed": 83, "text": "fully supervised"}]
[{"st": 7, "ed": 9, "text": "representation learning"}, {"st": 78, "ed": 80, "text": "discrete space"}, {"st": 87, "ed": 89, "text": "discrete space"}, {"st": 97, "ed": 99, "text": "continuous space"}, {"st": 103, "ed": 107, "text": "generative adversarial network gan"}, {"st": 116, "ed": 118, "text": "method yields"}, {"st": 134, "ed": 136, "text": "latent variable"}, {"st": 170, "ed": 173, "text": "semi supervised learning"}, {"st": 184, "ed": 186, "text": "style transfer"}, {"st": 191, "ed": 193, "text": "continuous space"}]
[{"st": 18, "ed": 20, "text": "error detection"}, {"st": 53, "ed": 55, "text": "joint learning"}, {"st": 64, "ed": 66, "text": "improves performance"}, {"st": 70, "ed": 72, "text": "error detection"}]
[{"st": 4, "ed": 6, "text": "word embedding"}, {"st": 6, "ed": 8, "text": "pre training"}, {"st": 48, "ed": 50, "text": "pre trained"}, {"st": 65, "ed": 67, "text": "neural network"}, {"st": 98, "ed": 100, "text": "approach outperforms"}, {"st": 120, "ed": 122, "text": "performance gains"}]
[{"st": 28, "ed": 30, "text": "cross entropy"}, {"st": 50, "ed": 52, "text": "cross entropy"}, {"st": 52, "ed": 54, "text": "trained models"}, {"st": 85, "ed": 87, "text": "training procedure"}, {"st": 150, "ed": 152, "text": "training objective"}, {"st": 160, "ed": 163, "text": "named entity recognition"}, {"st": 170, "ed": 172, "text": "cross entropy"}, {"st": 176, "ed": 178, "text": "cross entropy"}]
[{"st": 0, "ed": 4, "text": "recurrent neural networks rnns"}, {"st": 7, "ed": 10, "text": "short term memory"}, {"st": 16, "ed": 18, "text": "building block"}, {"st": 22, "ed": 24, "text": "tasks including"}, {"st": 24, "ed": 26, "text": "machine translation"}, {"st": 40, "ed": 42, "text": "word level"}, {"st": 118, "ed": 120, "text": "word level"}]
[{"st": 0, "ed": 2, "text": "speech separation"}, {"st": 13, "ed": 15, "text": "speech separation"}, {"st": 19, "ed": 21, "text": "signal processing"}, {"st": 27, "ed": 29, "text": "speech separation"}, {"st": 31, "ed": 33, "text": "supervised learning"}, {"st": 50, "ed": 53, "text": "the past decade"}, {"st": 67, "ed": 69, "text": "deep learning"}, {"st": 71, "ed": 73, "text": "speech separation"}, {"st": 91, "ed": 93, "text": "deep learning"}, {"st": 95, "ed": 97, "text": "speech separation"}, {"st": 108, "ed": 110, "text": "speech separation"}, {"st": 125, "ed": 127, "text": "learning machines"}, {"st": 146, "ed": 148, "text": "speech enhancement"}, {"st": 166, "ed": 168, "text": "important issue"}, {"st": 172, "ed": 174, "text": "supervised learning"}]
[{"st": 14, "ed": 16, "text": "natural language"}, {"st": 28, "ed": 30, "text": "hand engineered"}, {"st": 56, "ed": 58, "text": "deep learning"}, {"st": 71, "ed": 73, "text": "existing approaches"}]
[{"st": 1, "ed": 3, "text": "word embeddings"}, {"st": 9, "ed": 11, "text": "natural language"}, {"st": 25, "ed": 27, "text": "word embeddings"}, {"st": 48, "ed": 50, "text": "word embeddings"}, {"st": 76, "ed": 78, "text": "pre trained"}, {"st": 78, "ed": 80, "text": "word embeddings"}, {"st": 90, "ed": 92, "text": "previously proposed"}, {"st": 108, "ed": 110, "text": "proposed method"}, {"st": 131, "ed": 133, "text": "previously proposed"}, {"st": 133, "ed": 135, "text": "highly competitive"}, {"st": 139, "ed": 141, "text": "word embeddings"}, {"st": 146, "ed": 148, "text": "special case"}, {"st": 159, "ed": 161, "text": "relation classification"}, {"st": 163, "ed": 165, "text": "text classification"}, {"st": 172, "ed": 174, "text": "significantly outperform"}, {"st": 174, "ed": 176, "text": "prior methods"}, {"st": 178, "ed": 180, "text": "benchmark datasets"}]
[{"st": 0, "ed": 2, "text": "previous studies"}, {"st": 5, "ed": 7, "text": "empirical success"}, {"st": 8, "ed": 10, "text": "word embeddings"}, {"st": 22, "ed": 24, "text": "distributed representations"}, {"st": 25, "ed": 27, "text": "text documents"}, {"st": 29, "ed": 31, "text": "machine learning"}, {"st": 44, "ed": 46, "text": "neural network"}, {"st": 67, "ed": 69, "text": "low dimensional"}, {"st": 75, "ed": 77, "text": "important information"}, {"st": 87, "ed": 89, "text": "empirical evaluations"}]
[{"st": 0, "ed": 2, "text": "context information"}, {"st": 4, "ed": 6, "text": "important role"}, {"st": 30, "ed": 32, "text": "encoder decoder"}, {"st": 46, "ed": 49, "text": "encoder decoder architecture"}, {"st": 66, "ed": 68, "text": "significantly improve"}, {"st": 113, "ed": 115, "text": "sentence representations"}]
[{"st": 2, "ed": 4, "text": "neural network"}, {"st": 12, "ed": 15, "text": "image and text"}, {"st": 21, "ed": 25, "text": "recurrent neural networks rnns"}, {"st": 27, "ed": 30, "text": "short term memory"}, {"st": 90, "ed": 92, "text": "deep learning"}]
[{"st": 8, "ed": 10, "text": "natural language"}, {"st": 20, "ed": 22, "text": "representation learning"}, {"st": 31, "ed": 33, "text": "gradient based"}, {"st": 39, "ed": 41, "text": "hand crafted"}, {"st": 51, "ed": 53, "text": "task specific"}, {"st": 58, "ed": 60, "text": "representation learning"}, {"st": 66, "ed": 68, "text": "training data"}, {"st": 75, "ed": 78, "text": "labeled training data"}, {"st": 86, "ed": 88, "text": "formal logic"}, {"st": 100, "ed": 102, "text": "representation learning"}, {"st": 111, "ed": 113, "text": "training data"}]
[]
[{"st": 4, "ed": 6, "text": "deep learning"}, {"st": 19, "ed": 21, "text": "deep networks"}, {"st": 86, "ed": 88, "text": "bayesian inference"}, {"st": 91, "ed": 93, "text": "gibbs sampling"}]
[{"st": 20, "ed": 22, "text": "recently proposed"}, {"st": 24, "ed": 26, "text": "neural network"}, {"st": 58, "ed": 60, "text": "pattern recognition"}, {"st": 113, "ed": 115, "text": "pattern recognition"}, {"st": 117, "ed": 119, "text": "temporal patterns"}, {"st": 123, "ed": 125, "text": "bay area"}, {"st": 150, "ed": 152, "text": "improved accuracy"}, {"st": 153, "ed": 155, "text": "pattern recognition"}, {"st": 169, "ed": 171, "text": "machine learning"}, {"st": 174, "ed": 176, "text": "neural network"}]
[{"st": 8, "ed": 10, "text": "linked data"}, {"st": 18, "ed": 20, "text": "domain experts"}, {"st": 58, "ed": 60, "text": "evolutionary algorithm"}]
[{"st": 7, "ed": 9, "text": "neural networks"}, {"st": 15, "ed": 17, "text": "inner product"}, {"st": 39, "ed": 42, "text": "convolutional neural networks"}, {"st": 57, "ed": 59, "text": "higher order"}, {"st": 59, "ed": 61, "text": "loss functions"}, {"st": 75, "ed": 77, "text": "network structures"}]
[{"st": 0, "ed": 3, "text": "deep neural networks"}, {"st": 51, "ed": 53, "text": "based approaches"}, {"st": 89, "ed": 91, "text": "loss functions"}, {"st": 96, "ed": 98, "text": "inner product"}, {"st": 118, "ed": 120, "text": "multilayer perceptron"}]
[{"st": 15, "ed": 17, "text": "classification task"}, {"st": 48, "ed": 50, "text": "neural networks"}, {"st": 53, "ed": 55, "text": "generative models"}, {"st": 95, "ed": 97, "text": "proposed framework"}, {"st": 101, "ed": 105, "text": "markov chain monte carlo"}, {"st": 106, "ed": 108, "text": "method called"}, {"st": 138, "ed": 140, "text": "extensive simulations"}]
[{"st": 0, "ed": 3, "text": "artificial neural networks"}, {"st": 5, "ed": 8, "text": "received increasing attention"}, {"st": 9, "ed": 11, "text": "recent years"}, {"st": 32, "ed": 34, "text": "neural network"}, {"st": 36, "ed": 38, "text": "becoming increasingly"}]
[{"st": 2, "ed": 4, "text": "deep rl"}, {"st": 14, "ed": 16, "text": "agents learn"}, {"st": 61, "ed": 63, "text": "wide variety"}, {"st": 64, "ed": 66, "text": "zero shot"}]
[{"st": 10, "ed": 12, "text": "deep learning"}, {"st": 45, "ed": 47, "text": "image classification"}, {"st": 47, "ed": 49, "text": "sentiment analysis"}, {"st": 64, "ed": 66, "text": "highly successful"}, {"st": 66, "ed": 68, "text": "machine learning"}, {"st": 69, "ed": 71, "text": "artificial intelligence"}, {"st": 117, "ed": 119, "text": "deep learning"}, {"st": 128, "ed": 130, "text": "recent developments"}, {"st": 152, "ed": 154, "text": "deep learning"}]
[{"st": 0, "ed": 3, "text": "deep reinforcement learning"}, {"st": 6, "ed": 8, "text": "recent successes"}, {"st": 93, "ed": 95, "text": "low dimensional"}, {"st": 104, "ed": 106, "text": "closed form"}, {"st": 151, "ed": 153, "text": "supervised learning"}]
[{"st": 20, "ed": 22, "text": "entorhinal cortex"}, {"st": 67, "ed": 71, "text": "recurrent neural networks rnns"}, {"st": 167, "ed": 169, "text": "recurrent connections"}]
[{"st": 7, "ed": 9, "text": "neural network"}, {"st": 13, "ed": 16, "text": "linear dimensionality reduction"}, {"st": 17, "ed": 19, "text": "object recognition"}, {"st": 28, "ed": 30, "text": "dimensionality reduction"}, {"st": 39, "ed": 41, "text": "input vector"}, {"st": 44, "ed": 46, "text": "low dimensional"}, {"st": 49, "ed": 51, "text": "neural network"}, {"st": 137, "ed": 140, "text": "mean squared error"}, {"st": 142, "ed": 144, "text": "output layer"}, {"st": 170, "ed": 172, "text": "neural network"}]
[{"st": 4, "ed": 8, "text": "functional magnetic resonance imaging"}, {"st": 14, "ed": 17, "text": "general linear model"}, {"st": 19, "ed": 21, "text": "spectral clustering"}, {"st": 22, "ed": 24, "text": "recently proposed"}, {"st": 90, "ed": 92, "text": "spectral clustering"}, {"st": 95, "ed": 98, "text": "independent component analysis"}, {"st": 101, "ed": 103, "text": "least squares"}, {"st": 132, "ed": 134, "text": "latent variables"}, {"st": 151, "ed": 153, "text": "principal components"}, {"st": 167, "ed": 169, "text": "spectral clustering"}, {"st": 182, "ed": 184, "text": "proposed method"}]
[{"st": 40, "ed": 42, "text": "inter class"}, {"st": 47, "ed": 49, "text": "iris recognition"}, {"st": 59, "ed": 61, "text": "neural network"}, {"st": 77, "ed": 79, "text": "inter class"}]
[{"st": 7, "ed": 9, "text": "signal processing"}, {"st": 26, "ed": 28, "text": "active research"}, {"st": 44, "ed": 46, "text": "image analysis"}, {"st": 68, "ed": 71, "text": "computer aided diagnosis"}, {"st": 114, "ed": 117, "text": "digital signal processing"}, {"st": 134, "ed": 136, "text": "higher level"}, {"st": 162, "ed": 164, "text": "machine learning"}, {"st": 196, "ed": 198, "text": "computational costs"}, {"st": 219, "ed": 221, "text": "scientific journals"}]
[{"st": 0, "ed": 3, "text": "deep neural networks"}, {"st": 17, "ed": 19, "text": "pattern recognition"}, {"st": 139, "ed": 142, "text": "convolutional neural networks"}, {"st": 151, "ed": 153, "text": "mnist datasets"}, {"st": 158, "ed": 160, "text": "evolutionary algorithms"}]
[{"st": 25, "ed": 27, "text": "neural network"}, {"st": 90, "ed": 92, "text": "real world"}, {"st": 128, "ed": 130, "text": "handwritten digits"}, {"st": 151, "ed": 153, "text": "building block"}, {"st": 154, "ed": 156, "text": "large scale"}]
[{"st": 45, "ed": 47, "text": "computer vision"}, {"st": 53, "ed": 55, "text": "computer vision"}, {"st": 73, "ed": 75, "text": "complex systems"}, {"st": 124, "ed": 126, "text": "behavior analysis"}, {"st": 130, "ed": 132, "text": "biologically inspired"}]
[{"st": 0, "ed": 3, "text": "convolutional neural networks"}, {"st": 5, "ed": 7, "text": "empirical results"}, {"st": 8, "ed": 10, "text": "computer vision"}, {"st": 11, "ed": 13, "text": "speech recognition"}, {"st": 15, "ed": 18, "text": "labeled training data"}, {"st": 35, "ed": 37, "text": "annotated data"}, {"st": 64, "ed": 66, "text": "feature extractor"}, {"st": 79, "ed": 81, "text": "deep architecture"}, {"st": 99, "ed": 101, "text": "x ray"}]
[{"st": 3, "ed": 5, "text": "optimization technique"}, {"st": 18, "ed": 20, "text": "optimization problems"}, {"st": 39, "ed": 41, "text": "neural networks"}, {"st": 44, "ed": 46, "text": "deep learning"}, {"st": 51, "ed": 53, "text": "deep learning"}, {"st": 58, "ed": 60, "text": "machine learning"}, {"st": 72, "ed": 74, "text": "artificial intelligence"}, {"st": 108, "ed": 110, "text": "simulated annealing"}, {"st": 110, "ed": 112, "text": "differential evolution"}, {"st": 129, "ed": 132, "text": "mnist and cifar"}]
[{"st": 15, "ed": 17, "text": "visual tasks"}, {"st": 19, "ed": 21, "text": "object recognition"}, {"st": 23, "ed": 26, "text": "visual question answering"}, {"st": 33, "ed": 36, "text": "taking advantage of"}, {"st": 56, "ed": 58, "text": "low rank"}, {"st": 66, "ed": 68, "text": "attention mechanism"}, {"st": 81, "ed": 84, "text": "visual question answering"}]
[{"st": 4, "ed": 6, "text": "network quantization"}, {"st": 15, "ed": 17, "text": "pre trained"}, {"st": 19, "ed": 23, "text": "convolutional neural network cnn"}, {"st": 26, "ed": 28, "text": "low precision"}, {"st": 41, "ed": 43, "text": "unlike existing"}, {"st": 49, "ed": 51, "text": "accuracy loss"}, {"st": 97, "ed": 99, "text": "pre trained"}, {"st": 116, "ed": 118, "text": "low precision"}, {"st": 141, "ed": 143, "text": "accuracy loss"}, {"st": 181, "ed": 183, "text": "low precision"}, {"st": 188, "ed": 190, "text": "network quantization"}, {"st": 194, "ed": 196, "text": "extensive experiments"}, {"st": 198, "ed": 200, "text": "imagenet classification"}, {"st": 205, "ed": 207, "text": "deep cnn"}, {"st": 207, "ed": 209, "text": "architectures including"}, {"st": 210, "ed": 212, "text": "vgg 16"}, {"st": 231, "ed": 233, "text": "improved accuracy"}, {"st": 237, "ed": 239, "text": "floating point"}, {"st": 267, "ed": 269, "text": "similar accuracy"}, {"st": 273, "ed": 275, "text": "floating point"}, {"st": 277, "ed": 279, "text": "impressive results"}, {"st": 283, "ed": 285, "text": "network pruning"}, {"st": 293, "ed": 297, "text": "available at https github.com"}]
[{"st": 22, "ed": 25, "text": "fully convolutional network"}, {"st": 28, "ed": 32, "text": "trained end to end"}, {"st": 62, "ed": 65, "text": "field of view"}, {"st": 69, "ed": 71, "text": "receptive field"}, {"st": 99, "ed": 101, "text": "super resolution"}, {"st": 118, "ed": 120, "text": "validation set"}]
[{"st": 20, "ed": 22, "text": "human brain"}, {"st": 35, "ed": 37, "text": "spike timing"}, {"st": 40, "ed": 42, "text": "feature learning"}, {"st": 43, "ed": 45, "text": "biologically plausible"}, {"st": 51, "ed": 54, "text": "spiking neural networks"}, {"st": 90, "ed": 92, "text": "unsupervised learning"}, {"st": 98, "ed": 100, "text": "object recognition"}, {"st": 108, "ed": 110, "text": "fully connected"}, {"st": 132, "ed": 134, "text": "biologically plausible"}]
[{"st": 4, "ed": 6, "text": "generating adversarial"}, {"st": 26, "ed": 28, "text": "image pixels"}, {"st": 47, "ed": 49, "text": "generating adversarial"}, {"st": 64, "ed": 68, "text": "feed forward neural networks"}, {"st": 71, "ed": 73, "text": "supervised manner"}, {"st": 75, "ed": 77, "text": "adversarial examples"}, {"st": 101, "ed": 103, "text": "adversarial examples"}, {"st": 112, "ed": 114, "text": "original input"}]
[{"st": 0, "ed": 2, "text": "deep learning"}, {"st": 6, "ed": 8, "text": "outperform traditional"}, {"st": 8, "ed": 10, "text": "machine learning"}, {"st": 20, "ed": 22, "text": "deep learning"}, {"st": 34, "ed": 36, "text": "decision making"}, {"st": 48, "ed": 50, "text": "deep learning"}, {"st": 71, "ed": 73, "text": "decision trees"}, {"st": 82, "ed": 84, "text": "deep learning"}, {"st": 101, "ed": 103, "text": "deep learning"}, {"st": 113, "ed": 115, "text": "deep learning"}, {"st": 174, "ed": 176, "text": "decision making"}, {"st": 178, "ed": 180, "text": "deep learning"}]
[{"st": 0, "ed": 2, "text": "object detection"}, {"st": 8, "ed": 10, "text": "challenging problems"}, {"st": 14, "ed": 16, "text": "computer vision"}, {"st": 22, "ed": 24, "text": "object classification"}, {"st": 25, "ed": 27, "text": "object localization"}, {"st": 31, "ed": 34, "text": "deep neural networks"}, {"st": 41, "ed": 43, "text": "object detection"}, {"st": 66, "ed": 68, "text": "dnn based"}, {"st": 68, "ed": 70, "text": "object detection"}, {"st": 101, "ed": 103, "text": "object detection"}, {"st": 111, "ed": 113, "text": "computational power"}, {"st": 123, "ed": 125, "text": "framework called"}, {"st": 142, "ed": 144, "text": "object detection"}, {"st": 147, "ed": 149, "text": "embedded devices"}, {"st": 166, "ed": 168, "text": "network architecture"}, {"st": 182, "ed": 184, "text": "fewer parameters"}, {"st": 193, "ed": 195, "text": "power consumption"}, {"st": 196, "ed": 198, "text": "embedded devices"}]
[{"st": 0, "ed": 2, "text": "neural networks"}, {"st": 34, "ed": 36, "text": "conventional approaches"}, {"st": 141, "ed": 143, "text": "mnist dataset"}, {"st": 145, "ed": 147, "text": "network parameters"}, {"st": 151, "ed": 153, "text": "floating point"}, {"st": 167, "ed": 169, "text": "imagenet dataset"}, {"st": 171, "ed": 173, "text": "network parameters"}]
[{"st": 0, "ed": 2, "text": "breast cancer"}, {"st": 14, "ed": 16, "text": "image analysis"}, {"st": 26, "ed": 28, "text": "early detection"}, {"st": 70, "ed": 72, "text": "semi supervised"}, {"st": 107, "ed": 109, "text": "random walks"}, {"st": 136, "ed": 138, "text": "semi supervised"}]
[{"st": 2, "ed": 4, "text": "extensive experiments"}, {"st": 4, "ed": 7, "text": "training and testing"}, {"st": 7, "ed": 9, "text": "hidden units"}, {"st": 10, "ed": 12, "text": "deep networks"}, {"st": 27, "ed": 29, "text": "real world"}, {"st": 51, "ed": 53, "text": "recurrent network"}, {"st": 112, "ed": 115, "text": "classification and regression"}, {"st": 136, "ed": 138, "text": "unlike previous"}, {"st": 161, "ed": 163, "text": "existing approaches"}]
[{"st": 19, "ed": 21, "text": "reinforcement learning"}, {"st": 54, "ed": 56, "text": "highly successful"}, {"st": 56, "ed": 58, "text": "reinforcement learning"}, {"st": 66, "ed": 68, "text": "neural network"}, {"st": 80, "ed": 82, "text": "search spaces"}, {"st": 94, "ed": 96, "text": "higher accuracy"}, {"st": 115, "ed": 117, "text": "reinforcement learning"}, {"st": 143, "ed": 145, "text": "search algorithms"}, {"st": 169, "ed": 171, "text": "cifar 10"}, {"st": 198, "ed": 200, "text": "evolutionary algorithms"}]
[{"st": 0, "ed": 2, "text": "object detection"}, {"st": 7, "ed": 9, "text": "computer vision"}, {"st": 11, "ed": 13, "text": "object classification"}, {"st": 14, "ed": 16, "text": "object localization"}, {"st": 20, "ed": 23, "text": "deep neural networks"}, {"st": 27, "ed": 29, "text": "recent years"}, {"st": 39, "ed": 41, "text": "object detection"}, {"st": 49, "ed": 51, "text": "object detection"}, {"st": 56, "ed": 58, "text": "embedded devices"}, {"st": 74, "ed": 77, "text": "deep neural network"}, {"st": 79, "ed": 81, "text": "object detection"}, {"st": 86, "ed": 88, "text": "embedded devices"}, {"st": 107, "ed": 109, "text": "object detection"}, {"st": 128, "ed": 132, "text": "deep convolutional neural network"}, {"st": 136, "ed": 138, "text": "object detection"}, {"st": 175, "ed": 177, "text": "object detection"}, {"st": 216, "ed": 219, "text": "deep neural network"}, {"st": 226, "ed": 228, "text": "object detection"}]
[{"st": 1, "ed": 5, "text": "generative adversarial networks gans"}, {"st": 14, "ed": 17, "text": "shown promising results"}, {"st": 38, "ed": 40, "text": "disentangled representations"}, {"st": 55, "ed": 57, "text": "current approaches"}, {"st": 77, "ed": 79, "text": "generated data"}, {"st": 91, "ed": 93, "text": "disentangled representations"}]
[{"st": 3, "ed": 6, "text": "self organizing map"}, {"st": 10, "ed": 12, "text": "neural network"}, {"st": 16, "ed": 19, "text": "self organizing map"}, {"st": 27, "ed": 29, "text": "learning rate"}, {"st": 33, "ed": 35, "text": "learning rate"}]
[{"st": 11, "ed": 14, "text": "field of view"}]
[{"st": 3, "ed": 7, "text": "convolutional neural networks cnn"}, {"st": 10, "ed": 12, "text": "feature representations"}]
[{"st": 0, "ed": 3, "text": "deep neural networks"}, {"st": 14, "ed": 16, "text": "object recognition"}, {"st": 19, "ed": 21, "text": "scene understanding"}, {"st": 76, "ed": 78, "text": "deep features"}, {"st": 82, "ed": 84, "text": "object classification"}, {"st": 128, "ed": 130, "text": "deep features"}, {"st": 138, "ed": 140, "text": "image representations"}]
[{"st": 0, "ed": 2, "text": "neural networks"}, {"st": 18, "ed": 20, "text": "image classification"}, {"st": 32, "ed": 34, "text": "evolutionary algorithms"}, {"st": 40, "ed": 42, "text": "significant computational"}, {"st": 79, "ed": 84, "text": "cifar 10 and cifar 100"}, {"st": 88, "ed": 90, "text": "initial conditions"}, {"st": 114, "ed": 116, "text": "search spaces"}]
[{"st": 6, "ed": 8, "text": "network architecture"}, {"st": 19, "ed": 21, "text": "image classification"}, {"st": 66, "ed": 68, "text": "domain adaptation"}, {"st": 72, "ed": 74, "text": "classification performance"}, {"st": 82, "ed": 84, "text": "network architecture"}, {"st": 106, "ed": 108, "text": "feature extractors"}, {"st": 113, "ed": 115, "text": "unified framework"}, {"st": 120, "ed": 122, "text": "simultaneously learn"}, {"st": 128, "ed": 130, "text": "feature representations"}, {"st": 168, "ed": 170, "text": "outperforms previous"}]
[{"st": 0, "ed": 2, "text": "traditional approaches"}, {"st": 5, "ed": 7, "text": "large scale"}, {"st": 7, "ed": 9, "text": "knowledge graph"}, {"st": 13, "ed": 15, "text": "extracting information"}, {"st": 23, "ed": 25, "text": "unstructured text"}, {"st": 28, "ed": 30, "text": "recent advances"}, {"st": 31, "ed": 35, "text": "convolutional neural networks cnn"}, {"st": 43, "ed": 46, "text": "entities and relations"}, {"st": 58, "ed": 60, "text": "pre processing"}, {"st": 74, "ed": 76, "text": "spatial relations"}, {"st": 98, "ed": 100, "text": "spatial relations"}, {"st": 120, "ed": 122, "text": "pre trained"}, {"st": 133, "ed": 137, "text": "multi layer perceptron mlp"}, {"st": 154, "ed": 156, "text": "spatial relations"}, {"st": 158, "ed": 160, "text": "bounding box"}, {"st": 176, "ed": 178, "text": "spatial relations"}]
[{"st": 2, "ed": 4, "text": "object tracking"}, {"st": 13, "ed": 15, "text": "discriminative models"}, {"st": 25, "ed": 27, "text": "visual cortex"}, {"st": 41, "ed": 43, "text": "visual features"}, {"st": 53, "ed": 55, "text": "object tracking"}, {"st": 83, "ed": 85, "text": "visual features"}, {"st": 93, "ed": 95, "text": "fully differentiable"}, {"st": 115, "ed": 117, "text": "loss function"}, {"st": 143, "ed": 145, "text": "activity recognition"}, {"st": 151, "ed": 153, "text": "object tracking"}]
[{"st": 6, "ed": 8, "text": "important issue"}, {"st": 42, "ed": 45, "text": "encoder and decoder"}, {"st": 58, "ed": 60, "text": "conducted experiments"}, {"st": 65, "ed": 67, "text": "predict future"}, {"st": 92, "ed": 94, "text": "ground truth"}]
[{"st": 28, "ed": 30, "text": "image analysis"}, {"st": 40, "ed": 42, "text": "magnetic resonance"}, {"st": 49, "ed": 51, "text": "cerebrospinal fluid"}, {"st": 75, "ed": 77, "text": "classification methods"}, {"st": 80, "ed": 82, "text": "multilayer perceptrons"}, {"st": 114, "ed": 116, "text": "ground truth"}, {"st": 118, "ed": 120, "text": "classification results"}]
[{"st": 27, "ed": 29, "text": "face recognition"}, {"st": 101, "ed": 104, "text": "input and output"}, {"st": 132, "ed": 134, "text": "visual processing"}, {"st": 201, "ed": 203, "text": "poorly understood"}]
[{"st": 64, "ed": 66, "text": "classification models"}, {"st": 68, "ed": 70, "text": "evolutionary algorithms"}, {"st": 74, "ed": 76, "text": "neural networks"}, {"st": 80, "ed": 82, "text": "time consuming"}, {"st": 124, "ed": 126, "text": "evolutionary algorithms"}, {"st": 142, "ed": 145, "text": "high computational cost"}]
[{"st": 9, "ed": 12, "text": "simple yet effective"}, {"st": 19, "ed": 21, "text": "domain shift"}, {"st": 28, "ed": 31, "text": "source and target"}, {"st": 51, "ed": 55, "text": "source and target domains"}, {"st": 60, "ed": 62, "text": "lower dimensional"}, {"st": 70, "ed": 72, "text": "distribution matching"}, {"st": 92, "ed": 94, "text": "linear transformation"}, {"st": 107, "ed": 109, "text": "linear classifiers"}, {"st": 144, "ed": 147, "text": "linear discriminant analysis"}, {"st": 153, "ed": 155, "text": "large margin"}, {"st": 157, "ed": 159, "text": "domain adaptation"}, {"st": 176, "ed": 179, "text": "deep neural networks"}, {"st": 207, "ed": 209, "text": "https github.com"}]
[{"st": 41, "ed": 43, "text": "multi dimensional"}, {"st": 43, "ed": 46, "text": "recurrent neural networks"}]
[{"st": 0, "ed": 2, "text": "vision based"}, {"st": 2, "ed": 4, "text": "object detection"}, {"st": 33, "ed": 35, "text": "challenging task"}, {"st": 65, "ed": 67, "text": "deep network"}, {"st": 72, "ed": 74, "text": "object detection"}, {"st": 85, "ed": 87, "text": "object detection"}, {"st": 105, "ed": 107, "text": "deep networks"}, {"st": 111, "ed": 113, "text": "bounding boxes"}, {"st": 133, "ed": 135, "text": "deep networks"}, {"st": 181, "ed": 183, "text": "proposed approach"}, {"st": 185, "ed": 188, "text": "efficient and effective"}]
[{"st": 21, "ed": 24, "text": "deep neural network"}, {"st": 34, "ed": 36, "text": "driving styles"}, {"st": 42, "ed": 45, "text": "supervised and unsupervised"}, {"st": 45, "ed": 47, "text": "feature learning"}, {"st": 57, "ed": 59, "text": "estimation problem"}, {"st": 76, "ed": 79, "text": "significantly outperforms existing"}, {"st": 87, "ed": 89, "text": "estimation error"}, {"st": 109, "ed": 111, "text": "supervised learning"}]
[{"st": 0, "ed": 3, "text": "convolutional neural networks"}, {"st": 5, "ed": 7, "text": "highly successful"}, {"st": 12, "ed": 14, "text": "computer vision"}, {"st": 17, "ed": 19, "text": "object recognition"}, {"st": 19, "ed": 21, "text": "object detection"}, {"st": 21, "ed": 23, "text": "image segmentation"}, {"st": 68, "ed": 70, "text": "successfully applied"}, {"st": 90, "ed": 92, "text": "style transfer"}, {"st": 143, "ed": 145, "text": "generated images"}]
[{"st": 2, "ed": 4, "text": "remote sensing"}, {"st": 13, "ed": 15, "text": "visual data"}, {"st": 26, "ed": 30, "text": "multi layer neural networks"}, {"st": 50, "ed": 52, "text": "linear regression"}]
[{"st": 1, "ed": 3, "text": "excellent performance"}, {"st": 4, "ed": 7, "text": "deep neural networks"}, {"st": 36, "ed": 38, "text": "floating point"}, {"st": 47, "ed": 49, "text": "pre trained"}, {"st": 49, "ed": 51, "text": "deep network"}, {"st": 62, "ed": 64, "text": "pre trained"}, {"st": 72, "ed": 74, "text": "neural network"}, {"st": 97, "ed": 101, "text": "convolutional neural networks cnn"}]
[{"st": 3, "ed": 6, "text": "computer aided diagnosis"}, {"st": 12, "ed": 14, "text": "recent years"}, {"st": 21, "ed": 23, "text": "decision support"}, {"st": 24, "ed": 26, "text": "medical oncology"}, {"st": 44, "ed": 46, "text": "medical imaging"}, {"st": 63, "ed": 65, "text": "gain insight"}, {"st": 108, "ed": 110, "text": "decision support"}, {"st": 136, "ed": 138, "text": "decision making"}, {"st": 147, "ed": 149, "text": "decision making"}, {"st": 178, "ed": 180, "text": "diabetic retinopathy"}, {"st": 189, "ed": 191, "text": "powerful tool"}]
[{"st": 13, "ed": 15, "text": "augmented reality"}, {"st": 25, "ed": 28, "text": "generative adversarial networks"}, {"st": 38, "ed": 40, "text": "synthetic data"}, {"st": 42, "ed": 45, "text": "deep neural network"}, {"st": 65, "ed": 67, "text": "modified version"}, {"st": 70, "ed": 73, "text": "generative adversarial networks"}, {"st": 81, "ed": 83, "text": "loss function"}, {"st": 97, "ed": 100, "text": "probability density function"}, {"st": 121, "ed": 123, "text": "input sequence"}, {"st": 148, "ed": 150, "text": "quality assessment"}, {"st": 192, "ed": 194, "text": "predictive power"}]
[{"st": 13, "ed": 15, "text": "challenging task"}, {"st": 104, "ed": 106, "text": "don t"}, {"st": 113, "ed": 115, "text": "ground truth"}]
[{"st": 4, "ed": 6, "text": "highly efficient"}, {"st": 6, "ed": 9, "text": "deep neural networks"}, {"st": 69, "ed": 71, "text": "a 10"}, {"st": 101, "ed": 103, "text": "significantly reduced"}]
[{"st": 0, "ed": 2, "text": "recent studies"}, {"st": 7, "ed": 9, "text": "vision science"}, {"st": 33, "ed": 35, "text": "visual input"}, {"st": 50, "ed": 52, "text": "visual attention"}, {"st": 107, "ed": 109, "text": "most probable"}, {"st": 111, "ed": 113, "text": "sample based"}, {"st": 115, "ed": 119, "text": "markov chain monte carlo"}, {"st": 126, "ed": 128, "text": "multi core"}, {"st": 134, "ed": 136, "text": "visual attention"}, {"st": 153, "ed": 155, "text": "visual attention"}]
[{"st": 83, "ed": 86, "text": "convolutional neural networks"}, {"st": 88, "ed": 90, "text": "successfully applied"}, {"st": 92, "ed": 94, "text": "computer vision"}, {"st": 104, "ed": 107, "text": "convolutional neural networks"}, {"st": 120, "ed": 122, "text": "weighted sum"}, {"st": 124, "ed": 127, "text": "deep neural network"}, {"st": 138, "ed": 140, "text": "linear regression"}, {"st": 144, "ed": 146, "text": "least squares"}]
[{"st": 46, "ed": 48, "text": "feature extraction"}, {"st": 76, "ed": 78, "text": "spatio temporal"}, {"st": 89, "ed": 91, "text": "low level"}, {"st": 94, "ed": 96, "text": "main idea"}, {"st": 101, "ed": 103, "text": "spectral clustering"}, {"st": 117, "ed": 119, "text": "higher dimensional"}]
[{"st": 29, "ed": 31, "text": "sentiment analysis"}, {"st": 32, "ed": 34, "text": "textual data"}, {"st": 55, "ed": 57, "text": "visual sentiment"}, {"st": 61, "ed": 63, "text": "image understanding"}, {"st": 64, "ed": 69, "text": "deep convolutional neural networks cnn"}, {"st": 77, "ed": 79, "text": "transfer learning"}, {"st": 88, "ed": 90, "text": "pre trained"}, {"st": 91, "ed": 93, "text": "large scale"}, {"st": 97, "ed": 99, "text": "experiments conducted"}, {"st": 101, "ed": 103, "text": "real world"}, {"st": 114, "ed": 116, "text": "visual sentiment"}]
[{"st": 9, "ed": 13, "text": "signal to noise ratio"}, {"st": 79, "ed": 81, "text": "non negativity"}, {"st": 93, "ed": 95, "text": "non negativity"}, {"st": 104, "ed": 106, "text": "optimization problems"}, {"st": 107, "ed": 109, "text": "solved efficiently"}, {"st": 111, "ed": 117, "text": "alternating direction method of multipliers admm"}, {"st": 120, "ed": 123, "text": "synthetic and real"}]
[{"st": 33, "ed": 35, "text": "small sample"}, {"st": 38, "ed": 40, "text": "transfer learning"}, {"st": 56, "ed": 58, "text": "prediction task"}, {"st": 112, "ed": 114, "text": "transfer learning"}]
[{"st": 3, "ed": 5, "text": "edge detection"}, {"st": 16, "ed": 18, "text": "class labels"}, {"st": 61, "ed": 63, "text": "zero shot"}, {"st": 70, "ed": 72, "text": "neural net"}, {"st": 94, "ed": 96, "text": "class labels"}, {"st": 131, "ed": 133, "text": "layer wise"}]
[{"st": 3, "ed": 5, "text": "remote sensing"}, {"st": 44, "ed": 46, "text": "machine learning"}, {"st": 57, "ed": 59, "text": "prediction performance"}, {"st": 67, "ed": 69, "text": "genetic programming"}, {"st": 77, "ed": 79, "text": "feature construction"}, {"st": 93, "ed": 95, "text": "predictive models"}, {"st": 112, "ed": 114, "text": "real world"}]
[{"st": 8, "ed": 10, "text": "multi kernel"}, {"st": 11, "ed": 14, "text": "support vector machines"}, {"st": 41, "ed": 43, "text": "hand crafted"}, {"st": 48, "ed": 50, "text": "deep convolutional"}, {"st": 61, "ed": 63, "text": "gaussian processes"}, {"st": 66, "ed": 71, "text": "deep convolutional neural networks cnn"}, {"st": 77, "ed": 79, "text": "neural network"}, {"st": 91, "ed": 93, "text": "deep neural"}, {"st": 93, "ed": 95, "text": "convolutional network"}]
[{"st": 0, "ed": 2, "text": "unlike traditional"}, {"st": 2, "ed": 4, "text": "reinforcement learning"}, {"st": 16, "ed": 20, "text": "partially observable markov decision"}]
[{"st": 16, "ed": 18, "text": "generative models"}, {"st": 44, "ed": 46, "text": "goal oriented"}, {"st": 51, "ed": 53, "text": "structural similarity"}, {"st": 82, "ed": 84, "text": "implicit memory"}, {"st": 108, "ed": 110, "text": "reinforcement learning"}, {"st": 126, "ed": 128, "text": "near optimal"}]
[{"st": 2, "ed": 4, "text": "intelligent agent"}, {"st": 29, "ed": 31, "text": "current approaches"}, {"st": 34, "ed": 36, "text": "feature extraction"}, {"st": 37, "ed": 39, "text": "ad hoc"}, {"st": 78, "ed": 80, "text": "continuous state"}, {"st": 115, "ed": 118, "text": "ability to learn"}]
[{"st": 0, "ed": 3, "text": "recurrent neural networks"}, {"st": 8, "ed": 10, "text": "time series"}, {"st": 19, "ed": 21, "text": "learning task"}, {"st": 23, "ed": 25, "text": "minimization problem"}, {"st": 28, "ed": 30, "text": "least squares"}, {"st": 37, "ed": 39, "text": "cost function"}, {"st": 56, "ed": 58, "text": "policy iteration"}, {"st": 72, "ed": 74, "text": "reinforcement learning"}]
[{"st": 8, "ed": 10, "text": "multi class"}, {"st": 36, "ed": 38, "text": "hidden neurons"}, {"st": 106, "ed": 108, "text": "correctly classified"}]
[{"st": 5, "ed": 7, "text": "domain knowledge"}, {"st": 19, "ed": 21, "text": "domain knowledge"}, {"st": 24, "ed": 26, "text": "prior knowledge"}, {"st": 49, "ed": 51, "text": "real valued"}, {"st": 72, "ed": 74, "text": "neural network"}, {"st": 83, "ed": 85, "text": "neural network"}, {"st": 97, "ed": 99, "text": "molecular biology"}, {"st": 105, "ed": 107, "text": "learning algorithms"}, {"st": 108, "ed": 110, "text": "standard backpropagation"}, {"st": 136, "ed": 138, "text": "improve performance"}, {"st": 141, "ed": 143, "text": "learning algorithms"}]
[{"st": 7, "ed": 9, "text": "reinforcement learning"}, {"st": 21, "ed": 23, "text": "temporal difference"}, {"st": 25, "ed": 27, "text": "evolutionary algorithms"}, {"st": 44, "ed": 46, "text": "temporal difference"}, {"st": 54, "ed": 56, "text": "evolutionary algorithms"}, {"st": 58, "ed": 60, "text": "reinforcement learning"}, {"st": 69, "ed": 71, "text": "problem specific"}, {"st": 73, "ed": 76, "text": "strengths and weaknesses"}, {"st": 81, "ed": 83, "text": "reinforcement learning"}]
[{"st": 22, "ed": 24, "text": "joint training"}, {"st": 42, "ed": 44, "text": "regularization scheme"}, {"st": 47, "ed": 49, "text": "weight vectors"}, {"st": 52, "ed": 54, "text": "hidden unit"}, {"st": 70, "ed": 72, "text": "maximum likelihood"}, {"st": 76, "ed": 78, "text": "training strategy"}]
[{"st": 0, "ed": 2, "text": "fuzzy inference"}, {"st": 35, "ed": 37, "text": "fuzzy inference"}, {"st": 54, "ed": 56, "text": "multi layer"}]
[{"st": 14, "ed": 16, "text": "machine learning"}, {"st": 27, "ed": 29, "text": "neural networks"}, {"st": 41, "ed": 43, "text": "learning process"}, {"st": 80, "ed": 82, "text": "recently proposed"}, {"st": 82, "ed": 84, "text": "neural network"}, {"st": 86, "ed": 88, "text": "queueing theory"}, {"st": 90, "ed": 92, "text": "neural network"}, {"st": 112, "ed": 114, "text": "echo state"}, {"st": 152, "ed": 154, "text": "machine learning"}]
[{"st": 3, "ed": 5, "text": "key challenges"}, {"st": 6, "ed": 8, "text": "artificial intelligence"}, {"st": 52, "ed": 54, "text": "forward pass"}, {"st": 69, "ed": 73, "text": "trained end to end"}, {"st": 108, "ed": 110, "text": "accurate predictions"}, {"st": 112, "ed": 115, "text": "deep neural network"}]
[{"st": 7, "ed": 9, "text": "combinatorial optimization"}, {"st": 19, "ed": 21, "text": "quadratic programming"}, {"st": 50, "ed": 52, "text": "classification problems"}, {"st": 74, "ed": 76, "text": "binary classification"}, {"st": 109, "ed": 111, "text": "optimization problem"}, {"st": 130, "ed": 132, "text": "optimal solution"}, {"st": 152, "ed": 154, "text": "proposed method"}, {"st": 158, "ed": 160, "text": "neural network"}]
[{"st": 0, "ed": 4, "text": "recurrent neural networks rnns"}, {"st": 6, "ed": 9, "text": "short term memory"}, {"st": 85, "ed": 87, "text": "main result"}, {"st": 95, "ed": 97, "text": "character level"}]
[{"st": 8, "ed": 10, "text": "neural network"}, {"st": 32, "ed": 34, "text": "neural networks"}, {"st": 35, "ed": 37, "text": "weight parameters"}, {"st": 42, "ed": 44, "text": "hidden layer"}, {"st": 63, "ed": 65, "text": "neural network"}, {"st": 70, "ed": 72, "text": "resource constrained"}, {"st": 79, "ed": 82, "text": "fixed point arithmetic"}, {"st": 101, "ed": 103, "text": "power consumption"}, {"st": 140, "ed": 142, "text": "real valued"}, {"st": 150, "ed": 152, "text": "mnist dataset"}, {"st": 162, "ed": 164, "text": "competitive performance"}]
[{"st": 1, "ed": 3, "text": "recent years"}, {"st": 10, "ed": 12, "text": "deep representations"}, {"st": 26, "ed": 29, "text": "deep q networks"}, {"st": 44, "ed": 47, "text": "markov decision process"}, {"st": 62, "ed": 64, "text": "spatio temporal"}, {"st": 128, "ed": 131, "text": "deep neural networks"}]
[{"st": 4, "ed": 6, "text": "data science"}, {"st": 20, "ed": 22, "text": "machine learning"}, {"st": 47, "ed": 49, "text": "machine learning"}, {"st": 54, "ed": 56, "text": "open source"}, {"st": 72, "ed": 76, "text": "simulated and real world"}, {"st": 87, "ed": 89, "text": "machine learning"}, {"st": 93, "ed": 95, "text": "significant improvement"}, {"st": 98, "ed": 100, "text": "machine learning"}, {"st": 108, "ed": 110, "text": "prior knowledge"}, {"st": 133, "ed": 135, "text": "without sacrificing"}, {"st": 148, "ed": 150, "text": "machine learning"}]
[{"st": 7, "ed": 9, "text": "deep learning"}, {"st": 9, "ed": 11, "text": "approach called"}, {"st": 25, "ed": 27, "text": "neural networks"}, {"st": 37, "ed": 39, "text": "neural networks"}, {"st": 47, "ed": 49, "text": "conditional probability"}, {"st": 69, "ed": 71, "text": "conditional probabilities"}, {"st": 87, "ed": 89, "text": "case studies"}, {"st": 96, "ed": 99, "text": "deep neural networks"}, {"st": 103, "ed": 105, "text": "neural nets"}, {"st": 115, "ed": 117, "text": "textual entailment"}, {"st": 120, "ed": 122, "text": "multi relational"}, {"st": 153, "ed": 155, "text": "significantly outperform"}, {"st": 156, "ed": 158, "text": "conventional methods"}, {"st": 175, "ed": 177, "text": "pre trained"}, {"st": 216, "ed": 218, "text": "experiments conducted"}]
[{"st": 0, "ed": 3, "text": "deep reinforcement learning"}, {"st": 23, "ed": 25, "text": "complex tasks"}, {"st": 50, "ed": 53, "text": "deep reinforcement learning"}, {"st": 73, "ed": 75, "text": "significant improvements"}, {"st": 99, "ed": 101, "text": "final results"}, {"st": 104, "ed": 106, "text": "deep rl"}]
[{"st": 26, "ed": 29, "text": "recurrent neural networks"}, {"st": 66, "ed": 68, "text": "attention model"}, {"st": 72, "ed": 75, "text": "electronic health records"}, {"st": 92, "ed": 94, "text": "neural attention"}, {"st": 161, "ed": 163, "text": "predictive accuracy"}]
[{"st": 6, "ed": 8, "text": "neural network"}, {"st": 11, "ed": 14, "text": "extreme learning machines"}, {"st": 15, "ed": 18, "text": "multi label classification"}, {"st": 24, "ed": 27, "text": "multi label classification"}, {"st": 34, "ed": 37, "text": "multi class classification"}, {"st": 43, "ed": 46, "text": "extreme learning machine"}, {"st": 51, "ed": 53, "text": "multi label"}, {"st": 81, "ed": 84, "text": "multi label classification"}, {"st": 93, "ed": 95, "text": "multi label"}, {"st": 100, "ed": 102, "text": "benchmark datasets"}, {"st": 105, "ed": 107, "text": "application areas"}, {"st": 158, "ed": 160, "text": "multi label"}]
[{"st": 10, "ed": 12, "text": "input samples"}, {"st": 22, "ed": 24, "text": "classification problems"}, {"st": 34, "ed": 36, "text": "multi class"}, {"st": 50, "ed": 52, "text": "multi class"}, {"st": 53, "ed": 56, "text": "multi label classification"}, {"st": 106, "ed": 108, "text": "streaming data"}, {"st": 121, "ed": 123, "text": "multi class"}, {"st": 124, "ed": 126, "text": "multi label"}]
[{"st": 1, "ed": 3, "text": "machine learning"}, {"st": 26, "ed": 28, "text": "concept drift"}, {"st": 36, "ed": 39, "text": "extreme learning machine"}, {"st": 53, "ed": 56, "text": "classification and regression"}, {"st": 120, "ed": 122, "text": "classification problem"}, {"st": 124, "ed": 126, "text": "concept drift"}, {"st": 167, "ed": 169, "text": "hidden nodes"}, {"st": 180, "ed": 182, "text": "hidden nodes"}]
[{"st": 1, "ed": 3, "text": "big data"}, {"st": 41, "ed": 45, "text": "convolutional neural network cnn"}, {"st": 48, "ed": 51, "text": "extreme learning machine"}, {"st": 61, "ed": 63, "text": "concept drift"}, {"st": 95, "ed": 97, "text": "concept drift"}, {"st": 109, "ed": 111, "text": "concept drift"}, {"st": 136, "ed": 138, "text": "current methods"}, {"st": 185, "ed": 187, "text": "proposed method"}, {"st": 221, "ed": 223, "text": "image data"}]
[{"st": 9, "ed": 11, "text": "continuous state"}, {"st": 29, "ed": 31, "text": "problem specific"}, {"st": 31, "ed": 33, "text": "cost functions"}, {"st": 39, "ed": 41, "text": "optimal control"}, {"st": 53, "ed": 55, "text": "real world"}, {"st": 55, "ed": 57, "text": "reinforcement learning"}, {"st": 62, "ed": 64, "text": "online learning"}, {"st": 71, "ed": 73, "text": "online learning"}, {"st": 89, "ed": 91, "text": "reinforcement learning"}, {"st": 120, "ed": 122, "text": "machine learning"}, {"st": 146, "ed": 148, "text": "self organizing"}, {"st": 165, "ed": 167, "text": "online learning"}, {"st": 199, "ed": 201, "text": "proposed approach"}]
[{"st": 1, "ed": 4, "text": "long term memory"}, {"st": 46, "ed": 48, "text": "neural network"}, {"st": 51, "ed": 53, "text": "specific task"}, {"st": 66, "ed": 68, "text": "open range"}, {"st": 76, "ed": 78, "text": "lifelong learning"}, {"st": 86, "ed": 89, "text": "content addressable memory"}, {"st": 122, "ed": 124, "text": "transfer learning"}, {"st": 126, "ed": 128, "text": "prior knowledge"}]
[{"st": 43, "ed": 45, "text": "proposed method"}, {"st": 49, "ed": 51, "text": "learning task"}, {"st": 71, "ed": 73, "text": "technique called"}, {"st": 92, "ed": 95, "text": "long term memory"}, {"st": 103, "ed": 105, "text": "working memory"}, {"st": 107, "ed": 109, "text": "sensory input"}, {"st": 116, "ed": 118, "text": "sensory data"}, {"st": 150, "ed": 152, "text": "discriminative features"}, {"st": 158, "ed": 160, "text": "sensory input"}]
[{"st": 1, "ed": 4, "text": "deep neural networks"}, {"st": 6, "ed": 8, "text": "machine learning"}, {"st": 27, "ed": 29, "text": "recent developments"}, {"st": 40, "ed": 42, "text": "local minima"}, {"st": 80, "ed": 82, "text": "sufficient conditions"}, {"st": 86, "ed": 88, "text": "critical point"}, {"st": 92, "ed": 94, "text": "local minimum"}, {"st": 135, "ed": 137, "text": "global minimum"}]
[{"st": 5, "ed": 7, "text": "neural network"}, {"st": 14, "ed": 17, "text": "structure and parameters"}, {"st": 25, "ed": 27, "text": "machine learning"}, {"st": 41, "ed": 43, "text": "learning rates"}, {"st": 58, "ed": 60, "text": "fine tuned"}, {"st": 64, "ed": 66, "text": "time consuming"}, {"st": 100, "ed": 102, "text": "mathematical optimization"}, {"st": 123, "ed": 126, "text": "radial basis function"}, {"st": 129, "ed": 131, "text": "objective function"}, {"st": 132, "ed": 134, "text": "prediction accuracy"}]
[{"st": 4, "ed": 6, "text": "deep learning"}, {"st": 6, "ed": 9, "text": "artificial neural networks"}, {"st": 15, "ed": 17, "text": "artificial intelligence"}, {"st": 30, "ed": 32, "text": "particle physics"}, {"st": 32, "ed": 34, "text": "reinforcement learning"}, {"st": 34, "ed": 36, "text": "speech recognition"}, {"st": 36, "ed": 38, "text": "computer vision"}, {"st": 48, "ed": 50, "text": "biological neural"}, {"st": 62, "ed": 65, "text": "artificial neural networks"}, {"st": 109, "ed": 111, "text": "random graph"}, {"st": 119, "ed": 121, "text": "scale free"}, {"st": 165, "ed": 168, "text": "training and inference"}, {"st": 182, "ed": 185, "text": "restricted boltzmann machine"}, {"st": 186, "ed": 189, "text": "multi layer perceptron"}, {"st": 194, "ed": 197, "text": "supervised and unsupervised"}, {"st": 224, "ed": 226, "text": "real world"}]
[{"st": 0, "ed": 3, "text": "the past decade"}, {"st": 21, "ed": 23, "text": "recent literature"}, {"st": 33, "ed": 35, "text": "large scale"}, {"st": 76, "ed": 78, "text": "previous studies"}, {"st": 85, "ed": 87, "text": "input signals"}, {"st": 101, "ed": 103, "text": "attention based"}, {"st": 103, "ed": 105, "text": "deep learning"}, {"st": 134, "ed": 137, "text": "short term memory"}, {"st": 142, "ed": 144, "text": "input signals"}]
[{"st": 0, "ed": 4, "text": "recurrent neural networks rnns"}, {"st": 9, "ed": 11, "text": "sequential data"}, {"st": 79, "ed": 82, "text": "training and inference"}, {"st": 88, "ed": 90, "text": "rnn architectures"}, {"st": 113, "ed": 115, "text": "surrogate model"}, {"st": 118, "ed": 121, "text": "short term memory"}, {"st": 156, "ed": 158, "text": "classification task"}]
[{"st": 23, "ed": 25, "text": "feature learning"}, {"st": 28, "ed": 30, "text": "raw data"}, {"st": 40, "ed": 42, "text": "recent years"}, {"st": 56, "ed": 58, "text": "feature learning"}, {"st": 71, "ed": 74, "text": "unsupervised feature learning"}, {"st": 94, "ed": 96, "text": "multi objective"}, {"st": 96, "ed": 98, "text": "optimization process"}, {"st": 118, "ed": 120, "text": "nonlinear function"}, {"st": 131, "ed": 133, "text": "input data"}, {"st": 135, "ed": 137, "text": "higher dimensional"}, {"st": 141, "ed": 143, "text": "optimization problem"}, {"st": 151, "ed": 153, "text": "genetic algorithm"}, {"st": 158, "ed": 160, "text": "evolutionary algorithm"}, {"st": 180, "ed": 182, "text": "machine learning"}, {"st": 184, "ed": 187, "text": "advantages and disadvantages"}, {"st": 189, "ed": 191, "text": "optimization strategy"}]
[{"st": 22, "ed": 24, "text": "meta learning"}, {"st": 37, "ed": 40, "text": "input and output"}, {"st": 59, "ed": 61, "text": "meta learning"}, {"st": 64, "ed": 66, "text": "deep representations"}, {"st": 70, "ed": 72, "text": "fine tuned"}, {"st": 85, "ed": 87, "text": "meta learning"}, {"st": 97, "ed": 99, "text": "learning algorithm"}, {"st": 103, "ed": 105, "text": "expressive power"}, {"st": 108, "ed": 110, "text": "recurrent models"}, {"st": 113, "ed": 115, "text": "recent approaches"}, {"st": 146, "ed": 148, "text": "learning algorithm"}, {"st": 164, "ed": 166, "text": "meta learning"}]
[{"st": 4, "ed": 6, "text": "reinforcement learning"}, {"st": 38, "ed": 40, "text": "higher level"}, {"st": 67, "ed": 69, "text": "reinforcement learning"}, {"st": 100, "ed": 102, "text": "likelihood ratio"}, {"st": 102, "ed": 104, "text": "policy gradient"}, {"st": 113, "ed": 115, "text": "highly successful"}, {"st": 117, "ed": 119, "text": "preliminary experiments"}, {"st": 125, "ed": 127, "text": "sample efficiency"}, {"st": 128, "ed": 130, "text": "policy gradient"}]
[{"st": 1, "ed": 4, "text": "deep neural networks"}, {"st": 8, "ed": 10, "text": "recent years"}, {"st": 13, "ed": 15, "text": "machine learning"}, {"st": 29, "ed": 32, "text": "deep neural networks"}, {"st": 59, "ed": 62, "text": "deep neural network"}, {"st": 97, "ed": 99, "text": "machine learning"}, {"st": 130, "ed": 132, "text": "deep neural"}, {"st": 160, "ed": 162, "text": "a 10"}, {"st": 178, "ed": 181, "text": "deep neural networks"}, {"st": 247, "ed": 249, "text": "preliminary results"}, {"st": 270, "ed": 273, "text": "deep neural network"}]
[{"st": 15, "ed": 17, "text": "deep learning"}, {"st": 22, "ed": 24, "text": "artificial intelligence"}, {"st": 33, "ed": 35, "text": "intrinsic motivation"}, {"st": 35, "ed": 37, "text": "social learning"}, {"st": 57, "ed": 59, "text": "deep learning"}, {"st": 62, "ed": 64, "text": "deep learning"}, {"st": 70, "ed": 72, "text": "artificial intelligence"}, {"st": 93, "ed": 95, "text": "causal models"}, {"st": 161, "ed": 163, "text": "central role"}, {"st": 164, "ed": 166, "text": "artificial intelligence"}, {"st": 178, "ed": 180, "text": "task specific"}, {"st": 180, "ed": 182, "text": "objective function"}, {"st": 189, "ed": 191, "text": "off line"}, {"st": 202, "ed": 204, "text": "open ended"}, {"st": 223, "ed": 225, "text": "intrinsic motivation"}, {"st": 227, "ed": 229, "text": "social learning"}, {"st": 243, "ed": 245, "text": "child development"}, {"st": 315, "ed": 317, "text": "strong interaction"}, {"st": 318, "ed": 320, "text": "developmental psychology"}]
[{"st": 8, "ed": 10, "text": "optimal policy"}, {"st": 17, "ed": 19, "text": "partially observable"}, {"st": 19, "ed": 21, "text": "reinforcement learning"}, {"st": 65, "ed": 67, "text": "likelihood ratio"}, {"st": 119, "ed": 121, "text": "empirical results"}, {"st": 124, "ed": 126, "text": "sample complexity"}]
[{"st": 22, "ed": 25, "text": "bayesian optimization algorithm"}]
[{"st": 11, "ed": 13, "text": "neural network"}, {"st": 14, "ed": 16, "text": "decision tree"}, {"st": 22, "ed": 24, "text": "classification rules"}, {"st": 62, "ed": 64, "text": "decision tree"}, {"st": 68, "ed": 70, "text": "classification rules"}, {"st": 76, "ed": 78, "text": "classification error"}, {"st": 90, "ed": 92, "text": "machine learning"}]
[{"st": 28, "ed": 30, "text": "incremental learning"}, {"st": 32, "ed": 34, "text": "successfully applied"}, {"st": 36, "ed": 38, "text": "classification problems"}, {"st": 66, "ed": 68, "text": "incremental learning"}, {"st": 72, "ed": 74, "text": "proposed method"}, {"st": 75, "ed": 77, "text": "incremental learning"}, {"st": 78, "ed": 80, "text": "genetic algorithm"}, {"st": 86, "ed": 88, "text": "incremental learning"}, {"st": 90, "ed": 92, "text": "benchmark datasets"}, {"st": 106, "ed": 108, "text": "incremental learning"}, {"st": 128, "ed": 131, "text": "optical character recognition"}, {"st": 147, "ed": 149, "text": "a 4"}, {"st": 156, "ed": 158, "text": "multi class"}]
[{"st": 4, "ed": 6, "text": "unsupervised learning"}, {"st": 9, "ed": 11, "text": "multi layer"}, {"st": 12, "ed": 14, "text": "neural network"}, {"st": 19, "ed": 21, "text": "multi layer"}, {"st": 22, "ed": 24, "text": "neural network"}, {"st": 26, "ed": 28, "text": "neural network"}, {"st": 44, "ed": 47, "text": "linear dimensionality reduction"}, {"st": 50, "ed": 52, "text": "low dimensional"}, {"st": 57, "ed": 59, "text": "pattern classification"}, {"st": 67, "ed": 69, "text": "linear activation"}, {"st": 120, "ed": 122, "text": "neural network"}, {"st": 127, "ed": 129, "text": "input vector"}, {"st": 149, "ed": 151, "text": "output layer"}, {"st": 157, "ed": 159, "text": "feature set"}, {"st": 162, "ed": 164, "text": "hidden layer"}, {"st": 176, "ed": 178, "text": "input data"}, {"st": 215, "ed": 217, "text": "unsupervised learning"}, {"st": 227, "ed": 229, "text": "impressive results"}]
[{"st": 28, "ed": 30, "text": "classification problems"}, {"st": 86, "ed": 88, "text": "network structure"}, {"st": 111, "ed": 113, "text": "generalization capabilities"}]
[{"st": 0, "ed": 2, "text": "feature selection"}, {"st": 8, "ed": 10, "text": "relevant features"}, {"st": 18, "ed": 20, "text": "feature selection"}, {"st": 30, "ed": 32, "text": "rough set"}, {"st": 61, "ed": 63, "text": "feature selection"}, {"st": 66, "ed": 68, "text": "rough set"}, {"st": 108, "ed": 110, "text": "rough set"}, {"st": 113, "ed": 115, "text": "genetic algorithm"}]
[{"st": 10, "ed": 13, "text": "bag of words"}, {"st": 81, "ed": 84, "text": "the paper presents"}, {"st": 144, "ed": 146, "text": "significant improvement"}, {"st": 162, "ed": 164, "text": "proposed framework"}, {"st": 172, "ed": 174, "text": "based approach"}]
[{"st": 31, "ed": 33, "text": "probabilistic model"}, {"st": 41, "ed": 43, "text": "low dimensional"}, {"st": 54, "ed": 56, "text": "higher dimensional"}, {"st": 60, "ed": 63, "text": "curse of dimensionality"}, {"st": 75, "ed": 77, "text": "higher dimensional"}, {"st": 92, "ed": 94, "text": "probabilistic model"}, {"st": 150, "ed": 152, "text": "dependent variable"}, {"st": 173, "ed": 175, "text": "computational cost"}, {"st": 243, "ed": 245, "text": "large scale"}]
[{"st": 8, "ed": 10, "text": "transfer learning"}, {"st": 12, "ed": 14, "text": "hierarchical bayesian"}, {"st": 14, "ed": 16, "text": "optimization algorithm"}, {"st": 19, "ed": 21, "text": "distance based"}, {"st": 34, "ed": 36, "text": "probabilistic models"}, {"st": 69, "ed": 71, "text": "np complete"}, {"st": 71, "ed": 73, "text": "problems including"}, {"st": 78, "ed": 80, "text": "vertex cover"}, {"st": 100, "ed": 102, "text": "empirical evidence"}, {"st": 104, "ed": 106, "text": "transfer learning"}]
[{"st": 66, "ed": 68, "text": "open ended"}, {"st": 76, "ed": 78, "text": "dynamical systems"}]
[{"st": 19, "ed": 21, "text": "neural networks"}, {"st": 25, "ed": 27, "text": "genetic programming"}, {"st": 51, "ed": 53, "text": "fuzzy logic"}, {"st": 73, "ed": 75, "text": "open ended"}, {"st": 83, "ed": 85, "text": "dynamical systems"}]
[{"st": 13, "ed": 15, "text": "computational intelligence"}, {"st": 26, "ed": 28, "text": "based approaches"}, {"st": 38, "ed": 40, "text": "promising results"}, {"st": 41, "ed": 43, "text": "successful applications"}, {"st": 69, "ed": 71, "text": "framework named"}, {"st": 83, "ed": 85, "text": "challenging problems"}, {"st": 143, "ed": 146, "text": "proof of concept"}, {"st": 155, "ed": 157, "text": "open source"}, {"st": 157, "ed": 160, "text": "first person shooter"}]
[{"st": 12, "ed": 14, "text": "board games"}, {"st": 110, "ed": 112, "text": "learning curve"}]
[]
[{"st": 0, "ed": 3, "text": "the paper presents"}, {"st": 7, "ed": 9, "text": "motion planning"}, {"st": 14, "ed": 16, "text": "decision making"}, {"st": 24, "ed": 27, "text": "artificial neural network"}]
[{"st": 40, "ed": 42, "text": "equivalence classes"}, {"st": 101, "ed": 103, "text": "greedy search"}]
[{"st": 20, "ed": 22, "text": "inference algorithm"}, {"st": 36, "ed": 38, "text": "computation graph"}, {"st": 41, "ed": 43, "text": "neural network"}, {"st": 63, "ed": 65, "text": "neural network"}, {"st": 79, "ed": 81, "text": "question answering"}]
[{"st": 0, "ed": 2, "text": "machine learning"}, {"st": 20, "ed": 22, "text": "deep learning"}, {"st": 42, "ed": 44, "text": "network outputs"}, {"st": 76, "ed": 80, "text": "number of training samples"}, {"st": 87, "ed": 89, "text": "pre trained"}, {"st": 128, "ed": 130, "text": "supervised learning"}, {"st": 163, "ed": 165, "text": "state farm"}]
[{"st": 2, "ed": 4, "text": "machine learning"}, {"st": 12, "ed": 14, "text": "input output"}, {"st": 16, "ed": 18, "text": "source code"}, {"st": 25, "ed": 27, "text": "key contribution"}, {"st": 30, "ed": 33, "text": "domain specific language"}, {"st": 70, "ed": 72, "text": "input output"}, {"st": 105, "ed": 107, "text": "linear program"}, {"st": 149, "ed": 151, "text": "inference algorithm"}, {"st": 173, "ed": 175, "text": "extensive empirical"}, {"st": 178, "ed": 180, "text": "inference algorithms"}]
[{"st": 12, "ed": 14, "text": "neural networks"}, {"st": 82, "ed": 84, "text": "local learning"}, {"st": 124, "ed": 128, "text": "mnist and cifar 10"}, {"st": 153, "ed": 155, "text": "activation functions"}, {"st": 208, "ed": 210, "text": "fixed points"}, {"st": 219, "ed": 221, "text": "fixed points"}, {"st": 238, "ed": 241, "text": "single hidden layer"}, {"st": 244, "ed": 246, "text": "special cases"}, {"st": 250, "ed": 252, "text": "fixed points"}, {"st": 261, "ed": 263, "text": "activation functions"}]
[{"st": 14, "ed": 16, "text": "residual networks"}, {"st": 25, "ed": 27, "text": "feedforward networks"}, {"st": 62, "ed": 64, "text": "deep learning"}, {"st": 141, "ed": 143, "text": "preliminary experiments"}]
[{"st": 14, "ed": 16, "text": "cardiovascular disease"}, {"st": 30, "ed": 32, "text": "non invasive"}, {"st": 56, "ed": 59, "text": "end to end"}, {"st": 99, "ed": 102, "text": "coronary artery disease"}, {"st": 105, "ed": 107, "text": "chest pain"}, {"st": 117, "ed": 119, "text": "significantly outperform"}]
[{"st": 20, "ed": 23, "text": "simple and effective"}, {"st": 28, "ed": 30, "text": "neural network"}, {"st": 48, "ed": 50, "text": "benchmark datasets"}, {"st": 55, "ed": 57, "text": "neural network"}, {"st": 66, "ed": 68, "text": "gaussian processes"}, {"st": 69, "ed": 72, "text": "support vector machines"}, {"st": 73, "ed": 75, "text": "achieves comparable"}, {"st": 83, "ed": 85, "text": "neural networks"}, {"st": 111, "ed": 113, "text": "large scale"}, {"st": 123, "ed": 125, "text": "collaborative filtering"}]
[{"st": 3, "ed": 5, "text": "sight reading"}, {"st": 11, "ed": 13, "text": "cognitive psychology"}, {"st": 22, "ed": 24, "text": "reinforcement learning"}, {"st": 106, "ed": 110, "text": "partially observable markov decision"}, {"st": 119, "ed": 121, "text": "faster learning"}]
[{"st": 16, "ed": 18, "text": "neural networks"}, {"st": 52, "ed": 54, "text": "honey bee"}, {"st": 55, "ed": 57, "text": "multilayer perceptron"}, {"st": 62, "ed": 64, "text": "back propagation"}, {"st": 67, "ed": 69, "text": "computationally intensive"}, {"st": 98, "ed": 100, "text": "local optima"}, {"st": 121, "ed": 123, "text": "time series"}, {"st": 143, "ed": 145, "text": "result shows"}, {"st": 155, "ed": 157, "text": "time series"}]
[{"st": 24, "ed": 27, "text": "central pattern generator"}, {"st": 90, "ed": 92, "text": "simulated annealing"}]
[{"st": 7, "ed": 10, "text": "a long standing"}, {"st": 48, "ed": 50, "text": "pattern recognition"}, {"st": 61, "ed": 65, "text": "deep convolutional neural networks"}, {"st": 182, "ed": 185, "text": "convolutional neural networks"}, {"st": 208, "ed": 210, "text": "monte carlo"}]
[{"st": 10, "ed": 12, "text": "temporal dependencies"}, {"st": 19, "ed": 22, "text": "recurrent neural network"}, {"st": 46, "ed": 48, "text": "temporal information"}, {"st": 50, "ed": 52, "text": "multi layer"}, {"st": 88, "ed": 90, "text": "complex data"}, {"st": 102, "ed": 104, "text": "polyphonic music"}]
[{"st": 33, "ed": 35, "text": "neural network"}, {"st": 56, "ed": 59, "text": "deep q network"}]
[{"st": 27, "ed": 29, "text": "partially observable"}]
[{"st": 4, "ed": 6, "text": "deep learning"}, {"st": 10, "ed": 12, "text": "control policies"}, {"st": 16, "ed": 18, "text": "sensory input"}, {"st": 27, "ed": 30, "text": "deep q network"}, {"st": 31, "ed": 34, "text": "convolutional neural network"}, {"st": 44, "ed": 46, "text": "raw pixels"}, {"st": 67, "ed": 70, "text": "deep q network"}, {"st": 83, "ed": 85, "text": "reinforcement learning"}, {"st": 106, "ed": 109, "text": "deep q network"}]
[{"st": 9, "ed": 11, "text": "neural network"}, {"st": 20, "ed": 22, "text": "domain knowledge"}, {"st": 44, "ed": 47, "text": "deep neural networks"}, {"st": 75, "ed": 78, "text": "stochastic gradient descent"}, {"st": 95, "ed": 97, "text": "hidden layers"}]
[{"st": 5, "ed": 7, "text": "chess engine"}, {"st": 15, "ed": 17, "text": "domain specific"}, {"st": 20, "ed": 22, "text": "hand crafted"}, {"st": 27, "ed": 29, "text": "unlike previous"}, {"st": 31, "ed": 33, "text": "machine learning"}, {"st": 36, "ed": 38, "text": "parameter tuning"}, {"st": 39, "ed": 41, "text": "hand crafted"}, {"st": 50, "ed": 52, "text": "feature extraction"}, {"st": 57, "ed": 59, "text": "evaluation function"}, {"st": 59, "ed": 61, "text": "performs comparably"}, {"st": 81, "ed": 83, "text": "hand crafted"}, {"st": 91, "ed": 93, "text": "computer chess"}, {"st": 108, "ed": 111, "text": "end to end"}, {"st": 111, "ed": 113, "text": "machine learning"}]
[{"st": 17, "ed": 20, "text": "neural network based"}, {"st": 36, "ed": 38, "text": "encoder network"}, {"st": 40, "ed": 42, "text": "word level"}, {"st": 52, "ed": 54, "text": "recurrent network"}, {"st": 63, "ed": 65, "text": "decoder network"}, {"st": 67, "ed": 69, "text": "recurrent network"}, {"st": 92, "ed": 94, "text": "attention mechanism"}, {"st": 111, "ed": 115, "text": "trained end to end"}]
[{"st": 5, "ed": 8, "text": "deep neural networks"}, {"st": 18, "ed": 20, "text": "reinforcement learning"}, {"st": 22, "ed": 24, "text": "continuous state"}, {"st": 41, "ed": 44, "text": "deep neural networks"}, {"st": 47, "ed": 49, "text": "continuous action"}, {"st": 107, "ed": 110, "text": "deep reinforcement learning"}]
[{"st": 15, "ed": 17, "text": "machine learning"}, {"st": 48, "ed": 50, "text": "fully connected"}, {"st": 50, "ed": 52, "text": "convolutional network"}, {"st": 57, "ed": 59, "text": "reinforcement learning"}, {"st": 83, "ed": 86, "text": "directions for future"}, {"st": 112, "ed": 114, "text": "directly applied"}]
[{"st": 7, "ed": 9, "text": "reinforcement learning"}, {"st": 11, "ed": 13, "text": "partially observable"}, {"st": 19, "ed": 23, "text": "recurrent neural networks rnns"}, {"st": 49, "ed": 51, "text": "predictive model"}, {"st": 69, "ed": 72, "text": "algorithmic information theory"}, {"st": 74, "ed": 76, "text": "rnn based"}, {"st": 116, "ed": 118, "text": "rnn based"}, {"st": 125, "ed": 127, "text": "rnn based"}, {"st": 147, "ed": 149, "text": "decision making"}]
[{"st": 11, "ed": 13, "text": "reinforcement learning"}, {"st": 15, "ed": 17, "text": "partially observable"}, {"st": 20, "ed": 23, "text": "recurrent neural network"}, {"st": 25, "ed": 28, "text": "short term memory"}, {"st": 28, "ed": 31, "text": "gated recurrent unit"}, {"st": 34, "ed": 36, "text": "recurrent neural"}]
[{"st": 34, "ed": 38, "text": "recurrent neural networks rnns"}, {"st": 65, "ed": 67, "text": "specifically designed"}, {"st": 73, "ed": 76, "text": "static and dynamic"}, {"st": 80, "ed": 82, "text": "predict future"}, {"st": 139, "ed": 142, "text": "electronic health record"}, {"st": 183, "ed": 186, "text": "feedforward neural network"}, {"st": 188, "ed": 190, "text": "logistic regression"}, {"st": 201, "ed": 203, "text": "gated recurrent"}, {"st": 234, "ed": 237, "text": "feedforward neural network"}]
[{"st": 8, "ed": 10, "text": "weight vectors"}, {"st": 12, "ed": 14, "text": "neural network"}, {"st": 20, "ed": 22, "text": "weight vectors"}, {"st": 38, "ed": 40, "text": "optimization problem"}, {"st": 54, "ed": 56, "text": "batch normalization"}, {"st": 79, "ed": 81, "text": "recurrent models"}, {"st": 91, "ed": 94, "text": "deep reinforcement learning"}, {"st": 95, "ed": 97, "text": "generative models"}, {"st": 99, "ed": 101, "text": "batch normalization"}, {"st": 126, "ed": 128, "text": "computational overhead"}, {"st": 157, "ed": 159, "text": "image recognition"}]
[{"st": 2, "ed": 4, "text": "decision makers"}, {"st": 5, "ed": 7, "text": "sensory input"}, {"st": 16, "ed": 18, "text": "decision makers"}, {"st": 46, "ed": 48, "text": "weight update"}, {"st": 62, "ed": 64, "text": "decision maker"}, {"st": 72, "ed": 74, "text": "update rules"}, {"st": 74, "ed": 76, "text": "bounded rationality"}, {"st": 93, "ed": 95, "text": "classification task"}, {"st": 96, "ed": 98, "text": "handwritten digits"}]
[{"st": 16, "ed": 18, "text": "external memory"}, {"st": 27, "ed": 29, "text": "mathbb r"}, {"st": 60, "ed": 62, "text": "random access"}, {"st": 70, "ed": 72, "text": "lie group"}, {"st": 98, "ed": 100, "text": "lie group"}, {"st": 186, "ed": 189, "text": "orders of magnitude"}]
[{"st": 9, "ed": 11, "text": "learning algorithm"}]
[{"st": 0, "ed": 3, "text": "deep reinforcement learning"}, {"st": 13, "ed": 15, "text": "control policies"}, {"st": 76, "ed": 80, "text": "deep q network dqn"}, {"st": 82, "ed": 84, "text": "network architectures"}, {"st": 126, "ed": 129, "text": "deep q network"}]
[{"st": 4, "ed": 6, "text": "training data"}, {"st": 23, "ed": 25, "text": "prior knowledge"}, {"st": 37, "ed": 39, "text": "procedural knowledge"}, {"st": 40, "ed": 42, "text": "neural networks"}, {"st": 69, "ed": 72, "text": "end to end"}, {"st": 76, "ed": 78, "text": "programming language"}, {"st": 97, "ed": 99, "text": "input output"}, {"st": 160, "ed": 162, "text": "trained jointly"}, {"st": 171, "ed": 174, "text": "end to end"}, {"st": 179, "ed": 181, "text": "natural language"}]
[{"st": 0, "ed": 2, "text": "recent advances"}, {"st": 3, "ed": 5, "text": "deep learning"}, {"st": 10, "ed": 12, "text": "high level"}, {"st": 59, "ed": 61, "text": "promising results"}, {"st": 64, "ed": 66, "text": "higher level"}, {"st": 83, "ed": 87, "text": "deep recurrent neural network"}, {"st": 90, "ed": 92, "text": "motion capture"}]
[{"st": 52, "ed": 54, "text": "feature vectors"}, {"st": 89, "ed": 91, "text": "neural networks"}]
[{"st": 1, "ed": 3, "text": "data science"}, {"st": 16, "ed": 18, "text": "data science"}, {"st": 27, "ed": 29, "text": "machine learning"}, {"st": 40, "ed": 42, "text": "machine learning"}, {"st": 49, "ed": 51, "text": "genetic programming"}, {"st": 64, "ed": 66, "text": "machine learning"}, {"st": 72, "ed": 74, "text": "classification accuracy"}, {"st": 76, "ed": 78, "text": "supervised classification"}, {"st": 94, "ed": 96, "text": "supervised classification"}, {"st": 103, "ed": 105, "text": "machine learning"}, {"st": 115, "ed": 117, "text": "building blocks"}, {"st": 118, "ed": 120, "text": "machine learning"}, {"st": 124, "ed": 126, "text": "building blocks"}, {"st": 139, "ed": 141, "text": "significantly improves"}, {"st": 161, "ed": 163, "text": "machine learning"}, {"st": 164, "ed": 166, "text": "building blocks"}]
[{"st": 3, "ed": 6, "text": "learning from demonstration"}, {"st": 20, "ed": 23, "text": "learning from demonstration"}, {"st": 24, "ed": 27, "text": "inverse reinforcement learning"}, {"st": 39, "ed": 41, "text": "feature based"}, {"st": 45, "ed": 48, "text": "inverse reinforcement learning"}, {"st": 54, "ed": 56, "text": "neural networks"}, {"st": 64, "ed": 66, "text": "neural networks"}, {"st": 94, "ed": 96, "text": "feature based"}, {"st": 96, "ed": 99, "text": "inverse reinforcement learning"}, {"st": 106, "ed": 108, "text": "neural networks"}, {"st": 111, "ed": 113, "text": "linear combinations"}, {"st": 152, "ed": 154, "text": "feature construction"}, {"st": 155, "ed": 158, "text": "inverse reinforcement learning"}, {"st": 161, "ed": 163, "text": "linear combinations"}, {"st": 183, "ed": 186, "text": "markov decision processes"}]
[{"st": 2, "ed": 4, "text": "machine learning"}, {"st": 10, "ed": 12, "text": "input output"}, {"st": 17, "ed": 19, "text": "source code"}, {"st": 31, "ed": 33, "text": "machine learning"}, {"st": 36, "ed": 38, "text": "neural networks"}, {"st": 47, "ed": 49, "text": "machine learning"}, {"st": 67, "ed": 69, "text": "key contribution"}, {"st": 75, "ed": 78, "text": "domain specific language"}, {"st": 88, "ed": 90, "text": "probabilistic programming"}, {"st": 104, "ed": 106, "text": "random variables"}, {"st": 132, "ed": 134, "text": "input output"}, {"st": 168, "ed": 170, "text": "inference algorithm"}, {"st": 200, "ed": 202, "text": "linear program"}, {"st": 230, "ed": 232, "text": "empirical comparison"}, {"st": 258, "ed": 260, "text": "machine learning"}]
[{"st": 4, "ed": 7, "text": "extreme learning machine"}, {"st": 11, "ed": 14, "text": "multi label classification"}, {"st": 20, "ed": 23, "text": "multi label classification"}, {"st": 26, "ed": 28, "text": "input data"}, {"st": 42, "ed": 45, "text": "multi class classification"}, {"st": 51, "ed": 53, "text": "multi label"}, {"st": 70, "ed": 73, "text": "multi label classification"}, {"st": 80, "ed": 82, "text": "multi label"}, {"st": 103, "ed": 105, "text": "proposed method"}, {"st": 129, "ed": 131, "text": "multi label"}, {"st": 139, "ed": 142, "text": "extreme learning machine"}, {"st": 143, "ed": 146, "text": "multi label classification"}, {"st": 160, "ed": 162, "text": "multi label"}]
[{"st": 5, "ed": 8, "text": "extreme learning machine"}, {"st": 10, "ed": 12, "text": "multi label"}, {"st": 20, "ed": 23, "text": "multi label classification"}, {"st": 29, "ed": 31, "text": "machine learning"}, {"st": 39, "ed": 41, "text": "recent years"}, {"st": 46, "ed": 48, "text": "real world"}, {"st": 55, "ed": 57, "text": "multi class"}, {"st": 58, "ed": 61, "text": "multi label classification"}, {"st": 67, "ed": 69, "text": "input samples"}, {"st": 82, "ed": 84, "text": "neural network"}, {"st": 85, "ed": 87, "text": "multi label"}, {"st": 106, "ed": 109, "text": "extreme learning machines"}, {"st": 118, "ed": 120, "text": "multi label"}, {"st": 131, "ed": 133, "text": "application domains"}, {"st": 144, "ed": 146, "text": "proposed method"}, {"st": 157, "ed": 160, "text": "speed and accuracy"}, {"st": 163, "ed": 165, "text": "multi label"}]
[{"st": 8, "ed": 10, "text": "multi class"}, {"st": 55, "ed": 57, "text": "neural network"}, {"st": 89, "ed": 91, "text": "real world"}, {"st": 101, "ed": 103, "text": "online learning"}, {"st": 122, "ed": 124, "text": "standard datasets"}, {"st": 135, "ed": 137, "text": "comparative study"}]
[{"st": 7, "ed": 9, "text": "neural network"}, {"st": 12, "ed": 15, "text": "extreme learning machines"}, {"st": 16, "ed": 19, "text": "multi label classification"}, {"st": 22, "ed": 25, "text": "multi label classification"}, {"st": 28, "ed": 30, "text": "input data"}, {"st": 46, "ed": 48, "text": "multi class"}, {"st": 65, "ed": 68, "text": "multi label classification"}, {"st": 76, "ed": 79, "text": "multi class classification"}, {"st": 98, "ed": 100, "text": "input samples"}, {"st": 114, "ed": 117, "text": "extreme learning machines"}, {"st": 121, "ed": 124, "text": "multi label classification"}, {"st": 133, "ed": 135, "text": "learning algorithm"}, {"st": 141, "ed": 143, "text": "streaming data"}, {"st": 145, "ed": 147, "text": "multi label"}, {"st": 149, "ed": 151, "text": "proposed method"}, {"st": 159, "ed": 161, "text": "application domains"}, {"st": 200, "ed": 202, "text": "multi label"}]
[{"st": 6, "ed": 9, "text": "deep neural networks"}, {"st": 19, "ed": 21, "text": "ubiquitous computing"}, {"st": 24, "ed": 26, "text": "smart phones"}, {"st": 36, "ed": 38, "text": "neural networks"}, {"st": 43, "ed": 45, "text": "deep learning"}, {"st": 61, "ed": 63, "text": "layer wise"}, {"st": 70, "ed": 72, "text": "training procedure"}, {"st": 89, "ed": 91, "text": "batch normalization"}, {"st": 101, "ed": 104, "text": "weights and activations"}, {"st": 129, "ed": 131, "text": "neural network"}, {"st": 173, "ed": 175, "text": "benchmark datasets"}]
[{"st": 1, "ed": 3, "text": "deep learning"}, {"st": 11, "ed": 13, "text": "recent years"}, {"st": 18, "ed": 20, "text": "standard classification"}, {"st": 116, "ed": 118, "text": "training set"}, {"st": 147, "ed": 149, "text": "output layer"}, {"st": 150, "ed": 152, "text": "neural network"}, {"st": 154, "ed": 156, "text": "experiments demonstrate"}, {"st": 170, "ed": 172, "text": "neural networks"}, {"st": 173, "ed": 175, "text": "standard classification"}]
[{"st": 18, "ed": 21, "text": "artificial neural networks"}, {"st": 38, "ed": 40, "text": "recent methods"}, {"st": 42, "ed": 44, "text": "neural networks"}, {"st": 66, "ed": 68, "text": "neural networks"}, {"st": 139, "ed": 141, "text": "tasks including"}, {"st": 167, "ed": 169, "text": "temporal dynamics"}, {"st": 174, "ed": 176, "text": "continual learning"}]
[{"st": 10, "ed": 12, "text": "biologically inspired"}, {"st": 14, "ed": 16, "text": "neural networks"}, {"st": 21, "ed": 23, "text": "learning rules"}, {"st": 45, "ed": 47, "text": "biologically inspired"}, {"st": 81, "ed": 83, "text": "sufficient conditions"}, {"st": 148, "ed": 150, "text": "biological neural"}, {"st": 160, "ed": 162, "text": "learning rule"}, {"st": 179, "ed": 182, "text": "artificial neural networks"}, {"st": 205, "ed": 207, "text": "motor learning"}]
[{"st": 8, "ed": 11, "text": "recurrent neural networks"}]
[{"st": 0, "ed": 2, "text": "neural networks"}, {"st": 19, "ed": 21, "text": "natural language"}, {"st": 25, "ed": 27, "text": "neural networks"}, {"st": 38, "ed": 40, "text": "recurrent network"}, {"st": 46, "ed": 48, "text": "neural networks"}, {"st": 53, "ed": 55, "text": "reinforcement learning"}, {"st": 70, "ed": 72, "text": "cifar 10"}, {"st": 82, "ed": 84, "text": "network architecture"}, {"st": 98, "ed": 100, "text": "cifar 10"}, {"st": 103, "ed": 105, "text": "test error"}]
[{"st": 3, "ed": 5, "text": "neural attention"}, {"st": 18, "ed": 20, "text": "visual search"}, {"st": 30, "ed": 32, "text": "visual scene"}, {"st": 84, "ed": 86, "text": "low resolution"}]
[{"st": 4, "ed": 7, "text": "recurrent neural networks"}, {"st": 24, "ed": 26, "text": "scales linearly"}, {"st": 48, "ed": 50, "text": "vanishing gradients"}, {"st": 63, "ed": 65, "text": "doesn t"}, {"st": 82, "ed": 84, "text": "recurrent architecture"}, {"st": 92, "ed": 94, "text": "embedding vectors"}, {"st": 112, "ed": 114, "text": "vanishing gradients"}, {"st": 154, "ed": 156, "text": "forward pass"}, {"st": 210, "ed": 212, "text": "forward pass"}, {"st": 217, "ed": 219, "text": "forward pass"}, {"st": 242, "ed": 244, "text": "forward pass"}]
[{"st": 1, "ed": 3, "text": "recent years"}, {"st": 3, "ed": 5, "text": "machine learning"}, {"st": 8, "ed": 10, "text": "neural networks"}, {"st": 11, "ed": 13, "text": "mobile computing"}, {"st": 17, "ed": 21, "text": "multi layer neural networks"}, {"st": 22, "ed": 24, "text": "matrix multiplications"}, {"st": 54, "ed": 56, "text": "neural network"}, {"st": 71, "ed": 73, "text": "neural network"}, {"st": 154, "ed": 156, "text": "neural network"}, {"st": 164, "ed": 166, "text": "mnist dataset"}, {"st": 175, "ed": 177, "text": "neural networks"}, {"st": 183, "ed": 186, "text": "multi layer perceptron"}, {"st": 187, "ed": 190, "text": "convolutional neural networks"}]
[{"st": 0, "ed": 2, "text": "reinforcement learning"}, {"st": 9, "ed": 12, "text": "sequential decision making"}, {"st": 59, "ed": 61, "text": "fine grained"}, {"st": 88, "ed": 91, "text": "deep reinforcement learning"}, {"st": 107, "ed": 109, "text": "empirically demonstrate"}, {"st": 116, "ed": 118, "text": "performance improvements"}, {"st": 122, "ed": 124, "text": "policy search"}, {"st": 129, "ed": 132, "text": "advantage actor critic"}, {"st": 137, "ed": 139, "text": "trust region"}, {"st": 139, "ed": 141, "text": "policy optimization"}, {"st": 146, "ed": 148, "text": "deterministic policy"}]
[{"st": 12, "ed": 14, "text": "neural networks"}, {"st": 20, "ed": 22, "text": "image recognition"}, {"st": 33, "ed": 35, "text": "attention mechanism"}, {"st": 37, "ed": 41, "text": "recurrent neural network rnn"}, {"st": 44, "ed": 46, "text": "modified version"}, {"st": 85, "ed": 87, "text": "neural networks"}, {"st": 98, "ed": 100, "text": "sequential data"}, {"st": 103, "ed": 105, "text": "neural architectures"}, {"st": 115, "ed": 118, "text": "long term memory"}, {"st": 122, "ed": 124, "text": "neural networks"}, {"st": 125, "ed": 127, "text": "external memory"}, {"st": 132, "ed": 134, "text": "attention mechanism"}, {"st": 134, "ed": 137, "text": "end to end"}, {"st": 149, "ed": 151, "text": "preliminary results"}, {"st": 153, "ed": 155, "text": "neural architectures"}, {"st": 161, "ed": 163, "text": "question answering"}]
[{"st": 0, "ed": 2, "text": "imitation learning"}, {"st": 17, "ed": 19, "text": "feature engineering"}, {"st": 59, "ed": 61, "text": "task specific"}, {"st": 68, "ed": 70, "text": "meta learning"}, {"st": 172, "ed": 174, "text": "neural net"}, {"st": 211, "ed": 214, "text": "states and actions"}, {"st": 240, "ed": 242, "text": "neural net"}, {"st": 257, "ed": 259, "text": "soft attention"}]
[{"st": 6, "ed": 8, "text": "optimization problems"}, {"st": 32, "ed": 34, "text": "explicitly modeling"}, {"st": 56, "ed": 58, "text": "deep networks"}, {"st": 69, "ed": 71, "text": "search space"}, {"st": 75, "ed": 77, "text": "deep networks"}, {"st": 125, "ed": 127, "text": "local search"}, {"st": 136, "ed": 138, "text": "search space"}, {"st": 152, "ed": 154, "text": "optimization problems"}, {"st": 161, "ed": 164, "text": "real world domains"}, {"st": 176, "ed": 178, "text": "search spaces"}]
[{"st": 0, "ed": 3, "text": "deep reinforcement learning"}, {"st": 6, "ed": 8, "text": "impressive results"}, {"st": 14, "ed": 16, "text": "sparse rewards"}, {"st": 41, "ed": 43, "text": "pre training"}, {"st": 65, "ed": 67, "text": "intrinsic motivation"}, {"st": 89, "ed": 91, "text": "domain knowledge"}, {"st": 109, "ed": 111, "text": "significant improvement"}, {"st": 118, "ed": 120, "text": "sparse rewards"}, {"st": 126, "ed": 128, "text": "pre train"}, {"st": 136, "ed": 138, "text": "neural networks"}, {"st": 162, "ed": 164, "text": "sample efficient"}]
[{"st": 9, "ed": 11, "text": "recently introduced"}, {"st": 21, "ed": 23, "text": "reinforcement learning"}, {"st": 26, "ed": 28, "text": "off policy"}, {"st": 39, "ed": 41, "text": "real world"}, {"st": 56, "ed": 58, "text": "reinforcement learning"}, {"st": 76, "ed": 78, "text": "continuous state"}, {"st": 84, "ed": 86, "text": "partially observable"}, {"st": 106, "ed": 108, "text": "closed form"}, {"st": 108, "ed": 110, "text": "control policies"}, {"st": 117, "ed": 119, "text": "neural network"}, {"st": 146, "ed": 148, "text": "real world"}]
[{"st": 2, "ed": 4, "text": "neural networks"}, {"st": 5, "ed": 8, "text": "end to end"}, {"st": 24, "ed": 26, "text": "neural networks"}, {"st": 57, "ed": 60, "text": "radial basis function"}, {"st": 76, "ed": 78, "text": "neural network"}, {"st": 144, "ed": 146, "text": "link prediction"}, {"st": 164, "ed": 167, "text": "first order logic"}]
[{"st": 2, "ed": 5, "text": "deep reinforcement learning"}, {"st": 7, "ed": 9, "text": "multi agent"}, {"st": 21, "ed": 23, "text": "multi agent"}, {"st": 37, "ed": 39, "text": "policy gradient"}, {"st": 77, "ed": 79, "text": "multi agent"}, {"st": 99, "ed": 101, "text": "multi agent"}, {"st": 111, "ed": 113, "text": "existing methods"}]
[{"st": 6, "ed": 8, "text": "deep learning"}, {"st": 9, "ed": 11, "text": "becoming increasingly"}, {"st": 37, "ed": 40, "text": "input and output"}, {"st": 49, "ed": 52, "text": "difficult to train"}, {"st": 55, "ed": 57, "text": "limited memory"}, {"st": 59, "ed": 61, "text": "processing units"}, {"st": 88, "ed": 91, "text": "input and output"}, {"st": 92, "ed": 94, "text": "neural network"}, {"st": 106, "ed": 108, "text": "computationally efficient"}]
[{"st": 0, "ed": 2, "text": "monte carlo"}, {"st": 77, "ed": 79, "text": "neural network"}, {"st": 90, "ed": 94, "text": "deep convolutional neural network"}, {"st": 101, "ed": 103, "text": "convolutional layer"}, {"st": 119, "ed": 121, "text": "local features"}, {"st": 188, "ed": 190, "text": "cumulative reward"}, {"st": 241, "ed": 243, "text": "substantially improve"}]
[{"st": 2, "ed": 4, "text": "sparse rewards"}, {"st": 11, "ed": 13, "text": "reinforcement learning"}, {"st": 19, "ed": 21, "text": "technique called"}, {"st": 22, "ed": 24, "text": "experience replay"}, {"st": 26, "ed": 28, "text": "sample efficient"}, {"st": 52, "ed": 54, "text": "off policy"}, {"st": 116, "ed": 118, "text": "experience replay"}]
[{"st": 6, "ed": 8, "text": "complex tasks"}, {"st": 58, "ed": 60, "text": "reinforcement learning"}, {"st": 111, "ed": 113, "text": "atari games"}, {"st": 115, "ed": 117, "text": "deep rl"}, {"st": 153, "ed": 155, "text": "catastrophic forgetting"}, {"st": 180, "ed": 182, "text": "adversarial examples"}]
[{"st": 45, "ed": 47, "text": "goal oriented"}, {"st": 53, "ed": 55, "text": "reinforcement learning"}, {"st": 58, "ed": 60, "text": "reward function"}, {"st": 85, "ed": 87, "text": "expert demonstrations"}, {"st": 92, "ed": 94, "text": "task specific"}, {"st": 114, "ed": 116, "text": "prior knowledge"}, {"st": 172, "ed": 174, "text": "goal oriented"}, {"st": 184, "ed": 186, "text": "fine grained"}, {"st": 195, "ed": 197, "text": "reinforcement learning"}]
[{"st": 2, "ed": 4, "text": "based optimization"}, {"st": 65, "ed": 67, "text": "detection problem"}, {"st": 137, "ed": 139, "text": "step sizes"}, {"st": 151, "ed": 154, "text": "exploration and exploitation"}, {"st": 171, "ed": 173, "text": "step size"}, {"st": 178, "ed": 180, "text": "proposed algorithm"}, {"st": 216, "ed": 218, "text": "proposed algorithm"}, {"st": 248, "ed": 250, "text": "compressed sensing"}, {"st": 266, "ed": 268, "text": "evolutionary algorithms"}]
[{"st": 0, "ed": 3, "text": "deep neural networks"}, {"st": 10, "ed": 12, "text": "visual recognition"}, {"st": 24, "ed": 26, "text": "neural networks"}, {"st": 29, "ed": 31, "text": "mobile phones"}, {"st": 45, "ed": 47, "text": "limited memory"}, {"st": 58, "ed": 60, "text": "machine learning"}, {"st": 101, "ed": 103, "text": "neural networks"}, {"st": 127, "ed": 129, "text": "neural network"}, {"st": 133, "ed": 135, "text": "feed forward"}, {"st": 137, "ed": 139, "text": "lstm rnns"}, {"st": 147, "ed": 149, "text": "random projections"}, {"st": 179, "ed": 181, "text": "trained jointly"}, {"st": 231, "ed": 233, "text": "neural networks"}, {"st": 238, "ed": 240, "text": "visual recognition"}, {"st": 241, "ed": 243, "text": "text classification"}, {"st": 266, "ed": 268, "text": "empirical results"}]
[{"st": 54, "ed": 57, "text": "deep neural networks"}]
[{"st": 4, "ed": 6, "text": "e commerce"}, {"st": 7, "ed": 9, "text": "web 2.0"}, {"st": 30, "ed": 32, "text": "recommender systems"}, {"st": 82, "ed": 84, "text": "temporal patterns"}, {"st": 92, "ed": 95, "text": "visual and textual"}, {"st": 144, "ed": 146, "text": "proposed framework"}, {"st": 148, "ed": 150, "text": "signal processing"}, {"st": 154, "ed": 156, "text": "feature learning"}, {"st": 167, "ed": 169, "text": "neural network"}, {"st": 182, "ed": 184, "text": "neural network"}, {"st": 189, "ed": 191, "text": "feature embedding"}, {"st": 211, "ed": 213, "text": "user study"}]
[{"st": 4, "ed": 6, "text": "sparse rewards"}, {"st": 12, "ed": 14, "text": "reinforcement learning"}, {"st": 30, "ed": 32, "text": "reward function"}, {"st": 57, "ed": 59, "text": "real world"}, {"st": 85, "ed": 87, "text": "multi step"}, {"st": 90, "ed": 92, "text": "continuous control"}, {"st": 108, "ed": 110, "text": "deterministic policy"}, {"st": 113, "ed": 115, "text": "experience replay"}]
[{"st": 0, "ed": 3, "text": "recurrent neural networks"}, {"st": 5, "ed": 7, "text": "remarkable success"}, {"st": 31, "ed": 33, "text": "recurrent units"}, {"st": 41, "ed": 43, "text": "multi layer"}, {"st": 43, "ed": 45, "text": "recurrent models"}, {"st": 120, "ed": 122, "text": "convergence rates"}, {"st": 131, "ed": 134, "text": "publicly available datasets"}, {"st": 151, "ed": 153, "text": "convergence rates"}, {"st": 154, "ed": 156, "text": "statistical efficiency"}]
[{"st": 5, "ed": 7, "text": "deep learning"}, {"st": 34, "ed": 36, "text": "lifelong learning"}, {"st": 47, "ed": 49, "text": "non stationary"}, {"st": 52, "ed": 54, "text": "lifelong learning"}, {"st": 115, "ed": 118, "text": "efficient and effective"}]
[{"st": 18, "ed": 20, "text": "generative models"}, {"st": 26, "ed": 28, "text": "discriminative learning"}, {"st": 41, "ed": 43, "text": "generative models"}, {"st": 81, "ed": 83, "text": "generative models"}, {"st": 110, "ed": 112, "text": "proposed approach"}, {"st": 129, "ed": 131, "text": "probabilistic model"}]
[{"st": 1, "ed": 4, "text": "ability to learn"}, {"st": 15, "ed": 17, "text": "main challenges"}, {"st": 18, "ed": 21, "text": "deep reinforcement learning"}, {"st": 70, "ed": 72, "text": "reinforcement learning"}, {"st": 85, "ed": 87, "text": "problems involving"}, {"st": 87, "ed": 89, "text": "continuous action"}, {"st": 113, "ed": 115, "text": "faster learning"}, {"st": 116, "ed": 118, "text": "agents learn"}, {"st": 129, "ed": 132, "text": "end to end"}]
[{"st": 34, "ed": 37, "text": "degrees of freedom"}, {"st": 132, "ed": 135, "text": "degrees of freedom"}, {"st": 168, "ed": 170, "text": "motor learning"}, {"st": 178, "ed": 180, "text": "cross entropy"}, {"st": 181, "ed": 183, "text": "policy search"}]
[{"st": 4, "ed": 6, "text": "artificial intelligence"}, {"st": 48, "ed": 50, "text": "environmental change"}, {"st": 120, "ed": 122, "text": "learning machines"}, {"st": 169, "ed": 171, "text": "learning machines"}, {"st": 187, "ed": 189, "text": "control flow"}]
[{"st": 0, "ed": 4, "text": "restricted boltzmann machines rbms"}, {"st": 8, "ed": 11, "text": "deep belief networks"}, {"st": 13, "ed": 15, "text": "neural networks"}, {"st": 23, "ed": 25, "text": "machine learning"}, {"st": 43, "ed": 45, "text": "gibbs sampling"}, {"st": 46, "ed": 48, "text": "contrastive divergence"}, {"st": 98, "ed": 100, "text": "pattern recognition"}, {"st": 102, "ed": 104, "text": "modified version"}, {"st": 114, "ed": 116, "text": "phase space"}, {"st": 137, "ed": 139, "text": "significantly reduces"}, {"st": 157, "ed": 159, "text": "performance gain"}, {"st": 175, "ed": 178, "text": "number of iterations"}, {"st": 222, "ed": 224, "text": "supervised training"}, {"st": 225, "ed": 227, "text": "batch normalization"}, {"st": 252, "ed": 254, "text": "a 20"}, {"st": 256, "ed": 258, "text": "error rate"}, {"st": 284, "ed": 286, "text": "deep networks"}]
[{"st": 0, "ed": 2, "text": "multi view"}, {"st": 6, "ed": 8, "text": "real world"}, {"st": 33, "ed": 35, "text": "network embedding"}, {"st": 55, "ed": 57, "text": "multi view"}, {"st": 57, "ed": 59, "text": "network embedding"}, {"st": 81, "ed": 83, "text": "real world"}, {"st": 83, "ed": 85, "text": "multi view"}, {"st": 126, "ed": 128, "text": "synthetic datasets"}]
[{"st": 9, "ed": 11, "text": "input features"}, {"st": 23, "ed": 25, "text": "feature importance"}, {"st": 56, "ed": 59, "text": "mixture of experts"}, {"st": 72, "ed": 74, "text": "accurate predictions"}]
[{"st": 0, "ed": 2, "text": "neural networks"}, {"st": 5, "ed": 7, "text": "learning systems"}, {"st": 60, "ed": 62, "text": "generalization capabilities"}, {"st": 63, "ed": 67, "text": "recurrent neural networks rnns"}, {"st": 72, "ed": 74, "text": "lookup table"}, {"st": 118, "ed": 120, "text": "additional supervision"}]
[{"st": 6, "ed": 8, "text": "continual learning"}, {"st": 11, "ed": 14, "text": "artificial neural networks"}, {"st": 25, "ed": 27, "text": "catastrophic forgetting"}, {"st": 42, "ed": 44, "text": "neural network"}, {"st": 60, "ed": 62, "text": "complex network"}, {"st": 81, "ed": 84, "text": "deep reinforcement learning"}, {"st": 97, "ed": 99, "text": "catastrophic forgetting"}, {"st": 114, "ed": 116, "text": "continual learning"}, {"st": 139, "ed": 141, "text": "experience replay"}]
[{"st": 6, "ed": 8, "text": "reinforcement learning"}, {"st": 17, "ed": 19, "text": "deep rl"}, {"st": 35, "ed": 37, "text": "practical applications"}, {"st": 100, "ed": 102, "text": "exploration strategies"}, {"st": 131, "ed": 133, "text": "exploration strategies"}, {"st": 137, "ed": 139, "text": "prior knowledge"}, {"st": 157, "ed": 159, "text": "exploration strategies"}, {"st": 169, "ed": 171, "text": "exploration strategies"}]
[{"st": 3, "ed": 5, "text": "approximation algorithm"}, {"st": 10, "ed": 12, "text": "pre trained"}, {"st": 23, "ed": 25, "text": "similar accuracy"}, {"st": 36, "ed": 38, "text": "imagenet classification"}, {"st": 47, "ed": 49, "text": "floating point"}]
[{"st": 0, "ed": 2, "text": "neural networks"}, {"st": 12, "ed": 14, "text": "classification problems"}, {"st": 20, "ed": 22, "text": "feature space"}, {"st": 90, "ed": 92, "text": "training data"}, {"st": 120, "ed": 122, "text": "visualization method"}]
[{"st": 0, "ed": 2, "text": "common sense"}, {"st": 10, "ed": 12, "text": "intelligent agent"}, {"st": 45, "ed": 47, "text": "real world"}, {"st": 90, "ed": 92, "text": "prior knowledge"}, {"st": 97, "ed": 99, "text": "human perception"}]
[{"st": 61, "ed": 63, "text": "complex network"}]
[{"st": 3, "ed": 5, "text": "neural network"}, {"st": 11, "ed": 13, "text": "neural networks"}, {"st": 57, "ed": 59, "text": "random initialization"}, {"st": 198, "ed": 200, "text": "large networks"}]
[{"st": 19, "ed": 21, "text": "recurrent network"}, {"st": 51, "ed": 53, "text": "least squares"}, {"st": 110, "ed": 112, "text": "sufficient conditions"}, {"st": 145, "ed": 147, "text": "activity patterns"}]
[{"st": 28, "ed": 30, "text": "karl pearson"}, {"st": 31, "ed": 34, "text": "principal component analysis"}, {"st": 51, "ed": 53, "text": "k means"}, {"st": 120, "ed": 122, "text": "principal components"}, {"st": 123, "ed": 125, "text": "k means"}]
[{"st": 0, "ed": 3, "text": "deep belief networks"}, {"st": 6, "ed": 8, "text": "successfully applied"}, {"st": 10, "ed": 12, "text": "machine learning"}, {"st": 17, "ed": 19, "text": "hand written"}, {"st": 19, "ed": 21, "text": "digit recognition"}, {"st": 47, "ed": 49, "text": "recent advances"}, {"st": 93, "ed": 95, "text": "classification accuracy"}, {"st": 96, "ed": 98, "text": "digit recognition"}]
[{"st": 14, "ed": 17, "text": "multi layer perceptron"}, {"st": 24, "ed": 26, "text": "proposed method"}, {"st": 57, "ed": 59, "text": "proposed framework"}, {"st": 71, "ed": 73, "text": "hidden neurons"}, {"st": 87, "ed": 89, "text": "hidden layers"}]
[{"st": 10, "ed": 12, "text": "neural network"}, {"st": 31, "ed": 33, "text": "prior information"}, {"st": 58, "ed": 60, "text": "method called"}, {"st": 89, "ed": 91, "text": "lagrange multiplier"}, {"st": 165, "ed": 167, "text": "numerical examples"}]
[{"st": 13, "ed": 15, "text": "gaussian processes"}, {"st": 22, "ed": 24, "text": "gaussian processes"}, {"st": 26, "ed": 28, "text": "kernel based"}, {"st": 72, "ed": 74, "text": "proposed approach"}, {"st": 85, "ed": 87, "text": "genetic programming"}, {"st": 104, "ed": 106, "text": "proposed approach"}, {"st": 107, "ed": 109, "text": "synthetic data"}, {"st": 117, "ed": 119, "text": "mauna loa"}, {"st": 159, "ed": 161, "text": "hand tuned"}]
[{"st": 3, "ed": 5, "text": "theoretical analysis"}, {"st": 8, "ed": 11, "text": "restricted boltzmann machines"}, {"st": 35, "ed": 38, "text": "mixture of gaussians"}, {"st": 66, "ed": 69, "text": "blind source separation"}]
[{"st": 4, "ed": 6, "text": "maximum likelihood"}, {"st": 38, "ed": 40, "text": "contrastive divergence"}, {"st": 49, "ed": 51, "text": "training data"}, {"st": 77, "ed": 79, "text": "training data"}, {"st": 98, "ed": 100, "text": "local optima"}, {"st": 124, "ed": 126, "text": "closely related"}, {"st": 143, "ed": 145, "text": "hidden layer"}]
[{"st": 44, "ed": 46, "text": "randomly selected"}, {"st": 58, "ed": 60, "text": "nearest neighbor"}, {"st": 71, "ed": 73, "text": "input data"}, {"st": 79, "ed": 81, "text": "feature space"}, {"st": 91, "ed": 93, "text": "feature space"}, {"st": 135, "ed": 137, "text": "estimation error"}]
[{"st": 2, "ed": 4, "text": "classification problems"}, {"st": 32, "ed": 34, "text": "image classification"}, {"st": 64, "ed": 66, "text": "neural network"}, {"st": 84, "ed": 86, "text": "loss function"}, {"st": 105, "ed": 107, "text": "loss function"}, {"st": 123, "ed": 125, "text": "proposed algorithm"}, {"st": 134, "ed": 136, "text": "adversarial training"}, {"st": 154, "ed": 156, "text": "classification accuracy"}, {"st": 161, "ed": 165, "text": "mnist and cifar 10"}, {"st": 174, "ed": 176, "text": "proposed algorithm"}, {"st": 179, "ed": 181, "text": "dataset size"}]
[{"st": 14, "ed": 17, "text": "restricted boltzmann machines"}, {"st": 34, "ed": 37, "text": "linear discriminant analysis"}, {"st": 40, "ed": 42, "text": "hidden layer"}, {"st": 62, "ed": 64, "text": "maximum likelihood"}, {"st": 64, "ed": 66, "text": "parameter estimation"}]
[{"st": 0, "ed": 2, "text": "existing approaches"}, {"st": 19, "ed": 21, "text": "discrete optimization"}, {"st": 37, "ed": 39, "text": "computational resources"}, {"st": 45, "ed": 47, "text": "computational complexity"}, {"st": 56, "ed": 58, "text": "transfer function"}, {"st": 100, "ed": 102, "text": "standard backpropagation"}]
[{"st": 3, "ed": 5, "text": "probabilistic framework"}, {"st": 6, "ed": 8, "text": "deep learning"}, {"st": 13, "ed": 15, "text": "mixture model"}, {"st": 19, "ed": 21, "text": "probabilistic model"}, {"st": 51, "ed": 55, "text": "deep convolutional neural networks"}, {"st": 86, "ed": 90, "text": "expectation maximization em algorithm"}, {"st": 96, "ed": 98, "text": "back propagation"}, {"st": 116, "ed": 118, "text": "digit classification"}, {"st": 132, "ed": 134, "text": "semi supervised"}, {"st": 135, "ed": 137, "text": "unsupervised learning"}]
[{"st": 1, "ed": 3, "text": "machine learning"}, {"st": 6, "ed": 9, "text": "deep neural networks"}, {"st": 12, "ed": 14, "text": "remarkable success"}, {"st": 18, "ed": 20, "text": "complex data"}, {"st": 55, "ed": 57, "text": "biological neural"}, {"st": 96, "ed": 98, "text": "deep networks"}, {"st": 125, "ed": 128, "text": "artificial neural networks"}, {"st": 147, "ed": 149, "text": "mnist handwritten"}, {"st": 183, "ed": 185, "text": "machine learning"}]
[{"st": 0, "ed": 2, "text": "deep learning"}, {"st": 49, "ed": 51, "text": "deep learning"}, {"st": 82, "ed": 84, "text": "deep learning"}, {"st": 89, "ed": 91, "text": "brain imaging"}, {"st": 97, "ed": 99, "text": "constraint based"}, {"st": 122, "ed": 124, "text": "deep learning"}]
[{"st": 7, "ed": 9, "text": "recently introduced"}, {"st": 14, "ed": 17, "text": "deep neural networks"}, {"st": 53, "ed": 55, "text": "linear transformations"}, {"st": 75, "ed": 77, "text": "desirable properties"}, {"st": 85, "ed": 87, "text": "invariance properties"}, {"st": 106, "ed": 108, "text": "classification performance"}, {"st": 120, "ed": 122, "text": "image classification"}, {"st": 123, "ed": 127, "text": "cifar 10 cifar 100"}]
[{"st": 22, "ed": 24, "text": "machine learning"}, {"st": 61, "ed": 64, "text": "self organizing map"}]
[{"st": 0, "ed": 2, "text": "model based"}, {"st": 4, "ed": 7, "text": "deep neural networks"}, {"st": 17, "ed": 19, "text": "model based"}, {"st": 21, "ed": 23, "text": "domain knowledge"}, {"st": 43, "ed": 46, "text": "deep neural networks"}, {"st": 86, "ed": 88, "text": "model based"}, {"st": 92, "ed": 94, "text": "inference algorithm"}, {"st": 156, "ed": 158, "text": "mean field"}, {"st": 160, "ed": 163, "text": "markov random fields"}, {"st": 171, "ed": 173, "text": "belief propagation"}, {"st": 184, "ed": 188, "text": "non negative matrix factorization"}, {"st": 193, "ed": 195, "text": "domain knowledge"}, {"st": 212, "ed": 215, "text": "deep neural network"}, {"st": 228, "ed": 230, "text": "speech enhancement"}, {"st": 239, "ed": 241, "text": "neural networks"}]
[{"st": 1, "ed": 3, "text": "dynamical systems"}, {"st": 50, "ed": 52, "text": "latent space"}, {"st": 71, "ed": 73, "text": "practical applications"}, {"st": 116, "ed": 118, "text": "recent advances"}, {"st": 119, "ed": 121, "text": "deep learning"}, {"st": 127, "ed": 129, "text": "jointly learn"}, {"st": 130, "ed": 132, "text": "low dimensional"}, {"st": 140, "ed": 142, "text": "auto encoders"}, {"st": 145, "ed": 147, "text": "transition model"}, {"st": 149, "ed": 151, "text": "low dimensional"}, {"st": 160, "ed": 162, "text": "predictive models"}, {"st": 163, "ed": 165, "text": "dynamical systems"}]
[{"st": 4, "ed": 6, "text": "unsupervised learning"}, {"st": 34, "ed": 36, "text": "higher levels"}, {"st": 51, "ed": 53, "text": "latent variable"}, {"st": 67, "ed": 69, "text": "hierarchical latent"}, {"st": 73, "ed": 75, "text": "denoising autoencoder"}, {"st": 88, "ed": 90, "text": "cost function"}, {"st": 132, "ed": 134, "text": "higher levels"}, {"st": 139, "ed": 142, "text": "ability to learn"}, {"st": 142, "ed": 144, "text": "invariant features"}]
[{"st": 1, "ed": 3, "text": "neural networks"}, {"st": 5, "ed": 7, "text": "large scale"}, {"st": 7, "ed": 10, "text": "non convex optimization"}, {"st": 24, "ed": 26, "text": "local minima"}, {"st": 43, "ed": 45, "text": "neural networks"}, {"st": 50, "ed": 52, "text": "training error"}, {"st": 53, "ed": 55, "text": "complex tasks"}, {"st": 100, "ed": 102, "text": "neural networks"}]
[{"st": 2, "ed": 4, "text": "deep learning"}, {"st": 12, "ed": 14, "text": "higher order"}, {"st": 24, "ed": 26, "text": "group theory"}, {"st": 52, "ed": 54, "text": "pre training"}, {"st": 59, "ed": 61, "text": "generative model"}, {"st": 63, "ed": 65, "text": "input samples"}, {"st": 98, "ed": 100, "text": "neural networks"}, {"st": 124, "ed": 126, "text": "pre training"}, {"st": 162, "ed": 164, "text": "deep learning"}, {"st": 184, "ed": 186, "text": "higher order"}]
[{"st": 9, "ed": 11, "text": "large scale"}, {"st": 11, "ed": 13, "text": "learning problems"}, {"st": 32, "ed": 34, "text": "learning rate"}, {"st": 54, "ed": 56, "text": "learning rate"}, {"st": 76, "ed": 78, "text": "loss function"}, {"st": 95, "ed": 97, "text": "variance reduction"}, {"st": 105, "ed": 107, "text": "preliminary experiments"}, {"st": 108, "ed": 111, "text": "deep neural networks"}]
[{"st": 7, "ed": 9, "text": "neural network"}, {"st": 12, "ed": 15, "text": "multi task learning"}, {"st": 64, "ed": 67, "text": "zero shot learning"}, {"st": 93, "ed": 95, "text": "zero shot"}, {"st": 95, "ed": 97, "text": "domain adaptation"}]
[{"st": 13, "ed": 15, "text": "distance measure"}, {"st": 41, "ed": 43, "text": "k means"}, {"st": 71, "ed": 73, "text": "neural network"}]
[{"st": 2, "ed": 4, "text": "deep learning"}, {"st": 12, "ed": 14, "text": "higher order"}, {"st": 24, "ed": 26, "text": "group theory"}, {"st": 59, "ed": 61, "text": "generative model"}, {"st": 63, "ed": 65, "text": "input samples"}, {"st": 98, "ed": 100, "text": "neural networks"}, {"st": 124, "ed": 126, "text": "pre training"}, {"st": 162, "ed": 164, "text": "deep learning"}, {"st": 184, "ed": 186, "text": "higher order"}]
[{"st": 1, "ed": 3, "text": "generative model"}, {"st": 7, "ed": 9, "text": "multi layered"}, {"st": 21, "ed": 23, "text": "deep model"}, {"st": 44, "ed": 46, "text": "multi layer"}, {"st": 51, "ed": 53, "text": "classification results"}]
[{"st": 0, "ed": 2, "text": "pre training"}, {"st": 12, "ed": 14, "text": "pre training"}, {"st": 19, "ed": 22, "text": "restricted boltzmann machines"}, {"st": 26, "ed": 29, "text": "layer by layer"}, {"st": 35, "ed": 37, "text": "layer wise"}, {"st": 37, "ed": 39, "text": "pre training"}, {"st": 42, "ed": 44, "text": "theoretical foundation"}, {"st": 58, "ed": 60, "text": "pre train"}, {"st": 64, "ed": 66, "text": "multi layer"}, {"st": 68, "ed": 72, "text": "recurrent neural networks rnns"}, {"st": 78, "ed": 80, "text": "pre training"}, {"st": 83, "ed": 85, "text": "knowledge transfer"}, {"st": 90, "ed": 92, "text": "layer wise"}, {"st": 125, "ed": 127, "text": "trained model"}, {"st": 134, "ed": 136, "text": "layer wise"}, {"st": 152, "ed": 154, "text": "pre train"}, {"st": 160, "ed": 162, "text": "speech recognition"}, {"st": 177, "ed": 180, "text": "deep neural network"}, {"st": 191, "ed": 193, "text": "layer wise"}, {"st": 193, "ed": 195, "text": "pre training"}]
[{"st": 9, "ed": 11, "text": "auto encoders"}, {"st": 25, "ed": 27, "text": "semi supervised"}, {"st": 28, "ed": 30, "text": "unsupervised learning"}, {"st": 68, "ed": 70, "text": "objective function"}, {"st": 76, "ed": 78, "text": "hidden states"}, {"st": 90, "ed": 92, "text": "pooling layer"}]
[{"st": 10, "ed": 12, "text": "dynamical systems"}, {"st": 99, "ed": 101, "text": "loss function"}, {"st": 143, "ed": 146, "text": "recurrent neural networks"}, {"st": 156, "ed": 158, "text": "small scale"}, {"st": 158, "ed": 160, "text": "experiments confirm"}]
[{"st": 6, "ed": 8, "text": "source separation"}, {"st": 10, "ed": 12, "text": "deep learning"}, {"st": 28, "ed": 30, "text": "deep network"}, {"st": 45, "ed": 47, "text": "deep network"}, {"st": 78, "ed": 80, "text": "spectral clustering"}, {"st": 121, "ed": 123, "text": "objective function"}, {"st": 130, "ed": 132, "text": "low rank"}, {"st": 137, "ed": 139, "text": "affinity matrix"}, {"st": 179, "ed": 181, "text": "preliminary experiments"}, {"st": 184, "ed": 186, "text": "proposed method"}, {"st": 246, "ed": 248, "text": "class labels"}, {"st": 290, "ed": 292, "text": "image segmentation"}]
[{"st": 6, "ed": 8, "text": "representation learning"}, {"st": 9, "ed": 11, "text": "dimensionality reduction"}, {"st": 14, "ed": 16, "text": "computationally expensive"}, {"st": 24, "ed": 26, "text": "input data"}, {"st": 36, "ed": 38, "text": "training samples"}, {"st": 68, "ed": 71, "text": "deep neural networks"}, {"st": 101, "ed": 104, "text": "orders of magnitude"}]
[{"st": 8, "ed": 10, "text": "distributed computing"}, {"st": 14, "ed": 16, "text": "large scale"}, {"st": 24, "ed": 27, "text": "stochastic gradient descent"}, {"st": 40, "ed": 42, "text": "learning rates"}, {"st": 56, "ed": 58, "text": "learning rate"}, {"st": 94, "ed": 96, "text": "principled approach"}, {"st": 97, "ed": 99, "text": "distributed training"}, {"st": 100, "ed": 102, "text": "neural networks"}, {"st": 103, "ed": 105, "text": "mini batch"}, {"st": 131, "ed": 133, "text": "image classification"}]
[{"st": 3, "ed": 6, "text": "convolutional neural network"}, {"st": 14, "ed": 17, "text": "end to end"}, {"st": 37, "ed": 39, "text": "feature extraction"}, {"st": 57, "ed": 59, "text": "predictive performance"}]
[{"st": 2, "ed": 4, "text": "log likelihood"}, {"st": 12, "ed": 16, "text": "restricted boltzmann machine rbm"}, {"st": 16, "ed": 18, "text": "typically requires"}, {"st": 20, "ed": 24, "text": "markov chain monte carlo"}, {"st": 57, "ed": 59, "text": "contrastive divergence"}, {"st": 69, "ed": 71, "text": "monte carlo"}, {"st": 82, "ed": 84, "text": "contrastive divergence"}, {"st": 100, "ed": 102, "text": "significantly lower"}, {"st": 104, "ed": 106, "text": "computational overhead"}, {"st": 126, "ed": 128, "text": "significantly outperform"}, {"st": 140, "ed": 142, "text": "log likelihood"}, {"st": 150, "ed": 152, "text": "hidden neurons"}, {"st": 180, "ed": 182, "text": "theoretical properties"}]
[{"st": 0, "ed": 4, "text": "deep convolutional neural networks"}, {"st": 8, "ed": 11, "text": "deep neural networks"}, {"st": 19, "ed": 22, "text": "spatial and temporal"}, {"st": 28, "ed": 30, "text": "convolutional networks"}, {"st": 33, "ed": 35, "text": "predictive performance"}, {"st": 41, "ed": 43, "text": "image recognition"}, {"st": 47, "ed": 49, "text": "local features"}, {"st": 86, "ed": 90, "text": "deep convolutional neural network"}, {"st": 139, "ed": 141, "text": "convolutional filters"}, {"st": 162, "ed": 164, "text": "outperforms previous"}, {"st": 174, "ed": 176, "text": "large margin"}]
[{"st": 0, "ed": 2, "text": "deep learning"}, {"st": 13, "ed": 15, "text": "machine learning"}, {"st": 17, "ed": 19, "text": "recent studies"}, {"st": 22, "ed": 24, "text": "deep learning"}, {"st": 26, "ed": 28, "text": "machine learning"}, {"st": 30, "ed": 33, "text": "vulnerable to adversarial"}, {"st": 39, "ed": 42, "text": "deep neural network"}, {"st": 108, "ed": 110, "text": "adversarial samples"}, {"st": 215, "ed": 217, "text": "adversarial samples"}]
[{"st": 0, "ed": 2, "text": "variational inference"}, {"st": 4, "ed": 6, "text": "powerful tool"}, {"st": 7, "ed": 9, "text": "approximate inference"}, {"st": 16, "ed": 18, "text": "representation learning"}, {"st": 26, "ed": 28, "text": "gaussian process"}, {"st": 46, "ed": 48, "text": "approximate posterior"}, {"st": 105, "ed": 107, "text": "auto encoders"}, {"st": 128, "ed": 130, "text": "unsupervised learning"}, {"st": 141, "ed": 143, "text": "recently proposed"}]
[{"st": 13, "ed": 15, "text": "objective function"}, {"st": 19, "ed": 21, "text": "local optima"}, {"st": 96, "ed": 98, "text": "significant improvements"}, {"st": 113, "ed": 115, "text": "optimisation problems"}]
[{"st": 5, "ed": 8, "text": "deep neural networks"}, {"st": 15, "ed": 18, "text": "low dimensional manifold"}, {"st": 20, "ed": 22, "text": "high dimensional"}, {"st": 26, "ed": 28, "text": "deep networks"}, {"st": 33, "ed": 35, "text": "low dimensional"}, {"st": 49, "ed": 51, "text": "deep network"}, {"st": 71, "ed": 73, "text": "low dimensional"}]
[{"st": 8, "ed": 10, "text": "linearly separable"}, {"st": 23, "ed": 25, "text": "non trivial"}, {"st": 28, "ed": 30, "text": "real world"}, {"st": 144, "ed": 146, "text": "experiments conducted"}, {"st": 159, "ed": 161, "text": "classification problem"}]
[{"st": 15, "ed": 18, "text": "deep neural networks"}, {"st": 25, "ed": 27, "text": "objective functions"}, {"st": 48, "ed": 50, "text": "weight matrices"}, {"st": 51, "ed": 53, "text": "fully connected"}, {"st": 60, "ed": 62, "text": "nonnegative matrix"}, {"st": 75, "ed": 77, "text": "weight matrices"}, {"st": 105, "ed": 107, "text": "pre training"}, {"st": 124, "ed": 126, "text": "error rates"}]
[{"st": 1, "ed": 3, "text": "generalization error"}, {"st": 4, "ed": 7, "text": "deep neural networks"}, {"st": 26, "ed": 29, "text": "deep neural network"}, {"st": 37, "ed": 39, "text": "non linearities"}, {"st": 40, "ed": 42, "text": "pooling layers"}, {"st": 50, "ed": 52, "text": "feed forward"}, {"st": 78, "ed": 80, "text": "training samples"}, {"st": 84, "ed": 87, "text": "deep neural network"}, {"st": 89, "ed": 92, "text": "depth and width"}, {"st": 98, "ed": 100, "text": "significant improvement"}, {"st": 111, "ed": 113, "text": "generalization error"}, {"st": 129, "ed": 131, "text": "recently proposed"}, {"st": 131, "ed": 133, "text": "batch normalization"}, {"st": 140, "ed": 142, "text": "generalization properties"}, {"st": 165, "ed": 168, "text": "mnist cifar 10"}]
[{"st": 10, "ed": 12, "text": "training loss"}, {"st": 14, "ed": 16, "text": "neural networks"}, {"st": 27, "ed": 29, "text": "linear activation"}, {"st": 30, "ed": 32, "text": "quadratic loss"}, {"st": 48, "ed": 50, "text": "hidden layer"}, {"st": 51, "ed": 53, "text": "training error"}, {"st": 58, "ed": 60, "text": "local minimum"}, {"st": 84, "ed": 86, "text": "theoretical guarantees"}, {"st": 91, "ed": 93, "text": "training data"}, {"st": 104, "ed": 106, "text": "convex loss"}, {"st": 117, "ed": 120, "text": "stochastic gradient descent"}]
[{"st": 9, "ed": 11, "text": "large scale"}, {"st": 11, "ed": 14, "text": "deep neural networks"}, {"st": 16, "ed": 18, "text": "resource constrained"}, {"st": 25, "ed": 27, "text": "structured sparsity"}, {"st": 57, "ed": 59, "text": "computation cost"}, {"st": 64, "ed": 66, "text": "structured sparsity"}, {"st": 87, "ed": 89, "text": "convolutional layer"}, {"st": 110, "ed": 112, "text": "structured sparsity"}, {"st": 126, "ed": 128, "text": "cifar 10"}, {"st": 138, "ed": 140, "text": "deep residual"}, {"st": 180, "ed": 182, "text": "open source"}, {"st": 185, "ed": 187, "text": "https github.com"}]
[{"st": 9, "ed": 13, "text": "feed forward neural networks"}, {"st": 48, "ed": 50, "text": "linear functions"}, {"st": 57, "ed": 60, "text": "l 1 norm"}, {"st": 87, "ed": 89, "text": "neural networks"}]
[{"st": 4, "ed": 6, "text": "neural network"}, {"st": 15, "ed": 19, "text": "rectified linear unit relu"}, {"st": 31, "ed": 33, "text": "input vector"}, {"st": 46, "ed": 48, "text": "hidden unit"}, {"st": 66, "ed": 68, "text": "deep network"}, {"st": 76, "ed": 79, "text": "vanishing gradient problem"}, {"st": 111, "ed": 113, "text": "learning algorithm"}, {"st": 117, "ed": 119, "text": "representation learning"}]
[{"st": 4, "ed": 6, "text": "neural network"}, {"st": 11, "ed": 13, "text": "expressive power"}, {"st": 14, "ed": 17, "text": "deep neural networks"}, {"st": 56, "ed": 58, "text": "grows exponentially"}, {"st": 96, "ed": 98, "text": "expressive power"}, {"st": 138, "ed": 140, "text": "input output"}]
[{"st": 0, "ed": 4, "text": "generative adversarial networks gans"}, {"st": 43, "ed": 45, "text": "technique called"}, {"st": 59, "ed": 61, "text": "latent vector"}, {"st": 68, "ed": 70, "text": "experiments demonstrate"}]
[{"st": 4, "ed": 6, "text": "challenging task"}, {"st": 106, "ed": 109, "text": "uc san diego"}, {"st": 114, "ed": 116, "text": "united states"}, {"st": 159, "ed": 161, "text": "log likelihood"}]
[{"st": 1, "ed": 3, "text": "deep learning"}, {"st": 12, "ed": 14, "text": "convex loss"}, {"st": 21, "ed": 23, "text": "local minima"}, {"st": 37, "ed": 39, "text": "local minima"}, {"st": 43, "ed": 45, "text": "convex loss"}, {"st": 53, "ed": 55, "text": "recently proposed"}, {"st": 62, "ed": 64, "text": "local minima"}, {"st": 66, "ed": 68, "text": "deep linear"}, {"st": 68, "ed": 70, "text": "neural networks"}, {"st": 74, "ed": 76, "text": "theoretical results"}, {"st": 77, "ed": 79, "text": "previous results"}, {"st": 96, "ed": 98, "text": "deep linear"}]
[{"st": 3, "ed": 5, "text": "random features"}, {"st": 6, "ed": 8, "text": "nonlinear function"}, {"st": 22, "ed": 24, "text": "deep learning"}, {"st": 26, "ed": 28, "text": "random features"}, {"st": 34, "ed": 36, "text": "hidden layer"}, {"st": 39, "ed": 41, "text": "random features"}, {"st": 80, "ed": 82, "text": "generalization bound"}, {"st": 84, "ed": 86, "text": "deep models"}, {"st": 99, "ed": 101, "text": "approximation error"}, {"st": 114, "ed": 116, "text": "generalization bound"}, {"st": 119, "ed": 121, "text": "random features"}, {"st": 134, "ed": 136, "text": "generalization error"}, {"st": 160, "ed": 162, "text": "random features"}, {"st": 167, "ed": 169, "text": "neural networks"}, {"st": 177, "ed": 179, "text": "random features"}, {"st": 181, "ed": 183, "text": "significantly fewer"}, {"st": 190, "ed": 192, "text": "ensemble method"}]
[{"st": 96, "ed": 98, "text": "regularization scheme"}, {"st": 139, "ed": 141, "text": "optimization strategy"}, {"st": 157, "ed": 159, "text": "feature representations"}, {"st": 166, "ed": 168, "text": "image classification"}, {"st": 171, "ed": 173, "text": "network architectures"}, {"st": 176, "ed": 178, "text": "method named"}]
[{"st": 5, "ed": 7, "text": "neural networks"}, {"st": 32, "ed": 34, "text": "total number"}, {"st": 54, "ed": 57, "text": "deep neural networks"}, {"st": 58, "ed": 60, "text": "grows exponentially"}, {"st": 63, "ed": 66, "text": "single hidden layer"}, {"st": 70, "ed": 72, "text": "provide evidence"}, {"st": 77, "ed": 79, "text": "hidden layers"}, {"st": 88, "ed": 90, "text": "grows exponentially"}]
[{"st": 53, "ed": 55, "text": "supervised learning"}, {"st": 154, "ed": 156, "text": "supervised learning"}, {"st": 158, "ed": 161, "text": "spiking neural networks"}]
[{"st": 2, "ed": 4, "text": "supervised learning"}, {"st": 43, "ed": 45, "text": "recent developments"}, {"st": 82, "ed": 84, "text": "genetic programming"}, {"st": 97, "ed": 99, "text": "neural networks"}, {"st": 105, "ed": 107, "text": "real world"}, {"st": 141, "ed": 143, "text": "computationally efficient"}, {"st": 150, "ed": 152, "text": "neural networks"}]
[{"st": 6, "ed": 9, "text": "recurrent neural network"}, {"st": 12, "ed": 14, "text": "anomaly detection"}, {"st": 28, "ed": 32, "text": "recurrent neural network rnn"}, {"st": 33, "ed": 35, "text": "variational inference"}, {"st": 38, "ed": 40, "text": "spatial information"}, {"st": 55, "ed": 57, "text": "feature extractor"}, {"st": 62, "ed": 64, "text": "latent variables"}, {"st": 94, "ed": 96, "text": "traffic flow"}]
[{"st": 0, "ed": 4, "text": "gated recurrent unit gru"}, {"st": 6, "ed": 8, "text": "recently developed"}, {"st": 12, "ed": 15, "text": "short term memory"}, {"st": 23, "ed": 27, "text": "recurrent neural network rnn"}, {"st": 29, "ed": 31, "text": "empirical evidence"}, {"st": 41, "ed": 43, "text": "wide variety"}, {"st": 44, "ed": 46, "text": "machine learning"}, {"st": 49, "ed": 51, "text": "natural language"}, {"st": 56, "ed": 58, "text": "speech recognition"}, {"st": 63, "ed": 65, "text": "text classification"}, {"st": 73, "ed": 75, "text": "neural networks"}, {"st": 87, "ed": 89, "text": "final output"}, {"st": 95, "ed": 97, "text": "cross entropy"}, {"st": 115, "ed": 118, "text": "support vector machine"}, {"st": 126, "ed": 128, "text": "final output"}, {"st": 135, "ed": 137, "text": "cross entropy"}, {"st": 143, "ed": 145, "text": "margin based"}, {"st": 166, "ed": 168, "text": "intrusion detection"}, {"st": 238, "ed": 240, "text": "final output"}, {"st": 260, "ed": 263, "text": "training and testing"}]
[{"st": 0, "ed": 3, "text": "deep neural networks"}, {"st": 14, "ed": 16, "text": "computer vision"}, {"st": 16, "ed": 18, "text": "speech recognition"}, {"st": 18, "ed": 20, "text": "natural language"}, {"st": 23, "ed": 25, "text": "social network"}, {"st": 26, "ed": 28, "text": "machine translation"}, {"st": 48, "ed": 50, "text": "adversarial perturbations"}, {"st": 53, "ed": 55, "text": "correctly classified"}, {"st": 89, "ed": 91, "text": "existing techniques"}, {"st": 119, "ed": 121, "text": "input space"}, {"st": 209, "ed": 211, "text": "mnist dataset"}, {"st": 214, "ed": 216, "text": "neural network"}, {"st": 261, "ed": 263, "text": "adversarial perturbations"}]
[{"st": 0, "ed": 2, "text": "neural networks"}, {"st": 4, "ed": 6, "text": "hidden nodes"}, {"st": 7, "ed": 9, "text": "gained increasing"}, {"st": 39, "ed": 41, "text": "hidden nodes"}, {"st": 44, "ed": 46, "text": "feature mapping"}, {"st": 93, "ed": 95, "text": "method generates"}, {"st": 99, "ed": 101, "text": "hidden nodes"}, {"st": 110, "ed": 112, "text": "activation functions"}, {"st": 116, "ed": 118, "text": "input space"}, {"st": 142, "ed": 144, "text": "input data"}, {"st": 146, "ed": 148, "text": "activation function"}]
[{"st": 7, "ed": 9, "text": "associative memory"}, {"st": 14, "ed": 17, "text": "recurrent neural networks"}, {"st": 48, "ed": 50, "text": "successful applications"}, {"st": 66, "ed": 68, "text": "rnn model"}, {"st": 109, "ed": 112, "text": "vanishing and exploding"}, {"st": 130, "ed": 132, "text": "question answering"}, {"st": 161, "ed": 163, "text": "question answering"}, {"st": 191, "ed": 193, "text": "character level"}, {"st": 206, "ed": 208, "text": "real world"}, {"st": 229, "ed": 231, "text": "speech recognition"}]
[{"st": 11, "ed": 13, "text": "key idea"}, {"st": 25, "ed": 27, "text": "low resolution"}, {"st": 74, "ed": 76, "text": "generated images"}, {"st": 80, "ed": 82, "text": "inception score"}, {"st": 119, "ed": 121, "text": "image quality"}, {"st": 130, "ed": 132, "text": "higher quality"}]
[{"st": 1, "ed": 3, "text": "source separation"}, {"st": 6, "ed": 11, "text": "non negative matrix factorization nmf"}, {"st": 12, "ed": 14, "text": "auto encoders"}, {"st": 23, "ed": 27, "text": "generative adversarial networks gans"}, {"st": 45, "ed": 47, "text": "source separation"}, {"st": 50, "ed": 53, "text": "multi layer perceptron"}, {"st": 56, "ed": 58, "text": "wasserstein gan"}, {"st": 61, "ed": 63, "text": "auto encoders"}, {"st": 65, "ed": 67, "text": "maximum likelihood"}, {"st": 68, "ed": 71, "text": "variational auto encoders"}]
[{"st": 0, "ed": 2, "text": "neural networks"}, {"st": 4, "ed": 6, "text": "great potential"}, {"st": 10, "ed": 12, "text": "speech recognition"}, {"st": 14, "ed": 16, "text": "image classification"}, {"st": 19, "ed": 21, "text": "neural network"}, {"st": 25, "ed": 27, "text": "biological neural"}, {"st": 34, "ed": 36, "text": "machine learning"}, {"st": 49, "ed": 51, "text": "neural networks"}, {"st": 58, "ed": 60, "text": "machine learning"}, {"st": 66, "ed": 68, "text": "learning algorithm"}, {"st": 88, "ed": 90, "text": "handwritten digit"}]
[{"st": 68, "ed": 70, "text": "market capitalization"}, {"st": 124, "ed": 127, "text": "deep neural networks"}, {"st": 138, "ed": 140, "text": "quantitative analysis"}, {"st": 142, "ed": 144, "text": "significant improvement"}]
[{"st": 6, "ed": 8, "text": "genetic algorithms"}, {"st": 12, "ed": 14, "text": "reverse engineer"}, {"st": 15, "ed": 17, "text": "evaluation function"}, {"st": 50, "ed": 52, "text": "computer chess"}, {"st": 54, "ed": 56, "text": "performance gain"}, {"st": 70, "ed": 72, "text": "evaluation function"}]
[{"st": 8, "ed": 10, "text": "feed forward"}, {"st": 10, "ed": 12, "text": "network architecture"}, {"st": 20, "ed": 22, "text": "learning paradigm"}, {"st": 44, "ed": 46, "text": "previous tasks"}, {"st": 64, "ed": 66, "text": "computational resources"}, {"st": 80, "ed": 83, "text": "trained from scratch"}]
[{"st": 7, "ed": 9, "text": "large scale"}, {"st": 22, "ed": 24, "text": "shopping mall"}, {"st": 49, "ed": 52, "text": "deep neural networks"}, {"st": 65, "ed": 67, "text": "wi fi"}, {"st": 100, "ed": 102, "text": "feature space"}, {"st": 105, "ed": 107, "text": "feed forward"}, {"st": 109, "ed": 112, "text": "multi label classification"}, {"st": 129, "ed": 131, "text": "wi fi"}, {"st": 158, "ed": 160, "text": "dnn based"}]
[{"st": 7, "ed": 9, "text": "highly efficient"}, {"st": 11, "ed": 13, "text": "time series"}, {"st": 28, "ed": 30, "text": "gaussian distribution"}, {"st": 67, "ed": 69, "text": "gaussian distribution"}, {"st": 92, "ed": 94, "text": "time series"}, {"st": 103, "ed": 105, "text": "gaussian distribution"}, {"st": 108, "ed": 110, "text": "shape parameter"}, {"st": 115, "ed": 117, "text": "heavy tailed"}, {"st": 131, "ed": 133, "text": "performance improvement"}, {"st": 136, "ed": 138, "text": "time series"}]
[{"st": 18, "ed": 20, "text": "neural network"}, {"st": 32, "ed": 34, "text": "reinforcement learning"}, {"st": 59, "ed": 61, "text": "higher level"}, {"st": 115, "ed": 117, "text": "higher level"}]
[{"st": 0, "ed": 2, "text": "learning algorithms"}, {"st": 52, "ed": 55, "text": "restricted boltzmann machines"}, {"st": 58, "ed": 60, "text": "learning algorithm"}, {"st": 60, "ed": 62, "text": "contrastive divergence"}, {"st": 146, "ed": 148, "text": "significant improvement"}]
[{"st": 0, "ed": 3, "text": "deep neural networks"}, {"st": 6, "ed": 8, "text": "machine learning"}, {"st": 14, "ed": 16, "text": "artificial intelligence"}, {"st": 33, "ed": 35, "text": "network structure"}, {"st": 54, "ed": 56, "text": "network structure"}, {"st": 57, "ed": 59, "text": "weight parameters"}, {"st": 60, "ed": 62, "text": "neural network"}, {"st": 70, "ed": 72, "text": "network structures"}, {"st": 81, "ed": 83, "text": "directly optimizing"}, {"st": 87, "ed": 89, "text": "proposed method"}, {"st": 94, "ed": 96, "text": "network structure"}, {"st": 96, "ed": 98, "text": "optimization problems"}, {"st": 105, "ed": 107, "text": "proposed method"}, {"st": 110, "ed": 112, "text": "optimization problems"}, {"st": 127, "ed": 130, "text": "mnist cifar 10"}, {"st": 131, "ed": 133, "text": "cifar 100"}, {"st": 140, "ed": 142, "text": "proposed method"}]
[{"st": 16, "ed": 18, "text": "genetic programming"}, {"st": 20, "ed": 22, "text": "ensemble methods"}, {"st": 26, "ed": 28, "text": "machine learning"}, {"st": 54, "ed": 56, "text": "gp models"}, {"st": 90, "ed": 92, "text": "genetic programming"}, {"st": 97, "ed": 99, "text": "based method"}, {"st": 104, "ed": 106, "text": "recent literature"}, {"st": 115, "ed": 117, "text": "generalization ability"}, {"st": 128, "ed": 130, "text": "gp models"}, {"st": 178, "ed": 180, "text": "generalization ability"}, {"st": 188, "ed": 190, "text": "computational burden"}]
[{"st": 6, "ed": 8, "text": "natural gradient"}, {"st": 47, "ed": 49, "text": "natural gradient"}, {"st": 64, "ed": 67, "text": "log partition function"}, {"st": 93, "ed": 95, "text": "joint training"}, {"st": 115, "ed": 117, "text": "maximum likelihood"}]
[{"st": 3, "ed": 6, "text": "simple and effective"}, {"st": 35, "ed": 37, "text": "multinomial distribution"}, {"st": 48, "ed": 50, "text": "hyper parameter"}, {"st": 74, "ed": 76, "text": "image datasets"}]
[{"st": 18, "ed": 20, "text": "conjugate gradient"}, {"st": 99, "ed": 101, "text": "competitive performance"}, {"st": 105, "ed": 107, "text": "deep autoencoder"}]
[{"st": 6, "ed": 10, "text": "markov chain monte carlo"}, {"st": 11, "ed": 13, "text": "simulated annealing"}, {"st": 16, "ed": 19, "text": "radial basis function"}, {"st": 29, "ed": 31, "text": "posterior distribution"}, {"st": 33, "ed": 35, "text": "network parameters"}, {"st": 93, "ed": 96, "text": "theoretically and empirically"}, {"st": 106, "ed": 108, "text": "posterior distribution"}]
[{"st": 12, "ed": 14, "text": "deep learning"}, {"st": 28, "ed": 30, "text": "accurately predict"}, {"st": 41, "ed": 43, "text": "parameter values"}]
[{"st": 15, "ed": 18, "text": "factors of variation"}, {"st": 26, "ed": 29, "text": "spike and slab"}, {"st": 29, "ed": 32, "text": "restricted boltzmann machine"}, {"st": 37, "ed": 39, "text": "higher order"}, {"st": 71, "ed": 73, "text": "unlike previous"}, {"st": 76, "ed": 78, "text": "latent factors"}, {"st": 99, "ed": 101, "text": "facial expression"}]
[{"st": 0, "ed": 3, "text": "artificial neural networks"}, {"st": 4, "ed": 7, "text": "simple and efficient"}, {"st": 7, "ed": 9, "text": "machine learning"}, {"st": 20, "ed": 22, "text": "neural network"}, {"st": 33, "ed": 35, "text": "real world"}]
[{"st": 1, "ed": 4, "text": "artificial neural networks"}, {"st": 29, "ed": 31, "text": "machine learning"}, {"st": 41, "ed": 43, "text": "neural net"}, {"st": 46, "ed": 48, "text": "successful applications"}, {"st": 68, "ed": 70, "text": "neural networks"}, {"st": 78, "ed": 81, "text": "artificial neural network"}, {"st": 98, "ed": 100, "text": "conducted experiments"}, {"st": 101, "ed": 103, "text": "recent methods"}, {"st": 108, "ed": 110, "text": "neural networks"}, {"st": 117, "ed": 119, "text": "neural networks"}, {"st": 138, "ed": 140, "text": "neural net"}]
[{"st": 0, "ed": 2, "text": "collaborative filtering"}, {"st": 25, "ed": 27, "text": "collaborative filtering"}, {"st": 30, "ed": 33, "text": "cold start problem"}, {"st": 52, "ed": 54, "text": "additional information"}, {"st": 61, "ed": 63, "text": "collaborative filtering"}, {"st": 75, "ed": 77, "text": "neural network"}, {"st": 80, "ed": 82, "text": "input variables"}, {"st": 83, "ed": 85, "text": "neural network"}, {"st": 90, "ed": 92, "text": "collaborative filtering"}, {"st": 105, "ed": 107, "text": "content based"}, {"st": 128, "ed": 130, "text": "collaborative filtering"}]
[{"st": 2, "ed": 4, "text": "hidden units"}, {"st": 6, "ed": 10, "text": "multi layer perceptron mlp"}, {"st": 40, "ed": 42, "text": "structured prediction"}, {"st": 46, "ed": 48, "text": "internal structure"}, {"st": 65, "ed": 67, "text": "generalization performance"}, {"st": 95, "ed": 97, "text": "m 1"}, {"st": 128, "ed": 130, "text": "experiments confirm"}]
[{"st": 18, "ed": 21, "text": "extreme learning machines"}, {"st": 29, "ed": 32, "text": "linear discriminant analysis"}, {"st": 34, "ed": 36, "text": "bayes optimal"}]
[{"st": 8, "ed": 10, "text": "deep networks"}, {"st": 34, "ed": 36, "text": "larger datasets"}, {"st": 44, "ed": 46, "text": "learning algorithms"}, {"st": 48, "ed": 50, "text": "decision trees"}, {"st": 94, "ed": 97, "text": "deep neural network"}, {"st": 126, "ed": 128, "text": "weight matrices"}, {"st": 129, "ed": 131, "text": "neural networks"}, {"st": 150, "ed": 152, "text": "proposed approach"}, {"st": 159, "ed": 161, "text": "weight matrices"}, {"st": 166, "ed": 168, "text": "hidden unit"}]
[{"st": 28, "ed": 30, "text": "representational power"}, {"st": 34, "ed": 38, "text": "restricted boltzmann machines rbms"}, {"st": 64, "ed": 67, "text": "theoretical and empirical"}, {"st": 70, "ed": 72, "text": "representational power"}, {"st": 92, "ed": 94, "text": "representational power"}, {"st": 149, "ed": 151, "text": "distributed representations"}, {"st": 156, "ed": 158, "text": "experiments demonstrate"}, {"st": 166, "ed": 168, "text": "models including"}]
[{"st": 4, "ed": 6, "text": "representation learning"}, {"st": 8, "ed": 10, "text": "domain adaptation"}, {"st": 14, "ed": 17, "text": "training and test"}, {"st": 33, "ed": 35, "text": "domain adaptation"}, {"st": 39, "ed": 41, "text": "domain transfer"}, {"st": 71, "ed": 73, "text": "neural network"}, {"st": 78, "ed": 80, "text": "labeled data"}, {"st": 82, "ed": 84, "text": "source domain"}, {"st": 85, "ed": 87, "text": "unlabeled data"}, {"st": 117, "ed": 119, "text": "learning task"}, {"st": 121, "ed": 123, "text": "source domain"}, {"st": 146, "ed": 148, "text": "feed forward"}, {"st": 170, "ed": 172, "text": "standard backpropagation"}, {"st": 173, "ed": 176, "text": "stochastic gradient descent"}, {"st": 188, "ed": 190, "text": "deep learning"}, {"st": 201, "ed": 203, "text": "classification problems"}, {"st": 204, "ed": 206, "text": "sentiment analysis"}, {"st": 207, "ed": 209, "text": "image classification"}, {"st": 214, "ed": 216, "text": "domain adaptation"}, {"st": 218, "ed": 220, "text": "standard benchmarks"}, {"st": 229, "ed": 231, "text": "learning task"}, {"st": 235, "ed": 238, "text": "person re identification"}]
[{"st": 2, "ed": 4, "text": "convex optimization"}, {"st": 8, "ed": 10, "text": "building blocks"}, {"st": 11, "ed": 13, "text": "deep learning"}, {"st": 19, "ed": 21, "text": "empirical success"}, {"st": 25, "ed": 27, "text": "convolutional networks"}, {"st": 32, "ed": 34, "text": "max pooling"}, {"st": 50, "ed": 52, "text": "convergence rates"}, {"st": 86, "ed": 88, "text": "max pooling"}, {"st": 97, "ed": 99, "text": "neural networks"}, {"st": 118, "ed": 120, "text": "convergence properties"}, {"st": 128, "ed": 130, "text": "main result"}, {"st": 137, "ed": 139, "text": "critical point"}, {"st": 156, "ed": 158, "text": "main result"}, {"st": 170, "ed": 172, "text": "neural network"}, {"st": 179, "ed": 181, "text": "neural nets"}, {"st": 191, "ed": 193, "text": "neural nets"}, {"st": 198, "ed": 200, "text": "recently developed"}]
[{"st": 16, "ed": 18, "text": "deep learning"}, {"st": 23, "ed": 25, "text": "supervised learning"}, {"st": 28, "ed": 30, "text": "labeled data"}, {"st": 36, "ed": 40, "text": "deep convolutional neural networks"}, {"st": 60, "ed": 62, "text": "unsupervised learning"}, {"st": 79, "ed": 81, "text": "hidden units"}]
[{"st": 0, "ed": 2, "text": "icu mortality"}, {"st": 10, "ed": 12, "text": "predictive models"}, {"st": 58, "ed": 60, "text": "predictive models"}, {"st": 66, "ed": 68, "text": "variable selection"}, {"st": 74, "ed": 76, "text": "genetic algorithm"}, {"st": 81, "ed": 83, "text": "logistic regression"}, {"st": 126, "ed": 128, "text": "weighted average"}, {"st": 137, "ed": 139, "text": "genetic algorithm"}, {"st": 166, "ed": 168, "text": "improved performance"}, {"st": 172, "ed": 174, "text": "random forest"}, {"st": 194, "ed": 196, "text": "performance metric"}]
[{"st": 6, "ed": 9, "text": "deep neural networks"}, {"st": 19, "ed": 21, "text": "exploding gradient"}, {"st": 57, "ed": 59, "text": "preliminary experiments"}]
[{"st": 3, "ed": 5, "text": "multi layered"}, {"st": 8, "ed": 10, "text": "generative models"}, {"st": 14, "ed": 17, "text": "difficult to train"}, {"st": 24, "ed": 26, "text": "layer wise"}, {"st": 26, "ed": 28, "text": "training procedure"}, {"st": 30, "ed": 32, "text": "performance guarantee"}, {"st": 53, "ed": 55, "text": "auto encoders"}, {"st": 59, "ed": 61, "text": "generative models"}, {"st": 102, "ed": 104, "text": "deep architectures"}, {"st": 112, "ed": 114, "text": "hidden variables"}, {"st": 117, "ed": 119, "text": "generative model"}, {"st": 120, "ed": 122, "text": "hidden variables"}]
[{"st": 67, "ed": 70, "text": "deep artificial neural"}, {"st": 72, "ed": 74, "text": "object recognition"}, {"st": 76, "ed": 78, "text": "computer vision"}, {"st": 81, "ed": 83, "text": "front end"}, {"st": 107, "ed": 109, "text": "nonconvex optimization"}, {"st": 164, "ed": 166, "text": "problem involving"}, {"st": 168, "ed": 170, "text": "nested function"}, {"st": 173, "ed": 175, "text": "problem involving"}, {"st": 192, "ed": 194, "text": "based methods"}, {"st": 195, "ed": 197, "text": "alternating optimization"}, {"st": 209, "ed": 212, "text": "easy to implement"}, {"st": 213, "ed": 215, "text": "existing algorithms"}]
[{"st": 1, "ed": 3, "text": "dimensionality reduction"}, {"st": 11, "ed": 13, "text": "random variables"}, {"st": 26, "ed": 28, "text": "dimensionality reduction"}, {"st": 69, "ed": 71, "text": "fisher information"}, {"st": 108, "ed": 110, "text": "hidden units"}, {"st": 149, "ed": 152, "text": "deep neural network"}, {"st": 163, "ed": 165, "text": "layer wise"}, {"st": 171, "ed": 173, "text": "theoretical analysis"}, {"st": 180, "ed": 182, "text": "contrastive divergence"}]
[{"st": 0, "ed": 2, "text": "radial basis"}, {"st": 3, "ed": 5, "text": "neural networks"}, {"st": 39, "ed": 42, "text": "non convex optimization"}, {"st": 77, "ed": 79, "text": "nonconvex optimization"}, {"st": 96, "ed": 98, "text": "global optimal"}, {"st": 113, "ed": 115, "text": "radial basis"}, {"st": 117, "ed": 119, "text": "gaussian function"}, {"st": 158, "ed": 160, "text": "neural networks"}]
[{"st": 9, "ed": 12, "text": "feedforward neural networks"}, {"st": 30, "ed": 32, "text": "deep networks"}, {"st": 42, "ed": 44, "text": "input space"}, {"st": 51, "ed": 53, "text": "deep models"}, {"st": 65, "ed": 67, "text": "compositional structure"}, {"st": 99, "ed": 101, "text": "theoretical results"}, {"st": 107, "ed": 109, "text": "neural networks"}, {"st": 111, "ed": 113, "text": "linear activation"}, {"st": 141, "ed": 143, "text": "complexity bounds"}]
[{"st": 1, "ed": 4, "text": "restricted boltzmann machines"}, {"st": 7, "ed": 9, "text": "neural networks"}, {"st": 13, "ed": 16, "text": "input and output"}, {"st": 30, "ed": 32, "text": "conditional probability"}, {"st": 38, "ed": 40, "text": "output units"}, {"st": 56, "ed": 58, "text": "representational power"}, {"st": 68, "ed": 71, "text": "markov random fields"}, {"st": 72, "ed": 74, "text": "conditional distributions"}, {"st": 105, "ed": 107, "text": "conditional probability"}, {"st": 123, "ed": 126, "text": "restricted boltzmann machine"}]
[{"st": 2, "ed": 4, "text": "generative models"}, {"st": 9, "ed": 11, "text": "deep architectures"}, {"st": 12, "ed": 14, "text": "layer wise"}, {"st": 14, "ed": 16, "text": "pre training"}, {"st": 21, "ed": 23, "text": "trained model"}, {"st": 36, "ed": 38, "text": "hidden variables"}, {"st": 40, "ed": 42, "text": "higher layers"}, {"st": 98, "ed": 100, "text": "joint training"}, {"st": 145, "ed": 147, "text": "empirically evaluate"}, {"st": 151, "ed": 153, "text": "joint training"}, {"st": 163, "ed": 165, "text": "data model"}, {"st": 191, "ed": 193, "text": "joint training"}, {"st": 202, "ed": 204, "text": "supervised setting"}, {"st": 204, "ed": 206, "text": "joint training"}, {"st": 215, "ed": 217, "text": "joint training"}]
[{"st": 2, "ed": 4, "text": "neural architectures"}, {"st": 6, "ed": 8, "text": "learning framework"}, {"st": 62, "ed": 64, "text": "empirical studies"}, {"st": 84, "ed": 86, "text": "predictive power"}, {"st": 105, "ed": 107, "text": "total number"}]
[{"st": 7, "ed": 11, "text": "recurrent neural network rnn"}, {"st": 27, "ed": 29, "text": "recurrent layers"}, {"st": 37, "ed": 39, "text": "recurrent layers"}, {"st": 65, "ed": 67, "text": "hidden states"}, {"st": 81, "ed": 83, "text": "recurrent units"}, {"st": 87, "ed": 90, "text": "short term memory"}, {"st": 91, "ed": 93, "text": "gated recurrent"}, {"st": 98, "ed": 100, "text": "character level"}, {"st": 107, "ed": 109, "text": "empirical evaluation"}, {"st": 123, "ed": 125, "text": "conventional approaches"}]
[{"st": 2, "ed": 4, "text": "large scale"}, {"st": 4, "ed": 7, "text": "deep neural networks"}, {"st": 27, "ed": 29, "text": "neural network"}, {"st": 34, "ed": 36, "text": "low precision"}, {"st": 36, "ed": 38, "text": "fixed point"}, {"st": 61, "ed": 63, "text": "deep networks"}, {"st": 71, "ed": 73, "text": "fixed point"}, {"st": 99, "ed": 101, "text": "low precision"}, {"st": 101, "ed": 104, "text": "fixed point arithmetic"}]
[{"st": 10, "ed": 12, "text": "neural network"}, {"st": 29, "ed": 31, "text": "neural networks"}, {"st": 72, "ed": 74, "text": "conditional probabilities"}, {"st": 92, "ed": 94, "text": "joint probability"}, {"st": 106, "ed": 108, "text": "architectures including"}, {"st": 120, "ed": 122, "text": "experiments demonstrate"}, {"st": 141, "ed": 143, "text": "significantly faster"}]
[{"st": 0, "ed": 2, "text": "sparse coding"}, {"st": 10, "ed": 12, "text": "signal processing"}, {"st": 14, "ed": 16, "text": "machine learning"}, {"st": 27, "ed": 29, "text": "sparse representation"}, {"st": 44, "ed": 47, "text": "non convex optimization"}, {"st": 68, "ed": 70, "text": "sparse coding"}, {"st": 71, "ed": 73, "text": "provable guarantees"}, {"st": 82, "ed": 84, "text": "alternating minimization"}, {"st": 93, "ed": 95, "text": "alternating minimization"}, {"st": 119, "ed": 121, "text": "neural architectures"}, {"st": 143, "ed": 145, "text": "sparse coding"}, {"st": 155, "ed": 157, "text": "sparse recovery"}, {"st": 183, "ed": 185, "text": "sample complexity"}, {"st": 202, "ed": 204, "text": "iterative algorithms"}]
[{"st": 87, "ed": 89, "text": "prediction methods"}, {"st": 91, "ed": 93, "text": "specifically designed"}, {"st": 99, "ed": 101, "text": "deep learning"}, {"st": 130, "ed": 132, "text": "automatically learn"}, {"st": 140, "ed": 142, "text": "deep learning"}]
[{"st": 1, "ed": 4, "text": "deep belief networks"}, {"st": 9, "ed": 11, "text": "convex function"}, {"st": 13, "ed": 15, "text": "extremely large"}, {"st": 23, "ed": 25, "text": "based methods"}, {"st": 40, "ed": 42, "text": "local minima"}, {"st": 47, "ed": 49, "text": "approximation error"}, {"st": 75, "ed": 77, "text": "hidden layer"}, {"st": 77, "ed": 79, "text": "neural network"}, {"st": 80, "ed": 82, "text": "training objective"}, {"st": 87, "ed": 89, "text": "approximation error"}, {"st": 104, "ed": 106, "text": "empirical risk"}, {"st": 155, "ed": 157, "text": "generalization error"}, {"st": 160, "ed": 163, "text": "generalized linear models"}, {"st": 178, "ed": 180, "text": "differentially private"}, {"st": 195, "ed": 197, "text": "training points"}, {"st": 199, "ed": 201, "text": "accurate predictions"}, {"st": 207, "ed": 209, "text": "empirically validate"}, {"st": 225, "ed": 227, "text": "significantly outperforms"}, {"st": 230, "ed": 232, "text": "prediction accuracy"}, {"st": 233, "ed": 235, "text": "l2 regularization"}, {"st": 235, "ed": 237, "text": "based methods"}]
[{"st": 11, "ed": 13, "text": "machine learning"}, {"st": 45, "ed": 47, "text": "computationally expensive"}, {"st": 116, "ed": 118, "text": "significantly improve"}, {"st": 163, "ed": 165, "text": "fine grained"}, {"st": 173, "ed": 176, "text": "mixture of experts"}]
[{"st": 1, "ed": 3, "text": "complex valued"}, {"st": 3, "ed": 5, "text": "convolutional network"}, {"st": 23, "ed": 25, "text": "input vector"}, {"st": 32, "ed": 34, "text": "complex valued"}, {"st": 56, "ed": 58, "text": "real valued"}, {"st": 58, "ed": 60, "text": "random vectors"}, {"st": 60, "ed": 62, "text": "complex valued"}, {"st": 96, "ed": 98, "text": "complex valued"}, {"st": 110, "ed": 112, "text": "complex valued"}, {"st": 114, "ed": 116, "text": "real valued"}, {"st": 118, "ed": 121, "text": "rectified linear units"}, {"st": 146, "ed": 148, "text": "complex valued"}, {"st": 179, "ed": 181, "text": "complex valued"}]
[{"st": 7, "ed": 9, "text": "natural gradient"}, {"st": 11, "ed": 13, "text": "neural networks"}, {"st": 34, "ed": 36, "text": "neural network"}, {"st": 37, "ed": 39, "text": "fisher information"}, {"st": 45, "ed": 47, "text": "low rank"}, {"st": 117, "ed": 120, "text": "stochastic gradient descent"}, {"st": 127, "ed": 129, "text": "previously proposed"}, {"st": 130, "ed": 132, "text": "natural gradient"}, {"st": 196, "ed": 198, "text": "low rank"}]
[{"st": 7, "ed": 9, "text": "promising performance"}, {"st": 16, "ed": 18, "text": "compact representations"}, {"st": 124, "ed": 127, "text": "deep neural network"}, {"st": 146, "ed": 148, "text": "prediction accuracy"}, {"st": 156, "ed": 159, "text": "times faster than"}, {"st": 177, "ed": 179, "text": "unsupervised learning"}, {"st": 181, "ed": 184, "text": "effectiveness and efficiency"}, {"st": 187, "ed": 189, "text": "supervised learning"}, {"st": 192, "ed": 195, "text": "effectiveness and efficiency"}]
[{"st": 28, "ed": 31, "text": "short term memory"}, {"st": 31, "ed": 34, "text": "artificial neural network"}, {"st": 41, "ed": 44, "text": "intensive care unit"}, {"st": 66, "ed": 68, "text": "time series"}, {"st": 76, "ed": 78, "text": "ghent university"}, {"st": 80, "ed": 82, "text": "main goal"}, {"st": 87, "ed": 89, "text": "machine learning"}, {"st": 101, "ed": 103, "text": "early detection"}, {"st": 114, "ed": 116, "text": "precision recall"}, {"st": 122, "ed": 124, "text": "neural networks"}]
[{"st": 6, "ed": 8, "text": "neural networks"}, {"st": 65, "ed": 67, "text": "variational approximation"}, {"st": 70, "ed": 72, "text": "neural network"}, {"st": 92, "ed": 94, "text": "neural network"}, {"st": 95, "ed": 97, "text": "maximum likelihood"}, {"st": 119, "ed": 121, "text": "maximum likelihood"}, {"st": 128, "ed": 130, "text": "variational approximation"}]
[{"st": 0, "ed": 3, "text": "semi supervised learning"}, {"st": 10, "ed": 13, "text": "labeled training data"}, {"st": 16, "ed": 19, "text": "labeled and unlabeled"}, {"st": 22, "ed": 25, "text": "deep convolutional networks"}, {"st": 27, "ed": 30, "text": "achieved great success"}, {"st": 31, "ed": 33, "text": "supervised tasks"}, {"st": 51, "ed": 53, "text": "recently developed"}, {"st": 55, "ed": 57, "text": "mixture model"}, {"st": 60, "ed": 62, "text": "generative model"}, {"st": 69, "ed": 71, "text": "inference algorithm"}, {"st": 76, "ed": 78, "text": "em algorithm"}, {"st": 85, "ed": 88, "text": "labeled and unlabeled"}, {"st": 100, "ed": 102, "text": "non negativity"}, {"st": 105, "ed": 107, "text": "variational inference"}, {"st": 120, "ed": 122, "text": "competitive results"}, {"st": 135, "ed": 137, "text": "semi supervised"}, {"st": 152, "ed": 154, "text": "unified framework"}, {"st": 156, "ed": 160, "text": "unsupervised and semi supervised"}]
[{"st": 5, "ed": 7, "text": "biologically plausible"}, {"st": 7, "ed": 9, "text": "online algorithms"}, {"st": 14, "ed": 16, "text": "streaming data"}, {"st": 39, "ed": 41, "text": "singular values"}, {"st": 43, "ed": 45, "text": "input data"}, {"st": 57, "ed": 59, "text": "singular values"}, {"st": 64, "ed": 66, "text": "online algorithms"}, {"st": 75, "ed": 77, "text": "singular values"}, {"st": 90, "ed": 92, "text": "cost function"}, {"st": 99, "ed": 101, "text": "online algorithms"}, {"st": 108, "ed": 110, "text": "neural networks"}, {"st": 113, "ed": 115, "text": "learning rule"}, {"st": 131, "ed": 133, "text": "online algorithms"}]
[{"st": 7, "ed": 10, "text": "artificial neural networks"}, {"st": 42, "ed": 46, "text": "recurrent neural networks rnns"}, {"st": 60, "ed": 62, "text": "neural network"}, {"st": 99, "ed": 101, "text": "computational complexity"}, {"st": 107, "ed": 109, "text": "mathcal o"}, {"st": 128, "ed": 130, "text": "digit recognition"}, {"st": 145, "ed": 147, "text": "significantly outperforms"}, {"st": 157, "ed": 159, "text": "lstm architecture"}, {"st": 163, "ed": 165, "text": "final performance"}, {"st": 183, "ed": 185, "text": "wide variety"}]
[{"st": 1, "ed": 3, "text": "machine learning"}, {"st": 19, "ed": 21, "text": "speech recognition"}, {"st": 21, "ed": 23, "text": "machine translation"}, {"st": 28, "ed": 31, "text": "text to speech"}, {"st": 39, "ed": 41, "text": "key challenges"}, {"st": 50, "ed": 53, "text": "input and output"}, {"st": 69, "ed": 73, "text": "recurrent neural networks rnns"}, {"st": 92, "ed": 94, "text": "pre defined"}, {"st": 97, "ed": 100, "text": "input and output"}, {"st": 131, "ed": 133, "text": "output sequence"}, {"st": 140, "ed": 143, "text": "end to end"}, {"st": 159, "ed": 161, "text": "input sequence"}]
[{"st": 0, "ed": 4, "text": "recurrent neural networks rnns"}, {"st": 40, "ed": 43, "text": "vanishing and exploding"}, {"st": 67, "ed": 70, "text": "generalized linear models"}, {"st": 71, "ed": 73, "text": "neural networks"}, {"st": 75, "ed": 77, "text": "back propagation"}, {"st": 86, "ed": 88, "text": "quadratic form"}, {"st": 127, "ed": 129, "text": "training error"}]
[{"st": 79, "ed": 81, "text": "max pooling"}, {"st": 86, "ed": 90, "text": "convolutional neural networks cnn"}, {"st": 107, "ed": 109, "text": "recently proposed"}, {"st": 122, "ed": 124, "text": "object recognition"}, {"st": 139, "ed": 141, "text": "activation function"}, {"st": 156, "ed": 158, "text": "complex nonlinear"}, {"st": 219, "ed": 221, "text": "empirically evaluate"}, {"st": 234, "ed": 236, "text": "multilayer perceptrons"}, {"st": 266, "ed": 268, "text": "recently proposed"}, {"st": 268, "ed": 272, "text": "deep recurrent neural networks"}]
[{"st": 1, "ed": 3, "text": "data mining"}, {"st": 16, "ed": 19, "text": "real world data"}, {"st": 27, "ed": 29, "text": "classification algorithms"}, {"st": 35, "ed": 37, "text": "missing values"}, {"st": 45, "ed": 47, "text": "missing values"}, {"st": 63, "ed": 65, "text": "unsupervised learning"}, {"st": 72, "ed": 75, "text": "multi layer perceptron"}, {"st": 96, "ed": 98, "text": "missing values"}, {"st": 108, "ed": 110, "text": "missing values"}, {"st": 111, "ed": 113, "text": "significantly lower"}, {"st": 114, "ed": 116, "text": "squared error"}, {"st": 118, "ed": 120, "text": "collaborative filtering"}, {"st": 131, "ed": 133, "text": "supervised learning"}, {"st": 135, "ed": 137, "text": "classification accuracy"}]
[{"st": 0, "ed": 2, "text": "contrastive divergence"}, {"st": 5, "ed": 7, "text": "contrastive divergence"}, {"st": 114, "ed": 116, "text": "learning rates"}]
[{"st": 10, "ed": 14, "text": "recurrent neural network rnn"}, {"st": 104, "ed": 106, "text": "recurrent layers"}, {"st": 141, "ed": 143, "text": "empirically evaluated"}, {"st": 147, "ed": 149, "text": "polyphonic music"}]
[{"st": 0, "ed": 2, "text": "deep learning"}, {"st": 12, "ed": 14, "text": "image recognition"}, {"st": 21, "ed": 23, "text": "deep networks"}, {"st": 71, "ed": 73, "text": "spike timing"}, {"st": 88, "ed": 90, "text": "deep networks"}, {"st": 98, "ed": 100, "text": "neural network"}, {"st": 103, "ed": 105, "text": "complex valued"}, {"st": 120, "ed": 122, "text": "deep learning"}]
[{"st": 1, "ed": 3, "text": "recently introduced"}, {"st": 7, "ed": 9, "text": "neural networks"}, {"st": 33, "ed": 35, "text": "training procedure"}, {"st": 37, "ed": 39, "text": "exponentially large"}, {"st": 67, "ed": 69, "text": "rectified linear"}, {"st": 81, "ed": 83, "text": "inference procedure"}, {"st": 145, "ed": 147, "text": "maximum likelihood"}]
[{"st": 0, "ed": 2, "text": "catastrophic forgetting"}, {"st": 8, "ed": 10, "text": "machine learning"}, {"st": 25, "ed": 27, "text": "machine learning"}, {"st": 55, "ed": 57, "text": "catastrophic forgetting"}, {"st": 61, "ed": 63, "text": "neural networks"}, {"st": 148, "ed": 150, "text": "activation function"}, {"st": 156, "ed": 158, "text": "activation function"}]
[{"st": 2, "ed": 4, "text": "deep architectures"}, {"st": 7, "ed": 10, "text": "massive amounts of"}, {"st": 16, "ed": 18, "text": "labeled data"}, {"st": 22, "ed": 24, "text": "domain adaptation"}, {"st": 31, "ed": 33, "text": "labeled data"}, {"st": 53, "ed": 55, "text": "domain adaptation"}, {"st": 56, "ed": 58, "text": "deep architectures"}, {"st": 66, "ed": 68, "text": "labeled data"}, {"st": 70, "ed": 72, "text": "source domain"}, {"st": 76, "ed": 78, "text": "unlabeled data"}, {"st": 100, "ed": 102, "text": "deep features"}, {"st": 109, "ed": 111, "text": "learning task"}, {"st": 113, "ed": 115, "text": "source domain"}, {"st": 138, "ed": 140, "text": "feed forward"}, {"st": 178, "ed": 180, "text": "deep learning"}, {"st": 190, "ed": 192, "text": "image classification"}]
[{"st": 1, "ed": 3, "text": "discrete data"}, {"st": 67, "ed": 69, "text": "log likelihood"}, {"st": 113, "ed": 116, "text": "encoder and decoder"}, {"st": 121, "ed": 124, "text": "deep neural network"}, {"st": 133, "ed": 135, "text": "log likelihood"}, {"st": 203, "ed": 205, "text": "back propagation"}, {"st": 252, "ed": 254, "text": "pre training"}]
[{"st": 0, "ed": 2, "text": "deep learning"}, {"st": 10, "ed": 12, "text": "multiple layers"}, {"st": 15, "ed": 17, "text": "automatically learn"}, {"st": 17, "ed": 19, "text": "relevant features"}, {"st": 37, "ed": 39, "text": "machine learning"}, {"st": 41, "ed": 43, "text": "computer vision"}, {"st": 43, "ed": 45, "text": "speech recognition"}, {"st": 46, "ed": 48, "text": "natural language"}, {"st": 54, "ed": 56, "text": "deep learning"}, {"st": 69, "ed": 71, "text": "feature learning"}, {"st": 77, "ed": 79, "text": "deep learning"}, {"st": 92, "ed": 94, "text": "theoretical physics"}, {"st": 112, "ed": 114, "text": "relevant features"}, {"st": 141, "ed": 143, "text": "deep learning"}, {"st": 146, "ed": 150, "text": "restricted boltzmann machines rbms"}, {"st": 157, "ed": 159, "text": "nearest neighbor"}, {"st": 159, "ed": 161, "text": "ising model"}, {"st": 170, "ed": 172, "text": "deep learning"}, {"st": 183, "ed": 185, "text": "relevant features"}]
[{"st": 18, "ed": 20, "text": "observed data"}, {"st": 98, "ed": 100, "text": "underlying structure"}]
[{"st": 3, "ed": 5, "text": "neural network"}, {"st": 11, "ed": 13, "text": "speech recognition"}, {"st": 22, "ed": 26, "text": "amounts of training data"}, {"st": 31, "ed": 33, "text": "multi core"}, {"st": 62, "ed": 64, "text": "neural network"}, {"st": 104, "ed": 106, "text": "efficient implementation"}, {"st": 107, "ed": 109, "text": "natural gradient"}, {"st": 110, "ed": 113, "text": "stochastic gradient descent"}]
[{"st": 3, "ed": 6, "text": "hidden markov model"}, {"st": 13, "ed": 16, "text": "continuous speech recognition"}, {"st": 18, "ed": 20, "text": "bi directional"}, {"st": 20, "ed": 23, "text": "recurrent neural network"}, {"st": 27, "ed": 30, "text": "recurrent neural network"}, {"st": 42, "ed": 45, "text": "input and output"}, {"st": 50, "ed": 52, "text": "attention mechanism"}, {"st": 75, "ed": 77, "text": "initial results"}, {"st": 81, "ed": 83, "text": "approach achieves"}, {"st": 84, "ed": 86, "text": "error rates"}, {"st": 95, "ed": 97, "text": "hmm based"}]
[{"st": 7, "ed": 10, "text": "feedforward neural networks"}, {"st": 47, "ed": 49, "text": "score function"}, {"st": 60, "ed": 62, "text": "weight matrix"}, {"st": 68, "ed": 70, "text": "deep network"}]
[{"st": 4, "ed": 6, "text": "representation learning"}, {"st": 12, "ed": 14, "text": "domain adaptation"}, {"st": 18, "ed": 21, "text": "training and test"}, {"st": 36, "ed": 38, "text": "domain adaptation"}, {"st": 42, "ed": 44, "text": "domain transfer"}, {"st": 70, "ed": 72, "text": "training objective"}, {"st": 81, "ed": 83, "text": "neural network"}, {"st": 84, "ed": 86, "text": "hidden layer"}, {"st": 93, "ed": 95, "text": "classification task"}, {"st": 108, "ed": 110, "text": "sentiment analysis"}, {"st": 126, "ed": 128, "text": "neural network"}, {"st": 139, "ed": 141, "text": "neural network"}, {"st": 148, "ed": 150, "text": "input features"}, {"st": 159, "ed": 161, "text": "denoising autoencoders"}]
[{"st": 36, "ed": 39, "text": "deep neural network"}, {"st": 80, "ed": 82, "text": "fully supervised"}, {"st": 99, "ed": 101, "text": "semi supervised"}, {"st": 113, "ed": 115, "text": "case study"}, {"st": 134, "ed": 136, "text": "significantly improves"}, {"st": 140, "ed": 142, "text": "real world"}, {"st": 142, "ed": 144, "text": "sentiment analysis"}]
[{"st": 2, "ed": 4, "text": "deep networks"}, {"st": 7, "ed": 9, "text": "open problem"}, {"st": 38, "ed": 40, "text": "feed forward"}, {"st": 51, "ed": 53, "text": "back propagation"}, {"st": 57, "ed": 59, "text": "recurrent network"}, {"st": 72, "ed": 74, "text": "random matrix"}, {"st": 95, "ed": 97, "text": "random walk"}, {"st": 121, "ed": 123, "text": "random walk"}, {"st": 127, "ed": 129, "text": "network depth"}, {"st": 150, "ed": 152, "text": "square root"}, {"st": 154, "ed": 156, "text": "network depth"}, {"st": 160, "ed": 163, "text": "vanishing gradient problem"}, {"st": 179, "ed": 182, "text": "stochastic gradient descent"}]
[{"st": 18, "ed": 20, "text": "auto encoder"}, {"st": 30, "ed": 32, "text": "large scale"}, {"st": 32, "ed": 34, "text": "unsupervised learning"}, {"st": 35, "ed": 37, "text": "time series"}, {"st": 40, "ed": 42, "text": "time series"}, {"st": 45, "ed": 47, "text": "latent vector"}, {"st": 78, "ed": 80, "text": "unlabeled data"}, {"st": 84, "ed": 86, "text": "supervised training"}]
[{"st": 9, "ed": 12, "text": "deep neural networks"}, {"st": 47, "ed": 49, "text": "regularization technique"}, {"st": 70, "ed": 72, "text": "related methods"}, {"st": 125, "ed": 127, "text": "weight matrix"}]
[{"st": 51, "ed": 53, "text": "weight parameters"}]
[{"st": 1, "ed": 4, "text": "fully connected layers"}, {"st": 6, "ed": 10, "text": "deep convolutional neural network"}, {"st": 16, "ed": 18, "text": "network parameters"}, {"st": 41, "ed": 43, "text": "predictive performance"}, {"st": 48, "ed": 51, "text": "deep neural networks"}, {"st": 67, "ed": 69, "text": "kernel methods"}, {"st": 81, "ed": 84, "text": "fully connected layers"}, {"st": 96, "ed": 99, "text": "end to end"}, {"st": 103, "ed": 105, "text": "convolutional layers"}, {"st": 117, "ed": 119, "text": "convolutional networks"}, {"st": 126, "ed": 128, "text": "convolutional networks"}]
[{"st": 5, "ed": 7, "text": "denoising autoencoder"}, {"st": 8, "ed": 10, "text": "lateral connections"}, {"st": 16, "ed": 18, "text": "unsupervised learning"}, {"st": 34, "ed": 37, "text": "supervised and unsupervised"}, {"st": 37, "ed": 39, "text": "cost functions"}, {"st": 40, "ed": 42, "text": "back propagation"}, {"st": 46, "ed": 48, "text": "layer wise"}]
[{"st": 17, "ed": 19, "text": "input data"}, {"st": 21, "ed": 23, "text": "training examples"}, {"st": 26, "ed": 28, "text": "unseen data"}, {"st": 105, "ed": 107, "text": "theoretical analysis"}, {"st": 108, "ed": 110, "text": "deep networks"}, {"st": 117, "ed": 119, "text": "compressed sensing"}, {"st": 140, "ed": 142, "text": "metric learning"}, {"st": 163, "ed": 165, "text": "training set"}, {"st": 168, "ed": 170, "text": "training examples"}]
[{"st": 2, "ed": 4, "text": "recent successes"}, {"st": 5, "ed": 7, "text": "deep learning"}, {"st": 8, "ed": 10, "text": "computer vision"}, {"st": 17, "ed": 19, "text": "time series"}, {"st": 45, "ed": 47, "text": "computer vision"}, {"st": 48, "ed": 50, "text": "time series"}, {"st": 56, "ed": 59, "text": "convolutional neural networks"}, {"st": 63, "ed": 65, "text": "standard datasets"}, {"st": 67, "ed": 69, "text": "high level"}, {"st": 82, "ed": 84, "text": "highly competitive"}, {"st": 93, "ed": 95, "text": "time series"}, {"st": 112, "ed": 114, "text": "auto encoders"}]
[{"st": 7, "ed": 9, "text": "neural networks"}, {"st": 10, "ed": 12, "text": "large datasets"}, {"st": 21, "ed": 23, "text": "linear algebra"}, {"st": 33, "ed": 35, "text": "neural network"}, {"st": 75, "ed": 77, "text": "machine learning"}, {"st": 86, "ed": 88, "text": "large datasets"}, {"st": 92, "ed": 94, "text": "pre processing"}]
[{"st": 21, "ed": 24, "text": "non convex optimization"}, {"st": 27, "ed": 30, "text": "deep neural networks"}, {"st": 38, "ed": 41, "text": "global and local"}, {"st": 83, "ed": 86, "text": "deep neural networks"}, {"st": 88, "ed": 90, "text": "visual recognition"}, {"st": 93, "ed": 97, "text": "mnist and cifar 10"}, {"st": 114, "ed": 116, "text": "recent literature"}, {"st": 130, "ed": 132, "text": "multilayer perceptrons"}, {"st": 134, "ed": 136, "text": "auto encoders"}, {"st": 145, "ed": 147, "text": "quasi newton"}, {"st": 152, "ed": 154, "text": "regularization techniques"}, {"st": 172, "ed": 175, "text": "non convex optimization"}]
[{"st": 0, "ed": 3, "text": "restricted boltzmann machines"}, {"st": 5, "ed": 7, "text": "neural networks"}, {"st": 16, "ed": 18, "text": "applications including"}, {"st": 47, "ed": 49, "text": "contrastive divergence"}, {"st": 57, "ed": 59, "text": "iterative procedure"}, {"st": 63, "ed": 65, "text": "mean field"}, {"st": 67, "ed": 69, "text": "statistical physics"}, {"st": 90, "ed": 92, "text": "contrastive divergence"}, {"st": 121, "ed": 123, "text": "higher order"}]
[{"st": 4, "ed": 6, "text": "neural architecture"}, {"st": 9, "ed": 11, "text": "conditional probability"}, {"st": 13, "ed": 15, "text": "output sequence"}, {"st": 77, "ed": 79, "text": "combinatorial optimization"}, {"st": 96, "ed": 98, "text": "recently proposed"}, {"st": 117, "ed": 119, "text": "hidden units"}, {"st": 142, "ed": 144, "text": "input sequence"}, {"st": 183, "ed": 186, "text": "travelling salesman problem"}, {"st": 187, "ed": 189, "text": "training examples"}]
[{"st": 41, "ed": 43, "text": "local learning"}, {"st": 52, "ed": 54, "text": "local learning"}, {"st": 69, "ed": 71, "text": "functional form"}, {"st": 89, "ed": 91, "text": "learning rules"}, {"st": 96, "ed": 98, "text": "learning rules"}, {"st": 104, "ed": 106, "text": "local learning"}, {"st": 127, "ed": 129, "text": "local learning"}, {"st": 132, "ed": 134, "text": "feedforward networks"}, {"st": 141, "ed": 143, "text": "local learning"}, {"st": 151, "ed": 153, "text": "input output"}, {"st": 165, "ed": 167, "text": "input output"}, {"st": 170, "ed": 172, "text": "deep learning"}, {"st": 233, "ed": 235, "text": "computational cost"}, {"st": 253, "ed": 255, "text": "local learning"}, {"st": 278, "ed": 280, "text": "learning rules"}]
[{"st": 1, "ed": 3, "text": "neural networks"}, {"st": 6, "ed": 9, "text": "non convex optimization"}, {"st": 45, "ed": 47, "text": "proposed method"}, {"st": 50, "ed": 52, "text": "sample complexity"}, {"st": 70, "ed": 72, "text": "np hard"}, {"st": 97, "ed": 99, "text": "global optimum"}, {"st": 123, "ed": 126, "text": "stochastic gradient descent"}, {"st": 136, "ed": 138, "text": "computationally efficient"}, {"st": 145, "ed": 147, "text": "neural networks"}]
[{"st": 3, "ed": 5, "text": "neural networks"}, {"st": 17, "ed": 19, "text": "internal representation"}, {"st": 38, "ed": 41, "text": "simple and efficient"}, {"st": 44, "ed": 46, "text": "neural network"}, {"st": 59, "ed": 61, "text": "feed forward"}, {"st": 75, "ed": 77, "text": "natural gradient"}, {"st": 93, "ed": 95, "text": "closely related"}, {"st": 111, "ed": 114, "text": "unsupervised and supervised"}, {"st": 124, "ed": 126, "text": "large scale"}]
[{"st": 2, "ed": 4, "text": "supervised learning"}, {"st": 5, "ed": 7, "text": "unsupervised learning"}, {"st": 22, "ed": 25, "text": "supervised and unsupervised"}, {"st": 25, "ed": 27, "text": "cost functions"}, {"st": 33, "ed": 35, "text": "layer wise"}, {"st": 42, "ed": 44, "text": "ladder network"}, {"st": 70, "ed": 72, "text": "semi supervised"}, {"st": 72, "ed": 76, "text": "mnist and cifar 10"}]
[{"st": 8, "ed": 11, "text": "single hidden layer"}, {"st": 11, "ed": 13, "text": "neural networks"}, {"st": 45, "ed": 47, "text": "tikhonov regularization"}, {"st": 59, "ed": 61, "text": "condition number"}, {"st": 79, "ed": 81, "text": "theoretical analysis"}, {"st": 90, "ed": 92, "text": "regularization parameter"}, {"st": 104, "ed": 106, "text": "cross validation"}, {"st": 126, "ed": 128, "text": "proposed method"}, {"st": 156, "ed": 158, "text": "regularization parameter"}]
[{"st": 0, "ed": 2, "text": "deep learning"}, {"st": 5, "ed": 7, "text": "great promise"}, {"st": 9, "ed": 11, "text": "practical applications"}, {"st": 13, "ed": 15, "text": "speech recognition"}, {"st": 15, "ed": 18, "text": "visual object recognition"}, {"st": 26, "ed": 28, "text": "deep learning"}, {"st": 34, "ed": 36, "text": "large scale"}, {"st": 44, "ed": 46, "text": "small scale"}, {"st": 58, "ed": 60, "text": "machine learning"}, {"st": 61, "ed": 64, "text": "deep belief networks"}, {"st": 69, "ed": 71, "text": "classification performance"}, {"st": 77, "ed": 79, "text": "machine learning"}, {"st": 134, "ed": 136, "text": "hidden layer"}, {"st": 183, "ed": 185, "text": "handwritten digit"}, {"st": 189, "ed": 191, "text": "approach outperforms"}, {"st": 196, "ed": 198, "text": "test error"}]
[{"st": 1, "ed": 4, "text": "recurrent neural networks"}, {"st": 93, "ed": 95, "text": "em algorithm"}, {"st": 97, "ed": 99, "text": "fisher information"}, {"st": 119, "ed": 121, "text": "improved performance"}]
[{"st": 37, "ed": 39, "text": "empirical evidence"}, {"st": 62, "ed": 64, "text": "convolutional networks"}, {"st": 67, "ed": 69, "text": "deep learning"}, {"st": 78, "ed": 81, "text": "deep network architecture"}, {"st": 117, "ed": 119, "text": "deep network"}, {"st": 127, "ed": 129, "text": "measure theory"}, {"st": 147, "ed": 149, "text": "deep network"}, {"st": 182, "ed": 184, "text": "deep learning"}, {"st": 208, "ed": 210, "text": "deep learning"}]
[{"st": 52, "ed": 54, "text": "real world"}]
[{"st": 15, "ed": 17, "text": "d dimensional"}, {"st": 20, "ed": 22, "text": "mathbb r"}, {"st": 30, "ed": 32, "text": "neural network"}, {"st": 83, "ed": 86, "text": "rectified linear units"}]
[{"st": 0, "ed": 2, "text": "deep learning"}, {"st": 64, "ed": 66, "text": "deep learning"}, {"st": 143, "ed": 145, "text": "convergence guarantees"}, {"st": 172, "ed": 174, "text": "deep learning"}, {"st": 181, "ed": 183, "text": "optimization procedure"}, {"st": 193, "ed": 196, "text": "probabilistic graphical models"}]
[{"st": 4, "ed": 7, "text": "deep neural networks"}, {"st": 15, "ed": 17, "text": "central role"}, {"st": 91, "ed": 93, "text": "improves performance"}, {"st": 103, "ed": 105, "text": "experimentally demonstrate"}, {"st": 114, "ed": 116, "text": "invariance properties"}, {"st": 129, "ed": 131, "text": "widely adopted"}, {"st": 131, "ed": 133, "text": "benchmark datasets"}, {"st": 136, "ed": 139, "text": "easy to implement"}, {"st": 145, "ed": 148, "text": "deep neural network"}, {"st": 158, "ed": 160, "text": "computational overhead"}]
[{"st": 0, "ed": 4, "text": "recurrent neural networks rnns"}, {"st": 8, "ed": 10, "text": "sequential data"}, {"st": 22, "ed": 24, "text": "computationally expensive"}, {"st": 39, "ed": 41, "text": "neural networks"}, {"st": 42, "ed": 44, "text": "significantly improve"}, {"st": 44, "ed": 46, "text": "convergence rates"}, {"st": 47, "ed": 50, "text": "feedforward neural networks"}, {"st": 53, "ed": 55, "text": "batch normalization"}, {"st": 66, "ed": 68, "text": "significantly reduce"}, {"st": 77, "ed": 79, "text": "batch normalization"}, {"st": 88, "ed": 90, "text": "doesn t"}, {"st": 106, "ed": 108, "text": "batch normalization"}, {"st": 112, "ed": 114, "text": "faster convergence"}, {"st": 119, "ed": 121, "text": "doesn t"}, {"st": 125, "ed": 127, "text": "generalization performance"}, {"st": 133, "ed": 135, "text": "speech recognition"}, {"st": 140, "ed": 142, "text": "batch normalization"}, {"st": 154, "ed": 156, "text": "feedforward networks"}]
[{"st": 0, "ed": 4, "text": "recurrent neural networks rnns"}, {"st": 23, "ed": 25, "text": "optimization techniques"}, {"st": 51, "ed": 53, "text": "optimization method"}, {"st": 55, "ed": 58, "text": "shown promising results"}, {"st": 75, "ed": 77, "text": "rnn architectures"}, {"st": 81, "ed": 84, "text": "short term memory"}, {"st": 122, "ed": 124, "text": "larger scale"}, {"st": 127, "ed": 129, "text": "character level"}]
[{"st": 2, "ed": 5, "text": "multi class classification"}, {"st": 15, "ed": 17, "text": "neural network"}, {"st": 19, "ed": 21, "text": "categorical distribution"}, {"st": 49, "ed": 51, "text": "maximum likelihood"}, {"st": 86, "ed": 88, "text": "softmax loss"}, {"st": 101, "ed": 103, "text": "recently introduced"}, {"st": 106, "ed": 108, "text": "loss functions"}, {"st": 138, "ed": 140, "text": "loss functions"}, {"st": 163, "ed": 165, "text": "softmax loss"}, {"st": 169, "ed": 171, "text": "log likelihood"}, {"st": 203, "ed": 205, "text": "softmax loss"}, {"st": 218, "ed": 222, "text": "mnist and cifar 10"}]
[{"st": 11, "ed": 13, "text": "neural nets"}, {"st": 15, "ed": 17, "text": "robust optimization"}, {"st": 24, "ed": 26, "text": "alternating minimization"}, {"st": 50, "ed": 52, "text": "adversarial training"}, {"st": 65, "ed": 67, "text": "proposed framework"}, {"st": 68, "ed": 70, "text": "previous approaches"}, {"st": 90, "ed": 92, "text": "adversarial examples"}]
[{"st": 0, "ed": 4, "text": "recurrent neural networks rnns"}, {"st": 5, "ed": 7, "text": "notoriously difficult"}, {"st": 17, "ed": 19, "text": "weight matrix"}, {"st": 34, "ed": 37, "text": "vanishing and exploding"}, {"st": 59, "ed": 61, "text": "weight matrix"}, {"st": 99, "ed": 101, "text": "weight matrix"}, {"st": 109, "ed": 111, "text": "building blocks"}, {"st": 125, "ed": 127, "text": "hidden states"}, {"st": 148, "ed": 150, "text": "tasks involving"}]
[{"st": 0, "ed": 2, "text": "deep learning"}, {"st": 2, "ed": 5, "text": "takes advantage of"}, {"st": 5, "ed": 7, "text": "large datasets"}, {"st": 8, "ed": 10, "text": "computationally efficient"}, {"st": 18, "ed": 20, "text": "machine learning"}, {"st": 25, "ed": 27, "text": "training phase"}, {"st": 28, "ed": 31, "text": "deep neural networks"}, {"st": 33, "ed": 36, "text": "vulnerable to adversarial"}, {"st": 46, "ed": 49, "text": "deep neural networks"}, {"st": 61, "ed": 64, "text": "deep neural networks"}, {"st": 74, "ed": 76, "text": "adversarial samples"}, {"st": 94, "ed": 96, "text": "computer vision"}, {"st": 105, "ed": 107, "text": "correctly classified"}, {"st": 122, "ed": 124, "text": "success rate"}, {"st": 132, "ed": 134, "text": "input features"}, {"st": 146, "ed": 148, "text": "adversarial perturbations"}, {"st": 161, "ed": 163, "text": "adversarial samples"}]
[{"st": 19, "ed": 22, "text": "feedforward neural networks"}, {"st": 53, "ed": 55, "text": "activation functions"}, {"st": 56, "ed": 59, "text": "rectified linear units"}]
[{"st": 4, "ed": 6, "text": "deep learning"}, {"st": 12, "ed": 14, "text": "computational complexity"}, {"st": 18, "ed": 20, "text": "network pruning"}, {"st": 54, "ed": 56, "text": "structured sparsity"}, {"st": 60, "ed": 63, "text": "convolutional neural networks"}, {"st": 75, "ed": 77, "text": "structured sparsity"}, {"st": 88, "ed": 90, "text": "parallel computing"}, {"st": 105, "ed": 107, "text": "proposed method"}, {"st": 162, "ed": 164, "text": "significantly reduce"}, {"st": 169, "ed": 171, "text": "feature map"}, {"st": 177, "ed": 179, "text": "fixed point"}, {"st": 188, "ed": 190, "text": "significant reduction"}]
[{"st": 24, "ed": 26, "text": "neural networks"}, {"st": 54, "ed": 56, "text": "recent years"}, {"st": 59, "ed": 61, "text": "successfully applied"}, {"st": 62, "ed": 64, "text": "neural networks"}, {"st": 102, "ed": 104, "text": "empirical results"}]
[{"st": 14, "ed": 16, "text": "activation function"}, {"st": 34, "ed": 37, "text": "restricted boltzmann machines"}, {"st": 50, "ed": 52, "text": "exponential family"}, {"st": 60, "ed": 62, "text": "exponential family"}, {"st": 73, "ed": 75, "text": "non linearity"}, {"st": 79, "ed": 81, "text": "contrastive divergence"}]
[{"st": 30, "ed": 32, "text": "cost function"}, {"st": 34, "ed": 36, "text": "optimal solution"}, {"st": 96, "ed": 98, "text": "neural network"}, {"st": 116, "ed": 118, "text": "loss minimization"}, {"st": 126, "ed": 128, "text": "a 20"}, {"st": 131, "ed": 133, "text": "classification error"}]
[{"st": 24, "ed": 27, "text": "stochastic gradient descent"}, {"st": 73, "ed": 75, "text": "common practice"}]
[{"st": 2, "ed": 4, "text": "positive integer"}, {"st": 7, "ed": 9, "text": "neural networks"}, {"st": 32, "ed": 34, "text": "mathcal o"}, {"st": 39, "ed": 41, "text": "exponentially large"}, {"st": 90, "ed": 92, "text": "convolutional networks"}, {"st": 97, "ed": 100, "text": "sum product networks"}, {"st": 101, "ed": 104, "text": "boosted decision trees"}]
[{"st": 8, "ed": 11, "text": "component analysis pca"}, {"st": 19, "ed": 21, "text": "mathbb r"}, {"st": 86, "ed": 88, "text": "frac 1"}, {"st": 189, "ed": 191, "text": "update rule"}, {"st": 204, "ed": 206, "text": "accurately estimate"}, {"st": 221, "ed": 224, "text": "sheds light on"}]
[{"st": 8, "ed": 10, "text": "synthetic datasets"}, {"st": 18, "ed": 20, "text": "neural networks"}, {"st": 28, "ed": 30, "text": "mnist svhn"}, {"st": 58, "ed": 60, "text": "computational overhead"}, {"st": 79, "ed": 81, "text": "final performance"}, {"st": 85, "ed": 87, "text": "training epochs"}, {"st": 105, "ed": 107, "text": "neural networks"}]
[{"st": 1, "ed": 3, "text": "deep learning"}, {"st": 18, "ed": 20, "text": "matrix multiplication"}, {"st": 34, "ed": 36, "text": "deep learning"}, {"st": 37, "ed": 39, "text": "low power"}, {"st": 47, "ed": 50, "text": "training and testing"}, {"st": 51, "ed": 53, "text": "deep networks"}, {"st": 96, "ed": 98, "text": "inner product"}, {"st": 112, "ed": 114, "text": "deep learning"}, {"st": 117, "ed": 119, "text": "computational cost"}, {"st": 122, "ed": 124, "text": "back propagation"}, {"st": 127, "ed": 129, "text": "significantly fewer"}, {"st": 164, "ed": 166, "text": "back propagation"}, {"st": 209, "ed": 211, "text": "proposed algorithm"}]
[{"st": 2, "ed": 4, "text": "activation functions"}, {"st": 6, "ed": 8, "text": "neural networks"}, {"st": 19, "ed": 21, "text": "activation function"}, {"st": 44, "ed": 46, "text": "activation functions"}, {"st": 83, "ed": 85, "text": "activation function"}, {"st": 94, "ed": 96, "text": "noise free"}, {"st": 99, "ed": 102, "text": "stochastic gradient descent"}, {"st": 114, "ed": 116, "text": "activation function"}, {"st": 119, "ed": 121, "text": "optimization procedure"}, {"st": 143, "ed": 145, "text": "simulated annealing"}, {"st": 168, "ed": 170, "text": "activation functions"}, {"st": 184, "ed": 186, "text": "competitive results"}]
[{"st": 6, "ed": 8, "text": "input output"}, {"st": 8, "ed": 11, "text": "recurrent neural networks"}, {"st": 43, "ed": 45, "text": "linear transformation"}, {"st": 60, "ed": 62, "text": "computational complexity"}]
[{"st": 130, "ed": 132, "text": "case studies"}]
[{"st": 18, "ed": 20, "text": "related methods"}, {"st": 26, "ed": 28, "text": "convex optimization"}, {"st": 32, "ed": 35, "text": "stochastic gradient descent"}, {"st": 38, "ed": 40, "text": "theoretical analysis"}, {"st": 50, "ed": 53, "text": "rates of convergence"}, {"st": 54, "ed": 56, "text": "stationary points"}, {"st": 59, "ed": 61, "text": "nonconvex optimization"}, {"st": 85, "ed": 87, "text": "linear convergence"}]
[{"st": 0, "ed": 3, "text": "independent component analysis"}, {"st": 9, "ed": 12, "text": "blind source separation"}, {"st": 17, "ed": 19, "text": "de facto"}, {"st": 39, "ed": 41, "text": "linear transformations"}, {"st": 74, "ed": 76, "text": "dimensionality reduction"}, {"st": 80, "ed": 82, "text": "raw data"}, {"st": 94, "ed": 97, "text": "inference and learning"}, {"st": 110, "ed": 113, "text": "variational auto encoders"}, {"st": 119, "ed": 121, "text": "feature extraction"}, {"st": 122, "ed": 126, "text": "magnetic resonance imaging mri"}]
[{"st": 6, "ed": 8, "text": "deep learning"}, {"st": 12, "ed": 15, "text": "convolutional neural network"}, {"st": 39, "ed": 41, "text": "convolutional layers"}]
[{"st": 1, "ed": 3, "text": "recent years"}, {"st": 3, "ed": 6, "text": "deep neural networks"}, {"st": 13, "ed": 15, "text": "large scale"}, {"st": 17, "ed": 19, "text": "classification tasks"}, {"st": 21, "ed": 23, "text": "speech recognition"}, {"st": 23, "ed": 25, "text": "visual object"}, {"st": 38, "ed": 40, "text": "time consuming"}, {"st": 41, "ed": 43, "text": "computationally intensive"}, {"st": 48, "ed": 50, "text": "computational resources"}, {"st": 69, "ed": 71, "text": "dnn training"}, {"st": 72, "ed": 75, "text": "orders of magnitude"}, {"st": 127, "ed": 129, "text": "dnn training"}, {"st": 212, "ed": 214, "text": "big data"}, {"st": 229, "ed": 231, "text": "speech recognition"}, {"st": 253, "ed": 255, "text": "sensory data"}]
[{"st": 16, "ed": 18, "text": "challenging problem"}, {"st": 35, "ed": 37, "text": "biologically inspired"}, {"st": 64, "ed": 66, "text": "denoising autoencoders"}, {"st": 133, "ed": 135, "text": "digital radio"}, {"st": 147, "ed": 151, "text": "signal to noise ratio"}, {"st": 165, "ed": 167, "text": "experiments demonstrate"}, {"st": 178, "ed": 180, "text": "related tasks"}]
[{"st": 0, "ed": 2, "text": "neural networks"}, {"st": 20, "ed": 22, "text": "activation function"}, {"st": 30, "ed": 32, "text": "crucial step"}, {"st": 35, "ed": 37, "text": "open problem"}, {"st": 104, "ed": 106, "text": "activation functions"}]
[{"st": 0, "ed": 2, "text": "auto encoders"}, {"st": 11, "ed": 13, "text": "observed data"}, {"st": 34, "ed": 36, "text": "compressed sensing"}, {"st": 37, "ed": 39, "text": "sparse coding"}, {"st": 42, "ed": 44, "text": "generating process"}, {"st": 46, "ed": 48, "text": "observed data"}, {"st": 67, "ed": 69, "text": "auto encoders"}, {"st": 72, "ed": 74, "text": "signal recovery"}, {"st": 105, "ed": 107, "text": "weight matrices"}, {"st": 112, "ed": 114, "text": "ell 2"}, {"st": 149, "ed": 151, "text": "empirically demonstrate"}, {"st": 152, "ed": 154, "text": "auto encoders"}]
[{"st": 0, "ed": 3, "text": "multivariate time series"}, {"st": 5, "ed": 7, "text": "practical applications"}, {"st": 23, "ed": 25, "text": "time series"}, {"st": 28, "ed": 30, "text": "related tasks"}, {"st": 35, "ed": 37, "text": "missing values"}, {"st": 74, "ed": 76, "text": "deep learning"}, {"st": 91, "ed": 95, "text": "gated recurrent unit gru"}, {"st": 121, "ed": 123, "text": "deep model"}, {"st": 133, "ed": 135, "text": "temporal dependencies"}, {"st": 136, "ed": 138, "text": "time series"}, {"st": 151, "ed": 153, "text": "time series"}, {"st": 153, "ed": 155, "text": "classification tasks"}, {"st": 156, "ed": 158, "text": "real world"}, {"st": 164, "ed": 166, "text": "synthetic datasets"}, {"st": 186, "ed": 188, "text": "missing values"}, {"st": 189, "ed": 191, "text": "time series"}]
[{"st": 8, "ed": 10, "text": "missing data"}, {"st": 29, "ed": 32, "text": "intensive care unit"}, {"st": 37, "ed": 39, "text": "los angeles"}, {"st": 43, "ed": 46, "text": "multivariate time series"}, {"st": 72, "ed": 74, "text": "predictive performance"}, {"st": 81, "ed": 83, "text": "linear models"}, {"st": 83, "ed": 86, "text": "recurrent neural networks"}, {"st": 98, "ed": 100, "text": "linear models"}]
[{"st": 48, "ed": 50, "text": "smart meter"}, {"st": 68, "ed": 72, "text": "deep neural network dnn"}, {"st": 74, "ed": 76, "text": "traditional methods"}, {"st": 84, "ed": 86, "text": "methods outperform"}, {"st": 96, "ed": 98, "text": "computational complexity"}]
[{"st": 6, "ed": 8, "text": "recent advances"}, {"st": 9, "ed": 12, "text": "deep neural networks"}, {"st": 45, "ed": 47, "text": "recurrent models"}, {"st": 67, "ed": 69, "text": "deep networks"}, {"st": 88, "ed": 90, "text": "machine translation"}, {"st": 167, "ed": 169, "text": "learning speed"}]
[{"st": 6, "ed": 8, "text": "significantly improves"}, {"st": 15, "ed": 17, "text": "learning agents"}, {"st": 24, "ed": 26, "text": "thompson sampling"}, {"st": 27, "ed": 29, "text": "monte carlo"}, {"st": 44, "ed": 46, "text": "exploration strategies"}, {"st": 48, "ed": 50, "text": "epsilon greedy"}]
[{"st": 6, "ed": 8, "text": "deep learning"}, {"st": 25, "ed": 27, "text": "neural networks"}, {"st": 48, "ed": 50, "text": "fewer parameters"}, {"st": 97, "ed": 99, "text": "machine learning"}, {"st": 100, "ed": 103, "text": "deep neural network"}, {"st": 136, "ed": 138, "text": "deep networks"}]
[{"st": 1, "ed": 3, "text": "neural networks"}, {"st": 12, "ed": 14, "text": "gradient based"}, {"st": 17, "ed": 19, "text": "theoretical results"}, {"st": 30, "ed": 32, "text": "worst case"}, {"st": 83, "ed": 85, "text": "gradient based"}, {"st": 87, "ed": 89, "text": "provide evidence"}]
[{"st": 9, "ed": 11, "text": "latent spaces"}, {"st": 15, "ed": 17, "text": "linear interpolation"}, {"st": 19, "ed": 21, "text": "linear interpolation"}, {"st": 27, "ed": 29, "text": "prior distribution"}, {"st": 82, "ed": 84, "text": "quantitative analysis"}, {"st": 104, "ed": 106, "text": "variational autoencoders"}]
[{"st": 2, "ed": 4, "text": "content based"}, {"st": 4, "ed": 6, "text": "attention mechanism"}, {"st": 52, "ed": 54, "text": "fixed size"}, {"st": 73, "ed": 75, "text": "attention mechanism"}, {"st": 77, "ed": 79, "text": "small scale"}, {"st": 97, "ed": 99, "text": "attention mechanisms"}, {"st": 113, "ed": 115, "text": "non linearity"}, {"st": 126, "ed": 128, "text": "fixed size"}, {"st": 138, "ed": 140, "text": "attention mechanisms"}, {"st": 143, "ed": 145, "text": "large scale"}, {"st": 160, "ed": 162, "text": "question answering"}]
[{"st": 17, "ed": 19, "text": "recently introduced"}, {"st": 28, "ed": 30, "text": "generative model"}, {"st": 32, "ed": 34, "text": "multi dimensional"}, {"st": 48, "ed": 50, "text": "exact inference"}, {"st": 69, "ed": 71, "text": "fixed length"}, {"st": 156, "ed": 158, "text": "sampling based"}, {"st": 158, "ed": 160, "text": "learning rule"}, {"st": 166, "ed": 168, "text": "previously proposed"}, {"st": 168, "ed": 170, "text": "regularization techniques"}, {"st": 216, "ed": 218, "text": "mnist dataset"}]
[{"st": 1, "ed": 3, "text": "neural networks"}, {"st": 14, "ed": 16, "text": "existing methods"}, {"st": 18, "ed": 21, "text": "generative adversarial networks"}, {"st": 36, "ed": 38, "text": "generative models"}, {"st": 53, "ed": 55, "text": "generative model"}, {"st": 63, "ed": 65, "text": "convolutional networks"}, {"st": 80, "ed": 82, "text": "expressive power"}, {"st": 83, "ed": 85, "text": "convolutional networks"}, {"st": 105, "ed": 107, "text": "missing data"}, {"st": 140, "ed": 142, "text": "promising results"}]
[{"st": 4, "ed": 7, "text": "artificial neural networks"}, {"st": 59, "ed": 62, "text": "artificial neural networks"}, {"st": 132, "ed": 134, "text": "neural activity"}]
[{"st": 4, "ed": 6, "text": "neural network"}, {"st": 26, "ed": 28, "text": "rational choice"}, {"st": 30, "ed": 32, "text": "learning rate"}, {"st": 43, "ed": 45, "text": "clustering process"}, {"st": 61, "ed": 63, "text": "similarity measure"}, {"st": 83, "ed": 85, "text": "learning process"}, {"st": 104, "ed": 106, "text": "neural network"}]
[{"st": 0, "ed": 3, "text": "recurrent neural networks"}, {"st": 8, "ed": 10, "text": "sequential data"}, {"st": 16, "ed": 19, "text": "vanishing and exploding"}, {"st": 22, "ed": 25, "text": "recurrent neural networks"}, {"st": 61, "ed": 63, "text": "open question"}, {"st": 107, "ed": 109, "text": "significantly improved"}, {"st": 148, "ed": 150, "text": "recently proposed"}, {"st": 156, "ed": 158, "text": "hidden state"}, {"st": 178, "ed": 180, "text": "differentiable manifold"}, {"st": 198, "ed": 200, "text": "learning rate"}]
[{"st": 1, "ed": 3, "text": "residual network"}, {"st": 14, "ed": 16, "text": "significantly reduce"}, {"st": 56, "ed": 58, "text": "training error"}, {"st": 96, "ed": 98, "text": "condition number"}, {"st": 103, "ed": 105, "text": "loss function"}, {"st": 117, "ed": 119, "text": "deep models"}, {"st": 136, "ed": 138, "text": "stationary point"}, {"st": 142, "ed": 144, "text": "optimization algorithm"}, {"st": 161, "ed": 163, "text": "condition number"}, {"st": 191, "ed": 193, "text": "loss function"}, {"st": 198, "ed": 200, "text": "extensive experiments"}, {"st": 241, "ed": 243, "text": "learning dynamics"}]
[{"st": 19, "ed": 22, "text": "deep reinforcement learning"}, {"st": 80, "ed": 82, "text": "theoretical analysis"}, {"st": 115, "ed": 117, "text": "experiments demonstrate"}]
[{"st": 13, "ed": 15, "text": "neural networks"}, {"st": 32, "ed": 34, "text": "training process"}, {"st": 42, "ed": 45, "text": "unsupervised pre training"}, {"st": 71, "ed": 73, "text": "adversarial noise"}, {"st": 85, "ed": 87, "text": "neural network"}, {"st": 89, "ed": 91, "text": "ladder network"}, {"st": 93, "ed": 95, "text": "key idea"}, {"st": 103, "ed": 105, "text": "latent variable"}, {"st": 115, "ed": 117, "text": "neural network"}, {"st": 118, "ed": 121, "text": "supervised and unsupervised"}, {"st": 147, "ed": 149, "text": "adversarial noise"}, {"st": 151, "ed": 153, "text": "ladder network"}, {"st": 166, "ed": 168, "text": "adversarial noise"}, {"st": 186, "ed": 188, "text": "adversarial noise"}]
[{"st": 1, "ed": 3, "text": "convolutional networks"}, {"st": 6, "ed": 8, "text": "max pooling"}, {"st": 22, "ed": 24, "text": "convex optimization"}, {"st": 34, "ed": 36, "text": "building blocks"}, {"st": 37, "ed": 39, "text": "deep learning"}, {"st": 45, "ed": 47, "text": "convergence guarantee"}, {"st": 77, "ed": 79, "text": "neural networks"}, {"st": 93, "ed": 95, "text": "provide evidence"}]
[{"st": 6, "ed": 8, "text": "variational autoencoder"}, {"st": 12, "ed": 14, "text": "gaussian mixture"}, {"st": 16, "ed": 18, "text": "prior distribution"}, {"st": 23, "ed": 25, "text": "unsupervised clustering"}, {"st": 83, "ed": 85, "text": "unsupervised clustering"}, {"st": 119, "ed": 121, "text": "synthetic data"}, {"st": 136, "ed": 138, "text": "competitive performance"}, {"st": 139, "ed": 141, "text": "unsupervised clustering"}]
[{"st": 5, "ed": 7, "text": "deep learning"}, {"st": 13, "ed": 16, "text": "deep artificial neural"}, {"st": 37, "ed": 39, "text": "batch normalization"}, {"st": 49, "ed": 51, "text": "residual networks"}, {"st": 82, "ed": 84, "text": "deep linear"}, {"st": 84, "ed": 86, "text": "residual networks"}, {"st": 96, "ed": 98, "text": "feed forward"}, {"st": 111, "ed": 113, "text": "residual networks"}, {"st": 118, "ed": 120, "text": "finite sample"}, {"st": 162, "ed": 164, "text": "convolutional layers"}, {"st": 169, "ed": 171, "text": "batch normalization"}, {"st": 182, "ed": 184, "text": "convolutional networks"}, {"st": 189, "ed": 191, "text": "imagenet classification"}]
[{"st": 8, "ed": 10, "text": "deep learning"}, {"st": 18, "ed": 20, "text": "parameter sharing"}, {"st": 64, "ed": 66, "text": "semi supervised"}, {"st": 95, "ed": 98, "text": "semi supervised learning"}]
[{"st": 9, "ed": 13, "text": "feed forward neural networks"}, {"st": 16, "ed": 18, "text": "performance gains"}, {"st": 24, "ed": 26, "text": "dropout regularization"}, {"st": 30, "ed": 32, "text": "neural network"}, {"st": 38, "ed": 40, "text": "proposed method"}, {"st": 43, "ed": 45, "text": "sparsity inducing"}, {"st": 60, "ed": 62, "text": "hidden units"}, {"st": 87, "ed": 89, "text": "competing methods"}, {"st": 91, "ed": 93, "text": "real world"}, {"st": 93, "ed": 95, "text": "speech recognition"}, {"st": 102, "ed": 104, "text": "comparable accuracy"}, {"st": 110, "ed": 112, "text": "hidden units"}]
[{"st": 7, "ed": 9, "text": "feed forward"}, {"st": 9, "ed": 11, "text": "learning algorithm"}, {"st": 27, "ed": 29, "text": "weight vector"}, {"st": 76, "ed": 78, "text": "learning algorithm"}]
[{"st": 21, "ed": 24, "text": "a long standing"}, {"st": 27, "ed": 29, "text": "deep networks"}, {"st": 30, "ed": 32, "text": "highly nonlinear"}, {"st": 48, "ed": 50, "text": "local minima"}, {"st": 57, "ed": 59, "text": "deep models"}, {"st": 72, "ed": 74, "text": "local minima"}, {"st": 98, "ed": 100, "text": "strong assumptions"}, {"st": 121, "ed": 123, "text": "additional assumptions"}, {"st": 170, "ed": 172, "text": "local minima"}]
[{"st": 5, "ed": 7, "text": "strong baseline"}, {"st": 8, "ed": 10, "text": "time series"}, {"st": 19, "ed": 21, "text": "baseline models"}, {"st": 23, "ed": 26, "text": "end to end"}, {"st": 32, "ed": 34, "text": "raw data"}, {"st": 39, "ed": 42, "text": "fully convolutional network"}, {"st": 59, "ed": 62, "text": "deep neural networks"}, {"st": 94, "ed": 96, "text": "raw data"}, {"st": 108, "ed": 110, "text": "real world"}, {"st": 114, "ed": 116, "text": "starting point"}, {"st": 128, "ed": 130, "text": "generalization capability"}, {"st": 133, "ed": 135, "text": "learned features"}, {"st": 135, "ed": 137, "text": "network structures"}]
[{"st": 44, "ed": 46, "text": "neural network"}, {"st": 53, "ed": 55, "text": "cross entropy"}, {"st": 86, "ed": 88, "text": "training process"}, {"st": 128, "ed": 130, "text": "prediction error"}, {"st": 142, "ed": 144, "text": "cross entropy"}, {"st": 156, "ed": 158, "text": "neural network"}]
[{"st": 3, "ed": 5, "text": "deep learning"}, {"st": 8, "ed": 13, "text": "deep convolutional neural networks cnn"}, {"st": 23, "ed": 25, "text": "labeled datasets"}, {"st": 38, "ed": 40, "text": "labeled data"}, {"st": 60, "ed": 62, "text": "efficient learning"}, {"st": 64, "ed": 66, "text": "layer wise"}, {"st": 66, "ed": 68, "text": "unsupervised training"}, {"st": 69, "ed": 71, "text": "deep cnns"}, {"st": 90, "ed": 92, "text": "learning objective"}, {"st": 96, "ed": 98, "text": "large scale"}, {"st": 99, "ed": 101, "text": "minimization problem"}, {"st": 106, "ed": 108, "text": "coordinate descent"}, {"st": 116, "ed": 118, "text": "learning strategy"}, {"st": 134, "ed": 136, "text": "numerical experiments"}, {"st": 140, "ed": 142, "text": "learning strategy"}, {"st": 145, "ed": 147, "text": "worst case"}]
[{"st": 1, "ed": 3, "text": "empirical results"}, {"st": 11, "ed": 13, "text": "neural networks"}, {"st": 16, "ed": 18, "text": "external memory"}, {"st": 33, "ed": 36, "text": "recurrent neural networks"}, {"st": 43, "ed": 45, "text": "neural networks"}, {"st": 50, "ed": 52, "text": "vanishing gradients"}, {"st": 68, "ed": 70, "text": "neural network"}, {"st": 95, "ed": 97, "text": "hidden states"}, {"st": 99, "ed": 101, "text": "external memory"}, {"st": 183, "ed": 186, "text": "vanishing gradient problem"}, {"st": 226, "ed": 228, "text": "detailed analysis"}, {"st": 248, "ed": 250, "text": "competitive results"}]
[{"st": 0, "ed": 2, "text": "machine learning"}, {"st": 7, "ed": 9, "text": "recent years"}, {"st": 40, "ed": 42, "text": "machine learning"}, {"st": 45, "ed": 47, "text": "domain expertise"}, {"st": 59, "ed": 61, "text": "data science"}, {"st": 69, "ed": 71, "text": "machine learning"}, {"st": 85, "ed": 87, "text": "open source"}, {"st": 94, "ed": 96, "text": "genetic programming"}, {"st": 99, "ed": 101, "text": "machine learning"}, {"st": 111, "ed": 113, "text": "dimensionality reduction"}, {"st": 116, "ed": 118, "text": "feature construction"}, {"st": 121, "ed": 123, "text": "higher order"}, {"st": 131, "ed": 133, "text": "expert knowledge"}, {"st": 151, "ed": 156, "text": "simulated and real world data"}, {"st": 158, "ed": 160, "text": "human genetics"}, {"st": 165, "ed": 167, "text": "significantly outperforms"}, {"st": 167, "ed": 170, "text": "modern machine learning"}, {"st": 173, "ed": 175, "text": "logistic regression"}, {"st": 193, "ed": 195, "text": "real world"}]
[{"st": 0, "ed": 2, "text": "neural networks"}, {"st": 17, "ed": 19, "text": "domains including"}, {"st": 19, "ed": 21, "text": "natural language"}, {"st": 31, "ed": 33, "text": "computation graph"}, {"st": 60, "ed": 62, "text": "deep learning"}, {"st": 74, "ed": 76, "text": "technique called"}]
[{"st": 10, "ed": 12, "text": "neural network"}, {"st": 24, "ed": 26, "text": "input space"}, {"st": 31, "ed": 33, "text": "kernel space"}, {"st": 41, "ed": 43, "text": "kernel space"}, {"st": 47, "ed": 49, "text": "proposed method"}, {"st": 72, "ed": 74, "text": "input samples"}, {"st": 79, "ed": 81, "text": "kernel matrix"}]
[{"st": 0, "ed": 4, "text": "restricted boltzmann machines rbms"}, {"st": 7, "ed": 9, "text": "neural networks"}, {"st": 15, "ed": 17, "text": "building blocks"}, {"st": 18, "ed": 20, "text": "deep architectures"}, {"st": 45, "ed": 47, "text": "mean field"}, {"st": 57, "ed": 59, "text": "spin glass"}, {"st": 66, "ed": 68, "text": "extensively studied"}, {"st": 79, "ed": 81, "text": "latent variable"}, {"st": 88, "ed": 90, "text": "real valued"}, {"st": 97, "ed": 99, "text": "numerical experiments"}, {"st": 117, "ed": 119, "text": "unsupervised learning"}]
[{"st": 0, "ed": 2, "text": "neural networks"}, {"st": 7, "ed": 9, "text": "predictive models"}, {"st": 77, "ed": 79, "text": "least squares"}, {"st": 94, "ed": 96, "text": "randomly generated"}, {"st": 105, "ed": 108, "text": "kernel density estimation"}, {"st": 119, "ed": 121, "text": "training samples"}, {"st": 128, "ed": 130, "text": "noisy data"}, {"st": 141, "ed": 143, "text": "alternating optimization"}, {"st": 158, "ed": 161, "text": "kernel density estimation"}, {"st": 172, "ed": 174, "text": "benchmark datasets"}, {"st": 176, "ed": 178, "text": "case study"}, {"st": 193, "ed": 195, "text": "learning algorithm"}, {"st": 196, "ed": 198, "text": "neural networks"}, {"st": 219, "ed": 221, "text": "real world"}]
[{"st": 7, "ed": 9, "text": "temporal data"}, {"st": 71, "ed": 73, "text": "external memory"}, {"st": 79, "ed": 81, "text": "variational inference"}, {"st": 92, "ed": 94, "text": "gain insight"}, {"st": 109, "ed": 111, "text": "temporal dependencies"}, {"st": 135, "ed": 137, "text": "existing models"}, {"st": 141, "ed": 144, "text": "recurrent neural networks"}]
[{"st": 10, "ed": 12, "text": "deep learning"}, {"st": 19, "ed": 21, "text": "neural networks"}, {"st": 40, "ed": 42, "text": "deep learning"}, {"st": 43, "ed": 46, "text": "convolutional neural networks"}, {"st": 46, "ed": 49, "text": "deep belief networks"}, {"st": 118, "ed": 120, "text": "prior knowledge"}, {"st": 200, "ed": 202, "text": "future research"}]
[{"st": 0, "ed": 3, "text": "a long standing"}, {"st": 7, "ed": 9, "text": "deep learning"}, {"st": 13, "ed": 16, "text": "vanishing and exploding"}, {"st": 36, "ed": 38, "text": "skip connections"}, {"st": 74, "ed": 76, "text": "feedforward networks"}, {"st": 94, "ed": 96, "text": "skip connections"}, {"st": 105, "ed": 107, "text": "empirical evidence"}, {"st": 116, "ed": 118, "text": "fully connected"}, {"st": 133, "ed": 135, "text": "preliminary experiments"}, {"st": 143, "ed": 145, "text": "deep networks"}]
[{"st": 5, "ed": 9, "text": "generative adversarial network gan"}, {"st": 13, "ed": 15, "text": "generalization properties"}, {"st": 42, "ed": 44, "text": "neural net"}, {"st": 74, "ed": 76, "text": "training set"}, {"st": 94, "ed": 96, "text": "gan training"}]
[{"st": 1, "ed": 3, "text": "recently proposed"}, {"st": 14, "ed": 17, "text": "semi supervised learning"}, {"st": 22, "ed": 24, "text": "moving average"}, {"st": 96, "ed": 98, "text": "network architecture"}, {"st": 102, "ed": 104, "text": "error rate"}, {"st": 124, "ed": 126, "text": "network architecture"}, {"st": 134, "ed": 136, "text": "residual networks"}, {"st": 144, "ed": 146, "text": "cifar 10"}]
[{"st": 48, "ed": 50, "text": "deep networks"}, {"st": 67, "ed": 69, "text": "exponentially large"}, {"st": 78, "ed": 80, "text": "deep network"}, {"st": 124, "ed": 126, "text": "receptive field"}, {"st": 128, "ed": 130, "text": "theoretically analyze"}, {"st": 152, "ed": 154, "text": "demonstrate empirically"}, {"st": 165, "ed": 167, "text": "analysis shows"}, {"st": 171, "ed": 173, "text": "receptive fields"}]
[{"st": 54, "ed": 56, "text": "significantly reduced"}, {"st": 68, "ed": 70, "text": "rnn architecture"}, {"st": 121, "ed": 123, "text": "performs comparably"}, {"st": 129, "ed": 132, "text": "convolutional neural networks"}, {"st": 135, "ed": 137, "text": "neural networks"}, {"st": 155, "ed": 157, "text": "imagenet dataset"}, {"st": 161, "ed": 163, "text": "optimization problems"}, {"st": 179, "ed": 181, "text": "open source"}]
[{"st": 30, "ed": 32, "text": "sensory data"}, {"st": 69, "ed": 71, "text": "moving average"}, {"st": 77, "ed": 79, "text": "auto encoder"}]
[{"st": 5, "ed": 7, "text": "ensemble based"}, {"st": 7, "ed": 9, "text": "feature engineering"}, {"st": 18, "ed": 20, "text": "machine learning"}, {"st": 30, "ed": 32, "text": "supervised classification"}, {"st": 102, "ed": 104, "text": "classification methods"}, {"st": 118, "ed": 120, "text": "classifier performance"}]
[{"st": 1, "ed": 3, "text": "recent years"}, {"st": 3, "ed": 5, "text": "deep learning"}, {"st": 64, "ed": 66, "text": "deep learning"}, {"st": 81, "ed": 83, "text": "provide theoretical"}]
[{"st": 5, "ed": 9, "text": "recurrent neural network rnn"}]
[{"st": 7, "ed": 11, "text": "recurrent neural network rnn"}, {"st": 31, "ed": 33, "text": "faster convergence"}, {"st": 65, "ed": 67, "text": "rnn architecture"}]
[{"st": 0, "ed": 3, "text": "the paper presents"}, {"st": 5, "ed": 7, "text": "principled approach"}, {"st": 9, "ed": 12, "text": "recurrent neural networks"}, {"st": 25, "ed": 27, "text": "input features"}, {"st": 37, "ed": 39, "text": "dropout regularization"}, {"st": 66, "ed": 68, "text": "original input"}, {"st": 103, "ed": 106, "text": "wireless sensor networks"}, {"st": 115, "ed": 118, "text": "real world data"}, {"st": 120, "ed": 122, "text": "application domains"}]
[{"st": 17, "ed": 19, "text": "fully connected"}, {"st": 19, "ed": 22, "text": "deep neural networks"}, {"st": 45, "ed": 49, "text": "convolutional neural networks cnns"}, {"st": 56, "ed": 58, "text": "convolutional layers"}, {"st": 146, "ed": 148, "text": "reduction techniques"}, {"st": 205, "ed": 207, "text": "neural network"}]
[{"st": 57, "ed": 59, "text": "supervised learning"}, {"st": 62, "ed": 64, "text": "multi layer"}, {"st": 83, "ed": 85, "text": "learning rule"}, {"st": 88, "ed": 90, "text": "multi layer"}, {"st": 108, "ed": 110, "text": "recent results"}, {"st": 119, "ed": 121, "text": "learning rule"}, {"st": 155, "ed": 157, "text": "complex tasks"}, {"st": 178, "ed": 181, "text": "spiking neural networks"}, {"st": 191, "ed": 193, "text": "problems involving"}]
[{"st": 0, "ed": 2, "text": "recent years"}, {"st": 9, "ed": 12, "text": "deep neural networks"}, {"st": 23, "ed": 25, "text": "low cost"}, {"st": 25, "ed": 27, "text": "local minima"}, {"st": 48, "ed": 50, "text": "recent theoretical"}, {"st": 52, "ed": 54, "text": "empirical studies"}, {"st": 55, "ed": 58, "text": "deep neural network"}, {"st": 62, "ed": 64, "text": "saddle points"}, {"st": 86, "ed": 88, "text": "based methods"}, {"st": 98, "ed": 100, "text": "extensive experimental"}, {"st": 102, "ed": 104, "text": "standard datasets"}, {"st": 106, "ed": 110, "text": "mnist and cifar 10"}]
[{"st": 4, "ed": 8, "text": "recurrent neural network rnn"}, {"st": 94, "ed": 96, "text": "important role"}, {"st": 102, "ed": 104, "text": "competitive results"}, {"st": 115, "ed": 117, "text": "tasks including"}, {"st": 119, "ed": 121, "text": "question answering"}]
[{"st": 0, "ed": 2, "text": "neural networks"}, {"st": 24, "ed": 26, "text": "rational function"}, {"st": 41, "ed": 43, "text": "rational function"}, {"st": 82, "ed": 84, "text": "rational function"}]
[{"st": 0, "ed": 3, "text": "spiking neural networks"}, {"st": 11, "ed": 13, "text": "machine learning"}, {"st": 32, "ed": 34, "text": "spike timing"}, {"st": 44, "ed": 46, "text": "spike timing"}, {"st": 56, "ed": 58, "text": "temporal correlations"}, {"st": 73, "ed": 75, "text": "spike timing"}, {"st": 76, "ed": 78, "text": "learning rule"}, {"st": 146, "ed": 148, "text": "spike timing"}, {"st": 165, "ed": 167, "text": "real world"}, {"st": 171, "ed": 173, "text": "united states"}, {"st": 187, "ed": 189, "text": "phase change"}, {"st": 214, "ed": 216, "text": "real world"}]
[{"st": 3, "ed": 5, "text": "neural network"}, {"st": 13, "ed": 15, "text": "bipartite graph"}, {"st": 25, "ed": 27, "text": "speed ups"}, {"st": 36, "ed": 38, "text": "fully connected"}, {"st": 52, "ed": 54, "text": "neural network"}, {"st": 79, "ed": 81, "text": "training speed"}, {"st": 94, "ed": 96, "text": "previous approaches"}, {"st": 102, "ed": 104, "text": "fully connected"}, {"st": 104, "ed": 106, "text": "neural network"}, {"st": 111, "ed": 113, "text": "training phase"}, {"st": 127, "ed": 129, "text": "neural network"}, {"st": 143, "ed": 145, "text": "neural network"}, {"st": 157, "ed": 159, "text": "neural networks"}]
[{"st": 5, "ed": 7, "text": "neural networks"}, {"st": 8, "ed": 12, "text": "vulnerable to adversarial examples"}, {"st": 40, "ed": 42, "text": "adversarial attacks"}, {"st": 48, "ed": 50, "text": "deep learning"}, {"st": 61, "ed": 63, "text": "neural networks"}, {"st": 102, "ed": 104, "text": "neural networks"}, {"st": 134, "ed": 136, "text": "significantly improved"}, {"st": 182, "ed": 184, "text": "deep learning"}]
[{"st": 4, "ed": 6, "text": "margin based"}, {"st": 7, "ed": 9, "text": "generalization bound"}, {"st": 10, "ed": 12, "text": "neural networks"}, {"st": 32, "ed": 34, "text": "weight matrices"}, {"st": 54, "ed": 57, "text": "mnist and cifar10"}, {"st": 92, "ed": 94, "text": "learning task"}]
[{"st": 1, "ed": 3, "text": "neural nets"}, {"st": 5, "ed": 7, "text": "active research"}, {"st": 22, "ed": 24, "text": "object recognition"}, {"st": 50, "ed": 52, "text": "low rank"}, {"st": 54, "ed": 56, "text": "lossless compression"}, {"st": 164, "ed": 166, "text": "neural nets"}]
[{"st": 5, "ed": 7, "text": "deep neural"}, {"st": 21, "ed": 23, "text": "real valued"}, {"st": 33, "ed": 35, "text": "training loss"}, {"st": 46, "ed": 48, "text": "learned jointly"}, {"st": 98, "ed": 100, "text": "recently proposed"}, {"st": 105, "ed": 107, "text": "constrained optimization"}, {"st": 142, "ed": 145, "text": "guaranteed to converge"}, {"st": 146, "ed": 148, "text": "local optimum"}]
[{"st": 11, "ed": 13, "text": "input data"}, {"st": 33, "ed": 35, "text": "recurrent connections"}, {"st": 35, "ed": 37, "text": "tikhonov regularization"}, {"st": 45, "ed": 47, "text": "additive noise"}, {"st": 62, "ed": 65, "text": "feedforward neural networks"}, {"st": 72, "ed": 74, "text": "recurrent connections"}, {"st": 91, "ed": 96, "text": "long short term memory lstm"}, {"st": 135, "ed": 137, "text": "input data"}, {"st": 183, "ed": 186, "text": "recurrent neural networks"}, {"st": 190, "ed": 192, "text": "lipschitz continuous"}]
[{"st": 1, "ed": 3, "text": "real world"}, {"st": 46, "ed": 48, "text": "mixture model"}, {"st": 60, "ed": 62, "text": "expectation maximization"}, {"st": 68, "ed": 70, "text": "clustering method"}, {"st": 71, "ed": 73, "text": "simultaneously learns"}, {"st": 106, "ed": 108, "text": "learned representations"}]
[{"st": 4, "ed": 6, "text": "tensor based"}, {"st": 9, "ed": 11, "text": "supervised classification"}, {"st": 17, "ed": 19, "text": "echo state"}, {"st": 23, "ed": 25, "text": "supervised classification"}, {"st": 31, "ed": 33, "text": "echo state"}, {"st": 38, "ed": 40, "text": "hidden layer"}, {"st": 56, "ed": 58, "text": "linear algebra"}, {"st": 68, "ed": 70, "text": "hidden layer"}, {"st": 90, "ed": 93, "text": "spatial and temporal"}, {"st": 103, "ed": 105, "text": "supervised classification"}, {"st": 107, "ed": 109, "text": "echo state"}, {"st": 120, "ed": 122, "text": "hidden layer"}, {"st": 145, "ed": 147, "text": "numerical experiments"}, {"st": 158, "ed": 160, "text": "tensor based"}]
[{"st": 2, "ed": 5, "text": "convolutional neural networks"}, {"st": 18, "ed": 20, "text": "temple university"}, {"st": 33, "ed": 35, "text": "recently shown"}, {"st": 116, "ed": 118, "text": "max pooling"}, {"st": 157, "ed": 159, "text": "spectral analysis"}, {"st": 184, "ed": 186, "text": "contextual information"}, {"st": 197, "ed": 199, "text": "visualization techniques"}]
[{"st": 6, "ed": 8, "text": "important role"}, {"st": 27, "ed": 29, "text": "additional information"}, {"st": 35, "ed": 37, "text": "learning algorithms"}, {"st": 44, "ed": 46, "text": "learning paradigm"}, {"st": 79, "ed": 81, "text": "existing approaches"}, {"st": 89, "ed": 91, "text": "neural networks"}, {"st": 120, "ed": 122, "text": "feature learning"}, {"st": 145, "ed": 147, "text": "generalization error"}, {"st": 158, "ed": 160, "text": "real world"}, {"st": 164, "ed": 167, "text": "effectiveness and efficiency"}, {"st": 177, "ed": 179, "text": "generalization performance"}]
[{"st": 4, "ed": 7, "text": "deep neural networks"}, {"st": 7, "ed": 10, "text": "stochastic gradient descent"}, {"st": 50, "ed": 52, "text": "learning rate"}, {"st": 92, "ed": 95, "text": "change point detection"}, {"st": 108, "ed": 110, "text": "learning rate"}, {"st": 122, "ed": 124, "text": "learning rate"}, {"st": 126, "ed": 128, "text": "proposed approach"}, {"st": 141, "ed": 143, "text": "standard benchmarks"}, {"st": 151, "ed": 154, "text": "compares favorably to"}]
[{"st": 19, "ed": 21, "text": "gaussian noise"}, {"st": 37, "ed": 39, "text": "unsupervised learning"}]
[{"st": 10, "ed": 12, "text": "activation functions"}, {"st": 28, "ed": 30, "text": "weight initialization"}, {"st": 44, "ed": 48, "text": "recurrent neural networks rnns"}, {"st": 57, "ed": 59, "text": "activation functions"}, {"st": 75, "ed": 77, "text": "competitive results"}, {"st": 90, "ed": 93, "text": "convolutional neural networks"}, {"st": 94, "ed": 96, "text": "batch normalization"}, {"st": 106, "ed": 108, "text": "training error"}, {"st": 117, "ed": 119, "text": "cifar 10"}]
[{"st": 2, "ed": 4, "text": "neural network"}, {"st": 12, "ed": 14, "text": "back propagation"}, {"st": 18, "ed": 21, "text": "simple yet effective"}, {"st": 53, "ed": 57, "text": "convolutional neural network cnn"}, {"st": 61, "ed": 63, "text": "back propagation"}, {"st": 111, "ed": 113, "text": "back propagation"}, {"st": 116, "ed": 118, "text": "significant computational"}, {"st": 121, "ed": 123, "text": "computational complexity"}]
[{"st": 0, "ed": 3, "text": "deep neural networks"}, {"st": 6, "ed": 8, "text": "improved performance"}, {"st": 23, "ed": 25, "text": "network architectures"}, {"st": 37, "ed": 39, "text": "increasing complexity"}, {"st": 42, "ed": 44, "text": "deep networks"}, {"st": 46, "ed": 48, "text": "computationally efficient"}, {"st": 49, "ed": 51, "text": "evolutionary algorithm"}, {"st": 55, "ed": 58, "text": "deep neural network"}, {"st": 78, "ed": 80, "text": "max pooling"}, {"st": 81, "ed": 84, "text": "fully connected layers"}, {"st": 95, "ed": 97, "text": "sentiment classification"}, {"st": 140, "ed": 142, "text": "convolutional networks"}, {"st": 143, "ed": 145, "text": "sentiment analysis"}]
[{"st": 13, "ed": 15, "text": "early detection"}, {"st": 34, "ed": 36, "text": "streaming data"}, {"st": 61, "ed": 63, "text": "deep learning"}, {"st": 113, "ed": 115, "text": "performance metric"}, {"st": 119, "ed": 122, "text": "recurrent neural network"}, {"st": 124, "ed": 127, "text": "principal component analysis"}, {"st": 127, "ed": 130, "text": "support vector machine"}, {"st": 134, "ed": 136, "text": "anomaly detection"}]
[{"st": 1, "ed": 4, "text": "feedforward neural networks"}, {"st": 33, "ed": 35, "text": "generalization performance"}, {"st": 81, "ed": 83, "text": "extensive experimental"}]
[{"st": 1, "ed": 3, "text": "recurrent networks"}, {"st": 4, "ed": 6, "text": "powerful tools"}, {"st": 15, "ed": 17, "text": "based method"}, {"st": 25, "ed": 27, "text": "recurrent network"}, {"st": 32, "ed": 34, "text": "tasks involving"}, {"st": 36, "ed": 38, "text": "input output"}, {"st": 65, "ed": 67, "text": "method produces"}, {"st": 80, "ed": 82, "text": "least squares"}, {"st": 91, "ed": 93, "text": "input signals"}]
[{"st": 1, "ed": 3, "text": "widely studied"}, {"st": 7, "ed": 9, "text": "np hard"}, {"st": 43, "ed": 47, "text": "deep recurrent neural network"}, {"st": 78, "ed": 80, "text": "recurrent networks"}, {"st": 107, "ed": 109, "text": "loss function"}]
[{"st": 5, "ed": 7, "text": "generative model"}, {"st": 9, "ed": 13, "text": "sum product networks spns"}, {"st": 21, "ed": 23, "text": "sum product"}, {"st": 32, "ed": 34, "text": "conditional distributions"}, {"st": 55, "ed": 58, "text": "provide sufficient conditions"}, {"st": 124, "ed": 126, "text": "neural network"}]
[{"st": 9, "ed": 12, "text": "multivariate time series"}, {"st": 22, "ed": 24, "text": "low dimensional"}, {"st": 54, "ed": 56, "text": "time series"}, {"st": 80, "ed": 82, "text": "learned representations"}, {"st": 90, "ed": 92, "text": "classification problem"}, {"st": 95, "ed": 97, "text": "missing values"}]
[{"st": 7, "ed": 10, "text": "deep neural network"}, {"st": 12, "ed": 14, "text": "supervised learning"}, {"st": 95, "ed": 97, "text": "visual recognition"}, {"st": 100, "ed": 102, "text": "mnist handwritten"}]
[{"st": 20, "ed": 22, "text": "neural network"}, {"st": 24, "ed": 26, "text": "deep learning"}, {"st": 30, "ed": 32, "text": "feed forward"}, {"st": 32, "ed": 34, "text": "deep network"}, {"st": 46, "ed": 48, "text": "accurately predict"}, {"st": 50, "ed": 52, "text": "complex systems"}, {"st": 80, "ed": 82, "text": "neural networks"}, {"st": 84, "ed": 86, "text": "machine learning"}, {"st": 95, "ed": 97, "text": "existing approaches"}, {"st": 111, "ed": 113, "text": "finite difference"}, {"st": 163, "ed": 165, "text": "predictive power"}, {"st": 171, "ed": 173, "text": "carefully designed"}, {"st": 213, "ed": 215, "text": "computer vision"}, {"st": 223, "ed": 225, "text": "neural network"}, {"st": 227, "ed": 229, "text": "numerical experiments"}]
[{"st": 8, "ed": 10, "text": "neural network"}, {"st": 28, "ed": 30, "text": "neural network"}, {"st": 49, "ed": 51, "text": "desirable properties"}, {"st": 56, "ed": 58, "text": "generalization performance"}, {"st": 99, "ed": 101, "text": "numerical results"}, {"st": 110, "ed": 113, "text": "deep neural networks"}]
[{"st": 5, "ed": 8, "text": "semi supervised learning"}, {"st": 8, "ed": 10, "text": "method called"}, {"st": 47, "ed": 49, "text": "similarity measure"}, {"st": 77, "ed": 80, "text": "semi supervised learning"}, {"st": 82, "ed": 84, "text": "error rates"}, {"st": 88, "ed": 90, "text": "cifar 10"}, {"st": 118, "ed": 120, "text": "error rate"}]
[{"st": 10, "ed": 12, "text": "recent years"}, {"st": 12, "ed": 14, "text": "mobile phones"}, {"st": 17, "ed": 19, "text": "wi fi"}, {"st": 20, "ed": 22, "text": "electromagnetic spectrum"}, {"st": 72, "ed": 74, "text": "extremely high"}, {"st": 105, "ed": 107, "text": "hand engineered"}, {"st": 116, "ed": 118, "text": "machine learning"}, {"st": 144, "ed": 147, "text": "deep neural nets"}, {"st": 148, "ed": 150, "text": "convolutional neural"}, {"st": 152, "ed": 155, "text": "support vector machines"}, {"st": 157, "ed": 159, "text": "multi stage"}, {"st": 184, "ed": 186, "text": "classification accuracy"}]
[{"st": 19, "ed": 22, "text": "undirected graphical models"}, {"st": 36, "ed": 38, "text": "detailed balance"}, {"st": 75, "ed": 77, "text": "detailed balance"}, {"st": 86, "ed": 88, "text": "neural networks"}, {"st": 93, "ed": 95, "text": "biological neural"}, {"st": 118, "ed": 120, "text": "detailed balance"}, {"st": 126, "ed": 128, "text": "biologically plausible"}, {"st": 129, "ed": 131, "text": "neural networks"}, {"st": 141, "ed": 143, "text": "training objective"}, {"st": 148, "ed": 150, "text": "variational methods"}, {"st": 158, "ed": 160, "text": "multi step"}, {"st": 188, "ed": 190, "text": "proposed approach"}, {"st": 195, "ed": 198, "text": "mnist cifar 10"}, {"st": 262, "ed": 264, "text": "source code"}]
[{"st": 5, "ed": 7, "text": "neural networks"}, {"st": 46, "ed": 48, "text": "neural networks"}, {"st": 54, "ed": 56, "text": "network architectures"}, {"st": 68, "ed": 70, "text": "main results"}, {"st": 71, "ed": 73, "text": "neural nets"}, {"st": 118, "ed": 120, "text": "frac 1"}, {"st": 143, "ed": 145, "text": "frac 1"}, {"st": 178, "ed": 180, "text": "frac 1"}, {"st": 236, "ed": 238, "text": "mathbb r"}, {"st": 240, "ed": 242, "text": "mathbb r"}, {"st": 252, "ed": 254, "text": "neural nets"}]
[{"st": 0, "ed": 2, "text": "deep generative"}, {"st": 2, "ed": 4, "text": "neural networks"}, {"st": 14, "ed": 16, "text": "complex data"}, {"st": 56, "ed": 58, "text": "latent space"}, {"st": 72, "ed": 74, "text": "gradient based"}, {"st": 95, "ed": 97, "text": "generate realistic"}, {"st": 106, "ed": 108, "text": "gradient based"}, {"st": 120, "ed": 122, "text": "latent space"}, {"st": 138, "ed": 140, "text": "zero shot"}, {"st": 149, "ed": 151, "text": "labeled data"}]
[{"st": 4, "ed": 6, "text": "highly effective"}, {"st": 55, "ed": 57, "text": "biological neural"}, {"st": 108, "ed": 110, "text": "higher layers"}, {"st": 115, "ed": 118, "text": "layer by layer"}, {"st": 207, "ed": 209, "text": "deep networks"}]
[{"st": 6, "ed": 8, "text": "genetic algorithms"}, {"st": 13, "ed": 15, "text": "evaluation function"}, {"st": 24, "ed": 27, "text": "supervised and unsupervised"}, {"st": 30, "ed": 32, "text": "supervised learning"}, {"st": 47, "ed": 49, "text": "unsupervised learning"}, {"st": 77, "ed": 79, "text": "computer chess"}, {"st": 94, "ed": 96, "text": "evaluation function"}, {"st": 118, "ed": 120, "text": "computer chess"}]
[{"st": 6, "ed": 8, "text": "genetic algorithms"}, {"st": 12, "ed": 14, "text": "reverse engineer"}, {"st": 15, "ed": 17, "text": "evaluation function"}, {"st": 52, "ed": 54, "text": "computer chess"}, {"st": 56, "ed": 58, "text": "performance gain"}, {"st": 74, "ed": 76, "text": "evaluation function"}]
[{"st": 1, "ed": 3, "text": "recent years"}, {"st": 3, "ed": 5, "text": "deep learning"}, {"st": 7, "ed": 9, "text": "unsupervised learning"}, {"st": 14, "ed": 16, "text": "neural networks"}, {"st": 27, "ed": 29, "text": "genetic algorithms"}, {"st": 29, "ed": 31, "text": "based methods"}, {"st": 33, "ed": 35, "text": "successfully applied"}, {"st": 69, "ed": 71, "text": "deep autoencoder"}]
[{"st": 3, "ed": 5, "text": "biologically plausible"}, {"st": 5, "ed": 7, "text": "deep learning"}, {"st": 20, "ed": 22, "text": "biological neural"}, {"st": 67, "ed": 69, "text": "weight updates"}, {"st": 86, "ed": 88, "text": "contrastive divergence"}, {"st": 108, "ed": 110, "text": "learning algorithm"}, {"st": 163, "ed": 165, "text": "weight updates"}, {"st": 170, "ed": 172, "text": "spike timing"}]
[{"st": 5, "ed": 8, "text": "deep learning based"}, {"st": 20, "ed": 23, "text": "deep belief network"}, {"st": 30, "ed": 32, "text": "denoising autoencoders"}, {"st": 35, "ed": 37, "text": "compact representation"}, {"st": 46, "ed": 48, "text": "based methods"}, {"st": 97, "ed": 99, "text": "method achieves"}, {"st": 100, "ed": 102, "text": "classification accuracy"}, {"st": 150, "ed": 153, "text": "deep neural network"}]
[{"st": 6, "ed": 8, "text": "genetic algorithms"}, {"st": 14, "ed": 16, "text": "evaluation function"}, {"st": 26, "ed": 28, "text": "parameter values"}, {"st": 34, "ed": 36, "text": "evaluation function"}, {"st": 97, "ed": 99, "text": "computer chess"}, {"st": 108, "ed": 110, "text": "computer chess"}]
[{"st": 1, "ed": 3, "text": "recent years"}, {"st": 26, "ed": 28, "text": "nation state"}, {"st": 35, "ed": 37, "text": "nation state"}, {"st": 52, "ed": 54, "text": "authorship attribution"}, {"st": 67, "ed": 69, "text": "feature extraction"}, {"st": 86, "ed": 89, "text": "deep neural networks"}, {"st": 95, "ed": 97, "text": "nation state"}, {"st": 117, "ed": 119, "text": "neural network"}]
[{"st": 3, "ed": 6, "text": "end to end"}, {"st": 31, "ed": 34, "text": "deep neural network"}, {"st": 46, "ed": 48, "text": "unsupervised training"}, {"st": 49, "ed": 51, "text": "high level"}, {"st": 58, "ed": 60, "text": "supervised training"}, {"st": 86, "ed": 88, "text": "domain specific"}, {"st": 97, "ed": 99, "text": "neural network"}, {"st": 124, "ed": 126, "text": "feature selection"}, {"st": 132, "ed": 135, "text": "end to end"}, {"st": 135, "ed": 137, "text": "machine learning"}, {"st": 137, "ed": 139, "text": "based method"}]
[{"st": 5, "ed": 7, "text": "activation functions"}, {"st": 27, "ed": 29, "text": "gaussian process"}, {"st": 37, "ed": 39, "text": "gaussian process"}, {"st": 50, "ed": 52, "text": "building block"}, {"st": 53, "ed": 56, "text": "probabilistic graphical models"}, {"st": 82, "ed": 84, "text": "variational bayesian"}, {"st": 87, "ed": 90, "text": "central limit theorem"}, {"st": 93, "ed": 95, "text": "loss function"}, {"st": 107, "ed": 109, "text": "neural network"}, {"st": 115, "ed": 117, "text": "posterior distribution"}, {"st": 118, "ed": 120, "text": "activation functions"}, {"st": 124, "ed": 126, "text": "training data"}, {"st": 139, "ed": 141, "text": "gaussian processes"}, {"st": 152, "ed": 154, "text": "directly applied"}, {"st": 157, "ed": 159, "text": "convolutional network"}, {"st": 166, "ed": 168, "text": "image processing"}, {"st": 172, "ed": 174, "text": "empirical evaluation"}, {"st": 180, "ed": 182, "text": "classification tasks"}, {"st": 196, "ed": 198, "text": "neural network"}]
[{"st": 2, "ed": 5, "text": "event related potential"}, {"st": 31, "ed": 34, "text": "event related potential"}, {"st": 72, "ed": 76, "text": "signal to noise ratio"}, {"st": 91, "ed": 93, "text": "common practice"}, {"st": 138, "ed": 140, "text": "classification accuracy"}, {"st": 148, "ed": 151, "text": "principal component analysis"}, {"st": 158, "ed": 161, "text": "linear discriminant analysis"}, {"st": 163, "ed": 165, "text": "neural networks"}]
[{"st": 0, "ed": 3, "text": "principal component analysis"}, {"st": 8, "ed": 10, "text": "chemical process"}, {"st": 29, "ed": 31, "text": "based methods"}, {"st": 47, "ed": 50, "text": "principal component analysis"}, {"st": 62, "ed": 64, "text": "based methods"}, {"st": 131, "ed": 133, "text": "method called"}, {"st": 134, "ed": 136, "text": "component analysis"}, {"st": 142, "ed": 144, "text": "feedforward neural"}, {"st": 178, "ed": 181, "text": "extensive experimental results"}, {"st": 201, "ed": 203, "text": "false alarm"}, {"st": 207, "ed": 209, "text": "source code"}, {"st": 215, "ed": 217, "text": "https github.com"}]
[{"st": 0, "ed": 3, "text": "deep generative models"}, {"st": 10, "ed": 12, "text": "applications including"}, {"st": 20, "ed": 22, "text": "latent space"}, {"st": 37, "ed": 39, "text": "previous studies"}, {"st": 69, "ed": 71, "text": "activity patterns"}, {"st": 75, "ed": 77, "text": "latent space"}, {"st": 79, "ed": 81, "text": "deep generative"}, {"st": 84, "ed": 87, "text": "variational auto encoder"}, {"st": 103, "ed": 105, "text": "input data"}, {"st": 118, "ed": 120, "text": "latent space"}, {"st": 176, "ed": 178, "text": "input data"}, {"st": 192, "ed": 194, "text": "latent variables"}, {"st": 209, "ed": 211, "text": "generalization ability"}]
[{"st": 3, "ed": 5, "text": "sample complexity"}, {"st": 7, "ed": 9, "text": "neural networks"}, {"st": 15, "ed": 17, "text": "rademacher complexity"}, {"st": 32, "ed": 34, "text": "complexity bounds"}, {"st": 39, "ed": 41, "text": "network depth"}, {"st": 44, "ed": 46, "text": "additional assumptions"}, {"st": 51, "ed": 53, "text": "network size"}, {"st": 54, "ed": 57, "text": "depth and width"}]
[{"st": 6, "ed": 9, "text": "artificial neural networks"}, {"st": 64, "ed": 66, "text": "task specific"}, {"st": 86, "ed": 89, "text": "vision and language"}]
[{"st": 4, "ed": 7, "text": "deep neural networks"}, {"st": 36, "ed": 38, "text": "typically require"}, {"st": 62, "ed": 64, "text": "computational overhead"}, {"st": 74, "ed": 76, "text": "embedded systems"}, {"st": 136, "ed": 138, "text": "resnet 50"}]
[{"st": 1, "ed": 4, "text": "artificial neural network"}, {"st": 30, "ed": 32, "text": "neural network"}, {"st": 45, "ed": 47, "text": "local patterns"}, {"st": 53, "ed": 55, "text": "domain knowledge"}, {"st": 57, "ed": 59, "text": "local patterns"}, {"st": 74, "ed": 76, "text": "local patterns"}, {"st": 84, "ed": 86, "text": "optimization problem"}, {"st": 94, "ed": 96, "text": "optimization strategy"}, {"st": 110, "ed": 112, "text": "neural networks"}, {"st": 132, "ed": 134, "text": "neural networks"}, {"st": 157, "ed": 159, "text": "neural networks"}, {"st": 171, "ed": 173, "text": "local patterns"}, {"st": 189, "ed": 191, "text": "time series"}]
[{"st": 14, "ed": 16, "text": "neural networks"}, {"st": 49, "ed": 51, "text": "proposed method"}, {"st": 90, "ed": 93, "text": "labeled training data"}, {"st": 94, "ed": 96, "text": "proposed method"}, {"st": 108, "ed": 110, "text": "training examples"}, {"st": 123, "ed": 125, "text": "benchmark datasets"}, {"st": 128, "ed": 130, "text": "proposed method"}]
[{"st": 49, "ed": 51, "text": "junction tree"}, {"st": 51, "ed": 53, "text": "variational autoencoder"}, {"st": 79, "ed": 81, "text": "message passing"}, {"st": 102, "ed": 104, "text": "multiple tasks"}, {"st": 114, "ed": 116, "text": "model outperforms"}]
[{"st": 5, "ed": 7, "text": "neural network"}, {"st": 14, "ed": 16, "text": "complex data"}, {"st": 23, "ed": 25, "text": "deep networks"}, {"st": 61, "ed": 63, "text": "hidden layer"}, {"st": 91, "ed": 93, "text": "deep features"}, {"st": 104, "ed": 106, "text": "anomaly detection"}, {"st": 137, "ed": 139, "text": "experiments demonstrate"}, {"st": 141, "ed": 143, "text": "complex data"}, {"st": 150, "ed": 152, "text": "significantly outperforms"}, {"st": 157, "ed": 159, "text": "anomaly detection"}]
[{"st": 4, "ed": 7, "text": "empirical risk minimization"}, {"st": 8, "ed": 10, "text": "maximum likelihood"}, {"st": 38, "ed": 41, "text": "artificial neural networks"}, {"st": 47, "ed": 49, "text": "adversarial examples"}, {"st": 73, "ed": 76, "text": "spiking neural networks"}, {"st": 80, "ed": 82, "text": "neural networks"}, {"st": 83, "ed": 85, "text": "adversarial examples"}, {"st": 120, "ed": 122, "text": "white box"}]
[{"st": 1, "ed": 3, "text": "neural networks"}, {"st": 8, "ed": 10, "text": "neural networks"}, {"st": 46, "ed": 48, "text": "activation functions"}, {"st": 66, "ed": 68, "text": "convolutional networks"}, {"st": 74, "ed": 76, "text": "activation functions"}, {"st": 88, "ed": 90, "text": "recently proposed"}, {"st": 91, "ed": 93, "text": "activation function"}, {"st": 119, "ed": 121, "text": "experimental evaluation"}, {"st": 127, "ed": 129, "text": "significantly improve"}]
[{"st": 0, "ed": 3, "text": "generative adversarial networks"}, {"st": 9, "ed": 11, "text": "generative models"}, {"st": 41, "ed": 43, "text": "gan framework"}, {"st": 45, "ed": 48, "text": "generative adversarial networks"}, {"st": 52, "ed": 54, "text": "gan training"}, {"st": 58, "ed": 60, "text": "unlike existing"}, {"st": 64, "ed": 66, "text": "pre defined"}, {"st": 67, "ed": 69, "text": "objective function"}, {"st": 79, "ed": 81, "text": "adversarial training"}, {"st": 113, "ed": 115, "text": "generated samples"}, {"st": 140, "ed": 142, "text": "adversarial training"}]
[{"st": 4, "ed": 6, "text": "machine learning"}, {"st": 20, "ed": 22, "text": "evolutionary algorithm"}, {"st": 30, "ed": 32, "text": "domain knowledge"}, {"st": 61, "ed": 63, "text": "starting point"}, {"st": 76, "ed": 78, "text": "machine learning"}, {"st": 103, "ed": 105, "text": "competitive performance"}]
[{"st": 4, "ed": 7, "text": "online learning algorithm"}, {"st": 8, "ed": 11, "text": "restricted boltzmann machines"}, {"st": 17, "ed": 20, "text": "restricted boltzmann machine"}, {"st": 31, "ed": 33, "text": "network architecture"}, {"st": 57, "ed": 59, "text": "feature representation"}, {"st": 61, "ed": 63, "text": "hidden layer"}, {"st": 80, "ed": 82, "text": "hidden layer"}, {"st": 92, "ed": 94, "text": "streaming data"}, {"st": 98, "ed": 100, "text": "unsupervised manner"}, {"st": 103, "ed": 105, "text": "feature representation"}, {"st": 116, "ed": 119, "text": "stochastic gradient descent"}, {"st": 140, "ed": 142, "text": "binary classification"}, {"st": 161, "ed": 163, "text": "multi class"}, {"st": 163, "ed": 165, "text": "mnist dataset"}, {"st": 182, "ed": 184, "text": "network architecture"}, {"st": 192, "ed": 194, "text": "class labels"}, {"st": 205, "ed": 207, "text": "machine learning"}, {"st": 207, "ed": 209, "text": "neural network"}, {"st": 219, "ed": 221, "text": "non stationary"}, {"st": 267, "ed": 269, "text": "multiple layers"}, {"st": 271, "ed": 274, "text": "deep belief networks"}, {"st": 275, "ed": 277, "text": "non stationary"}]
[{"st": 13, "ed": 16, "text": "fully connected layers"}, {"st": 60, "ed": 62, "text": "fully connected"}, {"st": 68, "ed": 70, "text": "convex functions"}, {"st": 82, "ed": 84, "text": "theoretical analysis"}, {"st": 89, "ed": 91, "text": "generalization error"}, {"st": 103, "ed": 105, "text": "hidden layers"}, {"st": 107, "ed": 109, "text": "weight pruning"}, {"st": 117, "ed": 120, "text": "orders of magnitude"}, {"st": 122, "ed": 124, "text": "competing approaches"}, {"st": 126, "ed": 128, "text": "theoretical analysis"}, {"st": 128, "ed": 130, "text": "sheds light"}, {"st": 140, "ed": 143, "text": "feedforward neural networks"}]
[{"st": 0, "ed": 2, "text": "deep learning"}, {"st": 22, "ed": 24, "text": "intrusion detection"}, {"st": 24, "ed": 28, "text": "denial of service attack"}, {"st": 57, "ed": 59, "text": "machine learning"}, {"st": 60, "ed": 62, "text": "deep learning"}, {"st": 88, "ed": 90, "text": "impressive performance"}, {"st": 91, "ed": 93, "text": "deep learning"}, {"st": 106, "ed": 109, "text": "recurrent neural network"}, {"st": 116, "ed": 118, "text": "anomaly detection"}, {"st": 150, "ed": 152, "text": "without sacrificing"}, {"st": 167, "ed": 169, "text": "intrusion detection"}, {"st": 172, "ed": 176, "text": "los alamos national laboratory"}]
[{"st": 16, "ed": 18, "text": "training data"}, {"st": 33, "ed": 35, "text": "feature vectors"}, {"st": 54, "ed": 56, "text": "current approaches"}, {"st": 126, "ed": 128, "text": "context dependent"}, {"st": 140, "ed": 142, "text": "context dependent"}, {"st": 152, "ed": 154, "text": "neural network"}, {"st": 156, "ed": 158, "text": "demonstrate empirically"}, {"st": 160, "ed": 162, "text": "methods outperform"}, {"st": 162, "ed": 164, "text": "traditional approaches"}, {"st": 165, "ed": 167, "text": "benchmark tasks"}]
[{"st": 4, "ed": 6, "text": "real world"}, {"st": 9, "ed": 12, "text": "convolutional neural networks"}, {"st": 17, "ed": 19, "text": "feature learning"}, {"st": 19, "ed": 21, "text": "neural networks"}, {"st": 33, "ed": 35, "text": "hand designed"}, {"st": 37, "ed": 39, "text": "automatically learn"}, {"st": 57, "ed": 59, "text": "multi stage"}, {"st": 94, "ed": 96, "text": "multi stage"}, {"st": 100, "ed": 102, "text": "source code"}]
[{"st": 8, "ed": 10, "text": "learning representations"}, {"st": 54, "ed": 56, "text": "learning algorithms"}, {"st": 71, "ed": 73, "text": "visual representations"}, {"st": 100, "ed": 102, "text": "computer vision"}, {"st": 106, "ed": 108, "text": "feature space"}, {"st": 125, "ed": 127, "text": "classification loss"}, {"st": 133, "ed": 135, "text": "kernel matrix"}, {"st": 164, "ed": 166, "text": "learning algorithms"}, {"st": 238, "ed": 240, "text": "learning algorithm"}, {"st": 267, "ed": 269, "text": "visual cortex"}]
[{"st": 4, "ed": 6, "text": "labeled datasets"}, {"st": 8, "ed": 10, "text": "convolutional network"}, {"st": 20, "ed": 22, "text": "manual annotation"}, {"st": 31, "ed": 33, "text": "noisy labels"}, {"st": 79, "ed": 81, "text": "network outputs"}, {"st": 100, "ed": 102, "text": "training process"}, {"st": 121, "ed": 123, "text": "large scale"}, {"st": 126, "ed": 128, "text": "imagenet classification"}]
[{"st": 3, "ed": 5, "text": "object detection"}, {"st": 12, "ed": 17, "text": "deep convolutional neural network cnn"}, {"st": 80, "ed": 82, "text": "deep cnn"}]
[{"st": 8, "ed": 10, "text": "deep learning"}, {"st": 14, "ed": 16, "text": "convolutional network"}, {"st": 26, "ed": 28, "text": "feature extraction"}, {"st": 36, "ed": 38, "text": "feature extraction"}, {"st": 43, "ed": 45, "text": "convolutional layer"}, {"st": 52, "ed": 54, "text": "convolutional layer"}, {"st": 74, "ed": 76, "text": "convolutional layers"}, {"st": 83, "ed": 85, "text": "feature maps"}, {"st": 97, "ed": 99, "text": "extensive experiments"}, {"st": 101, "ed": 103, "text": "challenging tasks"}, {"st": 104, "ed": 106, "text": "handwritten digits"}, {"st": 107, "ed": 109, "text": "face recognition"}, {"st": 128, "ed": 130, "text": "deep learning"}, {"st": 137, "ed": 139, "text": "back propagation"}]
[{"st": 3, "ed": 5, "text": "empirical study"}, {"st": 10, "ed": 14, "text": "convolutional neural networks cnns"}, {"st": 23, "ed": 25, "text": "input data"}, {"st": 39, "ed": 41, "text": "convolutional architecture"}, {"st": 50, "ed": 52, "text": "posterior distributions"}, {"st": 57, "ed": 59, "text": "class conditional"}, {"st": 67, "ed": 69, "text": "bounding boxes"}, {"st": 87, "ed": 89, "text": "empirical evidence"}, {"st": 104, "ed": 106, "text": "convolutional architectures"}, {"st": 135, "ed": 137, "text": "classification task"}, {"st": 156, "ed": 159, "text": "end to end"}, {"st": 172, "ed": 174, "text": "classification task"}]
[{"st": 3, "ed": 5, "text": "neural networks"}, {"st": 14, "ed": 16, "text": "classification task"}, {"st": 31, "ed": 33, "text": "top 5"}, {"st": 33, "ed": 35, "text": "classification accuracy"}, {"st": 39, "ed": 41, "text": "validation set"}, {"st": 48, "ed": 50, "text": "published results"}]
[{"st": 12, "ed": 14, "text": "visual field"}, {"st": 17, "ed": 19, "text": "low fidelity"}, {"st": 30, "ed": 32, "text": "deep learning"}, {"st": 48, "ed": 50, "text": "image processing"}, {"st": 60, "ed": 62, "text": "visual reasoning"}]
[{"st": 16, "ed": 18, "text": "low cost"}, {"st": 63, "ed": 65, "text": "kalman filter"}, {"st": 67, "ed": 69, "text": "optimization problem"}, {"st": 107, "ed": 109, "text": "supervised learning"}, {"st": 118, "ed": 122, "text": "deep recurrent neural networks"}, {"st": 163, "ed": 165, "text": "proposed approach"}, {"st": 169, "ed": 171, "text": "ground truth"}, {"st": 178, "ed": 180, "text": "motion capture"}]
[{"st": 29, "ed": 31, "text": "quality assessment"}, {"st": 45, "ed": 47, "text": "multi task"}, {"st": 47, "ed": 49, "text": "deep model"}, {"st": 53, "ed": 55, "text": "recognition task"}, {"st": 64, "ed": 67, "text": "convolutional neural networks"}, {"st": 73, "ed": 75, "text": "multi task"}, {"st": 149, "ed": 151, "text": "extensive experiments"}, {"st": 167, "ed": 169, "text": "quality assessment"}, {"st": 172, "ed": 174, "text": "multi task"}, {"st": 174, "ed": 176, "text": "deep models"}]
[{"st": 0, "ed": 3, "text": "deep neural networks"}, {"st": 23, "ed": 26, "text": "deep neural network"}, {"st": 27, "ed": 29, "text": "embedded systems"}, {"st": 31, "ed": 33, "text": "challenging task"}, {"st": 36, "ed": 39, "text": "deep neural network"}, {"st": 42, "ed": 45, "text": "deep belief network"}, {"st": 73, "ed": 75, "text": "deep learning"}, {"st": 93, "ed": 95, "text": "proposed method"}, {"st": 142, "ed": 145, "text": "deep neural networks"}, {"st": 159, "ed": 161, "text": "mnist handwritten"}, {"st": 163, "ed": 165, "text": "features extracted"}, {"st": 180, "ed": 182, "text": "classification accuracy"}, {"st": 190, "ed": 193, "text": "deep belief network"}, {"st": 205, "ed": 208, "text": "deep neural network"}, {"st": 221, "ed": 223, "text": "classification accuracy"}, {"st": 224, "ed": 226, "text": "pattern recognition"}]
[{"st": 10, "ed": 12, "text": "deep learning"}, {"st": 20, "ed": 22, "text": "pre trained"}, {"st": 22, "ed": 25, "text": "convolutional neural network"}, {"st": 27, "ed": 29, "text": "originally designed"}, {"st": 30, "ed": 32, "text": "image classification"}, {"st": 94, "ed": 96, "text": "proposed method"}, {"st": 100, "ed": 102, "text": "ukiyo e"}, {"st": 105, "ed": 107, "text": "japanese painting"}]
[{"st": 1, "ed": 3, "text": "computer vision"}, {"st": 15, "ed": 18, "text": "convolutional neural networks"}, {"st": 24, "ed": 26, "text": "larger datasets"}, {"st": 35, "ed": 37, "text": "computer vision"}, {"st": 40, "ed": 42, "text": "pre trained"}, {"st": 84, "ed": 86, "text": "pre trained"}, {"st": 109, "ed": 111, "text": "learning rates"}, {"st": 126, "ed": 128, "text": "deep networks"}, {"st": 134, "ed": 137, "text": "ability to learn"}, {"st": 161, "ed": 163, "text": "additional supervision"}, {"st": 166, "ed": 168, "text": "pre trained"}, {"st": 181, "ed": 183, "text": "regularization techniques"}]
[{"st": 1, "ed": 3, "text": "handwriting recognition"}, {"st": 25, "ed": 27, "text": "line level"}, {"st": 59, "ed": 61, "text": "multi dimensional"}, {"st": 62, "ed": 65, "text": "short term memory"}, {"st": 65, "ed": 68, "text": "recurrent neural networks"}, {"st": 72, "ed": 75, "text": "end to end"}, {"st": 89, "ed": 91, "text": "dimensional representation"}, {"st": 113, "ed": 115, "text": "neural network"}, {"st": 124, "ed": 126, "text": "attention weights"}, {"st": 150, "ed": 152, "text": "line level"}, {"st": 158, "ed": 161, "text": "end to end"}]
[{"st": 4, "ed": 6, "text": "mid level"}, {"st": 18, "ed": 20, "text": "automatically learn"}, {"st": 21, "ed": 23, "text": "image features"}, {"st": 27, "ed": 29, "text": "unsupervised manner"}, {"st": 42, "ed": 44, "text": "mid level"}, {"st": 44, "ed": 46, "text": "feature learning"}, {"st": 55, "ed": 58, "text": "k means clustering"}, {"st": 70, "ed": 72, "text": "method generates"}, {"st": 88, "ed": 90, "text": "low level"}, {"st": 113, "ed": 115, "text": "mid level"}, {"st": 145, "ed": 147, "text": "fast inference"}, {"st": 153, "ed": 155, "text": "extensive experiments"}, {"st": 172, "ed": 174, "text": "face recognition"}, {"st": 198, "ed": 200, "text": "recently proposed"}, {"st": 200, "ed": 202, "text": "sparse coding"}]
[{"st": 1, "ed": 3, "text": "recent progress"}, {"st": 4, "ed": 6, "text": "sparse coding"}, {"st": 7, "ed": 9, "text": "deep learning"}, {"st": 11, "ed": 14, "text": "unsupervised feature learning"}, {"st": 19, "ed": 21, "text": "hand crafted"}, {"st": 23, "ed": 25, "text": "computer vision"}, {"st": 28, "ed": 30, "text": "learned features"}, {"st": 35, "ed": 37, "text": "object recognition"}, {"st": 46, "ed": 48, "text": "feature learning"}, {"st": 70, "ed": 72, "text": "multi scale"}, {"st": 79, "ed": 81, "text": "learned features"}, {"st": 85, "ed": 87, "text": "hand crafted"}]
[{"st": 3, "ed": 8, "text": "long short term memory lstm"}, {"st": 24, "ed": 26, "text": "input sequence"}, {"st": 28, "ed": 30, "text": "fixed length"}, {"st": 49, "ed": 51, "text": "input sequence"}, {"st": 66, "ed": 68, "text": "image pixels"}, {"st": 163, "ed": 165, "text": "supervised learning"}, {"st": 184, "ed": 186, "text": "classification accuracy"}, {"st": 208, "ed": 210, "text": "action recognition"}]
[{"st": 11, "ed": 13, "text": "image classification"}, {"st": 19, "ed": 24, "text": "deep convolutional neural networks cnns"}, {"st": 30, "ed": 33, "text": "deep neural nets"}, {"st": 75, "ed": 77, "text": "hand crafted"}, {"st": 83, "ed": 85, "text": "features extracted"}, {"st": 109, "ed": 111, "text": "feature learning"}]
[{"st": 44, "ed": 46, "text": "multiple layers"}, {"st": 59, "ed": 61, "text": "variational bayes"}, {"st": 66, "ed": 68, "text": "training procedure"}, {"st": 89, "ed": 91, "text": "input image"}, {"st": 109, "ed": 112, "text": "qualitative and quantitative"}]
[{"st": 3, "ed": 5, "text": "neural network"}, {"st": 22, "ed": 24, "text": "training speed"}, {"st": 48, "ed": 50, "text": "convolutional filters"}, {"st": 52, "ed": 54, "text": "biologically inspired"}, {"st": 54, "ed": 56, "text": "visual processing"}, {"st": 67, "ed": 69, "text": "least squares"}, {"st": 82, "ed": 84, "text": "linear classifier"}, {"st": 119, "ed": 121, "text": "image classification"}, {"st": 131, "ed": 133, "text": "deep network"}]
[{"st": 2, "ed": 4, "text": "adversarial attacks"}, {"st": 13, "ed": 15, "text": "input image"}, {"st": 30, "ed": 32, "text": "latent representations"}, {"st": 40, "ed": 42, "text": "internal representation"}, {"st": 114, "ed": 116, "text": "adversarial attack"}]
[{"st": 1, "ed": 3, "text": "significant computational"}, {"st": 6, "ed": 8, "text": "neural networks"}, {"st": 9, "ed": 11, "text": "large scale"}, {"st": 12, "ed": 14, "text": "resource constrained"}, {"st": 52, "ed": 54, "text": "feed forward"}, {"st": 55, "ed": 58, "text": "convolutional neural networks"}, {"st": 69, "ed": 72, "text": "recurrent neural networks"}, {"st": 90, "ed": 93, "text": "short term memory"}, {"st": 109, "ed": 112, "text": "singular value decomposition"}, {"st": 118, "ed": 121, "text": "gated recurrent unit"}, {"st": 176, "ed": 179, "text": "short term memory"}, {"st": 199, "ed": 201, "text": "temporal coherence"}, {"st": 229, "ed": 231, "text": "low power"}]
[{"st": 3, "ed": 7, "text": "deep convolutional neural network"}, {"st": 13, "ed": 16, "text": "image to image"}, {"st": 20, "ed": 24, "text": "trained end to end"}, {"st": 42, "ed": 44, "text": "image representation"}, {"st": 55, "ed": 57, "text": "parameter sharing"}, {"st": 95, "ed": 97, "text": "fully convolutional"}, {"st": 108, "ed": 110, "text": "quantitative results"}, {"st": 126, "ed": 128, "text": "comparable results"}, {"st": 146, "ed": 148, "text": "post processing"}, {"st": 149, "ed": 151, "text": "task specific"}]
[{"st": 1, "ed": 3, "text": "recent years"}, {"st": 3, "ed": 6, "text": "the research community"}, {"st": 9, "ed": 12, "text": "deep neural networks"}, {"st": 14, "ed": 18, "text": "convolutional neural networks cnns"}, {"st": 20, "ed": 22, "text": "higher accuracy"}, {"st": 31, "ed": 33, "text": "machine learning"}]
[{"st": 10, "ed": 12, "text": "image classification"}, {"st": 13, "ed": 15, "text": "image classification"}, {"st": 26, "ed": 28, "text": "image classification"}, {"st": 45, "ed": 47, "text": "neural network"}, {"st": 64, "ed": 66, "text": "image classification"}, {"st": 124, "ed": 126, "text": "image classification"}, {"st": 139, "ed": 141, "text": "proposed approach"}, {"st": 148, "ed": 150, "text": "image classification"}]
[{"st": 16, "ed": 18, "text": "low level"}, {"st": 33, "ed": 35, "text": "higher level"}, {"st": 74, "ed": 76, "text": "outlier detection"}, {"st": 79, "ed": 81, "text": "self organizing"}, {"st": 115, "ed": 117, "text": "proposed method"}, {"st": 129, "ed": 131, "text": "low level"}, {"st": 138, "ed": 140, "text": "higher level"}, {"st": 148, "ed": 150, "text": "at large"}]
[{"st": 1, "ed": 3, "text": "deep learning"}, {"st": 6, "ed": 9, "text": "visual object recognition"}, {"st": 17, "ed": 19, "text": "training data"}, {"st": 54, "ed": 57, "text": "unsupervised feature learning"}, {"st": 67, "ed": 69, "text": "image patches"}, {"st": 105, "ed": 108, "text": "convolutional neural network"}, {"st": 115, "ed": 117, "text": "feature representation"}, {"st": 134, "ed": 136, "text": "feature learning"}, {"st": 142, "ed": 144, "text": "classification results"}, {"st": 151, "ed": 153, "text": "cifar 10"}]
[{"st": 5, "ed": 7, "text": "multi modal"}, {"st": 46, "ed": 48, "text": "supervised classification"}, {"st": 61, "ed": 63, "text": "deep learning"}, {"st": 68, "ed": 70, "text": "input output"}, {"st": 74, "ed": 77, "text": "restricted boltzmann machines"}, {"st": 80, "ed": 82, "text": "associative memory"}, {"st": 95, "ed": 97, "text": "mnist handwritten"}, {"st": 121, "ed": 123, "text": "multi modal"}, {"st": 132, "ed": 134, "text": "back propagation"}, {"st": 141, "ed": 143, "text": "bi directional"}, {"st": 145, "ed": 147, "text": "unsupervised learning"}]
[{"st": 7, "ed": 10, "text": "human pose estimation"}, {"st": 12, "ed": 14, "text": "multi layer"}, {"st": 14, "ed": 16, "text": "convolutional network"}, {"st": 24, "ed": 26, "text": "low level"}, {"st": 28, "ed": 30, "text": "higher level"}, {"st": 34, "ed": 37, "text": "human pose estimation"}, {"st": 44, "ed": 46, "text": "computer vision"}, {"st": 53, "ed": 55, "text": "shows significant"}, {"st": 65, "ed": 67, "text": "main contribution"}, {"st": 81, "ed": 83, "text": "deep learning"}, {"st": 113, "ed": 115, "text": "low level"}, {"st": 130, "ed": 132, "text": "higher level"}, {"st": 197, "ed": 199, "text": "speech recognition"}, {"st": 199, "ed": 201, "text": "object recognition"}]
[{"st": 4, "ed": 6, "text": "multi class"}, {"st": 6, "ed": 8, "text": "visual recognition"}, {"st": 51, "ed": 53, "text": "multi class"}, {"st": 157, "ed": 159, "text": "multi class"}]
[{"st": 8, "ed": 10, "text": "deep learning"}, {"st": 12, "ed": 14, "text": "image classification"}, {"st": 24, "ed": 27, "text": "principal component analysis"}, {"st": 139, "ed": 141, "text": "face verification"}, {"st": 149, "ed": 151, "text": "face recognition"}, {"st": 156, "ed": 158, "text": "hand written"}, {"st": 183, "ed": 185, "text": "hand crafted"}, {"st": 200, "ed": 202, "text": "classification tasks"}, {"st": 230, "ed": 232, "text": "highly competitive"}, {"st": 234, "ed": 236, "text": "texture classification"}]
[{"st": 1, "ed": 4, "text": "hand crafted features"}, {"st": 5, "ed": 7, "text": "metric learning"}, {"st": 31, "ed": 33, "text": "similarity metric"}, {"st": 34, "ed": 36, "text": "image pixels"}, {"st": 41, "ed": 44, "text": "deep neural network"}, {"st": 45, "ed": 47, "text": "proposed method"}, {"st": 48, "ed": 50, "text": "jointly learn"}, {"st": 121, "ed": 124, "text": "training and test"}, {"st": 129, "ed": 132, "text": "person re identification"}, {"st": 145, "ed": 147, "text": "proposed method"}]
[{"st": 0, "ed": 2, "text": "topic modeling"}, {"st": 4, "ed": 7, "text": "latent dirichlet allocation"}, {"st": 17, "ed": 19, "text": "multimodal data"}, {"st": 22, "ed": 24, "text": "image annotation"}, {"st": 31, "ed": 33, "text": "multimodal data"}, {"st": 35, "ed": 38, "text": "deep neural networks"}, {"st": 51, "ed": 53, "text": "topic model"}, {"st": 88, "ed": 90, "text": "multimodal data"}, {"st": 93, "ed": 95, "text": "image classification"}, {"st": 109, "ed": 111, "text": "discriminative power"}, {"st": 126, "ed": 128, "text": "joint representation"}, {"st": 130, "ed": 132, "text": "visual words"}, {"st": 135, "ed": 137, "text": "class label"}, {"st": 154, "ed": 157, "text": "compares favorably to"}, {"st": 184, "ed": 186, "text": "deep model"}]
[{"st": 12, "ed": 15, "text": "human pose estimation"}, {"st": 19, "ed": 21, "text": "convolutional network"}]
[{"st": 2, "ed": 6, "text": "convolutional neural networks cnn"}, {"st": 13, "ed": 15, "text": "computer vision"}, {"st": 51, "ed": 54, "text": "convolutional neural network"}, {"st": 59, "ed": 61, "text": "multi scale"}, {"st": 83, "ed": 85, "text": "unlike previous"}]
[{"st": 5, "ed": 9, "text": "convolutional neural networks cnns"}, {"st": 25, "ed": 27, "text": "building blocks"}, {"st": 75, "ed": 77, "text": "complex models"}, {"st": 78, "ed": 80, "text": "large datasets"}]
[{"st": 0, "ed": 2, "text": "deep learning"}, {"st": 12, "ed": 15, "text": "ability to learn"}, {"st": 31, "ed": 34, "text": "factors of variation"}, {"st": 44, "ed": 46, "text": "regularization terms"}, {"st": 52, "ed": 54, "text": "deep architectures"}, {"st": 59, "ed": 62, "text": "factors of variation"}, {"st": 95, "ed": 97, "text": "mnist handwritten"}, {"st": 120, "ed": 122, "text": "deep networks"}]
[{"st": 0, "ed": 3, "text": "convolutional neural networks"}, {"st": 6, "ed": 8, "text": "object recognition"}, {"st": 13, "ed": 15, "text": "recent advances"}, {"st": 15, "ed": 18, "text": "rectified linear units"}, {"st": 38, "ed": 41, "text": "unsupervised pre training"}, {"st": 56, "ed": 59, "text": "unsupervised pre training"}, {"st": 62, "ed": 64, "text": "recent advances"}, {"st": 85, "ed": 87, "text": "regularization techniques"}, {"st": 92, "ed": 95, "text": "unsupervised pre training"}, {"st": 102, "ed": 104, "text": "cifar 10"}, {"st": 122, "ed": 125, "text": "unsupervised pre training"}, {"st": 149, "ed": 152, "text": "unsupervised pre training"}]
[{"st": 4, "ed": 6, "text": "convolutional layer"}, {"st": 18, "ed": 20, "text": "image recognition"}, {"st": 74, "ed": 76, "text": "image processing"}, {"st": 91, "ed": 93, "text": "convolutional network"}]
[{"st": 2, "ed": 6, "text": "convolutional neural networks cnns"}, {"st": 8, "ed": 10, "text": "object recognition"}, {"st": 19, "ed": 21, "text": "max pooling"}, {"st": 40, "ed": 42, "text": "object recognition"}, {"st": 46, "ed": 48, "text": "convolutional networks"}, {"st": 60, "ed": 62, "text": "max pooling"}, {"st": 68, "ed": 70, "text": "convolutional layer"}, {"st": 74, "ed": 77, "text": "loss in accuracy"}, {"st": 79, "ed": 81, "text": "image recognition"}, {"st": 94, "ed": 96, "text": "network structures"}, {"st": 105, "ed": 107, "text": "convolutional layers"}, {"st": 118, "ed": 120, "text": "object recognition"}, {"st": 121, "ed": 125, "text": "cifar 10 cifar 100"}, {"st": 155, "ed": 157, "text": "network structures"}]
[{"st": 1, "ed": 5, "text": "convolutional neural network cnn"}, {"st": 6, "ed": 9, "text": "achieved great success"}, {"st": 30, "ed": 32, "text": "object detection"}, {"st": 77, "ed": 79, "text": "face detection"}, {"st": 98, "ed": 100, "text": "small scale"}, {"st": 110, "ed": 112, "text": "feature map"}, {"st": 147, "ed": 149, "text": "highly correlated"}]
[{"st": 14, "ed": 17, "text": "point of view"}, {"st": 28, "ed": 30, "text": "mobile robot"}, {"st": 43, "ed": 46, "text": "images and videos"}, {"st": 53, "ed": 58, "text": "deep convolutional neural network cnn"}, {"st": 63, "ed": 66, "text": "images and videos"}, {"st": 68, "ed": 70, "text": "rgb d"}, {"st": 97, "ed": 99, "text": "edge detection"}, {"st": 100, "ed": 102, "text": "qualitative results"}, {"st": 114, "ed": 116, "text": "frame level"}]
[{"st": 27, "ed": 29, "text": "neural networks"}, {"st": 33, "ed": 35, "text": "benchmark datasets"}, {"st": 35, "ed": 38, "text": "mnist cifar 10"}, {"st": 47, "ed": 49, "text": "floating point"}, {"st": 49, "ed": 51, "text": "fixed point"}, {"st": 86, "ed": 88, "text": "low precision"}]
[{"st": 0, "ed": 4, "text": "deep convolutional neural networks"}, {"st": 6, "ed": 8, "text": "recently shown"}, {"st": 16, "ed": 18, "text": "vision tasks"}, {"st": 20, "ed": 22, "text": "image classification"}, {"st": 33, "ed": 36, "text": "probabilistic graphical models"}, {"st": 47, "ed": 49, "text": "image segmentation"}, {"st": 74, "ed": 76, "text": "invariance properties"}, {"st": 91, "ed": 93, "text": "deep networks"}, {"st": 104, "ed": 106, "text": "fully connected"}, {"st": 106, "ed": 109, "text": "conditional random field"}, {"st": 122, "ed": 124, "text": "a level"}, {"st": 142, "ed": 145, "text": "pascal voc 2012"}, {"st": 146, "ed": 148, "text": "image segmentation"}, {"st": 186, "ed": 188, "text": "neural net"}]
[{"st": 28, "ed": 30, "text": "training data"}, {"st": 32, "ed": 34, "text": "deep convolutional"}, {"st": 34, "ed": 36, "text": "neural net"}, {"st": 40, "ed": 42, "text": "synthetic data"}, {"st": 48, "ed": 50, "text": "training data"}, {"st": 73, "ed": 75, "text": "low level"}, {"st": 86, "ed": 88, "text": "detailed analysis"}, {"st": 117, "ed": 119, "text": "fine tuned"}, {"st": 122, "ed": 124, "text": "detection task"}, {"st": 133, "ed": 135, "text": "low level"}, {"st": 141, "ed": 143, "text": "imagenet classification"}, {"st": 148, "ed": 150, "text": "low level"}, {"st": 160, "ed": 163, "text": "approach significantly outperforms"}, {"st": 163, "ed": 165, "text": "previous methods"}, {"st": 174, "ed": 176, "text": "few shot"}, {"st": 178, "ed": 180, "text": "improves performance"}, {"st": 182, "ed": 184, "text": "domain shift"}]
[{"st": 30, "ed": 32, "text": "fully connected"}, {"st": 46, "ed": 48, "text": "convolutional layers"}]
[{"st": 0, "ed": 3, "text": "extreme learning machine"}, {"st": 16, "ed": 18, "text": "pattern recognition"}, {"st": 28, "ed": 30, "text": "generalization ability"}, {"st": 36, "ed": 38, "text": "hidden neurons"}, {"st": 59, "ed": 62, "text": "extreme learning machines"}, {"st": 66, "ed": 68, "text": "hidden neurons"}, {"st": 78, "ed": 80, "text": "hidden nodes"}, {"st": 86, "ed": 88, "text": "hidden nodes"}, {"st": 110, "ed": 112, "text": "generalization ability"}, {"st": 128, "ed": 130, "text": "learning speed"}]
[{"st": 5, "ed": 7, "text": "contour detection"}, {"st": 19, "ed": 21, "text": "proposed approach"}, {"st": 25, "ed": 27, "text": "efficient implementation"}, {"st": 29, "ed": 33, "text": "convolutional neural networks cnns"}, {"st": 37, "ed": 39, "text": "feature vector"}, {"st": 55, "ed": 57, "text": "contour detection"}]
[{"st": 0, "ed": 2, "text": "deep learning"}, {"st": 4, "ed": 8, "text": "convolutional neural network cnn"}, {"st": 10, "ed": 12, "text": "promising results"}, {"st": 13, "ed": 15, "text": "face recognition"}, {"st": 20, "ed": 22, "text": "open question"}, {"st": 34, "ed": 36, "text": "existing works"}, {"st": 47, "ed": 49, "text": "face recognition"}, {"st": 65, "ed": 67, "text": "face recognition"}, {"st": 147, "ed": 149, "text": "learned features"}, {"st": 151, "ed": 153, "text": "significantly reduced"}, {"st": 157, "ed": 159, "text": "face recognition"}, {"st": 163, "ed": 165, "text": "metric learning"}, {"st": 168, "ed": 170, "text": "learned features"}, {"st": 196, "ed": 198, "text": "source code"}]
[{"st": 10, "ed": 12, "text": "image data"}, {"st": 13, "ed": 15, "text": "without resorting"}, {"st": 21, "ed": 23, "text": "similarity function"}, {"st": 25, "ed": 27, "text": "image patches"}, {"st": 36, "ed": 38, "text": "computer vision"}, {"st": 58, "ed": 60, "text": "wide variety"}, {"st": 73, "ed": 75, "text": "neural network"}, {"st": 90, "ed": 92, "text": "significantly outperform"}]
[{"st": 3, "ed": 5, "text": "main challenges"}, {"st": 6, "ed": 9, "text": "zero shot learning"}, {"st": 72, "ed": 75, "text": "fully connected layers"}, {"st": 77, "ed": 82, "text": "deep convolutional neural network cnn"}, {"st": 102, "ed": 104, "text": "embedding space"}, {"st": 120, "ed": 122, "text": "automatically generate"}, {"st": 141, "ed": 144, "text": "end to end"}, {"st": 158, "ed": 160, "text": "precision recall"}, {"st": 162, "ed": 164, "text": "empirical results"}, {"st": 169, "ed": 171, "text": "significantly outperforms"}]
[{"st": 5, "ed": 7, "text": "learning rate"}, {"st": 11, "ed": 13, "text": "hyper parameter"}, {"st": 29, "ed": 31, "text": "learning rate"}, {"st": 33, "ed": 35, "text": "learning rates"}, {"st": 58, "ed": 60, "text": "learning rate"}, {"st": 64, "ed": 66, "text": "learning rate"}, {"st": 75, "ed": 77, "text": "learning rates"}, {"st": 83, "ed": 85, "text": "classification accuracy"}, {"st": 109, "ed": 111, "text": "learning rate"}, {"st": 121, "ed": 123, "text": "learning rates"}, {"st": 127, "ed": 132, "text": "cifar 10 and cifar 100"}, {"st": 142, "ed": 144, "text": "imagenet dataset"}]
[{"st": 0, "ed": 2, "text": "neural networks"}, {"st": 4, "ed": 6, "text": "computationally intensive"}, {"st": 50, "ed": 52, "text": "neural networks"}, {"st": 100, "ed": 102, "text": "fine tune"}, {"st": 110, "ed": 112, "text": "imagenet dataset"}, {"st": 139, "ed": 141, "text": "vgg 16"}]
[{"st": 0, "ed": 4, "text": "convolutional neural networks cnn"}, {"st": 26, "ed": 28, "text": "labeled data"}, {"st": 51, "ed": 53, "text": "network architecture"}, {"st": 64, "ed": 66, "text": "convolutional layers"}, {"st": 67, "ed": 70, "text": "fully connected layers"}, {"st": 72, "ed": 74, "text": "deep learning"}, {"st": 94, "ed": 96, "text": "convolutional filters"}, {"st": 113, "ed": 116, "text": "discrete cosine transform"}, {"st": 120, "ed": 122, "text": "low cost"}, {"st": 122, "ed": 124, "text": "hash function"}]
[{"st": 0, "ed": 2, "text": "recent years"}, {"st": 9, "ed": 12, "text": "deep neural networks"}, {"st": 18, "ed": 21, "text": "convolutional neural networks"}, {"st": 157, "ed": 159, "text": "regularization methods"}, {"st": 171, "ed": 173, "text": "open source"}, {"st": 177, "ed": 179, "text": "pre trained"}]
[{"st": 9, "ed": 11, "text": "error rate"}, {"st": 32, "ed": 34, "text": "hidden layers"}, {"st": 40, "ed": 42, "text": "graphics card"}]
[{"st": 19, "ed": 22, "text": "fully connected layers"}, {"st": 23, "ed": 25, "text": "neural networks"}, {"st": 41, "ed": 44, "text": "fully connected layers"}, {"st": 48, "ed": 51, "text": "fully connected layers"}, {"st": 53, "ed": 55, "text": "weight matrices"}, {"st": 62, "ed": 64, "text": "linear combinations"}, {"st": 93, "ed": 95, "text": "total number"}, {"st": 106, "ed": 108, "text": "low rank"}, {"st": 115, "ed": 117, "text": "total number"}, {"st": 134, "ed": 136, "text": "fully connected"}, {"st": 180, "ed": 182, "text": "mnist svhn"}, {"st": 184, "ed": 186, "text": "chinese character"}]
[{"st": 0, "ed": 2, "text": "deep learning"}, {"st": 8, "ed": 10, "text": "large datasets"}, {"st": 13, "ed": 15, "text": "training examples"}, {"st": 19, "ed": 21, "text": "real world"}, {"st": 39, "ed": 41, "text": "deep learning"}, {"st": 44, "ed": 46, "text": "object instances"}, {"st": 59, "ed": 62, "text": "feedforward neural networks"}, {"st": 120, "ed": 122, "text": "deep learning"}, {"st": 128, "ed": 130, "text": "training data"}, {"st": 136, "ed": 138, "text": "multi view"}]
[{"st": 2, "ed": 4, "text": "word representations"}, {"st": 6, "ed": 8, "text": "neural network"}, {"st": 60, "ed": 62, "text": "deep learning"}, {"st": 137, "ed": 139, "text": "prior knowledge"}]
[{"st": 1, "ed": 4, "text": "feedforward neural networks"}, {"st": 24, "ed": 26, "text": "deep learning"}, {"st": 29, "ed": 31, "text": "linear activation"}, {"st": 59, "ed": 61, "text": "training process"}, {"st": 63, "ed": 65, "text": "avoid overfitting"}, {"st": 68, "ed": 70, "text": "regularization scheme"}, {"st": 123, "ed": 125, "text": "linear activation"}, {"st": 133, "ed": 135, "text": "piecewise linear"}, {"st": 141, "ed": 143, "text": "linear activation"}, {"st": 149, "ed": 151, "text": "significant reduction"}, {"st": 186, "ed": 189, "text": "feedforward neural networks"}, {"st": 208, "ed": 210, "text": "activation function"}, {"st": 230, "ed": 232, "text": "deep learning"}, {"st": 255, "ed": 257, "text": "classification results"}, {"st": 271, "ed": 275, "text": "cifar 10 cifar 100"}]
[{"st": 9, "ed": 11, "text": "recent years"}, {"st": 18, "ed": 22, "text": "convolutional neural networks cnns"}, {"st": 23, "ed": 25, "text": "improve performance"}, {"st": 27, "ed": 29, "text": "expression recognition"}, {"st": 52, "ed": 54, "text": "strong performance"}, {"st": 80, "ed": 82, "text": "facial expression"}, {"st": 95, "ed": 97, "text": "expression recognition"}, {"st": 129, "ed": 131, "text": "convolutional layers"}, {"st": 136, "ed": 138, "text": "facial action"}]
[{"st": 35, "ed": 37, "text": "similarity measure"}, {"st": 39, "ed": 41, "text": "image patches"}, {"st": 52, "ed": 54, "text": "supervised manner"}, {"st": 73, "ed": 75, "text": "network architectures"}, {"st": 90, "ed": 93, "text": "convolutional neural network"}, {"st": 104, "ed": 106, "text": "post processing"}]
[{"st": 0, "ed": 2, "text": "convolutional networks"}, {"st": 26, "ed": 28, "text": "parallel algorithm"}, {"st": 80, "ed": 82, "text": "shared memory"}, {"st": 136, "ed": 138, "text": "multi core"}, {"st": 178, "ed": 180, "text": "network architectures"}, {"st": 230, "ed": 232, "text": "network architecture"}]
[{"st": 7, "ed": 9, "text": "fully convolutional"}, {"st": 9, "ed": 11, "text": "neural network"}, {"st": 27, "ed": 29, "text": "encoder network"}, {"st": 31, "ed": 33, "text": "decoder network"}, {"st": 44, "ed": 46, "text": "encoder network"}, {"st": 52, "ed": 54, "text": "convolutional layers"}, {"st": 62, "ed": 64, "text": "decoder network"}, {"st": 68, "ed": 70, "text": "low resolution"}, {"st": 71, "ed": 73, "text": "feature maps"}, {"st": 77, "ed": 79, "text": "feature maps"}, {"st": 101, "ed": 103, "text": "feature map"}, {"st": 114, "ed": 116, "text": "max pooling"}, {"st": 158, "ed": 160, "text": "widely adopted"}, {"st": 191, "ed": 193, "text": "scene understanding"}, {"st": 214, "ed": 216, "text": "significantly smaller"}, {"st": 220, "ed": 222, "text": "trainable parameters"}, {"st": 243, "ed": 245, "text": "rgb d"}, {"st": 262, "ed": 264, "text": "efficient inference"}]
[{"st": 11, "ed": 13, "text": "object recognition"}, {"st": 26, "ed": 28, "text": "vision tasks"}, {"st": 42, "ed": 45, "text": "visual question answering"}, {"st": 59, "ed": 61, "text": "previous works"}, {"st": 95, "ed": 97, "text": "image regions"}, {"st": 136, "ed": 138, "text": "multiple choice"}, {"st": 147, "ed": 149, "text": "baseline models"}]
[{"st": 16, "ed": 18, "text": "feature extraction"}, {"st": 26, "ed": 28, "text": "image representation"}, {"st": 51, "ed": 53, "text": "deep learning"}, {"st": 55, "ed": 57, "text": "jointly learns"}, {"st": 92, "ed": 96, "text": "convolutional neural network cnn"}, {"st": 98, "ed": 100, "text": "fully connected"}, {"st": 100, "ed": 102, "text": "neural network"}, {"st": 163, "ed": 165, "text": "classification error"}, {"st": 224, "ed": 226, "text": "impressive results"}]
[{"st": 0, "ed": 4, "text": "deep recurrent neural network"}, {"st": 16, "ed": 18, "text": "spatio temporal"}, {"st": 25, "ed": 27, "text": "computer vision"}, {"st": 39, "ed": 41, "text": "spatio temporal"}, {"st": 56, "ed": 58, "text": "real world"}, {"st": 73, "ed": 75, "text": "spatio temporal"}, {"st": 81, "ed": 85, "text": "recurrent neural networks rnns"}, {"st": 95, "ed": 97, "text": "spatio temporal"}, {"st": 106, "ed": 108, "text": "fully differentiable"}, {"st": 112, "ed": 114, "text": "proposed method"}, {"st": 126, "ed": 128, "text": "spatio temporal"}, {"st": 142, "ed": 144, "text": "proposed approach"}, {"st": 184, "ed": 186, "text": "spatio temporal"}]
[{"st": 0, "ed": 3, "text": "deep neural networks"}, {"st": 17, "ed": 19, "text": "machine learning"}, {"st": 22, "ed": 24, "text": "recent works"}, {"st": 56, "ed": 58, "text": "neural network"}, {"st": 86, "ed": 88, "text": "total number"}, {"st": 113, "ed": 115, "text": "large networks"}]
[{"st": 39, "ed": 41, "text": "low resolution"}, {"st": 98, "ed": 100, "text": "image compression"}, {"st": 122, "ed": 124, "text": "neural networks"}, {"st": 128, "ed": 130, "text": "image compression"}, {"st": 145, "ed": 147, "text": "input image"}, {"st": 169, "ed": 171, "text": "image reconstruction"}, {"st": 195, "ed": 197, "text": "large scale"}, {"st": 205, "ed": 207, "text": "based approaches"}, {"st": 209, "ed": 211, "text": "visual quality"}]
[{"st": 0, "ed": 3, "text": "convolutional neural networks"}, {"st": 40, "ed": 42, "text": "trained models"}, {"st": 88, "ed": 91, "text": "propose and evaluate"}, {"st": 95, "ed": 97, "text": "parameter sharing"}, {"st": 131, "ed": 135, "text": "trained end to end"}, {"st": 140, "ed": 142, "text": "significantly higher"}]
[{"st": 7, "ed": 9, "text": "computationally efficient"}, {"st": 9, "ed": 13, "text": "convolutional neural networks cnns"}, {"st": 15, "ed": 17, "text": "low rank"}, {"st": 69, "ed": 71, "text": "weight initialization"}, {"st": 82, "ed": 84, "text": "convolutional layers"}, {"st": 122, "ed": 124, "text": "higher accuracy"}, {"st": 144, "ed": 146, "text": "max pooling"}, {"st": 147, "ed": 149, "text": "achieve comparable"}, {"st": 186, "ed": 188, "text": "top 5"}, {"st": 217, "ed": 219, "text": "comparable accuracy"}, {"st": 243, "ed": 245, "text": "comparable accuracy"}]
[{"st": 8, "ed": 10, "text": "neural network"}, {"st": 16, "ed": 18, "text": "deep network"}, {"st": 86, "ed": 88, "text": "deep networks"}, {"st": 92, "ed": 94, "text": "conventional methods"}, {"st": 124, "ed": 126, "text": "mnist dataset"}, {"st": 131, "ed": 133, "text": "cifar 10"}, {"st": 146, "ed": 148, "text": "imagenet dataset"}, {"st": 158, "ed": 160, "text": "vgg 16"}]
[{"st": 4, "ed": 6, "text": "object localization"}, {"st": 20, "ed": 22, "text": "object detection"}, {"st": 33, "ed": 35, "text": "bounding box"}, {"st": 51, "ed": 53, "text": "conditional probabilities"}, {"st": 55, "ed": 58, "text": "row and column"}, {"st": 88, "ed": 90, "text": "bounding box"}, {"st": 105, "ed": 108, "text": "convolutional neural network"}, {"st": 126, "ed": 128, "text": "significant improvement"}, {"st": 154, "ed": 156, "text": "object detection"}, {"st": 171, "ed": 173, "text": "achieve high"}]
[{"st": 10, "ed": 14, "text": "deep convolutional neural networks"}, {"st": 30, "ed": 32, "text": "pooling layers"}, {"st": 40, "ed": 42, "text": "max pooling"}, {"st": 52, "ed": 54, "text": "multinomial distribution"}, {"st": 74, "ed": 76, "text": "max pooling"}, {"st": 84, "ed": 86, "text": "empirical evidence"}, {"st": 125, "ed": 127, "text": "max pooling"}, {"st": 128, "ed": 131, "text": "fully connected layers"}, {"st": 142, "ed": 144, "text": "competitive results"}, {"st": 145, "ed": 150, "text": "cifar 10 and cifar 100"}, {"st": 160, "ed": 162, "text": "max pooling"}]
[{"st": 12, "ed": 14, "text": "decision making"}, {"st": 57, "ed": 59, "text": "face images"}, {"st": 68, "ed": 72, "text": "convolutional neural network cnn"}, {"st": 111, "ed": 113, "text": "important features"}, {"st": 126, "ed": 129, "text": "positive and negative"}]
[{"st": 10, "ed": 14, "text": "deep convolutional neural networks"}, {"st": 28, "ed": 30, "text": "pooling layers"}, {"st": 38, "ed": 40, "text": "max pooling"}, {"st": 50, "ed": 52, "text": "multinomial distribution"}, {"st": 72, "ed": 74, "text": "max pooling"}, {"st": 82, "ed": 84, "text": "empirical evidence"}, {"st": 94, "ed": 96, "text": "max pooling"}]
[{"st": 6, "ed": 9, "text": "convolutional auto encoder"}, {"st": 12, "ed": 14, "text": "deep learning"}, {"st": 37, "ed": 40, "text": "convolutional auto encoder"}, {"st": 54, "ed": 56, "text": "comparable accuracy"}, {"st": 57, "ed": 59, "text": "dimensionality reduction"}, {"st": 64, "ed": 66, "text": "auto encoder"}]
[{"st": 9, "ed": 11, "text": "problems including"}, {"st": 11, "ed": 13, "text": "image segmentation"}, {"st": 19, "ed": 21, "text": "affinity matrix"}, {"st": 34, "ed": 36, "text": "complex valued"}, {"st": 45, "ed": 49, "text": "convolutional neural network cnn"}, {"st": 78, "ed": 80, "text": "experiments demonstrate"}, {"st": 88, "ed": 90, "text": "prior works"}, {"st": 97, "ed": 99, "text": "edge detection"}, {"st": 117, "ed": 120, "text": "conditional random field"}]
[{"st": 49, "ed": 51, "text": "neural network"}, {"st": 55, "ed": 57, "text": "feature space"}, {"st": 80, "ed": 82, "text": "pattern recognition"}, {"st": 109, "ed": 111, "text": "neural networks"}, {"st": 112, "ed": 114, "text": "deep learning"}, {"st": 136, "ed": 138, "text": "classification methods"}]
[{"st": 0, "ed": 3, "text": "deep neural networks"}, {"st": 10, "ed": 13, "text": "large training sets"}, {"st": 18, "ed": 22, "text": "number of training samples"}, {"st": 29, "ed": 31, "text": "prior methods"}, {"st": 36, "ed": 38, "text": "weight decay"}, {"st": 104, "ed": 106, "text": "weight decay"}, {"st": 126, "ed": 128, "text": "rademacher complexity"}, {"st": 133, "ed": 135, "text": "generalization error"}, {"st": 137, "ed": 139, "text": "neural network"}, {"st": 167, "ed": 169, "text": "benchmark datasets"}, {"st": 171, "ed": 173, "text": "theoretical analysis"}, {"st": 178, "ed": 182, "text": "number of training samples"}, {"st": 188, "ed": 190, "text": "significantly improve"}]
[{"st": 5, "ed": 7, "text": "deep learning"}, {"st": 20, "ed": 22, "text": "visual recognition"}, {"st": 22, "ed": 24, "text": "speech recognition"}, {"st": 25, "ed": 27, "text": "natural language"}, {"st": 32, "ed": 35, "text": "deep neural networks"}, {"st": 35, "ed": 38, "text": "convolutional neural networks"}, {"st": 53, "ed": 55, "text": "annotated data"}, {"st": 63, "ed": 65, "text": "graphics processor"}, {"st": 69, "ed": 72, "text": "convolutional neural networks"}, {"st": 96, "ed": 98, "text": "recent advances"}, {"st": 114, "ed": 116, "text": "activation function"}, {"st": 116, "ed": 118, "text": "loss function"}, {"st": 130, "ed": 133, "text": "convolutional neural networks"}, {"st": 134, "ed": 136, "text": "computer vision"}, {"st": 138, "ed": 140, "text": "natural language"}]
[{"st": 19, "ed": 21, "text": "visual perception"}, {"st": 27, "ed": 29, "text": "deep networks"}, {"st": 123, "ed": 125, "text": "task specific"}, {"st": 129, "ed": 131, "text": "performance improvement"}]
[{"st": 27, "ed": 29, "text": "deep learning"}, {"st": 44, "ed": 46, "text": "codling moth"}, {"st": 50, "ed": 52, "text": "promising performance"}, {"st": 58, "ed": 60, "text": "previous attempts"}]
[{"st": 108, "ed": 111, "text": "deep neural networks"}, {"st": 115, "ed": 118, "text": "quantitatively and qualitatively"}]
[{"st": 4, "ed": 7, "text": "end to end"}, {"st": 12, "ed": 14, "text": "early detection"}, {"st": 46, "ed": 48, "text": "gas turbine"}, {"st": 130, "ed": 132, "text": "training scheme"}, {"st": 160, "ed": 162, "text": "deep learning"}, {"st": 168, "ed": 170, "text": "early detection"}]
[{"st": 25, "ed": 27, "text": "classification tasks"}, {"st": 57, "ed": 59, "text": "pre defined"}, {"st": 110, "ed": 113, "text": "end to end"}, {"st": 114, "ed": 116, "text": "gradient based"}, {"st": 123, "ed": 125, "text": "handwritten digits"}, {"st": 127, "ed": 129, "text": "classification performance"}]
[{"st": 1, "ed": 4, "text": "deep convolutional networks"}, {"st": 35, "ed": 37, "text": "deep networks"}, {"st": 70, "ed": 72, "text": "training procedure"}, {"st": 84, "ed": 86, "text": "deep networks"}, {"st": 93, "ed": 95, "text": "deep networks"}, {"st": 155, "ed": 157, "text": "residual networks"}, {"st": 171, "ed": 173, "text": "cifar 10"}]
[{"st": 0, "ed": 2, "text": "deep learning"}, {"st": 6, "ed": 8, "text": "machine learning"}, {"st": 9, "ed": 11, "text": "deep learning"}, {"st": 18, "ed": 20, "text": "multiple layers"}, {"st": 28, "ed": 30, "text": "large scale"}, {"st": 46, "ed": 49, "text": "support vector machines"}, {"st": 61, "ed": 63, "text": "deep learning"}, {"st": 70, "ed": 72, "text": "deep learning"}, {"st": 76, "ed": 78, "text": "deep learning"}, {"st": 82, "ed": 85, "text": "deep neural network"}, {"st": 86, "ed": 89, "text": "deep belief network"}, {"st": 92, "ed": 94, "text": "neural network"}, {"st": 106, "ed": 108, "text": "deep learning"}]
[{"st": 0, "ed": 4, "text": "convolutional neural networks cnn"}, {"st": 14, "ed": 16, "text": "computer vision"}, {"st": 26, "ed": 29, "text": "deep neural networks"}, {"st": 30, "ed": 32, "text": "capture complex"}, {"st": 63, "ed": 65, "text": "embedded devices"}, {"st": 78, "ed": 80, "text": "deep learning"}, {"st": 120, "ed": 123, "text": "deep neural networks"}, {"st": 139, "ed": 141, "text": "deep networks"}, {"st": 174, "ed": 176, "text": "network parameters"}, {"st": 213, "ed": 215, "text": "achieve high"}, {"st": 220, "ed": 223, "text": "deep neural networks"}, {"st": 225, "ed": 227, "text": "time consuming"}]
[{"st": 5, "ed": 7, "text": "transfer learning"}, {"st": 20, "ed": 22, "text": "vision tasks"}, {"st": 33, "ed": 35, "text": "pre trained"}, {"st": 48, "ed": 50, "text": "transfer learning"}, {"st": 54, "ed": 56, "text": "pre trained"}, {"st": 68, "ed": 70, "text": "neural networks"}, {"st": 77, "ed": 79, "text": "transfer learning"}, {"st": 111, "ed": 113, "text": "pre trained"}, {"st": 127, "ed": 129, "text": "fine tuning"}, {"st": 146, "ed": 148, "text": "technique called"}, {"st": 149, "ed": 151, "text": "fine tuning"}]
[{"st": 16, "ed": 20, "text": "generative adversarial networks gans"}, {"st": 28, "ed": 31, "text": "semi supervised learning"}, {"st": 45, "ed": 47, "text": "generative models"}, {"st": 91, "ed": 93, "text": "semi supervised"}, {"st": 95, "ed": 98, "text": "mnist cifar 10"}, {"st": 101, "ed": 103, "text": "generated images"}, {"st": 124, "ed": 126, "text": "real data"}, {"st": 127, "ed": 129, "text": "cifar 10"}, {"st": 133, "ed": 135, "text": "human error"}]
[{"st": 32, "ed": 34, "text": "generative models"}, {"st": 52, "ed": 54, "text": "feature spaces"}, {"st": 92, "ed": 94, "text": "multiple layers"}, {"st": 136, "ed": 138, "text": "highly competitive"}, {"st": 158, "ed": 160, "text": "image representations"}, {"st": 178, "ed": 181, "text": "deep network architecture"}, {"st": 182, "ed": 185, "text": "sheds light on"}]
[{"st": 4, "ed": 6, "text": "neural networks"}, {"st": 30, "ed": 32, "text": "deep architecture"}, {"st": 43, "ed": 45, "text": "neural network"}, {"st": 166, "ed": 168, "text": "vgg 16"}, {"st": 172, "ed": 174, "text": "achieve high"}, {"st": 174, "ed": 176, "text": "compression ratio"}, {"st": 183, "ed": 185, "text": "higher accuracy"}]
[{"st": 1, "ed": 3, "text": "based approaches"}, {"st": 11, "ed": 13, "text": "optical flow"}, {"st": 33, "ed": 35, "text": "optical flow"}, {"st": 126, "ed": 128, "text": "low pass"}, {"st": 130, "ed": 132, "text": "feature maps"}, {"st": 144, "ed": 146, "text": "competitive performance"}]
[{"st": 3, "ed": 5, "text": "key challenges"}, {"st": 6, "ed": 8, "text": "machine learning"}, {"st": 12, "ed": 14, "text": "computationally efficient"}, {"st": 14, "ed": 16, "text": "multi class"}, {"st": 37, "ed": 39, "text": "large scale"}, {"st": 39, "ed": 41, "text": "image classification"}, {"st": 49, "ed": 51, "text": "visual attention"}, {"st": 54, "ed": 56, "text": "attention model"}, {"st": 66, "ed": 68, "text": "visual processing"}, {"st": 87, "ed": 89, "text": "optimization procedure"}, {"st": 122, "ed": 124, "text": "attention model"}, {"st": 133, "ed": 135, "text": "decision boundary"}, {"st": 144, "ed": 146, "text": "proposed framework"}, {"st": 165, "ed": 167, "text": "tree based"}, {"st": 169, "ed": 171, "text": "significantly lower"}]
[{"st": 10, "ed": 13, "text": "deep neural networks"}, {"st": 65, "ed": 67, "text": "feature maps"}, {"st": 85, "ed": 89, "text": "trained end to end"}, {"st": 117, "ed": 119, "text": "object detection"}, {"st": 120, "ed": 122, "text": "image retrieval"}, {"st": 126, "ed": 128, "text": "significantly faster"}]
[{"st": 0, "ed": 2, "text": "deep learning"}, {"st": 16, "ed": 18, "text": "deep models"}, {"st": 45, "ed": 47, "text": "method called"}, {"st": 66, "ed": 68, "text": "previous methods"}, {"st": 108, "ed": 110, "text": "accuracy loss"}, {"st": 152, "ed": 156, "text": "available at https github.com"}]
[{"st": 4, "ed": 6, "text": "challenging problem"}, {"st": 63, "ed": 66, "text": "trained from scratch"}, {"st": 99, "ed": 101, "text": "mutual information"}]
[{"st": 11, "ed": 13, "text": "higher order"}, {"st": 23, "ed": 25, "text": "visual attention"}, {"st": 90, "ed": 92, "text": "outperforms previous"}]
[{"st": 4, "ed": 7, "text": "deep neural networks"}, {"st": 33, "ed": 35, "text": "convolutional layers"}, {"st": 58, "ed": 60, "text": "network parameters"}, {"st": 84, "ed": 86, "text": "network parameters"}, {"st": 94, "ed": 99, "text": "cifar 10 and cifar 100"}, {"st": 110, "ed": 112, "text": "imagenet dataset"}]
[{"st": 13, "ed": 15, "text": "deep network"}, {"st": 37, "ed": 39, "text": "computation cost"}, {"st": 86, "ed": 88, "text": "deep network"}, {"st": 100, "ed": 102, "text": "group sparsity"}]
[{"st": 6, "ed": 10, "text": "convolutional neural networks cnns"}, {"st": 22, "ed": 24, "text": "network layers"}, {"st": 26, "ed": 28, "text": "scale space"}, {"st": 41, "ed": 43, "text": "convolutional filters"}, {"st": 86, "ed": 88, "text": "receptive field"}, {"st": 89, "ed": 91, "text": "grows exponentially"}, {"st": 123, "ed": 125, "text": "experiments demonstrate"}, {"st": 132, "ed": 135, "text": "cifar and imagenet"}, {"st": 135, "ed": 137, "text": "classification tasks"}]
[{"st": 1, "ed": 3, "text": "style transfer"}, {"st": 25, "ed": 27, "text": "style transfer"}, {"st": 47, "ed": 49, "text": "style transfer"}, {"st": 54, "ed": 56, "text": "domain adaptation"}, {"st": 67, "ed": 69, "text": "feature maps"}, {"st": 74, "ed": 78, "text": "maximum mean discrepancy mmd"}, {"st": 92, "ed": 94, "text": "style transfer"}]
[{"st": 10, "ed": 12, "text": "deep convolutional"}, {"st": 12, "ed": 14, "text": "auto encoder"}, {"st": 17, "ed": 19, "text": "deep learning"}, {"st": 37, "ed": 40, "text": "convolutional auto encoder"}, {"st": 55, "ed": 57, "text": "auto encoder"}, {"st": 58, "ed": 61, "text": "encoder and decoder"}, {"st": 74, "ed": 76, "text": "dimensionality reduction"}, {"st": 77, "ed": 79, "text": "unsupervised clustering"}, {"st": 97, "ed": 99, "text": "linear classifier"}, {"st": 118, "ed": 120, "text": "pooling layers"}, {"st": 152, "ed": 154, "text": "deep convolutional"}, {"st": 154, "ed": 156, "text": "auto encoder"}, {"st": 161, "ed": 163, "text": "deep learning"}, {"st": 182, "ed": 185, "text": "deep neural network"}]
[{"st": 0, "ed": 2, "text": "recent advances"}, {"st": 3, "ed": 5, "text": "optimization methods"}, {"st": 8, "ed": 12, "text": "convolutional neural networks cnns"}, {"st": 82, "ed": 84, "text": "geometric properties"}, {"st": 95, "ed": 97, "text": "theoretical results"}, {"st": 118, "ed": 120, "text": "convergence properties"}, {"st": 124, "ed": 126, "text": "geometric properties"}, {"st": 140, "ed": 144, "text": "cifar 10 cifar 100"}, {"st": 153, "ed": 155, "text": "step size"}, {"st": 162, "ed": 164, "text": "training loss"}, {"st": 165, "ed": 167, "text": "convergence properties"}, {"st": 173, "ed": 175, "text": "classification performance"}]
[{"st": 0, "ed": 3, "text": "deep neural network"}, {"st": 4, "ed": 7, "text": "difficult to train"}, {"st": 101, "ed": 106, "text": "deep convolutional neural network cnn"}, {"st": 136, "ed": 138, "text": "cifar 10"}, {"st": 163, "ed": 165, "text": "network structure"}]
[{"st": 6, "ed": 9, "text": "recurrent neural networks"}, {"st": 15, "ed": 17, "text": "impressive results"}, {"st": 21, "ed": 23, "text": "application areas"}, {"st": 24, "ed": 27, "text": "visual question answering"}, {"st": 27, "ed": 31, "text": "part of speech tagging"}, {"st": 47, "ed": 49, "text": "application areas"}, {"st": 89, "ed": 91, "text": "network architecture"}, {"st": 131, "ed": 133, "text": "temporal dependencies"}, {"st": 157, "ed": 159, "text": "practical problems"}]
[{"st": 6, "ed": 8, "text": "point cloud"}, {"st": 25, "ed": 27, "text": "deep learning"}, {"st": 38, "ed": 43, "text": "deep convolutional neural networks cnns"}, {"st": 51, "ed": 53, "text": "performance improvements"}, {"st": 62, "ed": 64, "text": "de facto"}, {"st": 69, "ed": 74, "text": "computer vision and machine learning"}, {"st": 78, "ed": 80, "text": "object detection"}, {"st": 93, "ed": 95, "text": "point cloud"}, {"st": 105, "ed": 107, "text": "massive data"}, {"st": 126, "ed": 128, "text": "deep learning"}, {"st": 140, "ed": 142, "text": "point clouds"}, {"st": 188, "ed": 190, "text": "point clouds"}, {"st": 210, "ed": 212, "text": "baseline method"}, {"st": 230, "ed": 232, "text": "deep learning"}, {"st": 235, "ed": 237, "text": "point cloud"}]
[{"st": 2, "ed": 4, "text": "unsupervised learning"}, {"st": 7, "ed": 9, "text": "generative modeling"}, {"st": 39, "ed": 41, "text": "generated samples"}, {"st": 50, "ed": 52, "text": "discriminative learning"}, {"st": 52, "ed": 54, "text": "desirable properties"}, {"st": 82, "ed": 84, "text": "encouraging results"}, {"st": 88, "ed": 90, "text": "applications including"}]
[{"st": 8, "ed": 10, "text": "computational performance"}, {"st": 18, "ed": 21, "text": "deep neural networks"}, {"st": 27, "ed": 29, "text": "neural networks"}, {"st": 48, "ed": 52, "text": "convolutional neural networks cnns"}, {"st": 139, "ed": 141, "text": "matrix multiplication"}, {"st": 161, "ed": 163, "text": "significantly faster"}, {"st": 169, "ed": 171, "text": "neural networks"}, {"st": 173, "ed": 176, "text": "orders of magnitude"}]
[{"st": 1, "ed": 3, "text": "neural networks"}, {"st": 24, "ed": 26, "text": "significantly improve"}, {"st": 44, "ed": 46, "text": "deep learning"}, {"st": 48, "ed": 50, "text": "low power"}, {"st": 54, "ed": 56, "text": "open source"}, {"st": 80, "ed": 82, "text": "standard library"}, {"st": 110, "ed": 112, "text": "extensive experiments"}, {"st": 130, "ed": 132, "text": "pre trained"}, {"st": 133, "ed": 135, "text": "deep models"}, {"st": 140, "ed": 142, "text": "https github.com"}]
[{"st": 13, "ed": 15, "text": "convolutional layers"}, {"st": 32, "ed": 34, "text": "adjacency matrix"}, {"st": 37, "ed": 39, "text": "weight sharing"}, {"st": 44, "ed": 46, "text": "underlying structure"}, {"st": 87, "ed": 89, "text": "image datasets"}]
[{"st": 6, "ed": 10, "text": "deep neural network architecture"}, {"st": 27, "ed": 29, "text": "computer vision"}, {"st": 30, "ed": 32, "text": "natural language"}, {"st": 98, "ed": 100, "text": "generalization ability"}, {"st": 131, "ed": 133, "text": "metric learning"}, {"st": 140, "ed": 143, "text": "learning to rank"}, {"st": 146, "ed": 148, "text": "metric learning"}, {"st": 157, "ed": 159, "text": "metric learning"}, {"st": 159, "ed": 161, "text": "tasks including"}, {"st": 164, "ed": 166, "text": "image retrieval"}, {"st": 180, "ed": 182, "text": "baseline method"}]
[{"st": 0, "ed": 3, "text": "deep neural networks"}, {"st": 28, "ed": 30, "text": "improved performance"}, {"st": 35, "ed": 37, "text": "deep network"}, {"st": 79, "ed": 82, "text": "deep network architecture"}, {"st": 93, "ed": 95, "text": "prediction results"}, {"st": 159, "ed": 161, "text": "network layers"}, {"st": 195, "ed": 197, "text": "significantly reduce"}]
[{"st": 3, "ed": 5, "text": "natural images"}, {"st": 8, "ed": 10, "text": "problem involving"}, {"st": 13, "ed": 17, "text": "deep convolutional neural networks"}, {"st": 19, "ed": 21, "text": "large datasets"}, {"st": 44, "ed": 46, "text": "deep networks"}, {"st": 67, "ed": 70, "text": "deep convolutional network"}, {"st": 100, "ed": 102, "text": "cifar 100"}, {"st": 119, "ed": 121, "text": "convolutional networks"}, {"st": 126, "ed": 128, "text": "feature extractors"}]
[{"st": 5, "ed": 7, "text": "computer vision"}, {"st": 46, "ed": 48, "text": "pose estimation"}, {"st": 87, "ed": 89, "text": "internal representation"}, {"st": 91, "ed": 93, "text": "multi task"}, {"st": 111, "ed": 113, "text": "pose estimation"}, {"st": 120, "ed": 122, "text": "fine tuning"}, {"st": 131, "ed": 133, "text": "pose estimation"}, {"st": 140, "ed": 142, "text": "supervised tasks"}, {"st": 166, "ed": 168, "text": "learned features"}, {"st": 174, "ed": 176, "text": "pose estimation"}, {"st": 186, "ed": 188, "text": "supervised tasks"}, {"st": 196, "ed": 198, "text": "large scale"}, {"st": 224, "ed": 226, "text": "open research"}]
[{"st": 19, "ed": 22, "text": "end to end"}, {"st": 30, "ed": 33, "text": "fully convolutional network"}, {"st": 67, "ed": 69, "text": "adversarial training"}, {"st": 81, "ed": 83, "text": "multi scale"}, {"st": 104, "ed": 107, "text": "end to end"}, {"st": 118, "ed": 120, "text": "https github.com"}]
[{"st": 1, "ed": 3, "text": "neural networks"}, {"st": 14, "ed": 16, "text": "becoming increasingly"}, {"st": 19, "ed": 21, "text": "network quantization"}, {"st": 36, "ed": 38, "text": "deep networks"}, {"st": 47, "ed": 50, "text": "vanishing and exploding"}, {"st": 89, "ed": 91, "text": "hidden units"}, {"st": 98, "ed": 100, "text": "discrete optimization"}, {"st": 108, "ed": 110, "text": "discrete optimization"}, {"st": 127, "ed": 129, "text": "linearly separable"}, {"st": 187, "ed": 189, "text": "classification accuracy"}]
[{"st": 0, "ed": 3, "text": "multi task learning"}, {"st": 5, "ed": 7, "text": "neural networks"}, {"st": 12, "ed": 14, "text": "improve performance"}, {"st": 38, "ed": 40, "text": "neural network"}, {"st": 50, "ed": 52, "text": "self organizing"}, {"st": 52, "ed": 54, "text": "neural network"}, {"st": 75, "ed": 77, "text": "neural network"}, {"st": 80, "ed": 82, "text": "fully connected"}, {"st": 136, "ed": 138, "text": "multi agent"}, {"st": 138, "ed": 140, "text": "reinforcement learning"}, {"st": 155, "ed": 157, "text": "cross stitch"}, {"st": 163, "ed": 165, "text": "multi task"}, {"st": 172, "ed": 174, "text": "cifar 100"}, {"st": 176, "ed": 178, "text": "experiments demonstrate"}, {"st": 179, "ed": 181, "text": "significant improvement"}, {"st": 198, "ed": 200, "text": "cross stitch"}, {"st": 209, "ed": 211, "text": "cifar 100"}, {"st": 215, "ed": 217, "text": "cross stitch"}]
[{"st": 0, "ed": 2, "text": "deep learning"}, {"st": 11, "ed": 13, "text": "computer vision"}, {"st": 15, "ed": 17, "text": "image classification"}, {"st": 44, "ed": 46, "text": "resource constrained"}, {"st": 51, "ed": 53, "text": "deep networks"}, {"st": 63, "ed": 65, "text": "low precision"}, {"st": 105, "ed": 107, "text": "low precision"}, {"st": 110, "ed": 112, "text": "significantly improved"}]
[{"st": 0, "ed": 2, "text": "deep learning"}, {"st": 9, "ed": 11, "text": "super resolution"}, {"st": 28, "ed": 30, "text": "training data"}, {"st": 35, "ed": 37, "text": "low resolution"}, {"st": 57, "ed": 59, "text": "image compression"}, {"st": 89, "ed": 91, "text": "zero shot"}, {"st": 97, "ed": 99, "text": "deep learning"}, {"st": 133, "ed": 135, "text": "input image"}, {"st": 176, "ed": 178, "text": "method outperforms"}]
[{"st": 9, "ed": 13, "text": "convolutional neural networks cnn"}, {"st": 49, "ed": 51, "text": "inner product"}, {"st": 79, "ed": 83, "text": "dynamic time warping dtw"}, {"st": 98, "ed": 100, "text": "time series"}, {"st": 123, "ed": 125, "text": "handwritten digit"}]
[{"st": 13, "ed": 15, "text": "clinical research"}, {"st": 26, "ed": 28, "text": "big data"}, {"st": 29, "ed": 31, "text": "machine learning"}, {"st": 51, "ed": 53, "text": "patient data"}, {"st": 55, "ed": 57, "text": "deep learning"}, {"st": 89, "ed": 91, "text": "clinical research"}, {"st": 96, "ed": 98, "text": "clinical research"}, {"st": 101, "ed": 103, "text": "patient data"}, {"st": 111, "ed": 115, "text": "convolutional neural network cnn"}, {"st": 116, "ed": 118, "text": "automatically identify"}, {"st": 144, "ed": 146, "text": "ct images"}, {"st": 155, "ed": 159, "text": "head and neck cancer"}, {"st": 187, "ed": 189, "text": "deep learning"}, {"st": 190, "ed": 192, "text": "patient data"}]
[{"st": 1, "ed": 3, "text": "computational complexity"}, {"st": 5, "ed": 8, "text": "deep neural networks"}, {"st": 11, "ed": 13, "text": "feature representations"}, {"st": 43, "ed": 46, "text": "deep neural networks"}, {"st": 52, "ed": 54, "text": "highly efficient"}, {"st": 54, "ed": 57, "text": "deep neural networks"}, {"st": 73, "ed": 75, "text": "feature extraction"}, {"st": 95, "ed": 98, "text": "deep neural network"}, {"st": 114, "ed": 117, "text": "deep neural networks"}, {"st": 142, "ed": 145, "text": "deep neural network"}, {"st": 153, "ed": 155, "text": "object classification"}, {"st": 156, "ed": 158, "text": "object detection"}, {"st": 172, "ed": 174, "text": "proposed framework"}, {"st": 178, "ed": 180, "text": "significant improvement"}, {"st": 181, "ed": 183, "text": "network architecture"}]
[{"st": 6, "ed": 8, "text": "fully automated"}, {"st": 9, "ed": 11, "text": "computed tomography"}, {"st": 21, "ed": 23, "text": "nodule detection"}, {"st": 62, "ed": 64, "text": "nodule detection"}, {"st": 73, "ed": 75, "text": "convolutional neural"}, {"st": 81, "ed": 83, "text": "nodule detection"}, {"st": 90, "ed": 92, "text": "u net"}, {"st": 93, "ed": 95, "text": "encoder decoder"}, {"st": 127, "ed": 129, "text": "lidc idri"}, {"st": 163, "ed": 165, "text": "nodule detection"}, {"st": 175, "ed": 178, "text": "extensive experimental results"}, {"st": 198, "ed": 200, "text": "lidc idri"}, {"st": 202, "ed": 204, "text": "https github.com"}]
[{"st": 13, "ed": 15, "text": "great importance"}, {"st": 34, "ed": 36, "text": "widely applied"}, {"st": 54, "ed": 56, "text": "deep learning"}, {"st": 97, "ed": 99, "text": "neural network"}, {"st": 112, "ed": 115, "text": "fully connected layers"}, {"st": 157, "ed": 159, "text": "real world"}, {"st": 163, "ed": 165, "text": "experiment results"}, {"st": 182, "ed": 184, "text": "proposed method"}]
[{"st": 2, "ed": 5, "text": "deep neural networks"}, {"st": 12, "ed": 14, "text": "feature representations"}, {"st": 159, "ed": 161, "text": "fine tuning"}, {"st": 167, "ed": 169, "text": "performance loss"}, {"st": 180, "ed": 182, "text": "compression scheme"}]
[{"st": 7, "ed": 9, "text": "feature spaces"}, {"st": 27, "ed": 29, "text": "optimization problem"}, {"st": 35, "ed": 37, "text": "representational power"}, {"st": 38, "ed": 40, "text": "neural networks"}, {"st": 50, "ed": 52, "text": "estimation error"}, {"st": 67, "ed": 69, "text": "extensive experiments"}, {"st": 76, "ed": 78, "text": "existing approaches"}, {"st": 82, "ed": 85, "text": "generative adversarial networks"}]
[{"st": 6, "ed": 8, "text": "computational cost"}, {"st": 8, "ed": 11, "text": "deep neural networks"}, {"st": 34, "ed": 36, "text": "systolic array"}, {"st": 37, "ed": 39, "text": "matrix multiplication"}, {"st": 50, "ed": 52, "text": "fault tolerant"}, {"st": 52, "ed": 54, "text": "systolic array"}, {"st": 70, "ed": 72, "text": "classification accuracy"}, {"st": 123, "ed": 125, "text": "classification accuracy"}]
[{"st": 0, "ed": 4, "text": "convolutional neural networks cnns"}, {"st": 5, "ed": 7, "text": "computationally intensive"}, {"st": 36, "ed": 38, "text": "network pruning"}, {"st": 120, "ed": 124, "text": "cifar 10 cifar 100"}]
[{"st": 18, "ed": 20, "text": "time series"}, {"st": 52, "ed": 54, "text": "adversarial perturbations"}, {"st": 57, "ed": 62, "text": "convolutional neural network cnn architecture"}, {"st": 74, "ed": 76, "text": "empirically demonstrate"}, {"st": 94, "ed": 96, "text": "discriminative features"}, {"st": 123, "ed": 125, "text": "false alarms"}, {"st": 127, "ed": 129, "text": "input images"}]
[{"st": 0, "ed": 3, "text": "deep neural networks"}, {"st": 27, "ed": 29, "text": "low level"}, {"st": 74, "ed": 77, "text": "stochastic gradient descent"}]
[{"st": 4, "ed": 6, "text": "multiple classifiers"}, {"st": 14, "ed": 16, "text": "substantial improvements"}, {"st": 19, "ed": 21, "text": "pattern recognition"}, {"st": 45, "ed": 47, "text": "randomly selected"}, {"st": 119, "ed": 121, "text": "decision boundaries"}, {"st": 174, "ed": 176, "text": "linear combinations"}]
[{"st": 0, "ed": 2, "text": "recent theoretical"}, {"st": 6, "ed": 9, "text": "statistical machine learning"}, {"st": 14, "ed": 16, "text": "learning algorithms"}, {"st": 17, "ed": 19, "text": "deep architectures"}, {"st": 33, "ed": 35, "text": "unlabeled examples"}, {"st": 129, "ed": 131, "text": "object classes"}, {"st": 173, "ed": 175, "text": "previously published"}, {"st": 183, "ed": 185, "text": "handwritten digit"}]
[{"st": 23, "ed": 25, "text": "computed tomography"}, {"st": 51, "ed": 53, "text": "false positives"}, {"st": 143, "ed": 148, "text": "deep convolutional neural network cnn"}]
[{"st": 4, "ed": 7, "text": "multi task learning"}, {"st": 9, "ed": 12, "text": "human pose estimation"}, {"st": 23, "ed": 25, "text": "simultaneously learn"}, {"st": 31, "ed": 33, "text": "sliding window"}, {"st": 48, "ed": 50, "text": "detection task"}]
[{"st": 0, "ed": 3, "text": "deep convolutional networks"}, {"st": 10, "ed": 12, "text": "learning task"}, {"st": 21, "ed": 23, "text": "computer vision"}, {"st": 31, "ed": 33, "text": "supervised learning"}, {"st": 37, "ed": 39, "text": "input output"}, {"st": 46, "ed": 49, "text": "large training sets"}, {"st": 53, "ed": 55, "text": "key challenges"}, {"st": 67, "ed": 69, "text": "feature learning"}, {"st": 76, "ed": 78, "text": "convolutional network"}, {"st": 123, "ed": 125, "text": "feature representation"}, {"st": 144, "ed": 146, "text": "feature representation"}, {"st": 148, "ed": 150, "text": "classification results"}, {"st": 158, "ed": 160, "text": "unsupervised learning"}, {"st": 166, "ed": 168, "text": "cifar 10"}, {"st": 180, "ed": 182, "text": "class specific"}, {"st": 184, "ed": 186, "text": "supervised training"}, {"st": 188, "ed": 190, "text": "classification task"}]
[{"st": 3, "ed": 6, "text": "feedforward neural network"}, {"st": 11, "ed": 13, "text": "training set"}, {"st": 98, "ed": 100, "text": "benchmark tasks"}]
[{"st": 2, "ed": 4, "text": "distributed memory"}]
[{"st": 10, "ed": 14, "text": "convolutional neural networks cnns"}, {"st": 16, "ed": 18, "text": "deep cnns"}, {"st": 23, "ed": 25, "text": "computer vision"}, {"st": 26, "ed": 28, "text": "unlike previous"}, {"st": 56, "ed": 58, "text": "optimization problem"}, {"st": 62, "ed": 65, "text": "stochastic gradient descent"}, {"st": 70, "ed": 72, "text": "previous methods"}, {"st": 106, "ed": 108, "text": "vgg 16"}, {"st": 110, "ed": 112, "text": "method achieves"}, {"st": 124, "ed": 126, "text": "top 5"}, {"st": 133, "ed": 135, "text": "vgg 16"}, {"st": 143, "ed": 145, "text": "object detection"}]
[{"st": 5, "ed": 7, "text": "deep learning"}, {"st": 18, "ed": 20, "text": "training samples"}, {"st": 35, "ed": 37, "text": "deep learning"}, {"st": 40, "ed": 42, "text": "training samples"}, {"st": 66, "ed": 68, "text": "open data"}, {"st": 84, "ed": 86, "text": "approach achieves"}, {"st": 104, "ed": 106, "text": "benchmark dataset"}]
[{"st": 1, "ed": 3, "text": "computer vision"}, {"st": 21, "ed": 23, "text": "challenging problem"}, {"st": 43, "ed": 45, "text": "object categorization"}, {"st": 67, "ed": 70, "text": "multiple kernel learning"}, {"st": 89, "ed": 91, "text": "linear combination"}, {"st": 92, "ed": 94, "text": "base kernels"}, {"st": 102, "ed": 104, "text": "real world"}, {"st": 104, "ed": 106, "text": "object categorization"}, {"st": 123, "ed": 125, "text": "linear functions"}, {"st": 126, "ed": 128, "text": "base kernels"}, {"st": 129, "ed": 131, "text": "genetic programming"}, {"st": 136, "ed": 138, "text": "experiment results"}, {"st": 144, "ed": 146, "text": "genetic programming"}, {"st": 152, "ed": 154, "text": "linear combination"}]
[{"st": 3, "ed": 6, "text": "deep neural networks"}, {"st": 10, "ed": 13, "text": "ability to learn"}, {"st": 22, "ed": 24, "text": "hidden layers"}, {"st": 25, "ed": 28, "text": "deep neural networks"}, {"st": 30, "ed": 32, "text": "computer vision"}, {"st": 40, "ed": 42, "text": "mid level"}, {"st": 61, "ed": 63, "text": "regularization technique"}, {"st": 70, "ed": 72, "text": "hidden nodes"}, {"st": 78, "ed": 80, "text": "special case"}, {"st": 86, "ed": 88, "text": "classification accuracy"}, {"st": 102, "ed": 104, "text": "benchmark datasets"}]
[{"st": 4, "ed": 6, "text": "recently introduced"}, {"st": 6, "ed": 8, "text": "machine learning"}, {"st": 36, "ed": 39, "text": "recurrent neural networks"}, {"st": 48, "ed": 50, "text": "hidden layer"}, {"st": 58, "ed": 60, "text": "output layer"}, {"st": 72, "ed": 74, "text": "classification tasks"}, {"st": 78, "ed": 80, "text": "weight training"}, {"st": 88, "ed": 90, "text": "clustering method"}, {"st": 92, "ed": 94, "text": "principal components"}, {"st": 100, "ed": 102, "text": "proposed method"}, {"st": 108, "ed": 110, "text": "numerical experiments"}, {"st": 131, "ed": 133, "text": "classification accuracy"}]
[{"st": 0, "ed": 2, "text": "visual representation"}, {"st": 6, "ed": 8, "text": "visual tracking"}, {"st": 12, "ed": 14, "text": "visual representations"}, {"st": 16, "ed": 18, "text": "visual tracking"}, {"st": 20, "ed": 22, "text": "hand crafted"}, {"st": 22, "ed": 24, "text": "computer vision"}, {"st": 42, "ed": 44, "text": "complex valued"}, {"st": 44, "ed": 46, "text": "invariant representations"}, {"st": 49, "ed": 51, "text": "image patches"}, {"st": 69, "ed": 71, "text": "unlabeled data"}, {"st": 87, "ed": 89, "text": "training samples"}, {"st": 94, "ed": 96, "text": "negative samples"}, {"st": 114, "ed": 116, "text": "training samples"}, {"st": 117, "ed": 119, "text": "logistic regression"}]
[{"st": 1, "ed": 6, "text": "deep convolutional neural networks cnns"}, {"st": 9, "ed": 11, "text": "great success"}, {"st": 14, "ed": 16, "text": "image classification"}, {"st": 22, "ed": 24, "text": "real world"}, {"st": 27, "ed": 29, "text": "multiple labels"}, {"st": 42, "ed": 44, "text": "traditional approaches"}, {"st": 45, "ed": 47, "text": "multi label"}, {"st": 47, "ed": 49, "text": "image classification"}, {"st": 84, "ed": 88, "text": "recurrent neural networks rnns"}, {"st": 123, "ed": 127, "text": "trained end to end"}, {"st": 141, "ed": 143, "text": "benchmark datasets"}, {"st": 157, "ed": 160, "text": "multi label classification"}]
[{"st": 4, "ed": 6, "text": "sparse coding"}, {"st": 18, "ed": 20, "text": "image processing"}, {"st": 29, "ed": 31, "text": "image data"}, {"st": 67, "ed": 69, "text": "optimization problem"}, {"st": 86, "ed": 88, "text": "space complexity"}, {"st": 160, "ed": 162, "text": "learning algorithm"}, {"st": 191, "ed": 193, "text": "learning algorithms"}, {"st": 218, "ed": 220, "text": "efficient online"}, {"st": 220, "ed": 222, "text": "dictionary learning"}]
[{"st": 3, "ed": 7, "text": "convolutional neural network cnn"}, {"st": 9, "ed": 11, "text": "raw pixels"}, {"st": 22, "ed": 25, "text": "end to end"}, {"st": 31, "ed": 33, "text": "training data"}, {"st": 73, "ed": 75, "text": "automatically learns"}, {"st": 75, "ed": 77, "text": "internal representations"}, {"st": 80, "ed": 82, "text": "processing steps"}, {"st": 123, "ed": 125, "text": "path planning"}, {"st": 128, "ed": 131, "text": "end to end"}, {"st": 134, "ed": 136, "text": "processing steps"}, {"st": 186, "ed": 188, "text": "doesn t"}]
[{"st": 27, "ed": 29, "text": "neural network"}, {"st": 40, "ed": 43, "text": "deep neural networks"}, {"st": 63, "ed": 65, "text": "auto encoders"}, {"st": 67, "ed": 69, "text": "auto encoder"}, {"st": 71, "ed": 73, "text": "maximum margin"}, {"st": 76, "ed": 78, "text": "intra class"}, {"st": 80, "ed": 82, "text": "inter class"}, {"st": 117, "ed": 119, "text": "extensive experiments"}, {"st": 120, "ed": 122, "text": "cross view"}, {"st": 122, "ed": 124, "text": "image classification"}]
[{"st": 5, "ed": 7, "text": "spatial resolution"}, {"st": 63, "ed": 65, "text": "neural network"}, {"st": 73, "ed": 75, "text": "remote sensing"}, {"st": 179, "ed": 181, "text": "ground truth"}]
[{"st": 8, "ed": 10, "text": "deep architectures"}, {"st": 13, "ed": 16, "text": "deep belief networks"}, {"st": 18, "ed": 20, "text": "deep architectures"}, {"st": 24, "ed": 27, "text": "restricted boltzmann machines"}, {"st": 32, "ed": 34, "text": "generative model"}, {"st": 42, "ed": 44, "text": "feature extraction"}, {"st": 53, "ed": 55, "text": "image processing"}, {"st": 55, "ed": 57, "text": "speech processing"}, {"st": 64, "ed": 66, "text": "object oriented"}, {"st": 95, "ed": 97, "text": "experiments conducted"}, {"st": 123, "ed": 125, "text": "unlabeled data"}, {"st": 155, "ed": 157, "text": "sampling methods"}, {"st": 180, "ed": 183, "text": "generative and discriminative"}, {"st": 192, "ed": 195, "text": "open source software"}]
[{"st": 12, "ed": 15, "text": "deep convolutional network"}, {"st": 32, "ed": 34, "text": "recognition rate"}, {"st": 67, "ed": 69, "text": "facial expressions"}]
[{"st": 17, "ed": 19, "text": "deep learning"}, {"st": 45, "ed": 48, "text": "convolutional neural networks"}, {"st": 50, "ed": 52, "text": "deep models"}, {"st": 144, "ed": 146, "text": "large scale"}]
[{"st": 9, "ed": 11, "text": "neural network"}, {"st": 21, "ed": 23, "text": "attention mechanism"}, {"st": 34, "ed": 36, "text": "variational auto"}, {"st": 49, "ed": 51, "text": "substantially improves"}, {"st": 58, "ed": 60, "text": "generative models"}, {"st": 80, "ed": 82, "text": "real data"}]
[{"st": 3, "ed": 5, "text": "neural networks"}, {"st": 10, "ed": 12, "text": "neural networks"}, {"st": 29, "ed": 31, "text": "binary classification"}, {"st": 57, "ed": 59, "text": "image classification"}, {"st": 64, "ed": 66, "text": "neural networks"}, {"st": 68, "ed": 70, "text": "hidden layers"}, {"st": 74, "ed": 76, "text": "hidden units"}, {"st": 98, "ed": 100, "text": "mnist dataset"}, {"st": 125, "ed": 127, "text": "standard backpropagation"}, {"st": 129, "ed": 131, "text": "fully connected"}]
[{"st": 0, "ed": 2, "text": "network quantization"}, {"st": 23, "ed": 25, "text": "parameter values"}, {"st": 40, "ed": 42, "text": "network quantization"}, {"st": 46, "ed": 48, "text": "performance loss"}, {"st": 53, "ed": 55, "text": "compression ratio"}, {"st": 66, "ed": 68, "text": "neural network"}, {"st": 68, "ed": 70, "text": "loss function"}, {"st": 82, "ed": 84, "text": "objective function"}, {"st": 95, "ed": 98, "text": "k means clustering"}, {"st": 102, "ed": 104, "text": "network parameters"}, {"st": 124, "ed": 126, "text": "network quantization"}, {"st": 149, "ed": 151, "text": "network quantization"}, {"st": 171, "ed": 173, "text": "huffman coding"}]
[{"st": 7, "ed": 10, "text": "primary motor cortex"}, {"st": 20, "ed": 22, "text": "mirror neuron"}, {"st": 44, "ed": 46, "text": "crucial step"}, {"st": 50, "ed": 52, "text": "intelligent systems"}, {"st": 87, "ed": 89, "text": "vision problems"}, {"st": 112, "ed": 114, "text": "computer vision"}, {"st": 212, "ed": 214, "text": "motion features"}]
[{"st": 1, "ed": 3, "text": "recent progress"}, {"st": 39, "ed": 42, "text": "synthetic and real"}, {"st": 74, "ed": 76, "text": "real data"}, {"st": 99, "ed": 103, "text": "generative adversarial networks gans"}, {"st": 134, "ed": 136, "text": "regularization term"}, {"st": 139, "ed": 141, "text": "adversarial loss"}, {"st": 160, "ed": 162, "text": "realistic images"}, {"st": 176, "ed": 178, "text": "generated images"}, {"st": 191, "ed": 193, "text": "significant improvement"}]
[{"st": 3, "ed": 6, "text": "supervised machine learning"}, {"st": 22, "ed": 24, "text": "image reconstruction"}, {"st": 61, "ed": 63, "text": "bias variance"}, {"st": 73, "ed": 77, "text": "feed forward neural network"}, {"st": 84, "ed": 86, "text": "numerical experiments"}, {"st": 97, "ed": 99, "text": "iterative reconstruction"}]
[{"st": 4, "ed": 6, "text": "deep network"}, {"st": 25, "ed": 27, "text": "convolutional layer"}, {"st": 34, "ed": 36, "text": "activation function"}, {"st": 44, "ed": 46, "text": "neural networks"}, {"st": 62, "ed": 64, "text": "neural network"}, {"st": 66, "ed": 68, "text": "multilayer perceptron"}, {"st": 75, "ed": 77, "text": "feature maps"}, {"st": 131, "ed": 133, "text": "feature maps"}, {"st": 164, "ed": 169, "text": "cifar 10 and cifar 100"}]
[{"st": 0, "ed": 4, "text": "recurrent neural networks rnns"}, {"st": 6, "ed": 9, "text": "short term memory"}, {"st": 32, "ed": 34, "text": "recently proposed"}, {"st": 40, "ed": 42, "text": "previous works"}, {"st": 52, "ed": 54, "text": "convolutional networks"}, {"st": 78, "ed": 80, "text": "recurrent connections"}, {"st": 90, "ed": 92, "text": "extensive experiments"}, {"st": 105, "ed": 107, "text": "deep architectures"}]
[{"st": 7, "ed": 10, "text": "unsupervised feature learning"}, {"st": 13, "ed": 15, "text": "sparse coding"}, {"st": 18, "ed": 20, "text": "sparse coding"}, {"st": 22, "ed": 24, "text": "multi layer"}, {"st": 26, "ed": 29, "text": "visual object recognition"}, {"st": 62, "ed": 64, "text": "local spatial"}, {"st": 68, "ed": 70, "text": "low dimensional"}, {"st": 73, "ed": 76, "text": "takes advantage of"}, {"st": 96, "ed": 98, "text": "sparse representation"}, {"st": 123, "ed": 125, "text": "feature representations"}, {"st": 126, "ed": 128, "text": "multiple layers"}, {"st": 138, "ed": 140, "text": "object recognition"}]
[{"st": 0, "ed": 2, "text": "convolutional networks"}, {"st": 21, "ed": 24, "text": "ability to learn"}, {"st": 37, "ed": 39, "text": "convolutional network"}, {"st": 80, "ed": 83, "text": "training and inference"}, {"st": 123, "ed": 125, "text": "feature map"}]
[{"st": 0, "ed": 3, "text": "deep belief networks"}, {"st": 6, "ed": 8, "text": "generative models"}, {"st": 12, "ed": 14, "text": "feature representation"}, {"st": 25, "ed": 27, "text": "machine learning"}, {"st": 40, "ed": 42, "text": "image denoising"}, {"st": 96, "ed": 98, "text": "image content"}, {"st": 158, "ed": 160, "text": "mnist dataset"}, {"st": 161, "ed": 163, "text": "handwritten digits"}, {"st": 169, "ed": 171, "text": "gaussian noise"}, {"st": 179, "ed": 182, "text": "mean square error"}, {"st": 187, "ed": 189, "text": "proposed method"}]
[{"st": 4, "ed": 6, "text": "large scale"}, {"st": 6, "ed": 8, "text": "neural networks"}, {"st": 46, "ed": 48, "text": "computer vision"}, {"st": 56, "ed": 58, "text": "large scale"}, {"st": 101, "ed": 104, "text": "convolutional neural networks"}, {"st": 123, "ed": 125, "text": "training sets"}]
[{"st": 0, "ed": 3, "text": "deep neural networks"}, {"st": 9, "ed": 11, "text": "recently achieved"}, {"st": 19, "ed": 21, "text": "visual recognition"}, {"st": 67, "ed": 69, "text": "linear combinations"}, {"st": 109, "ed": 112, "text": "deep neural networks"}, {"st": 113, "ed": 115, "text": "input output"}]
[{"st": 0, "ed": 3, "text": "convolutional neural networks"}, {"st": 11, "ed": 13, "text": "recognition tasks"}, {"st": 62, "ed": 64, "text": "hierarchical clustering"}, {"st": 83, "ed": 85, "text": "low dimensional"}, {"st": 91, "ed": 93, "text": "convolutional layers"}]
[{"st": 8, "ed": 10, "text": "real world"}, {"st": 10, "ed": 12, "text": "computer vision"}, {"st": 14, "ed": 17, "text": "deep convolutional networks"}, {"st": 25, "ed": 27, "text": "image classification"}, {"st": 43, "ed": 45, "text": "fine tuning"}, {"st": 50, "ed": 52, "text": "deep model"}, {"st": 96, "ed": 98, "text": "single image"}, {"st": 111, "ed": 113, "text": "deep models"}, {"st": 126, "ed": 128, "text": "deep cnns"}, {"st": 133, "ed": 135, "text": "labeled data"}, {"st": 141, "ed": 143, "text": "previous methods"}, {"st": 154, "ed": 156, "text": "deep cnn"}, {"st": 177, "ed": 179, "text": "deep models"}, {"st": 193, "ed": 195, "text": "domain specific"}, {"st": 202, "ed": 204, "text": "deep models"}, {"st": 207, "ed": 209, "text": "domain adaptation"}]
[{"st": 5, "ed": 8, "text": "deep neural networks"}, {"st": 20, "ed": 22, "text": "neural networks"}, {"st": 22, "ed": 24, "text": "originally designed"}, {"st": 25, "ed": 27, "text": "image recognition"}, {"st": 44, "ed": 46, "text": "bounding box"}, {"st": 55, "ed": 57, "text": "bounding box"}]
[{"st": 1, "ed": 5, "text": "convolutional neural networks cnn"}, {"st": 58, "ed": 60, "text": "classification performance"}, {"st": 81, "ed": 83, "text": "policy search"}, {"st": 88, "ed": 90, "text": "parameter space"}, {"st": 99, "ed": 104, "text": "cifar 10 and cifar 100"}]
[{"st": 14, "ed": 16, "text": "convolutional networks"}, {"st": 59, "ed": 61, "text": "image representations"}, {"st": 62, "ed": 64, "text": "object category"}, {"st": 66, "ed": 69, "text": "standard benchmark datasets"}, {"st": 98, "ed": 100, "text": "image representations"}, {"st": 129, "ed": 131, "text": "large scale"}, {"st": 199, "ed": 201, "text": "significantly outperforms"}]
[{"st": 16, "ed": 19, "text": "convolutional neural network"}, {"st": 24, "ed": 26, "text": "image patches"}, {"st": 64, "ed": 66, "text": "method achieves"}, {"st": 67, "ed": 69, "text": "error rate"}]
[{"st": 1, "ed": 3, "text": "image classification"}, {"st": 7, "ed": 9, "text": "object categories"}, {"st": 31, "ed": 36, "text": "deep convolutional neural networks cnn"}, {"st": 52, "ed": 54, "text": "hierarchical structure"}, {"st": 62, "ed": 64, "text": "deep cnns"}, {"st": 68, "ed": 70, "text": "deep cnns"}, {"st": 108, "ed": 110, "text": "logistic loss"}, {"st": 134, "ed": 136, "text": "large scale"}, {"st": 149, "ed": 151, "text": "large scale"}]
[{"st": 4, "ed": 6, "text": "visual sentiment"}, {"st": 11, "ed": 16, "text": "deep convolutional neural networks cnns"}, {"st": 18, "ed": 20, "text": "visual sentiment"}, {"st": 73, "ed": 77, "text": "deep convolutional neural networks"}, {"st": 81, "ed": 83, "text": "performance improvement"}, {"st": 85, "ed": 87, "text": "large scale"}, {"st": 87, "ed": 89, "text": "web based"}, {"st": 89, "ed": 91, "text": "image dataset"}, {"st": 95, "ed": 97, "text": "deep cnns"}, {"st": 106, "ed": 108, "text": "deep learning"}, {"st": 114, "ed": 116, "text": "training data"}, {"st": 125, "ed": 127, "text": "prevent overfitting"}, {"st": 139, "ed": 141, "text": "evaluation shows"}, {"st": 144, "ed": 146, "text": "deep cnns"}, {"st": 153, "ed": 155, "text": "significantly improved"}, {"st": 160, "ed": 162, "text": "retrieval performance"}]
[{"st": 0, "ed": 2, "text": "convolutional neural"}, {"st": 7, "ed": 9, "text": "labeled datasets"}, {"st": 18, "ed": 20, "text": "image classification"}, {"st": 97, "ed": 99, "text": "receptive field"}, {"st": 113, "ed": 115, "text": "hand engineered"}, {"st": 128, "ed": 130, "text": "pascal voc"}]
[{"st": 1, "ed": 5, "text": "convolutional neural networks cnns"}, {"st": 102, "ed": 104, "text": "benchmark dataset"}]
[{"st": 27, "ed": 29, "text": "contextual information"}, {"st": 56, "ed": 59, "text": "local and global"}, {"st": 59, "ed": 61, "text": "contextual information"}, {"st": 77, "ed": 79, "text": "prediction error"}, {"st": 152, "ed": 154, "text": "predict future"}, {"st": 156, "ed": 158, "text": "image sequences"}, {"st": 174, "ed": 176, "text": "prediction accuracy"}]
[{"st": 4, "ed": 6, "text": "image representations"}, {"st": 13, "ed": 18, "text": "deep convolutional neural networks cnn"}, {"st": 19, "ed": 21, "text": "theoretical understanding"}, {"st": 48, "ed": 50, "text": "input image"}, {"st": 58, "ed": 60, "text": "special case"}, {"st": 82, "ed": 84, "text": "visual information"}, {"st": 144, "ed": 146, "text": "structured output"}]
[{"st": 4, "ed": 7, "text": "convolutional neural networks"}, {"st": 85, "ed": 87, "text": "training set"}, {"st": 89, "ed": 91, "text": "training instances"}, {"st": 117, "ed": 119, "text": "existing approaches"}]
[{"st": 2, "ed": 4, "text": "image representation"}, {"st": 6, "ed": 8, "text": "low level"}, {"st": 10, "ed": 12, "text": "deep neural"}, {"st": 14, "ed": 18, "text": "convolutional neural networks cnns"}, {"st": 21, "ed": 23, "text": "mid level"}, {"st": 40, "ed": 42, "text": "image representation"}, {"st": 64, "ed": 66, "text": "multi scale"}, {"st": 71, "ed": 73, "text": "pre trained"}, {"st": 115, "ed": 117, "text": "performance improvements"}, {"st": 142, "ed": 144, "text": "image representation"}, {"st": 148, "ed": 150, "text": "visual recognition"}]
[{"st": 4, "ed": 7, "text": "deep neural networks"}, {"st": 44, "ed": 46, "text": "neural networks"}, {"st": 51, "ed": 53, "text": "recent advances"}, {"st": 59, "ed": 61, "text": "recently developed"}, {"st": 61, "ed": 63, "text": "neural network"}, {"st": 65, "ed": 67, "text": "benchmark datasets"}]
[{"st": 4, "ed": 7, "text": "deep neural networks"}, {"st": 15, "ed": 17, "text": "small perturbations"}, {"st": 55, "ed": 57, "text": "adversarial examples"}, {"st": 59, "ed": 61, "text": "network topology"}, {"st": 61, "ed": 63, "text": "pre processing"}, {"st": 81, "ed": 83, "text": "adversarial examples"}, {"st": 89, "ed": 91, "text": "pre processing"}, {"st": 92, "ed": 94, "text": "denoising autoencoders"}, {"st": 127, "ed": 129, "text": "adversarial examples"}, {"st": 146, "ed": 149, "text": "end to end"}, {"st": 149, "ed": 151, "text": "training procedure"}, {"st": 169, "ed": 171, "text": "adversarial examples"}]
[{"st": 0, "ed": 3, "text": "convolutional neural networks"}, {"st": 6, "ed": 8, "text": "excellent results"}, {"st": 34, "ed": 36, "text": "feature learning"}, {"st": 63, "ed": 65, "text": "recent results"}, {"st": 67, "ed": 69, "text": "imagenet dataset"}, {"st": 81, "ed": 83, "text": "discriminative features"}, {"st": 97, "ed": 99, "text": "discriminative features"}, {"st": 106, "ed": 108, "text": "learning process"}, {"st": 125, "ed": 127, "text": "scale invariant"}, {"st": 140, "ed": 142, "text": "mnist dataset"}, {"st": 150, "ed": 152, "text": "scale invariance"}, {"st": 157, "ed": 159, "text": "discriminative features"}]
[{"st": 0, "ed": 5, "text": "deep convolutional neural networks cnn"}, {"st": 12, "ed": 14, "text": "object recognition"}, {"st": 20, "ed": 22, "text": "image classification"}, {"st": 23, "ed": 25, "text": "object detection"}, {"st": 31, "ed": 33, "text": "deep cnn"}, {"st": 57, "ed": 59, "text": "deep cnns"}, {"st": 106, "ed": 108, "text": "connected layers"}, {"st": 117, "ed": 119, "text": "matrix factorization"}, {"st": 122, "ed": 125, "text": "k means clustering"}, {"st": 149, "ed": 151, "text": "classification task"}, {"st": 172, "ed": 174, "text": "classification accuracy"}]
[{"st": 1, "ed": 5, "text": "convolutional neural networks cnns"}, {"st": 10, "ed": 12, "text": "powerful tool"}, {"st": 50, "ed": 52, "text": "generative modeling"}, {"st": 55, "ed": 57, "text": "main contributions"}, {"st": 62, "ed": 64, "text": "generative model"}, {"st": 84, "ed": 86, "text": "pre training"}, {"st": 91, "ed": 93, "text": "importance sampling"}, {"st": 121, "ed": 123, "text": "visualization method"}, {"st": 136, "ed": 138, "text": "visualization method"}, {"st": 149, "ed": 151, "text": "trained cnn"}, {"st": 154, "ed": 156, "text": "monte carlo"}, {"st": 158, "ed": 160, "text": "without resorting"}, {"st": 178, "ed": 180, "text": "pre training"}, {"st": 191, "ed": 193, "text": "visualization method"}, {"st": 203, "ed": 205, "text": "large scale"}]
[{"st": 5, "ed": 7, "text": "deep learning"}, {"st": 9, "ed": 12, "text": "visual object recognition"}, {"st": 16, "ed": 18, "text": "supervised training"}, {"st": 34, "ed": 36, "text": "labeled examples"}, {"st": 59, "ed": 61, "text": "class labels"}, {"st": 127, "ed": 129, "text": "deep network"}, {"st": 141, "ed": 143, "text": "approach yields"}, {"st": 152, "ed": 154, "text": "mnist handwritten"}, {"st": 168, "ed": 170, "text": "face database"}, {"st": 197, "ed": 199, "text": "face images"}, {"st": 219, "ed": 221, "text": "deep networks"}, {"st": 225, "ed": 227, "text": "structured outputs"}]
[{"st": 5, "ed": 7, "text": "contour detection"}, {"st": 19, "ed": 21, "text": "proposed approach"}, {"st": 25, "ed": 27, "text": "efficient implementation"}, {"st": 29, "ed": 33, "text": "convolutional neural networks cnns"}, {"st": 37, "ed": 39, "text": "feature vector"}, {"st": 58, "ed": 60, "text": "pre trained"}, {"st": 81, "ed": 83, "text": "fine tuning"}, {"st": 87, "ed": 89, "text": "cost sensitive"}, {"st": 110, "ed": 112, "text": "contour detection"}]
[{"st": 21, "ed": 23, "text": "spatio temporal"}, {"st": 44, "ed": 46, "text": "multi modal"}, {"st": 55, "ed": 58, "text": "decision support systems"}, {"st": 93, "ed": 95, "text": "deep learning"}, {"st": 123, "ed": 125, "text": "driving test"}, {"st": 140, "ed": 142, "text": "multi modal"}, {"st": 142, "ed": 146, "text": "deep convolutional neural networks"}]
[{"st": 13, "ed": 15, "text": "recurrent neural"}, {"st": 24, "ed": 26, "text": "fine grained"}, {"st": 53, "ed": 55, "text": "large scale"}, {"st": 55, "ed": 57, "text": "pre training"}, {"st": 69, "ed": 71, "text": "attention models"}, {"st": 88, "ed": 90, "text": "fine grained"}, {"st": 123, "ed": 125, "text": "bounding boxes"}, {"st": 131, "ed": 133, "text": "fine grained"}, {"st": 143, "ed": 145, "text": "low resolution"}, {"st": 159, "ed": 161, "text": "attention models"}, {"st": 167, "ed": 171, "text": "trained end to end"}, {"st": 181, "ed": 183, "text": "hand engineered"}]
[{"st": 0, "ed": 4, "text": "multiple instance learning mil"}, {"st": 31, "ed": 33, "text": "multi class"}, {"st": 55, "ed": 57, "text": "image level"}, {"st": 61, "ed": 65, "text": "trained end to end"}, {"st": 77, "ed": 79, "text": "fully convolutional"}, {"st": 90, "ed": 92, "text": "pre processing"}, {"st": 103, "ed": 105, "text": "multi class"}, {"st": 122, "ed": 124, "text": "preliminary experiments"}, {"st": 126, "ed": 128, "text": "pascal voc"}]
[{"st": 8, "ed": 11, "text": "convolutional neural networks"}, {"st": 27, "ed": 29, "text": "feature representations"}, {"st": 75, "ed": 77, "text": "object detection"}, {"st": 78, "ed": 80, "text": "pose estimation"}, {"st": 106, "ed": 108, "text": "object detection"}, {"st": 109, "ed": 111, "text": "pose estimation"}, {"st": 112, "ed": 114, "text": "significantly outperforms"}]
[{"st": 9, "ed": 11, "text": "feature representation"}, {"st": 12, "ed": 14, "text": "unlabeled data"}, {"st": 18, "ed": 20, "text": "k means"}, {"st": 22, "ed": 24, "text": "k means"}, {"st": 27, "ed": 29, "text": "input data"}, {"st": 31, "ed": 33, "text": "feature representation"}, {"st": 61, "ed": 63, "text": "feature mapping"}, {"st": 72, "ed": 74, "text": "training data"}, {"st": 85, "ed": 87, "text": "feature learning"}, {"st": 98, "ed": 100, "text": "k means"}, {"st": 151, "ed": 153, "text": "linear programming"}, {"st": 156, "ed": 159, "text": "unsupervised feature learning"}, {"st": 181, "ed": 183, "text": "spatial information"}, {"st": 201, "ed": 203, "text": "receptive field"}, {"st": 208, "ed": 210, "text": "extensive experiments"}, {"st": 213, "ed": 215, "text": "object recognition"}, {"st": 230, "ed": 232, "text": "method yields"}]
[{"st": 22, "ed": 24, "text": "visual representation"}, {"st": 36, "ed": 38, "text": "visual representation"}, {"st": 78, "ed": 80, "text": "dependency structure"}, {"st": 94, "ed": 96, "text": "partial observability"}, {"st": 133, "ed": 135, "text": "latent representation"}, {"st": 153, "ed": 155, "text": "latent representation"}, {"st": 160, "ed": 162, "text": "rotation group"}]
[{"st": 3, "ed": 5, "text": "attention based"}, {"st": 8, "ed": 10, "text": "multiple objects"}, {"st": 17, "ed": 21, "text": "deep recurrent neural network"}, {"st": 23, "ed": 25, "text": "reinforcement learning"}, {"st": 47, "ed": 49, "text": "multiple objects"}, {"st": 53, "ed": 55, "text": "class labels"}, {"st": 63, "ed": 65, "text": "challenging task"}, {"st": 89, "ed": 91, "text": "convolutional networks"}, {"st": 93, "ed": 95, "text": "fewer parameters"}]
[{"st": 0, "ed": 2, "text": "recent advances"}, {"st": 5, "ed": 7, "text": "multi layer"}, {"st": 13, "ed": 15, "text": "neural network"}, {"st": 18, "ed": 21, "text": "deep convolutional networks"}, {"st": 30, "ed": 32, "text": "large datasets"}, {"st": 43, "ed": 45, "text": "error rates"}, {"st": 49, "ed": 51, "text": "mnist handwritten"}, {"st": 71, "ed": 74, "text": "extreme learning machine"}, {"st": 96, "ed": 98, "text": "error rates"}, {"st": 113, "ed": 115, "text": "error rates"}, {"st": 139, "ed": 141, "text": "significantly improve"}, {"st": 149, "ed": 151, "text": "hidden unit"}, {"st": 167, "ed": 169, "text": "receptive field"}, {"st": 176, "ed": 178, "text": "weight matrix"}, {"st": 195, "ed": 198, "text": "number of iterations"}, {"st": 205, "ed": 207, "text": "significantly reduce"}, {"st": 208, "ed": 212, "text": "number of hidden units"}, {"st": 245, "ed": 248, "text": "single hidden layer"}, {"st": 248, "ed": 250, "text": "neural network"}, {"st": 274, "ed": 277, "text": "deep neural networks"}]
[{"st": 0, "ed": 2, "text": "sparse coding"}, {"st": 12, "ed": 14, "text": "higher order"}, {"st": 20, "ed": 22, "text": "inference algorithm"}, {"st": 23, "ed": 25, "text": "computationally expensive"}, {"st": 39, "ed": 41, "text": "sparse coding"}, {"st": 45, "ed": 47, "text": "neural network"}, {"st": 82, "ed": 84, "text": "norm regularization"}, {"st": 118, "ed": 120, "text": "ar 15"}, {"st": 131, "ed": 133, "text": "classification methods"}, {"st": 146, "ed": 148, "text": "recognition accuracy"}]
[{"st": 0, "ed": 2, "text": "deep learning"}, {"st": 8, "ed": 10, "text": "machine learning"}, {"st": 18, "ed": 22, "text": "convolutional neural networks cnn"}, {"st": 39, "ed": 41, "text": "detailed analysis"}, {"st": 59, "ed": 62, "text": "convolutional neural network"}, {"st": 66, "ed": 68, "text": "face recognition"}]
[{"st": 0, "ed": 2, "text": "feature representations"}, {"st": 3, "ed": 5, "text": "hand designed"}, {"st": 30, "ed": 32, "text": "image representations"}, {"st": 67, "ed": 69, "text": "existing methods"}, {"st": 82, "ed": 84, "text": "deep network"}, {"st": 95, "ed": 97, "text": "feature representation"}, {"st": 119, "ed": 121, "text": "network layers"}]
[{"st": 22, "ed": 24, "text": "classification problem"}, {"st": 30, "ed": 32, "text": "recent years"}, {"st": 47, "ed": 49, "text": "recent successes"}, {"st": 50, "ed": 52, "text": "deep learning"}, {"st": 56, "ed": 59, "text": "end to end"}, {"st": 68, "ed": 70, "text": "deep architectures"}, {"st": 96, "ed": 98, "text": "multiple layers"}, {"st": 121, "ed": 124, "text": "deep neural network"}, {"st": 135, "ed": 137, "text": "deep architecture"}, {"st": 172, "ed": 175, "text": "end to end"}, {"st": 182, "ed": 184, "text": "multi layer"}, {"st": 208, "ed": 211, "text": "achieve competitive results"}]
[{"st": 0, "ed": 2, "text": "deep learning"}, {"st": 3, "ed": 5, "text": "recent successes"}, {"st": 9, "ed": 11, "text": "convolutional networks"}, {"st": 14, "ed": 16, "text": "statistical properties"}, {"st": 26, "ed": 28, "text": "multi scale"}, {"st": 28, "ed": 30, "text": "compositional structure"}, {"st": 50, "ed": 52, "text": "text documents"}, {"st": 78, "ed": 80, "text": "deep architectures"}, {"st": 120, "ed": 122, "text": "large scale"}, {"st": 122, "ed": 124, "text": "classification problems"}]
[{"st": 12, "ed": 15, "text": "hand crafted features"}, {"st": 37, "ed": 39, "text": "learning process"}, {"st": 46, "ed": 48, "text": "loss function"}, {"st": 56, "ed": 58, "text": "saliency map"}, {"st": 67, "ed": 69, "text": "large datasets"}, {"st": 70, "ed": 72, "text": "saliency prediction"}, {"st": 81, "ed": 83, "text": "deep architecture"}, {"st": 90, "ed": 92, "text": "convolutional network"}, {"st": 103, "ed": 105, "text": "saliency prediction"}]
[{"st": 1, "ed": 3, "text": "feature extractors"}, {"st": 5, "ed": 7, "text": "convolutional networks"}, {"st": 9, "ed": 11, "text": "achieved impressive"}, {"st": 16, "ed": 18, "text": "classification tasks"}, {"st": 30, "ed": 32, "text": "input space"}, {"st": 35, "ed": 37, "text": "explicitly model"}, {"st": 40, "ed": 42, "text": "output spaces"}, {"st": 51, "ed": 54, "text": "human pose estimation"}, {"st": 65, "ed": 67, "text": "expressive power"}, {"st": 69, "ed": 71, "text": "feature extractors"}, {"st": 74, "ed": 77, "text": "input and output"}, {"st": 121, "ed": 123, "text": "excellent performance"}, {"st": 128, "ed": 130, "text": "pose estimation"}, {"st": 145, "ed": 147, "text": "ground truth"}]
[{"st": 1, "ed": 3, "text": "object recognition"}, {"st": 12, "ed": 14, "text": "real world"}, {"st": 19, "ed": 21, "text": "recent progress"}, {"st": 22, "ed": 26, "text": "convolutional neural networks cnns"}, {"st": 30, "ed": 32, "text": "rgb d"}, {"st": 71, "ed": 73, "text": "real world"}, {"st": 81, "ed": 83, "text": "multi stage"}, {"st": 143, "ed": 145, "text": "rgb d"}, {"st": 152, "ed": 154, "text": "rgb d"}, {"st": 154, "ed": 156, "text": "real world"}]
[{"st": 6, "ed": 8, "text": "image classification"}, {"st": 14, "ed": 16, "text": "deep architecture"}, {"st": 18, "ed": 20, "text": "recent successes"}, {"st": 32, "ed": 35, "text": "layer by layer"}, {"st": 62, "ed": 64, "text": "image representation"}, {"st": 74, "ed": 76, "text": "classification error"}, {"st": 80, "ed": 82, "text": "image generation"}, {"st": 96, "ed": 98, "text": "classification error"}, {"st": 101, "ed": 103, "text": "discriminative features"}, {"st": 112, "ed": 114, "text": "feature selection"}, {"st": 123, "ed": 125, "text": "image representation"}, {"st": 153, "ed": 155, "text": "layer wise"}, {"st": 166, "ed": 168, "text": "generated image"}, {"st": 175, "ed": 177, "text": "visual recognition"}, {"st": 180, "ed": 182, "text": "outperforms existing"}]
[{"st": 0, "ed": 3, "text": "deep neural networks"}, {"st": 7, "ed": 9, "text": "machine learning"}, {"st": 30, "ed": 32, "text": "highly complex"}, {"st": 36, "ed": 39, "text": "deep neural networks"}, {"st": 63, "ed": 65, "text": "functional connectivity"}, {"st": 92, "ed": 95, "text": "deep neural networks"}, {"st": 108, "ed": 111, "text": "deep neural networks"}, {"st": 130, "ed": 134, "text": "deep neural network architecture"}, {"st": 152, "ed": 156, "text": "deep neural network architecture"}, {"st": 163, "ed": 165, "text": "image datasets"}, {"st": 165, "ed": 167, "text": "cifar 10"}, {"st": 167, "ed": 169, "text": "mnist svhn"}, {"st": 191, "ed": 194, "text": "deep neural network"}, {"st": 194, "ed": 196, "text": "achieves comparable"}, {"st": 202, "ed": 204, "text": "cifar 10"}, {"st": 221, "ed": 223, "text": "higher accuracy"}, {"st": 223, "ed": 225, "text": "relative improvement"}, {"st": 226, "ed": 228, "text": "test error"}, {"st": 255, "ed": 257, "text": "similar accuracy"}]
[{"st": 23, "ed": 25, "text": "neural networks"}, {"st": 36, "ed": 39, "text": "restricted boltzmann machines"}, {"st": 48, "ed": 51, "text": "convolutional neural networks"}, {"st": 72, "ed": 74, "text": "facial expression"}, {"st": 154, "ed": 156, "text": "facial action"}, {"st": 160, "ed": 162, "text": "existing approaches"}]
[{"st": 10, "ed": 12, "text": "neural nets"}, {"st": 20, "ed": 23, "text": "variational auto encoders"}, {"st": 33, "ed": 35, "text": "learned features"}, {"st": 42, "ed": 44, "text": "statistical physics"}, {"st": 79, "ed": 81, "text": "neural nets"}]
[{"st": 8, "ed": 10, "text": "signal processing"}, {"st": 18, "ed": 21, "text": "loss in accuracy"}, {"st": 25, "ed": 27, "text": "related methods"}, {"st": 57, "ed": 59, "text": "sliding window"}, {"st": 70, "ed": 73, "text": "convolutional neural networks"}, {"st": 102, "ed": 104, "text": "special case"}, {"st": 125, "ed": 127, "text": "computational complexity"}]
[{"st": 2, "ed": 5, "text": "visual object recognition"}, {"st": 8, "ed": 11, "text": "convolutional neural networks"}, {"st": 12, "ed": 15, "text": "deep belief networks"}, {"st": 26, "ed": 28, "text": "real world"}, {"st": 29, "ed": 31, "text": "visual features"}, {"st": 33, "ed": 35, "text": "unsupervised learning"}, {"st": 65, "ed": 67, "text": "deep learning"}, {"st": 76, "ed": 78, "text": "large scale"}, {"st": 106, "ed": 108, "text": "deep learning"}, {"st": 190, "ed": 192, "text": "unsupervised learning"}, {"st": 193, "ed": 195, "text": "unsupervised learning"}, {"st": 198, "ed": 200, "text": "labelled data"}, {"st": 244, "ed": 246, "text": "top 5"}, {"st": 246, "ed": 248, "text": "error rate"}, {"st": 265, "ed": 267, "text": "validation error"}]
[{"st": 8, "ed": 11, "text": "convolutional neural network"}, {"st": 17, "ed": 19, "text": "visual recognition"}, {"st": 21, "ed": 23, "text": "promising results"}, {"st": 28, "ed": 30, "text": "empirical studies"}, {"st": 96, "ed": 98, "text": "compact representation"}, {"st": 106, "ed": 108, "text": "max pooling"}, {"st": 156, "ed": 158, "text": "context aware"}, {"st": 168, "ed": 170, "text": "context aware"}, {"st": 182, "ed": 184, "text": "extensive experiments"}, {"st": 187, "ed": 189, "text": "standard benchmarks"}, {"st": 191, "ed": 193, "text": "visual recognition"}, {"st": 195, "ed": 197, "text": "image classification"}, {"st": 197, "ed": 199, "text": "fine grained"}, {"st": 217, "ed": 219, "text": "outperforms existing"}, {"st": 236, "ed": 238, "text": "pre trained"}]
[{"st": 4, "ed": 6, "text": "unlabelled data"}, {"st": 17, "ed": 19, "text": "machine learning"}, {"st": 25, "ed": 28, "text": "benchmark data sets"}, {"st": 32, "ed": 34, "text": "unsupervised learning"}, {"st": 36, "ed": 38, "text": "deep learning"}, {"st": 65, "ed": 67, "text": "hand crafted"}, {"st": 72, "ed": 74, "text": "unsupervised learning"}, {"st": 79, "ed": 81, "text": "auto encoder"}, {"st": 85, "ed": 87, "text": "building block"}, {"st": 158, "ed": 160, "text": "invariant representations"}, {"st": 176, "ed": 178, "text": "deep learning"}, {"st": 181, "ed": 183, "text": "feature learning"}, {"st": 197, "ed": 199, "text": "digit recognition"}, {"st": 204, "ed": 206, "text": "object recognition"}, {"st": 211, "ed": 213, "text": "face verification"}, {"st": 220, "ed": 222, "text": "unsupervised learning"}, {"st": 226, "ed": 228, "text": "mnist dataset"}]
[{"st": 8, "ed": 11, "text": "deep neural networks"}, {"st": 65, "ed": 68, "text": "deep neural networks"}, {"st": 81, "ed": 83, "text": "regularization term"}, {"st": 85, "ed": 87, "text": "cost function"}, {"st": 117, "ed": 120, "text": "mnist and cifar10"}]
[{"st": 0, "ed": 3, "text": "deep neural networks"}, {"st": 23, "ed": 26, "text": "large training sets"}, {"st": 49, "ed": 52, "text": "training and test"}, {"st": 66, "ed": 68, "text": "low power"}, {"st": 84, "ed": 86, "text": "deep learning"}, {"st": 196, "ed": 199, "text": "mnist cifar 10"}]
[{"st": 3, "ed": 5, "text": "large scale"}, {"st": 11, "ed": 14, "text": "deep neural networks"}, {"st": 62, "ed": 64, "text": "neural architectures"}, {"st": 65, "ed": 67, "text": "efficient learning"}, {"st": 69, "ed": 71, "text": "multi modal"}, {"st": 94, "ed": 96, "text": "generative framework"}, {"st": 96, "ed": 99, "text": "taking into account"}, {"st": 108, "ed": 110, "text": "important information"}, {"st": 121, "ed": 123, "text": "multi modal"}]
[{"st": 7, "ed": 10, "text": "deep neural networks"}, {"st": 12, "ed": 14, "text": "supervised learning"}, {"st": 17, "ed": 19, "text": "existing methods"}, {"st": 32, "ed": 34, "text": "back propagation"}, {"st": 35, "ed": 37, "text": "vanishing gradients"}, {"st": 52, "ed": 58, "text": "alternating direction method of multipliers admm"}, {"st": 68, "ed": 70, "text": "training process"}, {"st": 72, "ed": 74, "text": "layer wise"}, {"st": 90, "ed": 92, "text": "computational complexity"}, {"st": 111, "ed": 113, "text": "hidden layers"}, {"st": 119, "ed": 121, "text": "supervised hashing"}, {"st": 133, "ed": 135, "text": "supervised hashing"}, {"st": 137, "ed": 139, "text": "significantly outperforms"}]
[{"st": 10, "ed": 13, "text": "deep neural network"}, {"st": 22, "ed": 24, "text": "natural images"}, {"st": 33, "ed": 35, "text": "previous methods"}, {"st": 36, "ed": 38, "text": "generating adversarial"}, {"st": 47, "ed": 49, "text": "class labels"}, {"st": 66, "ed": 68, "text": "adversarial images"}, {"st": 82, "ed": 84, "text": "internal representation"}, {"st": 129, "ed": 131, "text": "natural images"}]
[{"st": 2, "ed": 4, "text": "interstellar dust"}, {"st": 26, "ed": 28, "text": "building blocks"}, {"st": 82, "ed": 84, "text": "statistical methods"}, {"st": 119, "ed": 121, "text": "deep learning"}, {"st": 144, "ed": 147, "text": "deep learning based"}]
[{"st": 7, "ed": 11, "text": "deep convolutional neural network"}, {"st": 20, "ed": 22, "text": "multi scale"}, {"st": 47, "ed": 49, "text": "multi scale"}, {"st": 77, "ed": 79, "text": "multi scale"}, {"st": 106, "ed": 108, "text": "learning problems"}, {"st": 121, "ed": 123, "text": "multi scale"}, {"st": 138, "ed": 140, "text": "classification results"}, {"st": 156, "ed": 158, "text": "benchmark datasets"}, {"st": 158, "ed": 161, "text": "mnist cifar 10"}, {"st": 161, "ed": 163, "text": "cifar 100"}]
[{"st": 5, "ed": 7, "text": "deep learning"}, {"st": 38, "ed": 40, "text": "generalization performance"}, {"st": 41, "ed": 43, "text": "unlabeled data"}, {"st": 44, "ed": 46, "text": "supervised tasks"}, {"st": 55, "ed": 60, "text": "graph based semi supervised learning"}, {"st": 70, "ed": 74, "text": "deep convolutional neural network"}, {"st": 82, "ed": 85, "text": "semi supervised learning"}, {"st": 86, "ed": 89, "text": "deep neural networks"}, {"st": 90, "ed": 92, "text": "forward pass"}, {"st": 116, "ed": 118, "text": "back propagation"}, {"st": 145, "ed": 147, "text": "feature representations"}]
[{"st": 3, "ed": 7, "text": "convolutional neural networks cnn"}, {"st": 8, "ed": 10, "text": "natural image"}, {"st": 30, "ed": 32, "text": "deep learning"}, {"st": 39, "ed": 41, "text": "image classification"}, {"st": 80, "ed": 82, "text": "training data"}, {"st": 85, "ed": 87, "text": "achieve high"}, {"st": 87, "ed": 89, "text": "classification accuracy"}, {"st": 90, "ed": 92, "text": "low variance"}, {"st": 94, "ed": 96, "text": "image classification"}, {"st": 104, "ed": 107, "text": "computed tomography ct"}, {"st": 121, "ed": 123, "text": "training data"}, {"st": 150, "ed": 153, "text": "massachusetts general hospital"}, {"st": 167, "ed": 169, "text": "learning curve"}, {"st": 172, "ed": 174, "text": "classification accuracy"}, {"st": 177, "ed": 179, "text": "training sample"}, {"st": 190, "ed": 192, "text": "training data"}, {"st": 192, "ed": 194, "text": "set size"}, {"st": 200, "ed": 202, "text": "classification accuracy"}]
[{"st": 6, "ed": 8, "text": "spatio temporal"}, {"st": 13, "ed": 15, "text": "visual representations"}, {"st": 19, "ed": 22, "text": "gated recurrent unit"}, {"st": 22, "ed": 24, "text": "recurrent networks"}, {"st": 38, "ed": 41, "text": "deep convolutional network"}, {"st": 63, "ed": 65, "text": "low level"}, {"st": 73, "ed": 75, "text": "spatial resolution"}, {"st": 84, "ed": 86, "text": "low level"}, {"st": 135, "ed": 137, "text": "empirically validate"}]
[{"st": 3, "ed": 5, "text": "machine learning"}, {"st": 18, "ed": 21, "text": "artificial neural network"}, {"st": 24, "ed": 26, "text": "biologically inspired"}, {"st": 26, "ed": 28, "text": "computational models"}, {"st": 39, "ed": 41, "text": "artificial intelligence"}, {"st": 43, "ed": 45, "text": "machine learning"}, {"st": 59, "ed": 63, "text": "convolutional neural network cnn"}, {"st": 73, "ed": 75, "text": "pattern recognition"}, {"st": 101, "ed": 103, "text": "recently published"}, {"st": 113, "ed": 115, "text": "image recognition"}]
[{"st": 4, "ed": 7, "text": "end to end"}, {"st": 7, "ed": 11, "text": "convolutional neural network cnn"}, {"st": 26, "ed": 28, "text": "natural images"}, {"st": 78, "ed": 81, "text": "accuracy and speed"}, {"st": 83, "ed": 85, "text": "recent advances"}, {"st": 86, "ed": 88, "text": "deep learning"}, {"st": 110, "ed": 112, "text": "object category"}, {"st": 128, "ed": 130, "text": "pascal voc"}]
[{"st": 4, "ed": 6, "text": "natural images"}, {"st": 30, "ed": 33, "text": "deep neural network"}, {"st": 73, "ed": 75, "text": "recurrent layers"}, {"st": 80, "ed": 82, "text": "residual connections"}, {"st": 88, "ed": 90, "text": "log likelihood"}, {"st": 92, "ed": 94, "text": "natural images"}, {"st": 106, "ed": 108, "text": "main results"}]
[{"st": 2, "ed": 4, "text": "recent successes"}, {"st": 5, "ed": 7, "text": "deep learning"}, {"st": 8, "ed": 10, "text": "computer vision"}, {"st": 16, "ed": 20, "text": "deep convolutional neural networks"}, {"st": 21, "ed": 23, "text": "facial expression"}, {"st": 43, "ed": 45, "text": "facial action"}, {"st": 49, "ed": 51, "text": "existing approaches"}, {"st": 53, "ed": 55, "text": "hand crafted"}]
[{"st": 2, "ed": 4, "text": "machine learning"}, {"st": 9, "ed": 11, "text": "loss functions"}, {"st": 30, "ed": 32, "text": "loss functions"}, {"st": 50, "ed": 52, "text": "image space"}, {"st": 56, "ed": 58, "text": "image features"}, {"st": 87, "ed": 89, "text": "variational autoencoder"}, {"st": 99, "ed": 101, "text": "generated images"}]
[{"st": 7, "ed": 10, "text": "convolutional neural networks"}, {"st": 38, "ed": 40, "text": "network architecture"}, {"st": 52, "ed": 54, "text": "convolutional layers"}, {"st": 81, "ed": 83, "text": "neural network"}, {"st": 102, "ed": 104, "text": "parameter sharing"}, {"st": 120, "ed": 122, "text": "rotational symmetry"}, {"st": 124, "ed": 126, "text": "improved performance"}]
[{"st": 0, "ed": 3, "text": "deep artificial neural"}, {"st": 19, "ed": 21, "text": "empirical analysis"}, {"st": 40, "ed": 42, "text": "deep learning"}, {"st": 62, "ed": 64, "text": "generalization ability"}, {"st": 75, "ed": 79, "text": "convolutional neural networks cnn"}, {"st": 87, "ed": 89, "text": "object recognition"}, {"st": 110, "ed": 112, "text": "computational models"}, {"st": 131, "ed": 135, "text": "extensive set of experiments"}, {"st": 145, "ed": 147, "text": "classification results"}, {"st": 160, "ed": 162, "text": "significant improvement"}, {"st": 165, "ed": 167, "text": "fine tuning"}]
[{"st": 2, "ed": 5, "text": "deep neural network"}, {"st": 8, "ed": 11, "text": "artificial neural network"}, {"st": 46, "ed": 49, "text": "deep neural network"}, {"st": 57, "ed": 59, "text": "network structure"}, {"st": 77, "ed": 80, "text": "artificial neural network"}, {"st": 109, "ed": 112, "text": "save a lot"}]
[{"st": 52, "ed": 55, "text": "principal components analysis"}, {"st": 71, "ed": 73, "text": "receptive fields"}, {"st": 96, "ed": 98, "text": "receptive fields"}]
[{"st": 14, "ed": 17, "text": "trained neural network"}, {"st": 145, "ed": 147, "text": "non linearity"}, {"st": 156, "ed": 158, "text": "activation functions"}, {"st": 166, "ed": 168, "text": "linear activation"}, {"st": 172, "ed": 174, "text": "benchmark datasets"}, {"st": 176, "ed": 178, "text": "neural networks"}]
[{"st": 6, "ed": 8, "text": "deep learning"}, {"st": 28, "ed": 31, "text": "field of view"}, {"st": 51, "ed": 54, "text": "hand crafted features"}, {"st": 68, "ed": 72, "text": "convolutional neural network cnn"}, {"st": 100, "ed": 102, "text": "significantly improve"}, {"st": 122, "ed": 124, "text": "training data"}]
[{"st": 2, "ed": 5, "text": "convolutional neural network"}]
[{"st": 0, "ed": 2, "text": "pattern recognition"}, {"st": 16, "ed": 18, "text": "key challenge"}, {"st": 19, "ed": 22, "text": "image and video"}, {"st": 27, "ed": 29, "text": "computational cost"}, {"st": 30, "ed": 32, "text": "image processing"}, {"st": 32, "ed": 34, "text": "scales linearly"}, {"st": 92, "ed": 94, "text": "hand written"}, {"st": 94, "ed": 96, "text": "digit classification"}, {"st": 100, "ed": 103, "text": "proof of concept"}, {"st": 109, "ed": 111, "text": "multi class"}, {"st": 136, "ed": 138, "text": "future research"}]
[{"st": 27, "ed": 29, "text": "neural networks"}, {"st": 45, "ed": 47, "text": "important features"}, {"st": 48, "ed": 51, "text": "efficient and effective"}, {"st": 86, "ed": 88, "text": "natural images"}, {"st": 96, "ed": 98, "text": "gradient based"}]
[{"st": 9, "ed": 11, "text": "deep learning"}, {"st": 32, "ed": 34, "text": "deep learning"}, {"st": 40, "ed": 42, "text": "deep learning"}, {"st": 45, "ed": 47, "text": "multilayer perceptron"}, {"st": 49, "ed": 53, "text": "convolutional neural networks cnn"}, {"st": 54, "ed": 57, "text": "recurrent neural networks"}, {"st": 78, "ed": 80, "text": "computer vision"}, {"st": 80, "ed": 82, "text": "natural language"}]
[{"st": 0, "ed": 2, "text": "deep learning"}, {"st": 13, "ed": 15, "text": "wide variety"}, {"st": 16, "ed": 18, "text": "predictive analytics"}, {"st": 22, "ed": 25, "text": "recurrent neural networks"}, {"st": 92, "ed": 94, "text": "neural networks"}, {"st": 140, "ed": 142, "text": "path planning"}, {"st": 142, "ed": 145, "text": "sequential decision making"}, {"st": 193, "ed": 195, "text": "material science"}]
[{"st": 13, "ed": 15, "text": "network structure"}, {"st": 16, "ed": 18, "text": "impressive results"}, {"st": 31, "ed": 33, "text": "architectures including"}, {"st": 97, "ed": 99, "text": "existing methods"}, {"st": 141, "ed": 146, "text": "cifar 10 and cifar 100"}]
[{"st": 7, "ed": 9, "text": "computationally efficient"}, {"st": 11, "ed": 15, "text": "convolutional neural networks cnns"}, {"st": 29, "ed": 31, "text": "significant reduction"}, {"st": 32, "ed": 34, "text": "computational cost"}, {"st": 44, "ed": 46, "text": "deep cnns"}, {"st": 46, "ed": 48, "text": "without compromising"}, {"st": 89, "ed": 91, "text": "higher accuracy"}, {"st": 109, "ed": 111, "text": "resnet 50"}, {"st": 115, "ed": 117, "text": "fewer parameters"}, {"st": 119, "ed": 121, "text": "floating point"}, {"st": 142, "ed": 144, "text": "floating point"}, {"st": 147, "ed": 149, "text": "fewer parameters"}, {"st": 162, "ed": 164, "text": "fewer parameters"}]
[{"st": 0, "ed": 3, "text": "deep residual networks"}, {"st": 27, "ed": 29, "text": "improved accuracy"}, {"st": 40, "ed": 43, "text": "deep residual networks"}, {"st": 99, "ed": 101, "text": "network structures"}, {"st": 102, "ed": 104, "text": "residual networks"}, {"st": 133, "ed": 135, "text": "residual network"}, {"st": 137, "ed": 140, "text": "accuracy and efficiency"}, {"st": 142, "ed": 145, "text": "deep residual networks"}, {"st": 148, "ed": 150, "text": "deep networks"}, {"st": 162, "ed": 164, "text": "significant improvements"}, {"st": 171, "ed": 175, "text": "available at https github.com"}, {"st": 177, "ed": 179, "text": "residual networks"}]
[{"st": 4, "ed": 6, "text": "neural nets"}, {"st": 13, "ed": 15, "text": "adversarial examples"}, {"st": 37, "ed": 39, "text": "neural net"}, {"st": 71, "ed": 74, "text": "deep neural nets"}, {"st": 78, "ed": 82, "text": "mnist and cifar 10"}, {"st": 103, "ed": 105, "text": "existing approaches"}, {"st": 110, "ed": 112, "text": "adversarial examples"}, {"st": 129, "ed": 131, "text": "neural net"}, {"st": 144, "ed": 146, "text": "previously proposed"}]
[{"st": 0, "ed": 2, "text": "object recognition"}, {"st": 21, "ed": 23, "text": "linear unit"}, {"st": 36, "ed": 40, "text": "convolutional neural networks cnns"}, {"st": 78, "ed": 81, "text": "mnist cifar 10"}, {"st": 117, "ed": 119, "text": "relative error"}]
[{"st": 4, "ed": 6, "text": "polyphonic music"}, {"st": 45, "ed": 48, "text": "convolutional neural network"}, {"st": 54, "ed": 56, "text": "real world"}, {"st": 63, "ed": 65, "text": "fixed length"}, {"st": 83, "ed": 85, "text": "audio signal"}, {"st": 138, "ed": 140, "text": "extensive experiments"}, {"st": 155, "ed": 157, "text": "activation functions"}, {"st": 158, "ed": 160, "text": "neural networks"}, {"st": 182, "ed": 185, "text": "convolutional neural networks"}, {"st": 189, "ed": 191, "text": "conventional methods"}, {"st": 196, "ed": 198, "text": "source separation"}, {"st": 208, "ed": 210, "text": "convolutional network"}, {"st": 229, "ed": 231, "text": "performance improvement"}]
[{"st": 1, "ed": 3, "text": "visual recognition"}, {"st": 6, "ed": 8, "text": "image classification"}, {"st": 8, "ed": 10, "text": "unsupervised learning"}, {"st": 12, "ed": 14, "text": "unlabeled data"}, {"st": 35, "ed": 37, "text": "unsupervised methods"}, {"st": 46, "ed": 49, "text": "convolutional neural networks"}, {"st": 59, "ed": 61, "text": "previous works"}, {"st": 76, "ed": 78, "text": "total number"}, {"st": 79, "ed": 81, "text": "trainable parameters"}, {"st": 96, "ed": 99, "text": "mnist cifar 10"}, {"st": 99, "ed": 101, "text": "cifar 100"}, {"st": 104, "ed": 106, "text": "image classification"}]
[{"st": 0, "ed": 4, "text": "convolutional neural networks cnns"}, {"st": 12, "ed": 14, "text": "supervised learning"}, {"st": 17, "ed": 19, "text": "convolutional filters"}, {"st": 88, "ed": 90, "text": "weight sharing"}, {"st": 111, "ed": 113, "text": "multilayer perceptrons"}]
[{"st": 0, "ed": 2, "text": "recent results"}, {"st": 4, "ed": 7, "text": "deep neural networks"}, {"st": 8, "ed": 10, "text": "excellent performance"}, {"st": 83, "ed": 85, "text": "cifar 10"}, {"st": 117, "ed": 119, "text": "base level"}, {"st": 152, "ed": 154, "text": "cifar 10"}, {"st": 183, "ed": 185, "text": "cifar 10"}]
[{"st": 10, "ed": 12, "text": "recent advances"}, {"st": 20, "ed": 22, "text": "object categorization"}, {"st": 36, "ed": 38, "text": "non linearity"}, {"st": 43, "ed": 45, "text": "batch normalization"}, {"st": 56, "ed": 58, "text": "fully connected"}, {"st": 60, "ed": 62, "text": "pre processing"}, {"st": 64, "ed": 66, "text": "learning parameters"}, {"st": 66, "ed": 68, "text": "learning rate"}, {"st": 68, "ed": 70, "text": "batch size"}, {"st": 76, "ed": 78, "text": "performance gains"}, {"st": 133, "ed": 135, "text": "network structure"}]
[{"st": 33, "ed": 35, "text": "exponentially large"}, {"st": 64, "ed": 66, "text": "hyper parameters"}, {"st": 110, "ed": 112, "text": "back propagation"}, {"st": 116, "ed": 118, "text": "scales linearly"}, {"st": 134, "ed": 136, "text": "image classification"}, {"st": 137, "ed": 140, "text": "mnist and cifar10"}]
[{"st": 48, "ed": 50, "text": "main result"}, {"st": 63, "ed": 65, "text": "denoising autoencoder"}, {"st": 79, "ed": 81, "text": "fully connected"}, {"st": 86, "ed": 89, "text": "orders of magnitude"}, {"st": 102, "ed": 104, "text": "convolutional network"}, {"st": 108, "ed": 110, "text": "fully connected"}, {"st": 112, "ed": 114, "text": "convolutional architectures"}, {"st": 123, "ed": 125, "text": "computer vision"}, {"st": 165, "ed": 167, "text": "fully connected"}]
[{"st": 0, "ed": 3, "text": "deep neural networks"}, {"st": 29, "ed": 31, "text": "highly complex"}, {"st": 48, "ed": 50, "text": "neural networks"}, {"st": 53, "ed": 55, "text": "low dimensional"}, {"st": 99, "ed": 101, "text": "neural network"}, {"st": 105, "ed": 107, "text": "minimization problem"}, {"st": 125, "ed": 127, "text": "neural network"}]
[{"st": 25, "ed": 30, "text": "convolutional neural network cnn architecture"}, {"st": 34, "ed": 37, "text": "short term memory"}, {"st": 43, "ed": 45, "text": "visual features"}, {"st": 79, "ed": 81, "text": "false positive"}, {"st": 82, "ed": 84, "text": "false negative"}, {"st": 116, "ed": 118, "text": "recent developments"}, {"st": 168, "ed": 170, "text": "training set"}]
[{"st": 4, "ed": 6, "text": "framework called"}, {"st": 7, "ed": 9, "text": "multi resolution"}, {"st": 93, "ed": 95, "text": "ridge regression"}, {"st": 153, "ed": 155, "text": "complementary information"}, {"st": 206, "ed": 208, "text": "network topologies"}, {"st": 244, "ed": 246, "text": "complementary information"}]
[{"st": 1, "ed": 3, "text": "machine learning"}, {"st": 5, "ed": 8, "text": "vulnerable to adversarial"}, {"st": 10, "ed": 12, "text": "adversarial perturbation"}, {"st": 60, "ed": 62, "text": "detection method"}, {"st": 64, "ed": 66, "text": "adversarial images"}, {"st": 73, "ed": 75, "text": "principal components"}, {"st": 82, "ed": 84, "text": "saliency map"}]
[{"st": 0, "ed": 4, "text": "deep neural network dnn"}, {"st": 12, "ed": 14, "text": "prediction accuracy"}, {"st": 16, "ed": 18, "text": "transcription factor"}, {"st": 25, "ed": 27, "text": "remains unclear"}, {"st": 32, "ed": 34, "text": "dna sequence"}, {"st": 74, "ed": 77, "text": "deep neural network"}, {"st": 100, "ed": 102, "text": "visualization method"}, {"st": 108, "ed": 110, "text": "saliency map"}, {"st": 129, "ed": 131, "text": "recurrent models"}, {"st": 167, "ed": 169, "text": "class specific"}, {"st": 174, "ed": 176, "text": "input sequence"}, {"st": 180, "ed": 182, "text": "positive class"}, {"st": 193, "ed": 195, "text": "recurrent architecture"}, {"st": 203, "ed": 205, "text": "visualization techniques"}]
[{"st": 22, "ed": 24, "text": "deep learning"}, {"st": 41, "ed": 44, "text": "recurrent neural networks"}, {"st": 80, "ed": 82, "text": "machine learning"}, {"st": 129, "ed": 131, "text": "machine learning"}, {"st": 143, "ed": 145, "text": "deep learning"}, {"st": 152, "ed": 154, "text": "feature based"}, {"st": 154, "ed": 156, "text": "machine learning"}]
[{"st": 3, "ed": 8, "text": "deep convolutional neural networks cnn"}, {"st": 12, "ed": 14, "text": "temporal patterns"}, {"st": 27, "ed": 29, "text": "labeled data"}, {"st": 50, "ed": 54, "text": "deep convolutional neural network"}, {"st": 111, "ed": 113, "text": "improved performance"}, {"st": 126, "ed": 128, "text": "training set"}, {"st": 157, "ed": 159, "text": "classification accuracy"}, {"st": 189, "ed": 191, "text": "class conditional"}]
[{"st": 0, "ed": 4, "text": "convolutional neural networks cnns"}, {"st": 5, "ed": 7, "text": "recently shown"}, {"st": 15, "ed": 17, "text": "object category"}, {"st": 31, "ed": 33, "text": "competing approaches"}, {"st": 57, "ed": 59, "text": "detailed analysis"}, {"st": 71, "ed": 73, "text": "joint training"}, {"st": 76, "ed": 78, "text": "detection task"}, {"st": 101, "ed": 103, "text": "training data"}, {"st": 106, "ed": 108, "text": "synthetic data"}, {"st": 148, "ed": 150, "text": "classification task"}]
[{"st": 4, "ed": 6, "text": "related problems"}, {"st": 28, "ed": 30, "text": "correctly classified"}, {"st": 58, "ed": 60, "text": "computer vision"}, {"st": 60, "ed": 62, "text": "natural language"}, {"st": 64, "ed": 67, "text": "automatic speech recognition"}, {"st": 88, "ed": 90, "text": "future research"}]
[{"st": 0, "ed": 3, "text": "optical coherence tomography"}, {"st": 29, "ed": 33, "text": "age related macular degeneration"}, {"st": 51, "ed": 53, "text": "time consuming"}, {"st": 73, "ed": 75, "text": "ground truth"}, {"st": 96, "ed": 98, "text": "ground truth"}, {"st": 105, "ed": 107, "text": "based methods"}, {"st": 141, "ed": 143, "text": "semi supervised"}, {"st": 153, "ed": 158, "text": "convolutional neural network cnn architecture"}, {"st": 162, "ed": 164, "text": "level labels"}]
[{"st": 141, "ed": 143, "text": "neural network"}, {"st": 147, "ed": 152, "text": "long short term memory lstm"}, {"st": 154, "ed": 156, "text": "sequential data"}, {"st": 159, "ed": 161, "text": "classification performance"}, {"st": 224, "ed": 226, "text": "existing approaches"}, {"st": 238, "ed": 240, "text": "neural network"}]
[{"st": 9, "ed": 12, "text": "convolutional neural networks"}, {"st": 43, "ed": 45, "text": "tone mapping"}]
[{"st": 0, "ed": 2, "text": "recent research"}, {"st": 4, "ed": 6, "text": "deep learning"}, {"st": 25, "ed": 27, "text": "deep learning"}, {"st": 41, "ed": 43, "text": "deep learning"}, {"st": 80, "ed": 82, "text": "deep learning"}, {"st": 89, "ed": 91, "text": "neural network"}, {"st": 108, "ed": 110, "text": "taylor series"}, {"st": 118, "ed": 122, "text": "available at https github.com"}]
[{"st": 0, "ed": 2, "text": "recent advances"}, {"st": 13, "ed": 15, "text": "input distributions"}, {"st": 45, "ed": 47, "text": "class distributions"}, {"st": 82, "ed": 85, "text": "sequential decision making"}, {"st": 89, "ed": 91, "text": "bandit problem"}, {"st": 111, "ed": 114, "text": "end to end"}, {"st": 131, "ed": 134, "text": "convolutional neural network"}]
[{"st": 1, "ed": 3, "text": "machine learning"}, {"st": 49, "ed": 51, "text": "computer vision"}, {"st": 52, "ed": 54, "text": "hand crafted"}, {"st": 110, "ed": 112, "text": "neural network"}, {"st": 141, "ed": 143, "text": "rgb image"}, {"st": 148, "ed": 150, "text": "cross modal"}]
[{"st": 2, "ed": 5, "text": "deep neural networks"}]
[{"st": 4, "ed": 6, "text": "deep learning"}, {"st": 7, "ed": 9, "text": "neural network"}, {"st": 15, "ed": 17, "text": "saddle points"}, {"st": 35, "ed": 39, "text": "deep convolutional neural networks"}, {"st": 45, "ed": 47, "text": "network size"}, {"st": 50, "ed": 52, "text": "training data"}, {"st": 93, "ed": 95, "text": "empirical risk"}, {"st": 193, "ed": 195, "text": "empirical risk"}, {"st": 199, "ed": 201, "text": "cifar 10"}, {"st": 204, "ed": 206, "text": "training process"}, {"st": 230, "ed": 232, "text": "empirical loss"}]
[{"st": 3, "ed": 5, "text": "deep learning"}, {"st": 8, "ed": 10, "text": "diabetic retinopathy"}, {"st": 18, "ed": 20, "text": "proposed method"}, {"st": 33, "ed": 35, "text": "pooling layer"}, {"st": 37, "ed": 39, "text": "convolutional networks"}, {"st": 75, "ed": 77, "text": "deep learning"}, {"st": 94, "ed": 96, "text": "prediction performance"}, {"st": 115, "ed": 117, "text": "experiments conducted"}, {"st": 119, "ed": 121, "text": "large scale"}, {"st": 123, "ed": 125, "text": "image dataset"}, {"st": 133, "ed": 135, "text": "achieve high"}]
[{"st": 0, "ed": 3, "text": "deep neural networks"}, {"st": 15, "ed": 17, "text": "machine learning"}, {"st": 30, "ed": 32, "text": "computational requirements"}, {"st": 36, "ed": 38, "text": "large scale"}, {"st": 67, "ed": 70, "text": "deep neural networks"}, {"st": 74, "ed": 76, "text": "computational requirements"}, {"st": 193, "ed": 195, "text": "image recognition"}, {"st": 198, "ed": 200, "text": "cifar 10"}, {"st": 207, "ed": 209, "text": "vgg 16"}, {"st": 232, "ed": 234, "text": "performance improvement"}]
[{"st": 6, "ed": 8, "text": "neural networks"}, {"st": 23, "ed": 25, "text": "deep learning"}, {"st": 25, "ed": 27, "text": "important features"}, {"st": 36, "ed": 38, "text": "neural network"}, {"st": 83, "ed": 86, "text": "positive and negative"}, {"st": 125, "ed": 127, "text": "gradient based"}]
[{"st": 75, "ed": 78, "text": "deep neural networks"}, {"st": 91, "ed": 93, "text": "point cloud"}, {"st": 113, "ed": 115, "text": "deep learning"}, {"st": 117, "ed": 119, "text": "source code"}, {"st": 120, "ed": 124, "text": "available at https github.com"}]
[{"st": 3, "ed": 5, "text": "convolutional networks"}, {"st": 12, "ed": 15, "text": "convolutional neural networks"}, {"st": 45, "ed": 47, "text": "negative samples"}, {"st": 79, "ed": 81, "text": "conduct experiments"}, {"st": 82, "ed": 84, "text": "benchmark datasets"}, {"st": 85, "ed": 88, "text": "mnist cifar 10"}]
[{"st": 8, "ed": 10, "text": "autonomous driving"}, {"st": 14, "ed": 16, "text": "neural network"}, {"st": 55, "ed": 57, "text": "domain knowledge"}, {"st": 97, "ed": 99, "text": "wide variety"}]
[{"st": 1, "ed": 3, "text": "lung cancer"}, {"st": 31, "ed": 33, "text": "detection methods"}, {"st": 43, "ed": 46, "text": "hand crafted features"}, {"st": 102, "ed": 104, "text": "artificial intelligence"}, {"st": 197, "ed": 199, "text": "lidc idri"}]
[{"st": 6, "ed": 9, "text": "deep neural networks"}, {"st": 13, "ed": 15, "text": "real world"}, {"st": 33, "ed": 35, "text": "promising results"}, {"st": 36, "ed": 38, "text": "existing methods"}, {"st": 46, "ed": 48, "text": "deep network"}, {"st": 57, "ed": 59, "text": "deep network"}, {"st": 72, "ed": 74, "text": "layer wise"}, {"st": 82, "ed": 84, "text": "proposed method"}, {"st": 99, "ed": 101, "text": "layer wise"}, {"st": 101, "ed": 103, "text": "error function"}, {"st": 113, "ed": 115, "text": "final prediction"}, {"st": 123, "ed": 125, "text": "linear combination"}, {"st": 159, "ed": 162, "text": "conduct extensive experiments"}, {"st": 163, "ed": 165, "text": "benchmark datasets"}]
[{"st": 1, "ed": 3, "text": "machine learning"}, {"st": 17, "ed": 19, "text": "low level"}, {"st": 39, "ed": 42, "text": "convolutional neural network"}, {"st": 49, "ed": 51, "text": "low level"}, {"st": 64, "ed": 66, "text": "ground truth"}, {"st": 155, "ed": 158, "text": "achieve competitive results"}]
[{"st": 6, "ed": 9, "text": "computer aided diagnosis"}, {"st": 12, "ed": 14, "text": "traditional methods"}, {"st": 32, "ed": 34, "text": "deep convolutional"}, {"st": 36, "ed": 38, "text": "natural image"}, {"st": 40, "ed": 42, "text": "multi instance"}, {"st": 53, "ed": 56, "text": "end to end"}, {"st": 58, "ed": 60, "text": "multi instance"}, {"st": 80, "ed": 82, "text": "multi instance"}]
[{"st": 2, "ed": 4, "text": "neural network"}, {"st": 4, "ed": 6, "text": "hyperparameter optimization"}, {"st": 10, "ed": 12, "text": "computationally expensive"}, {"st": 37, "ed": 39, "text": "final performance"}, {"st": 41, "ed": 43, "text": "trained model"}, {"st": 48, "ed": 50, "text": "network architectures"}, {"st": 52, "ed": 54, "text": "time series"}, {"st": 63, "ed": 65, "text": "prediction models"}, {"st": 86, "ed": 88, "text": "final performance"}, {"st": 115, "ed": 117, "text": "prediction models"}, {"st": 121, "ed": 123, "text": "early stopping"}, {"st": 125, "ed": 127, "text": "hyperparameter optimization"}, {"st": 142, "ed": 144, "text": "hyperparameter optimization"}, {"st": 153, "ed": 155, "text": "early stopping"}, {"st": 162, "ed": 164, "text": "reinforcement learning"}, {"st": 166, "ed": 168, "text": "selection algorithms"}, {"st": 181, "ed": 183, "text": "prediction models"}, {"st": 184, "ed": 186, "text": "early stopping"}, {"st": 195, "ed": 197, "text": "prediction accuracy"}]
[{"st": 8, "ed": 11, "text": "deep neural networks"}, {"st": 18, "ed": 20, "text": "parallel computing"}, {"st": 31, "ed": 34, "text": "deep neural networks"}, {"st": 36, "ed": 38, "text": "significant improvements"}, {"st": 46, "ed": 49, "text": "deep neural networks"}, {"st": 54, "ed": 56, "text": "computational complexity"}, {"st": 74, "ed": 77, "text": "deep neural networks"}, {"st": 82, "ed": 85, "text": "deep neural network"}, {"st": 95, "ed": 97, "text": "generative modeling"}, {"st": 108, "ed": 111, "text": "deep neural networks"}, {"st": 122, "ed": 124, "text": "natural selection"}, {"st": 148, "ed": 151, "text": "deep neural networks"}, {"st": 155, "ed": 157, "text": "network architectures"}, {"st": 165, "ed": 167, "text": "significant improvements"}]
[{"st": 3, "ed": 5, "text": "deep learning"}, {"st": 18, "ed": 20, "text": "classification performance"}, {"st": 23, "ed": 27, "text": "multi layer perceptron mlp"}, {"st": 29, "ed": 32, "text": "deep neural network"}, {"st": 56, "ed": 58, "text": "classification accuracy"}, {"st": 62, "ed": 65, "text": "deep neural network"}, {"st": 81, "ed": 84, "text": "deep neural network"}, {"st": 86, "ed": 89, "text": "orders of magnitude"}]
[{"st": 0, "ed": 4, "text": "deep convolutional neural networks"}, {"st": 26, "ed": 28, "text": "real world"}, {"st": 48, "ed": 50, "text": "expert knowledge"}, {"st": 56, "ed": 59, "text": "simple and effective"}, {"st": 64, "ed": 66, "text": "deep models"}, {"st": 68, "ed": 71, "text": "end to end"}, {"st": 110, "ed": 112, "text": "optimization problem"}, {"st": 117, "ed": 119, "text": "proximal gradient"}, {"st": 148, "ed": 150, "text": "selection methods"}, {"st": 158, "ed": 160, "text": "fine tuning"}, {"st": 165, "ed": 168, "text": "end to end"}, {"st": 193, "ed": 195, "text": "promising results"}, {"st": 197, "ed": 200, "text": "depth and width"}]
[{"st": 1, "ed": 4, "text": "deep neural networks"}, {"st": 29, "ed": 31, "text": "neural networks"}, {"st": 32, "ed": 35, "text": "attracted much attention"}, {"st": 76, "ed": 78, "text": "distribution matching"}, {"st": 102, "ed": 104, "text": "loss function"}, {"st": 107, "ed": 111, "text": "maximum mean discrepancy mmd"}, {"st": 119, "ed": 121, "text": "loss function"}, {"st": 124, "ed": 126, "text": "significantly improve"}, {"st": 160, "ed": 162, "text": "fine tune"}]
[{"st": 1, "ed": 3, "text": "computer vision"}, {"st": 4, "ed": 6, "text": "prior works"}, {"st": 17, "ed": 19, "text": "network weights"}, {"st": 34, "ed": 37, "text": "training and inference"}, {"st": 130, "ed": 132, "text": "significantly improve"}, {"st": 149, "ed": 152, "text": "training and inference"}, {"st": 177, "ed": 179, "text": "previously reported"}, {"st": 191, "ed": 193, "text": "previously reported"}]
[{"st": 1, "ed": 3, "text": "image analysis"}, {"st": 13, "ed": 15, "text": "deep learning"}, {"st": 18, "ed": 20, "text": "deep learning"}, {"st": 31, "ed": 33, "text": "image analysis"}, {"st": 63, "ed": 65, "text": "open source"}, {"st": 68, "ed": 70, "text": "deep learning"}, {"st": 110, "ed": 112, "text": "deep learning"}, {"st": 119, "ed": 121, "text": "applications including"}, {"st": 123, "ed": 125, "text": "image generation"}, {"st": 126, "ed": 128, "text": "representation learning"}, {"st": 139, "ed": 141, "text": "network architectures"}, {"st": 141, "ed": 143, "text": "loss functions"}, {"st": 144, "ed": 146, "text": "evaluation metrics"}, {"st": 157, "ed": 159, "text": "image analysis"}, {"st": 173, "ed": 176, "text": "2d and 3d"}, {"st": 187, "ed": 189, "text": "image analysis"}, {"st": 200, "ed": 202, "text": "computed tomography"}, {"st": 207, "ed": 209, "text": "computed tomography"}, {"st": 213, "ed": 215, "text": "magnetic resonance"}, {"st": 235, "ed": 237, "text": "deep learning"}, {"st": 241, "ed": 243, "text": "image generation"}, {"st": 244, "ed": 246, "text": "representation learning"}]
[{"st": 6, "ed": 8, "text": "fully automated"}, {"st": 18, "ed": 20, "text": "nodule detection"}, {"st": 37, "ed": 39, "text": "nodule detection"}, {"st": 51, "ed": 53, "text": "nodule detection"}, {"st": 55, "ed": 57, "text": "u net"}, {"st": 58, "ed": 60, "text": "encoder decoder"}, {"st": 93, "ed": 95, "text": "lidc idri"}, {"st": 127, "ed": 129, "text": "nodule detection"}, {"st": 139, "ed": 142, "text": "extensive experimental results"}, {"st": 162, "ed": 164, "text": "lidc idri"}]
[{"st": 6, "ed": 8, "text": "generative models"}, {"st": 22, "ed": 24, "text": "generative models"}, {"st": 28, "ed": 30, "text": "objective function"}, {"st": 46, "ed": 48, "text": "objective functions"}, {"st": 88, "ed": 90, "text": "generative model"}, {"st": 117, "ed": 119, "text": "loss function"}, {"st": 139, "ed": 141, "text": "simulated data"}]
[{"st": 3, "ed": 5, "text": "activation functions"}, {"st": 6, "ed": 8, "text": "deep networks"}, {"st": 26, "ed": 28, "text": "activation function"}, {"st": 30, "ed": 34, "text": "rectified linear unit relu"}, {"st": 37, "ed": 39, "text": "hand designed"}, {"st": 76, "ed": 78, "text": "reinforcement learning"}, {"st": 96, "ed": 98, "text": "empirical evaluation"}, {"st": 111, "ed": 113, "text": "activation function"}, {"st": 151, "ed": 153, "text": "classification accuracy"}]
[{"st": 7, "ed": 9, "text": "neural networks"}, {"st": 31, "ed": 33, "text": "convergence speed"}, {"st": 43, "ed": 45, "text": "method called"}, {"st": 49, "ed": 51, "text": "multi level"}, {"st": 59, "ed": 61, "text": "neural network"}, {"st": 64, "ed": 66, "text": "method called"}, {"st": 79, "ed": 81, "text": "jointly learn"}, {"st": 90, "ed": 92, "text": "convergence rate"}]
[{"st": 15, "ed": 17, "text": "prior knowledge"}, {"st": 26, "ed": 28, "text": "based approaches"}, {"st": 43, "ed": 45, "text": "synthetic data"}, {"st": 46, "ed": 48, "text": "ground truth"}, {"st": 53, "ed": 55, "text": "domain adaptation"}, {"st": 67, "ed": 70, "text": "end to end"}, {"st": 149, "ed": 151, "text": "object appearance"}, {"st": 153, "ed": 155, "text": "real images"}, {"st": 163, "ed": 165, "text": "domain adaptation"}, {"st": 182, "ed": 185, "text": "end to end"}, {"st": 187, "ed": 189, "text": "real images"}]
[{"st": 13, "ed": 15, "text": "supervised learning"}, {"st": 40, "ed": 42, "text": "chromatic aberration"}, {"st": 53, "ed": 55, "text": "mid level"}, {"st": 96, "ed": 99, "text": "pascal voc 2007"}, {"st": 101, "ed": 104, "text": "pascal voc 2012"}, {"st": 120, "ed": 122, "text": "baseline method"}, {"st": 130, "ed": 132, "text": "transfer learning"}, {"st": 141, "ed": 143, "text": "network architectures"}]
[{"st": 4, "ed": 6, "text": "deep learning"}, {"st": 16, "ed": 18, "text": "large scale"}, {"st": 18, "ed": 20, "text": "point clouds"}, {"st": 31, "ed": 33, "text": "point clouds"}]
[{"st": 0, "ed": 3, "text": "stochastic gradient descent"}, {"st": 15, "ed": 17, "text": "empirical performance"}, {"st": 54, "ed": 56, "text": "non linearity"}, {"st": 66, "ed": 68, "text": "non linearity"}, {"st": 80, "ed": 82, "text": "multi layer"}, {"st": 122, "ed": 124, "text": "faster convergence"}, {"st": 126, "ed": 128, "text": "theoretical properties"}, {"st": 136, "ed": 138, "text": "large scale"}]
[{"st": 22, "ed": 24, "text": "modern architecture"}, {"st": 58, "ed": 61, "text": "convolutional neural networks"}, {"st": 70, "ed": 73, "text": "convolutional neural network"}]
[{"st": 5, "ed": 7, "text": "convolutional networks"}, {"st": 19, "ed": 21, "text": "computational budget"}, {"st": 89, "ed": 91, "text": "feature map"}, {"st": 119, "ed": 121, "text": "computational costs"}, {"st": 135, "ed": 137, "text": "network parameters"}, {"st": 175, "ed": 177, "text": "extensive experiments"}]
[{"st": 0, "ed": 2, "text": "deep learning"}, {"st": 30, "ed": 32, "text": "generative models"}, {"st": 67, "ed": 69, "text": "fully connected"}, {"st": 90, "ed": 92, "text": "challenging task"}]
[{"st": 4, "ed": 6, "text": "deep learning"}, {"st": 13, "ed": 15, "text": "low precision"}, {"st": 53, "ed": 55, "text": "efficient inference"}, {"st": 56, "ed": 58, "text": "low precision"}, {"st": 76, "ed": 78, "text": "low precision"}]
[{"st": 18, "ed": 20, "text": "kalman filter"}, {"st": 31, "ed": 33, "text": "kalman filter"}, {"st": 70, "ed": 72, "text": "image registration"}, {"st": 134, "ed": 136, "text": "prior knowledge"}, {"st": 141, "ed": 143, "text": "optimization process"}, {"st": 147, "ed": 150, "text": "accuracy and speed"}]
[{"st": 10, "ed": 12, "text": "autonomous systems"}, {"st": 90, "ed": 92, "text": "generative modeling"}, {"st": 129, "ed": 131, "text": "noisy data"}, {"st": 203, "ed": 205, "text": "generative model"}]
[{"st": 6, "ed": 8, "text": "word sense"}, {"st": 28, "ed": 30, "text": "unlabeled data"}, {"st": 34, "ed": 37, "text": "latent dirichlet allocation"}, {"st": 38, "ed": 40, "text": "topic model"}, {"st": 87, "ed": 89, "text": "word sense"}]
[{"st": 5, "ed": 7, "text": "machine learning"}, {"st": 18, "ed": 20, "text": "natural language"}, {"st": 64, "ed": 66, "text": "answer questions"}, {"st": 99, "ed": 101, "text": "learning systems"}, {"st": 137, "ed": 139, "text": "recently introduced"}]
[{"st": 20, "ed": 23, "text": "long term memory"}, {"st": 32, "ed": 35, "text": "long term memory"}, {"st": 57, "ed": 59, "text": "question answering"}, {"st": 62, "ed": 65, "text": "long term memory"}, {"st": 84, "ed": 86, "text": "large scale"}, {"st": 118, "ed": 120, "text": "answer questions"}]
[{"st": 18, "ed": 20, "text": "natural language"}, {"st": 49, "ed": 51, "text": "conditional probabilities"}, {"st": 59, "ed": 61, "text": "evaluation measures"}, {"st": 164, "ed": 166, "text": "evaluation measures"}]
[{"st": 4, "ed": 6, "text": "knowledge graph"}, {"st": 78, "ed": 80, "text": "training objective"}]
[{"st": 82, "ed": 84, "text": "conversational agents"}]
[{"st": 1, "ed": 3, "text": "sentiment analysis"}, {"st": 22, "ed": 24, "text": "user generated"}, {"st": 49, "ed": 51, "text": "social networks"}, {"st": 64, "ed": 66, "text": "unstructured text"}, {"st": 105, "ed": 108, "text": "bag of words"}, {"st": 129, "ed": 131, "text": "machine learning"}, {"st": 131, "ed": 133, "text": "classification algorithms"}, {"st": 146, "ed": 148, "text": "classification accuracy"}]
[{"st": 0, "ed": 2, "text": "word embeddings"}, {"st": 16, "ed": 18, "text": "natural language"}, {"st": 22, "ed": 24, "text": "question answering"}, {"st": 33, "ed": 35, "text": "word embeddings"}, {"st": 39, "ed": 41, "text": "modeling language"}, {"st": 58, "ed": 60, "text": "set theoretic"}, {"st": 63, "ed": 65, "text": "least squares"}, {"st": 78, "ed": 80, "text": "relative improvement"}, {"st": 94, "ed": 96, "text": "competitive results"}]
[{"st": 14, "ed": 16, "text": "rnn based"}, {"st": 16, "ed": 18, "text": "encoder decoder"}, {"st": 51, "ed": 53, "text": "shows significant"}, {"st": 57, "ed": 59, "text": "rnn based"}, {"st": 59, "ed": 61, "text": "encoder decoder"}]
[{"st": 0, "ed": 2, "text": "recent research"}, {"st": 60, "ed": 62, "text": "common sense"}, {"st": 73, "ed": 75, "text": "significantly improves"}]
[{"st": 46, "ed": 48, "text": "tasks including"}, {"st": 64, "ed": 67, "text": "multi task learning"}, {"st": 74, "ed": 76, "text": "multi task"}]
[{"st": 15, "ed": 17, "text": "sentiment analysis"}, {"st": 22, "ed": 24, "text": "supervised learning"}, {"st": 49, "ed": 51, "text": "supervised learning"}, {"st": 55, "ed": 57, "text": "specifically designed"}, {"st": 76, "ed": 78, "text": "traditional approaches"}, {"st": 88, "ed": 90, "text": "text classification"}, {"st": 103, "ed": 106, "text": "easy to implement"}, {"st": 112, "ed": 114, "text": "supervised learning"}, {"st": 139, "ed": 141, "text": "extensive experimental"}, {"st": 168, "ed": 170, "text": "competitive results"}, {"st": 211, "ed": 213, "text": "machine learning"}, {"st": 214, "ed": 216, "text": "natural language"}]
[]
[{"st": 35, "ed": 37, "text": "real world"}, {"st": 48, "ed": 50, "text": "previous approaches"}, {"st": 61, "ed": 63, "text": "predicate calculus"}, {"st": 69, "ed": 71, "text": "machine learning"}, {"st": 76, "ed": 79, "text": "scale to large"}, {"st": 79, "ed": 81, "text": "natural language"}, {"st": 87, "ed": 89, "text": "structural similarity"}, {"st": 132, "ed": 135, "text": "recurrent neural networks"}, {"st": 158, "ed": 161, "text": "precision and recall"}]
[{"st": 4, "ed": 6, "text": "united nations"}, {"st": 58, "ed": 60, "text": "english language"}]
[{"st": 0, "ed": 2, "text": "foreign policy"}, {"st": 36, "ed": 38, "text": "word embedding"}, {"st": 53, "ed": 55, "text": "united nations"}, {"st": 168, "ed": 170, "text": "foreign policy"}]
[{"st": 11, "ed": 13, "text": "multiple choice"}, {"st": 45, "ed": 47, "text": "domain specific"}, {"st": 84, "ed": 86, "text": "multiple choice"}, {"st": 100, "ed": 102, "text": "method produces"}, {"st": 131, "ed": 133, "text": "training data"}]
[{"st": 71, "ed": 73, "text": "short text"}]
[{"st": 13, "ed": 15, "text": "n gram"}, {"st": 29, "ed": 31, "text": "word embeddings"}, {"st": 78, "ed": 80, "text": "word embeddings"}, {"st": 92, "ed": 94, "text": "sentiment classification"}]
[{"st": 11, "ed": 15, "text": "recurrent neural networks rnns"}, {"st": 20, "ed": 25, "text": "long short term memory lstm"}, {"st": 61, "ed": 64, "text": "vanishing gradient problem"}, {"st": 79, "ed": 81, "text": "speech recognition"}, {"st": 83, "ed": 85, "text": "british english"}, {"st": 97, "ed": 100, "text": "rectified linear unit"}, {"st": 102, "ed": 104, "text": "activation functions"}, {"st": 106, "ed": 108, "text": "error rates"}]
[{"st": 5, "ed": 7, "text": "geometric properties"}, {"st": 13, "ed": 15, "text": "word embeddings"}, {"st": 15, "ed": 19, "text": "trained end to end"}, {"st": 21, "ed": 23, "text": "text classification"}, {"st": 28, "ed": 30, "text": "word embedding"}, {"st": 36, "ed": 38, "text": "linear combination"}, {"st": 53, "ed": 55, "text": "lie group"}, {"st": 77, "ed": 79, "text": "neural networks"}, {"st": 81, "ed": 83, "text": "word embedding"}]
[{"st": 33, "ed": 35, "text": "deep learning"}, {"st": 44, "ed": 46, "text": "natural language"}, {"st": 68, "ed": 70, "text": "unsupervised learning"}, {"st": 85, "ed": 87, "text": "financial risk"}, {"st": 151, "ed": 153, "text": "systemic risk"}]
[{"st": 56, "ed": 58, "text": "symbolic computation"}, {"st": 108, "ed": 110, "text": "dynamical systems"}, {"st": 206, "ed": 209, "text": "central pattern generator"}, {"st": 211, "ed": 213, "text": "finite state"}]
[{"st": 0, "ed": 2, "text": "recent works"}, {"st": 3, "ed": 5, "text": "representation learning"}, {"st": 6, "ed": 9, "text": "graph structured data"}, {"st": 13, "ed": 15, "text": "distributed representations"}, {"st": 30, "ed": 32, "text": "graph classification"}, {"st": 39, "ed": 41, "text": "fixed length"}, {"st": 70, "ed": 72, "text": "handcrafted features"}, {"st": 99, "ed": 101, "text": "framework named"}, {"st": 106, "ed": 108, "text": "distributed representations"}, {"st": 119, "ed": 121, "text": "unsupervised manner"}, {"st": 143, "ed": 145, "text": "representation learning"}, {"st": 153, "ed": 155, "text": "real world"}, {"st": 160, "ed": 162, "text": "significant improvements"}, {"st": 163, "ed": 166, "text": "classification and clustering"}, {"st": 169, "ed": 171, "text": "representation learning"}]
[{"st": 3, "ed": 5, "text": "question answering"}, {"st": 7, "ed": 9, "text": "real world"}, {"st": 24, "ed": 26, "text": "image representation"}, {"st": 27, "ed": 29, "text": "natural language"}, {"st": 36, "ed": 39, "text": "end to end"}, {"st": 59, "ed": 61, "text": "multi modal"}, {"st": 72, "ed": 74, "text": "natural language"}, {"st": 74, "ed": 76, "text": "input image"}, {"st": 135, "ed": 137, "text": "challenging task"}]
[{"st": 0, "ed": 2, "text": "deep models"}, {"st": 13, "ed": 15, "text": "impressive performance"}, {"st": 44, "ed": 46, "text": "natural language"}, {"st": 63, "ed": 65, "text": "deep models"}, {"st": 137, "ed": 140, "text": "visual question answering"}, {"st": 150, "ed": 152, "text": "natural language"}, {"st": 172, "ed": 174, "text": "extensively evaluate"}]
[{"st": 3, "ed": 5, "text": "natural language"}, {"st": 20, "ed": 22, "text": "unified framework"}, {"st": 57, "ed": 59, "text": "reward function"}, {"st": 71, "ed": 73, "text": "trained jointly"}, {"st": 75, "ed": 78, "text": "end to end"}, {"st": 78, "ed": 80, "text": "learning framework"}, {"st": 104, "ed": 106, "text": "unified framework"}]
[{"st": 6, "ed": 8, "text": "bi directional"}, {"st": 26, "ed": 28, "text": "unlike previous"}, {"st": 58, "ed": 60, "text": "visual features"}, {"st": 73, "ed": 75, "text": "visual memory"}, {"st": 76, "ed": 78, "text": "automatically learns"}, {"st": 82, "ed": 84, "text": "visual concepts"}, {"st": 132, "ed": 134, "text": "automatically generated"}, {"st": 158, "ed": 161, "text": "image and sentence"}, {"st": 161, "ed": 163, "text": "retrieval tasks"}]
[{"st": 1, "ed": 4, "text": "vision and language"}, {"st": 12, "ed": 15, "text": "artificial intelligence ai"}, {"st": 31, "ed": 34, "text": "vision and language"}, {"st": 69, "ed": 71, "text": "vision language"}, {"st": 92, "ed": 94, "text": "abstract concepts"}, {"st": 98, "ed": 101, "text": "strengths and weaknesses"}]
[{"st": 14, "ed": 16, "text": "natural language"}, {"st": 35, "ed": 37, "text": "spatio temporal"}, {"st": 37, "ed": 39, "text": "feature based"}, {"st": 46, "ed": 48, "text": "logic programming"}, {"st": 102, "ed": 104, "text": "natural language"}, {"st": 118, "ed": 120, "text": "evidence based"}, {"st": 120, "ed": 122, "text": "qualitative analysis"}, {"st": 138, "ed": 140, "text": "computational models"}]
[{"st": 58, "ed": 60, "text": "machine learning"}, {"st": 111, "ed": 113, "text": "embodied cognition"}, {"st": 114, "ed": 116, "text": "social interaction"}, {"st": 216, "ed": 218, "text": "visual information"}, {"st": 224, "ed": 226, "text": "speech signals"}, {"st": 234, "ed": 236, "text": "future directions"}]
[{"st": 41, "ed": 43, "text": "commonsense reasoning"}, {"st": 63, "ed": 65, "text": "natural language"}, {"st": 79, "ed": 82, "text": "amazon mechanical turk"}, {"st": 89, "ed": 91, "text": "ms coco"}, {"st": 123, "ed": 125, "text": "image caption"}]
[{"st": 9, "ed": 11, "text": "vision language"}, {"st": 18, "ed": 20, "text": "image captioning"}, {"st": 77, "ed": 79, "text": "visual question"}, {"st": 117, "ed": 119, "text": "training data"}]
[{"st": 12, "ed": 14, "text": "contextual information"}, {"st": 34, "ed": 36, "text": "visual scene"}]
[{"st": 6, "ed": 8, "text": "classification decision"}, {"st": 20, "ed": 22, "text": "existing approaches"}, {"st": 24, "ed": 26, "text": "visual recognition"}, {"st": 37, "ed": 39, "text": "vision language"}, {"st": 42, "ed": 44, "text": "image content"}, {"st": 76, "ed": 78, "text": "class label"}, {"st": 93, "ed": 95, "text": "loss function"}, {"st": 99, "ed": 101, "text": "reinforcement learning"}, {"st": 120, "ed": 122, "text": "fine grained"}]
[{"st": 3, "ed": 5, "text": "question answering"}, {"st": 7, "ed": 9, "text": "real world"}, {"st": 24, "ed": 26, "text": "image representation"}, {"st": 27, "ed": 29, "text": "natural language"}, {"st": 37, "ed": 39, "text": "jointly trained"}, {"st": 39, "ed": 42, "text": "end to end"}, {"st": 55, "ed": 57, "text": "multi modal"}, {"st": 68, "ed": 70, "text": "natural language"}, {"st": 115, "ed": 117, "text": "challenging task"}, {"st": 144, "ed": 146, "text": "large scale"}, {"st": 146, "ed": 148, "text": "question answering"}, {"st": 172, "ed": 174, "text": "strong performance"}]
[{"st": 3, "ed": 5, "text": "visual information"}, {"st": 26, "ed": 29, "text": "visual question answering"}, {"st": 53, "ed": 56, "text": "visual and textual"}, {"st": 68, "ed": 70, "text": "outer product"}, {"st": 72, "ed": 75, "text": "visual and textual"}, {"st": 78, "ed": 80, "text": "outer product"}, {"st": 105, "ed": 107, "text": "extensively evaluate"}, {"st": 110, "ed": 113, "text": "visual question answering"}, {"st": 128, "ed": 131, "text": "visual question answering"}, {"st": 144, "ed": 146, "text": "spatial features"}]
[{"st": 0, "ed": 4, "text": "visual question answering vqa"}, {"st": 16, "ed": 18, "text": "natural language"}, {"st": 20, "ed": 22, "text": "visual content"}, {"st": 67, "ed": 69, "text": "visual question"}]
[{"st": 10, "ed": 12, "text": "clustering results"}, {"st": 15, "ed": 19, "text": "nonnegative matrix factorization nmf"}, {"st": 30, "ed": 33, "text": "the research community"}]
[{"st": 5, "ed": 9, "text": "visual question answering vqa"}, {"st": 19, "ed": 21, "text": "key challenge"}, {"st": 38, "ed": 40, "text": "based approach"}, {"st": 63, "ed": 65, "text": "feature vectors"}, {"st": 73, "ed": 75, "text": "object instances"}, {"st": 112, "ed": 115, "text": "deep neural network"}, {"st": 123, "ed": 125, "text": "shows significant"}, {"st": 141, "ed": 143, "text": "significant improvements"}, {"st": 159, "ed": 161, "text": "multiple choice"}, {"st": 177, "ed": 179, "text": "fine grained"}]
[{"st": 0, "ed": 4, "text": "visual question answering vqa"}, {"st": 9, "ed": 11, "text": "computer vision"}, {"st": 12, "ed": 14, "text": "natural language"}, {"st": 25, "ed": 27, "text": "deep learning"}, {"st": 27, "ed": 29, "text": "computer vision"}, {"st": 30, "ed": 32, "text": "natural language"}, {"st": 41, "ed": 43, "text": "text based"}, {"st": 85, "ed": 87, "text": "evaluation metrics"}, {"st": 114, "ed": 116, "text": "existing algorithms"}, {"st": 122, "ed": 124, "text": "future directions"}, {"st": 127, "ed": 129, "text": "image understanding"}]
[{"st": 8, "ed": 10, "text": "time series"}, {"st": 25, "ed": 27, "text": "approximate inference"}, {"st": 38, "ed": 40, "text": "search space"}, {"st": 129, "ed": 132, "text": "exploration and exploitation"}, {"st": 134, "ed": 136, "text": "search space"}, {"st": 143, "ed": 145, "text": "search algorithm"}, {"st": 174, "ed": 176, "text": "image captioning"}, {"st": 176, "ed": 178, "text": "machine translation"}, {"st": 179, "ed": 181, "text": "visual question"}, {"st": 193, "ed": 195, "text": "consistently outperforms"}, {"st": 197, "ed": 199, "text": "previously proposed"}]
[{"st": 1, "ed": 3, "text": "open ended"}, {"st": 18, "ed": 20, "text": "open ended"}, {"st": 20, "ed": 22, "text": "question answering"}, {"st": 24, "ed": 28, "text": "visual question answering vqa"}, {"st": 41, "ed": 43, "text": "natural language"}, {"st": 91, "ed": 93, "text": "open ended"}, {"st": 115, "ed": 117, "text": "models including"}, {"st": 119, "ed": 121, "text": "generative model"}]
[{"st": 5, "ed": 9, "text": "visual question answering vqa"}, {"st": 17, "ed": 19, "text": "previously unseen"}, {"st": 21, "ed": 23, "text": "current methods"}, {"st": 43, "ed": 45, "text": "training data"}, {"st": 50, "ed": 53, "text": "questions about images"}, {"st": 57, "ed": 59, "text": "zero shot"}, {"st": 65, "ed": 67, "text": "answer questions"}, {"st": 89, "ed": 91, "text": "zero shot"}, {"st": 101, "ed": 103, "text": "current approaches"}, {"st": 115, "ed": 118, "text": "propose and evaluate"}, {"st": 122, "ed": 124, "text": "zero shot"}, {"st": 130, "ed": 132, "text": "word embeddings"}, {"st": 145, "ed": 147, "text": "extensive experiments"}, {"st": 154, "ed": 156, "text": "zero shot"}]
[{"st": 6, "ed": 8, "text": "social media"}, {"st": 17, "ed": 19, "text": "important role"}, {"st": 86, "ed": 88, "text": "event driven"}, {"st": 94, "ed": 96, "text": "social media"}, {"st": 102, "ed": 105, "text": "visual and textual"}, {"st": 131, "ed": 133, "text": "multi modal"}]
[{"st": 7, "ed": 9, "text": "visual scene"}, {"st": 19, "ed": 21, "text": "natural language"}]
[{"st": 1, "ed": 5, "text": "visual question answering vqa"}, {"st": 9, "ed": 11, "text": "text based"}, {"st": 15, "ed": 17, "text": "multiple datasets"}, {"st": 125, "ed": 128, "text": "strengths and weaknesses"}, {"st": 142, "ed": 144, "text": "vqa models"}, {"st": 145, "ed": 147, "text": "multi modal"}, {"st": 186, "ed": 188, "text": "complex models"}]
[{"st": 7, "ed": 9, "text": "image captioning"}, {"st": 80, "ed": 82, "text": "training objective"}, {"st": 112, "ed": 114, "text": "adversarial training"}, {"st": 133, "ed": 135, "text": "method achieves"}, {"st": 135, "ed": 137, "text": "comparable performance"}]
[{"st": 50, "ed": 53, "text": "artificial intelligence ai"}, {"st": 160, "ed": 164, "text": "visual question answering vqa"}, {"st": 241, "ed": 243, "text": "attention maps"}]
[{"st": 46, "ed": 48, "text": "raw data"}, {"st": 53, "ed": 55, "text": "higher level"}, {"st": 58, "ed": 60, "text": "natural language"}, {"st": 71, "ed": 74, "text": "ability to learn"}, {"st": 87, "ed": 89, "text": "cognitive linguistics"}, {"st": 145, "ed": 147, "text": "attention mechanisms"}]
[{"st": 8, "ed": 10, "text": "deep learning"}, {"st": 26, "ed": 28, "text": "automatically generated"}, {"st": 81, "ed": 84, "text": "visual question answering"}]
[{"st": 17, "ed": 19, "text": "recent years"}, {"st": 65, "ed": 67, "text": "video prediction"}, {"st": 71, "ed": 73, "text": "context aware"}, {"st": 100, "ed": 103, "text": "multi task learning"}, {"st": 118, "ed": 120, "text": "significant improvements"}, {"st": 143, "ed": 145, "text": "multi task"}]
[{"st": 34, "ed": 36, "text": "computational models"}, {"st": 65, "ed": 67, "text": "baseline approaches"}, {"st": 72, "ed": 74, "text": "turing test"}]
[{"st": 0, "ed": 3, "text": "visual question answering"}, {"st": 13, "ed": 15, "text": "natural language"}, {"st": 17, "ed": 19, "text": "computer vision"}, {"st": 74, "ed": 76, "text": "deep learning"}, {"st": 77, "ed": 79, "text": "deep learning"}, {"st": 82, "ed": 84, "text": "deep learning"}, {"st": 109, "ed": 112, "text": "directions for future"}]
[{"st": 45, "ed": 47, "text": "learning agent"}, {"st": 92, "ed": 94, "text": "image captioning"}, {"st": 112, "ed": 114, "text": "phrase based"}, {"st": 118, "ed": 120, "text": "policy gradients"}]
[{"st": 23, "ed": 25, "text": "maximum likelihood"}, {"st": 30, "ed": 32, "text": "cross entropy"}, {"st": 63, "ed": 65, "text": "don t"}, {"st": 138, "ed": 140, "text": "strong performance"}, {"st": 154, "ed": 157, "text": "end to end"}, {"st": 159, "ed": 161, "text": "visual dialog"}, {"st": 172, "ed": 174, "text": "adversarial loss"}, {"st": 183, "ed": 185, "text": "recently proposed"}, {"st": 185, "ed": 187, "text": "gumbel softmax"}, {"st": 212, "ed": 215, "text": "end to end"}, {"st": 223, "ed": 225, "text": "visual dialog"}, {"st": 229, "ed": 231, "text": "attention mechanism"}, {"st": 237, "ed": 239, "text": "metric learning"}, {"st": 274, "ed": 276, "text": "source code"}, {"st": 280, "ed": 282, "text": "https github.com"}]
[{"st": 0, "ed": 2, "text": "question answering"}, {"st": 19, "ed": 22, "text": "vision and language"}, {"st": 23, "ed": 25, "text": "real world"}, {"st": 73, "ed": 75, "text": "latent embedding"}, {"st": 86, "ed": 89, "text": "long term memory"}, {"st": 97, "ed": 99, "text": "attention model"}, {"st": 101, "ed": 104, "text": "long term memory"}, {"st": 150, "ed": 152, "text": "fine grained"}, {"st": 194, "ed": 196, "text": "latent embedding"}]
[{"st": 1, "ed": 3, "text": "image description"}, {"st": 11, "ed": 13, "text": "image description"}]
[]
[{"st": 6, "ed": 8, "text": "crime drama"}, {"st": 14, "ed": 18, "text": "csi crime scene investigation"}, {"st": 24, "ed": 26, "text": "real world"}, {"st": 26, "ed": 28, "text": "natural language"}, {"st": 40, "ed": 42, "text": "crime drama"}, {"st": 100, "ed": 102, "text": "multi modal"}]
[{"st": 2, "ed": 6, "text": "visual question answering vqa"}, {"st": 8, "ed": 10, "text": "gained increasing"}, {"st": 20, "ed": 22, "text": "visual attention"}, {"st": 30, "ed": 32, "text": "image regions"}, {"st": 45, "ed": 47, "text": "visual attention"}, {"st": 58, "ed": 60, "text": "image regions"}, {"st": 75, "ed": 77, "text": "attention mechanisms"}, {"st": 81, "ed": 83, "text": "complementary information"}, {"st": 101, "ed": 104, "text": "deep neural network"}, {"st": 112, "ed": 114, "text": "proposed framework"}, {"st": 120, "ed": 122, "text": "image regions"}, {"st": 129, "ed": 131, "text": "multi modal"}, {"st": 132, "ed": 134, "text": "feature embedding"}, {"st": 142, "ed": 144, "text": "image regions"}, {"st": 153, "ed": 155, "text": "proposed method"}, {"st": 160, "ed": 163, "text": "publicly available datasets"}, {"st": 174, "ed": 176, "text": "source code"}, {"st": 177, "ed": 181, "text": "available at https github.com"}]
[{"st": 7, "ed": 9, "text": "natural language"}, {"st": 42, "ed": 44, "text": "recent advances"}, {"st": 45, "ed": 48, "text": "vision and language"}, {"st": 54, "ed": 56, "text": "closely related"}, {"st": 65, "ed": 67, "text": "natural language"}, {"st": 80, "ed": 83, "text": "vision and language"}, {"st": 119, "ed": 122, "text": "vision and language"}, {"st": 138, "ed": 140, "text": "large scale"}, {"st": 140, "ed": 142, "text": "reinforcement learning"}, {"st": 159, "ed": 162, "text": "vision and language"}, {"st": 167, "ed": 169, "text": "benchmark dataset"}, {"st": 172, "ed": 174, "text": "natural language"}]
[{"st": 24, "ed": 27, "text": "visual question answering"}, {"st": 57, "ed": 59, "text": "key challenge"}, {"st": 73, "ed": 75, "text": "answer questions"}, {"st": 83, "ed": 85, "text": "reinforcement learning"}, {"st": 86, "ed": 90, "text": "generative adversarial networks gans"}, {"st": 106, "ed": 108, "text": "training data"}, {"st": 115, "ed": 117, "text": "based approach"}, {"st": 130, "ed": 132, "text": "attention mechanism"}, {"st": 144, "ed": 146, "text": "discriminative model"}, {"st": 176, "ed": 178, "text": "generative model"}]
[{"st": 88, "ed": 90, "text": "imitation learning"}, {"st": 91, "ed": 94, "text": "generative adversarial networks"}, {"st": 144, "ed": 146, "text": "proposed approach"}, {"st": 160, "ed": 162, "text": "competing methods"}]
[{"st": 15, "ed": 19, "text": "visual question answering vqa"}, {"st": 32, "ed": 34, "text": "fixed length"}, {"st": 59, "ed": 61, "text": "sequential decision"}, {"st": 113, "ed": 115, "text": "method outperforms"}]
[{"st": 6, "ed": 8, "text": "text based"}, {"st": 13, "ed": 15, "text": "becoming increasingly"}, {"st": 20, "ed": 22, "text": "multi modal"}, {"st": 36, "ed": 38, "text": "visual representations"}, {"st": 69, "ed": 71, "text": "multi modal"}, {"st": 80, "ed": 82, "text": "classification accuracy"}, {"st": 94, "ed": 96, "text": "improves performance"}, {"st": 103, "ed": 105, "text": "multi modal"}, {"st": 105, "ed": 107, "text": "classification tasks"}, {"st": 150, "ed": 152, "text": "computational cost"}, {"st": 154, "ed": 156, "text": "multi modal"}]
[{"st": 0, "ed": 2, "text": "deep models"}, {"st": 25, "ed": 27, "text": "attention weights"}, {"st": 28, "ed": 30, "text": "text based"}, {"st": 81, "ed": 84, "text": "visual and textual"}, {"st": 87, "ed": 89, "text": "classification decision"}, {"st": 90, "ed": 92, "text": "activity recognition"}, {"st": 97, "ed": 100, "text": "visual question answering"}, {"st": 144, "ed": 147, "text": "and vice versa"}]
[{"st": 52, "ed": 54, "text": "fine grained"}, {"st": 88, "ed": 90, "text": "face images"}]
[{"st": 0, "ed": 4, "text": "visual question answering vqa"}, {"st": 6, "ed": 8, "text": "increasingly popular"}, {"st": 10, "ed": 12, "text": "deep learning"}, {"st": 16, "ed": 18, "text": "natural language"}, {"st": 20, "ed": 22, "text": "computer vision"}, {"st": 43, "ed": 45, "text": "attention mechanisms"}]
[{"st": 4, "ed": 7, "text": "vision and language"}, {"st": 16, "ed": 19, "text": "deep reinforcement learning"}, {"st": 36, "ed": 38, "text": "real world"}, {"st": 64, "ed": 66, "text": "real world"}, {"st": 74, "ed": 76, "text": "reinforcement learning"}, {"st": 84, "ed": 86, "text": "reinforcement learning"}, {"st": 89, "ed": 91, "text": "real world"}, {"st": 91, "ed": 93, "text": "vision language"}, {"st": 96, "ed": 98, "text": "look ahead"}, {"st": 102, "ed": 104, "text": "look ahead"}, {"st": 123, "ed": 125, "text": "proposed method"}, {"st": 125, "ed": 127, "text": "significantly outperforms"}, {"st": 135, "ed": 137, "text": "real world"}, {"st": 156, "ed": 158, "text": "success rate"}]
[{"st": 8, "ed": 10, "text": "compositional structure"}, {"st": 16, "ed": 18, "text": "compositional structure"}, {"st": 47, "ed": 49, "text": "multi modal"}, {"st": 78, "ed": 80, "text": "spatial relations"}, {"st": 96, "ed": 98, "text": "activity recognition"}]
[{"st": 52, "ed": 54, "text": "strong baselines"}, {"st": 90, "ed": 92, "text": "artificial intelligence"}]
[{"st": 0, "ed": 3, "text": "the paper presents"}, {"st": 13, "ed": 15, "text": "south slavic"}, {"st": 56, "ed": 58, "text": "statistical analysis"}, {"st": 62, "ed": 64, "text": "local binary"}, {"st": 73, "ed": 75, "text": "result shows"}, {"st": 79, "ed": 81, "text": "extracted features"}, {"st": 118, "ed": 120, "text": "proposed method"}, {"st": 143, "ed": 145, "text": "experiments demonstrate"}, {"st": 146, "ed": 148, "text": "positive results"}]
[{"st": 32, "ed": 35, "text": "convolutional neural networks"}, {"st": 40, "ed": 42, "text": "speech signals"}, {"st": 44, "ed": 46, "text": "word level"}]
[{"st": 5, "ed": 7, "text": "holy grail"}, {"st": 40, "ed": 42, "text": "natural language"}, {"st": 50, "ed": 52, "text": "a level"}, {"st": 60, "ed": 62, "text": "cognitive science"}, {"st": 113, "ed": 115, "text": "computational linguistics"}, {"st": 116, "ed": 118, "text": "computer vision"}, {"st": 146, "ed": 148, "text": "multi modal"}]
[{"st": 7, "ed": 10, "text": "neural machine translation"}, {"st": 17, "ed": 19, "text": "parallel corpora"}, {"st": 32, "ed": 34, "text": "text documents"}, {"st": 136, "ed": 138, "text": "source language"}, {"st": 173, "ed": 176, "text": "end to end"}, {"st": 187, "ed": 190, "text": "cross entropy loss"}]
[{"st": 3, "ed": 5, "text": "recent works"}, {"st": 9, "ed": 12, "text": "end to end"}, {"st": 18, "ed": 20, "text": "multi agent"}, {"st": 100, "ed": 102, "text": "natural language"}, {"st": 112, "ed": 114, "text": "natural language"}]
[{"st": 1, "ed": 3, "text": "question answering"}, {"st": 5, "ed": 7, "text": "challenging problem"}, {"st": 8, "ed": 10, "text": "visual information"}, {"st": 27, "ed": 30, "text": "visual question answering"}, {"st": 45, "ed": 47, "text": "question answering"}, {"st": 54, "ed": 56, "text": "temporal dynamics"}, {"st": 68, "ed": 70, "text": "question answering"}, {"st": 73, "ed": 75, "text": "temporal dynamics"}, {"st": 76, "ed": 78, "text": "frame level"}, {"st": 87, "ed": 89, "text": "learning framework"}, {"st": 93, "ed": 95, "text": "frame level"}, {"st": 100, "ed": 102, "text": "representation learning"}, {"st": 110, "ed": 112, "text": "multi step"}, {"st": 127, "ed": 129, "text": "large scale"}, {"st": 130, "ed": 132, "text": "question answering"}, {"st": 139, "ed": 141, "text": "multiple choice"}, {"st": 142, "ed": 144, "text": "open ended"}, {"st": 145, "ed": 147, "text": "question answering"}]
[{"st": 6, "ed": 9, "text": "a long standing"}, {"st": 12, "ed": 14, "text": "natural language"}, {"st": 50, "ed": 52, "text": "word representations"}, {"st": 68, "ed": 70, "text": "text based"}, {"st": 72, "ed": 74, "text": "vision based"}, {"st": 102, "ed": 105, "text": "end to end"}, {"st": 114, "ed": 116, "text": "skip gram"}]
[{"st": 24, "ed": 26, "text": "existing works"}, {"st": 28, "ed": 30, "text": "image caption"}, {"st": 34, "ed": 36, "text": "natural language"}, {"st": 54, "ed": 56, "text": "phrase based"}, {"st": 58, "ed": 61, "text": "short term memory"}, {"st": 86, "ed": 88, "text": "image caption"}, {"st": 129, "ed": 131, "text": "image caption"}, {"st": 159, "ed": 161, "text": "ms coco"}, {"st": 189, "ed": 191, "text": "training data"}]
[{"st": 1, "ed": 3, "text": "significant progress"}, {"st": 7, "ed": 10, "text": "vision and language"}, {"st": 18, "ed": 20, "text": "goal oriented"}, {"st": 20, "ed": 23, "text": "questions about images"}, {"st": 36, "ed": 39, "text": "deep reinforcement learning"}, {"st": 63, "ed": 65, "text": "valuable information"}, {"st": 70, "ed": 72, "text": "directly optimizing"}, {"st": 87, "ed": 89, "text": "existing methods"}]
[{"st": 9, "ed": 11, "text": "textual description"}, {"st": 26, "ed": 29, "text": "shown promising results"}, {"st": 49, "ed": 51, "text": "fine grained"}, {"st": 68, "ed": 70, "text": "reinforcement learning"}, {"st": 87, "ed": 89, "text": "low level"}, {"st": 112, "ed": 115, "text": "approach significantly outperforms"}, {"st": 117, "ed": 119, "text": "baseline methods"}, {"st": 123, "ed": 125, "text": "large scale"}, {"st": 127, "ed": 129, "text": "fine grained"}]
[{"st": 15, "ed": 17, "text": "natural language"}, {"st": 62, "ed": 65, "text": "end to end"}, {"st": 105, "ed": 107, "text": "visual attention"}, {"st": 138, "ed": 140, "text": "outperforms previous"}, {"st": 146, "ed": 148, "text": "large margin"}, {"st": 150, "ed": 152, "text": "bounding box"}]
[{"st": 12, "ed": 14, "text": "computational linguistics"}, {"st": 30, "ed": 32, "text": "multi modal"}, {"st": 60, "ed": 62, "text": "real life"}, {"st": 73, "ed": 75, "text": "real life"}, {"st": 81, "ed": 83, "text": "outperforms existing"}]
[{"st": 4, "ed": 7, "text": "images and videos"}, {"st": 9, "ed": 11, "text": "natural language"}, {"st": 47, "ed": 49, "text": "standard datasets"}, {"st": 113, "ed": 115, "text": "visual concepts"}, {"st": 117, "ed": 119, "text": "visual concepts"}, {"st": 120, "ed": 122, "text": "natural language"}, {"st": 159, "ed": 161, "text": "ms coco"}, {"st": 230, "ed": 233, "text": "image and video"}, {"st": 237, "ed": 239, "text": "visual concepts"}]
[{"st": 10, "ed": 12, "text": "attention based"}, {"st": 12, "ed": 14, "text": "encoder decoder"}, {"st": 39, "ed": 41, "text": "attention mechanisms"}, {"st": 58, "ed": 60, "text": "rnn based"}]
[{"st": 3, "ed": 5, "text": "open source"}, {"st": 16, "ed": 18, "text": "natural language"}]
[{"st": 68, "ed": 70, "text": "learned representations"}]
[{"st": 9, "ed": 11, "text": "accurately predict"}, {"st": 64, "ed": 66, "text": "word representations"}, {"st": 147, "ed": 149, "text": "attention mechanism"}, {"st": 160, "ed": 162, "text": "large scale"}, {"st": 173, "ed": 175, "text": "significant improvements"}]
[{"st": 1, "ed": 3, "text": "classification problems"}, {"st": 30, "ed": 32, "text": "ad hoc"}]
[{"st": 6, "ed": 8, "text": "supervised learning"}, {"st": 12, "ed": 14, "text": "scoring functions"}, {"st": 28, "ed": 30, "text": "large margin"}, {"st": 35, "ed": 37, "text": "convex relaxation"}, {"st": 64, "ed": 66, "text": "scoring functions"}, {"st": 82, "ed": 84, "text": "significantly improves"}]
[{"st": 21, "ed": 24, "text": "vector space model"}, {"st": 126, "ed": 128, "text": "dual space"}, {"st": 149, "ed": 151, "text": "dual space"}]
[{"st": 6, "ed": 8, "text": "engineered features"}, {"st": 13, "ed": 15, "text": "contextual information"}, {"st": 48, "ed": 50, "text": "contextual information"}, {"st": 64, "ed": 66, "text": "low rank"}, {"st": 76, "ed": 78, "text": "parameter space"}, {"st": 92, "ed": 94, "text": "n grams"}, {"st": 108, "ed": 110, "text": "relation extraction"}]
[{"st": 117, "ed": 119, "text": "previous approaches"}, {"st": 158, "ed": 160, "text": "approach outperforms"}, {"st": 161, "ed": 163, "text": "conventional methods"}]
[{"st": 0, "ed": 2, "text": "domain knowledge"}, {"st": 52, "ed": 54, "text": "domain knowledge"}, {"st": 63, "ed": 65, "text": "text analysis"}, {"st": 69, "ed": 71, "text": "learned jointly"}, {"st": 137, "ed": 139, "text": "text analysis"}, {"st": 144, "ed": 146, "text": "multi layer"}, {"st": 146, "ed": 148, "text": "neural network"}, {"st": 152, "ed": 154, "text": "latent variables"}, {"st": 156, "ed": 158, "text": "hidden layers"}, {"st": 169, "ed": 171, "text": "monte carlo"}, {"st": 189, "ed": 191, "text": "strategy game"}, {"st": 191, "ed": 193, "text": "civilization ii"}, {"st": 212, "ed": 214, "text": "significantly outperforms"}]
[{"st": 2, "ed": 4, "text": "natural language"}, {"st": 40, "ed": 42, "text": "similarity measure"}, {"st": 63, "ed": 66, "text": "supervised machine learning"}, {"st": 67, "ed": 69, "text": "relation classification"}, {"st": 94, "ed": 96, "text": "similarity measure"}, {"st": 133, "ed": 135, "text": "relation classification"}, {"st": 138, "ed": 140, "text": "word pair"}, {"st": 144, "ed": 146, "text": "feature vector"}, {"st": 161, "ed": 163, "text": "supervised learning"}, {"st": 165, "ed": 167, "text": "training set"}, {"st": 184, "ed": 186, "text": "relation classification"}, {"st": 192, "ed": 194, "text": "word pair"}, {"st": 198, "ed": 200, "text": "feature vector"}]
[{"st": 8, "ed": 10, "text": "neural network"}, {"st": 19, "ed": 22, "text": "short term memory"}, {"st": 57, "ed": 60, "text": "vanishing gradient problem"}, {"st": 78, "ed": 80, "text": "neural network"}]
[{"st": 3, "ed": 5, "text": "artificial intelligence"}, {"st": 94, "ed": 96, "text": "sentence pairs"}]
[{"st": 4, "ed": 6, "text": "reinforcement learning"}, {"st": 7, "ed": 9, "text": "task oriented"}, {"st": 9, "ed": 11, "text": "dialogue systems"}, {"st": 22, "ed": 24, "text": "typically require"}, {"st": 46, "ed": 48, "text": "task specific"}, {"st": 61, "ed": 63, "text": "task oriented"}, {"st": 79, "ed": 81, "text": "time consuming"}, {"st": 101, "ed": 103, "text": "reinforcement learning"}, {"st": 106, "ed": 108, "text": "online fashion"}]
[{"st": 5, "ed": 7, "text": "prediction models"}, {"st": 28, "ed": 30, "text": "unstructured text"}, {"st": 42, "ed": 45, "text": "veterans health administration"}, {"st": 53, "ed": 55, "text": "genetic programming"}, {"st": 58, "ed": 61, "text": "bag of words"}, {"st": 70, "ed": 72, "text": "structured data"}, {"st": 83, "ed": 85, "text": "dataset size"}, {"st": 93, "ed": 95, "text": "cross validation"}, {"st": 126, "ed": 128, "text": "word pairs"}, {"st": 131, "ed": 133, "text": "classification accuracy"}, {"st": 163, "ed": 165, "text": "word pairs"}, {"st": 171, "ed": 174, "text": "bag of words"}]
[{"st": 7, "ed": 9, "text": "link prediction"}, {"st": 30, "ed": 32, "text": "previous attempts"}, {"st": 43, "ed": 45, "text": "connectivity patterns"}, {"st": 93, "ed": 95, "text": "pre trained"}, {"st": 118, "ed": 120, "text": "approach outperforms"}, {"st": 120, "ed": 122, "text": "existing methods"}]
[{"st": 0, "ed": 2, "text": "paragraph vectors"}, {"st": 4, "ed": 6, "text": "recently proposed"}, {"st": 12, "ed": 14, "text": "distributed representations"}, {"st": 43, "ed": 46, "text": "proof of concept"}, {"st": 57, "ed": 59, "text": "sentiment analysis"}, {"st": 65, "ed": 67, "text": "paragraph vectors"}, {"st": 74, "ed": 77, "text": "latent dirichlet allocation"}, {"st": 138, "ed": 140, "text": "word embeddings"}, {"st": 143, "ed": 145, "text": "paragraph vectors"}]
[{"st": 9, "ed": 11, "text": "sentiment classification"}, {"st": 14, "ed": 16, "text": "sentence level"}, {"st": 22, "ed": 25, "text": "takes into account"}, {"st": 30, "ed": 33, "text": "positive and negative"}, {"st": 43, "ed": 45, "text": "machine learning"}, {"st": 79, "ed": 81, "text": "baseline models"}, {"st": 92, "ed": 94, "text": "f measure"}]
[{"st": 36, "ed": 38, "text": "max margin"}, {"st": 50, "ed": 53, "text": "question answer pairs"}, {"st": 68, "ed": 70, "text": "evaluation shows"}]
[{"st": 1, "ed": 3, "text": "dialog state"}, {"st": 31, "ed": 33, "text": "spoken language"}, {"st": 50, "ed": 52, "text": "dialog state"}]
[{"st": 6, "ed": 9, "text": "end to end"}, {"st": 11, "ed": 13, "text": "task oriented"}, {"st": 23, "ed": 26, "text": "recurrent neural network"}, {"st": 60, "ed": 62, "text": "feature engineering"}, {"st": 90, "ed": 92, "text": "real world"}, {"st": 103, "ed": 105, "text": "supervised learning"}, {"st": 120, "ed": 122, "text": "reinforcement learning"}, {"st": 169, "ed": 171, "text": "learning rate"}]
[{"st": 40, "ed": 42, "text": "neural network"}, {"st": 55, "ed": 57, "text": "natural images"}]
[{"st": 4, "ed": 6, "text": "domain adaptation"}, {"st": 8, "ed": 10, "text": "neural networks"}, {"st": 15, "ed": 17, "text": "domain adaptation"}, {"st": 23, "ed": 25, "text": "generalization performance"}, {"st": 32, "ed": 34, "text": "source domain"}, {"st": 44, "ed": 47, "text": "recurrent neural networks"}, {"st": 61, "ed": 63, "text": "caption generation"}, {"st": 66, "ed": 68, "text": "domain adaptation"}, {"st": 100, "ed": 102, "text": "source domain"}, {"st": 114, "ed": 116, "text": "domain adaptation"}, {"st": 125, "ed": 127, "text": "domain adaptation"}, {"st": 133, "ed": 135, "text": "neural networks"}, {"st": 146, "ed": 148, "text": "performance improvements"}, {"st": 150, "ed": 152, "text": "domain adaptation"}]
[{"st": 4, "ed": 6, "text": "bi directional"}, {"st": 6, "ed": 8, "text": "attention model"}, {"st": 71, "ed": 73, "text": "conduct experiments"}]
[{"st": 0, "ed": 4, "text": "neural machine translation nmt"}, {"st": 6, "ed": 9, "text": "end to end"}, {"st": 25, "ed": 27, "text": "phrase based"}, {"st": 36, "ed": 38, "text": "computationally expensive"}, {"st": 68, "ed": 71, "text": "accuracy and speed"}, {"st": 81, "ed": 84, "text": "neural machine translation"}, {"st": 98, "ed": 100, "text": "deep lstm"}, {"st": 122, "ed": 124, "text": "attention mechanism"}, {"st": 146, "ed": 148, "text": "low precision"}, {"st": 259, "ed": 261, "text": "competitive results"}, {"st": 293, "ed": 295, "text": "phrase based"}]
[{"st": 20, "ed": 22, "text": "natural language"}, {"st": 79, "ed": 81, "text": "natural language"}, {"st": 123, "ed": 125, "text": "natural language"}, {"st": 141, "ed": 143, "text": "natural language"}, {"st": 180, "ed": 182, "text": "method generates"}]
[{"st": 1, "ed": 4, "text": "deep neural networks"}, {"st": 11, "ed": 14, "text": "gaussian mixture models"}, {"st": 16, "ed": 19, "text": "hidden markov model"}, {"st": 23, "ed": 25, "text": "class labels"}, {"st": 34, "ed": 36, "text": "speech recognition"}, {"st": 39, "ed": 41, "text": "context dependent"}, {"st": 108, "ed": 111, "text": "component analysis pca"}, {"st": 112, "ed": 114, "text": "sparse coding"}, {"st": 123, "ed": 125, "text": "low rank"}, {"st": 144, "ed": 146, "text": "experiments conducted"}]
[{"st": 39, "ed": 41, "text": "natural language"}, {"st": 103, "ed": 105, "text": "non trivial"}, {"st": 130, "ed": 132, "text": "agents learn"}, {"st": 147, "ed": 149, "text": "agents learn"}, {"st": 152, "ed": 154, "text": "multi step"}]
[{"st": 33, "ed": 35, "text": "social media"}, {"st": 61, "ed": 63, "text": "clustering approach"}, {"st": 83, "ed": 85, "text": "software engineer"}, {"st": 151, "ed": 153, "text": "features extracted"}]
[{"st": 28, "ed": 31, "text": "neural machine translation"}, {"st": 68, "ed": 70, "text": "natural language"}, {"st": 115, "ed": 118, "text": "neural machine translation"}]
[{"st": 24, "ed": 26, "text": "high school"}, {"st": 45, "ed": 47, "text": "100 000"}, {"st": 62, "ed": 64, "text": "carefully designed"}, {"st": 91, "ed": 93, "text": "benchmark datasets"}, {"st": 153, "ed": 157, "text": "available at https github.com"}]
[{"st": 29, "ed": 31, "text": "parallel corpora"}, {"st": 80, "ed": 82, "text": "parallel corpora"}]
[{"st": 0, "ed": 2, "text": "spoken language"}, {"st": 9, "ed": 11, "text": "goal oriented"}, {"st": 11, "ed": 13, "text": "dialogue systems"}, {"st": 57, "ed": 61, "text": "recurrent neural network rnn"}, {"st": 70, "ed": 72, "text": "encoder network"}]
[{"st": 34, "ed": 36, "text": "learning representations"}, {"st": 46, "ed": 48, "text": "representation learning"}, {"st": 88, "ed": 90, "text": "task specific"}, {"st": 93, "ed": 95, "text": "benchmark tasks"}, {"st": 107, "ed": 109, "text": "image classification"}, {"st": 113, "ed": 115, "text": "proposed framework"}, {"st": 117, "ed": 119, "text": "invariant representation"}]
[{"st": 12, "ed": 14, "text": "word vectors"}, {"st": 31, "ed": 33, "text": "cross lingual"}, {"st": 37, "ed": 39, "text": "cross lingual"}, {"st": 42, "ed": 44, "text": "evaluation shows"}, {"st": 52, "ed": 54, "text": "cross lingual"}, {"st": 122, "ed": 124, "text": "cross lingual"}]
[{"st": 0, "ed": 4, "text": "deep reinforcement learning rl"}, {"st": 9, "ed": 11, "text": "dialogue policy"}, {"st": 17, "ed": 19, "text": "poor performance"}, {"st": 49, "ed": 51, "text": "learning process"}, {"st": 52, "ed": 54, "text": "sample efficient"}, {"st": 54, "ed": 56, "text": "neural networks"}, {"st": 57, "ed": 59, "text": "trust region"}, {"st": 62, "ed": 64, "text": "experience replay"}, {"st": 71, "ed": 73, "text": "experience replay"}, {"st": 79, "ed": 81, "text": "trust region"}, {"st": 86, "ed": 88, "text": "step size"}, {"st": 96, "ed": 98, "text": "natural gradient"}, {"st": 114, "ed": 116, "text": "off policy"}, {"st": 118, "ed": 120, "text": "experience replay"}, {"st": 128, "ed": 130, "text": "cold start"}, {"st": 134, "ed": 136, "text": "demonstration data"}, {"st": 139, "ed": 141, "text": "pre train"}, {"st": 160, "ed": 162, "text": "deep rl"}, {"st": 171, "ed": 173, "text": "task oriented"}]
[{"st": 22, "ed": 24, "text": "natural language"}, {"st": 31, "ed": 33, "text": "main goal"}, {"st": 52, "ed": 54, "text": "sentiment analysis"}, {"st": 73, "ed": 75, "text": "time consuming"}, {"st": 77, "ed": 79, "text": "recent years"}, {"st": 79, "ed": 81, "text": "distant supervision"}, {"st": 136, "ed": 138, "text": "cross domain"}, {"st": 143, "ed": 145, "text": "competitive results"}]
[{"st": 4, "ed": 6, "text": "natural language"}, {"st": 108, "ed": 110, "text": "representation learning"}, {"st": 117, "ed": 119, "text": "attention models"}, {"st": 143, "ed": 145, "text": "policy gradient"}]
[{"st": 0, "ed": 2, "text": "machine translation"}, {"st": 8, "ed": 10, "text": "reinforcement learning"}, {"st": 29, "ed": 32, "text": "neural machine translation"}, {"st": 43, "ed": 45, "text": "reinforcement learning"}, {"st": 48, "ed": 51, "text": "neural machine translation"}, {"st": 60, "ed": 63, "text": "advantage actor critic"}, {"st": 70, "ed": 72, "text": "attention based"}, {"st": 73, "ed": 76, "text": "encoder decoder architecture"}, {"st": 103, "ed": 105, "text": "machine translation"}]
[{"st": 0, "ed": 3, "text": "deep neural networks"}, {"st": 25, "ed": 27, "text": "strong performance"}, {"st": 45, "ed": 47, "text": "sensitive information"}, {"st": 63, "ed": 66, "text": "the research community"}, {"st": 72, "ed": 74, "text": "large scale"}, {"st": 95, "ed": 97, "text": "trained model"}, {"st": 104, "ed": 106, "text": "training data"}, {"st": 115, "ed": 117, "text": "preliminary experiments"}]
[{"st": 3, "ed": 5, "text": "structured data"}, {"st": 12, "ed": 14, "text": "natural language"}, {"st": 18, "ed": 20, "text": "question answering"}, {"st": 24, "ed": 26, "text": "recent studies"}, {"st": 28, "ed": 31, "text": "neural language models"}, {"st": 32, "ed": 34, "text": "encoder decoder"}, {"st": 42, "ed": 45, "text": "neural network based"}, {"st": 104, "ed": 106, "text": "text generation"}, {"st": 128, "ed": 130, "text": "conducted experiments"}, {"st": 136, "ed": 138, "text": "significantly higher"}, {"st": 140, "ed": 142, "text": "previous methods"}]
[{"st": 18, "ed": 20, "text": "recent successes"}, {"st": 24, "ed": 26, "text": "machine learning"}, {"st": 51, "ed": 53, "text": "neural network"}, {"st": 64, "ed": 66, "text": "learning algorithm"}, {"st": 71, "ed": 73, "text": "learning algorithm"}, {"st": 90, "ed": 92, "text": "auto encoder"}, {"st": 94, "ed": 96, "text": "expressive power"}, {"st": 102, "ed": 104, "text": "learning algorithm"}, {"st": 108, "ed": 112, "text": "synthetic and real world"}]
[{"st": 4, "ed": 6, "text": "multi modal"}, {"st": 9, "ed": 11, "text": "interactive learning"}, {"st": 31, "ed": 33, "text": "interactive learning"}, {"st": 38, "ed": 40, "text": "reinforcement learning"}, {"st": 142, "ed": 144, "text": "hand crafted"}, {"st": 144, "ed": 146, "text": "rule based"}]
[{"st": 66, "ed": 68, "text": "zero shot"}, {"st": 68, "ed": 70, "text": "generalization capabilities"}, {"st": 74, "ed": 78, "text": "recurrent neural networks rnns"}, {"st": 93, "ed": 95, "text": "zero shot"}, {"st": 100, "ed": 103, "text": "training and test"}, {"st": 139, "ed": 142, "text": "proof of concept"}, {"st": 144, "ed": 147, "text": "neural machine translation"}, {"st": 157, "ed": 159, "text": "neural networks"}, {"st": 160, "ed": 162, "text": "training data"}]
[{"st": 52, "ed": 54, "text": "automatically generate"}, {"st": 92, "ed": 94, "text": "visual reasoning"}]
[{"st": 6, "ed": 8, "text": "significantly improves"}, {"st": 15, "ed": 17, "text": "learning agents"}, {"st": 24, "ed": 26, "text": "thompson sampling"}, {"st": 27, "ed": 29, "text": "monte carlo"}, {"st": 44, "ed": 46, "text": "exploration strategies"}, {"st": 48, "ed": 50, "text": "epsilon greedy"}]
[{"st": 48, "ed": 51, "text": "artificial intelligence ai"}, {"st": 73, "ed": 75, "text": "neural architecture"}, {"st": 83, "ed": 85, "text": "recurrent network"}, {"st": 137, "ed": 140, "text": "publicly available datasets"}, {"st": 142, "ed": 144, "text": "sentiment analysis"}]
[{"st": 58, "ed": 60, "text": "visual representations"}, {"st": 156, "ed": 158, "text": "significantly outperforms"}, {"st": 163, "ed": 165, "text": "zero shot"}]
[{"st": 0, "ed": 2, "text": "attention based"}, {"st": 10, "ed": 14, "text": "neural machine translation nmt"}, {"st": 33, "ed": 35, "text": "attention mechanism"}, {"st": 49, "ed": 51, "text": "adaptive control"}, {"st": 109, "ed": 111, "text": "significantly outperforms"}, {"st": 115, "ed": 117, "text": "analysis shows"}, {"st": 135, "ed": 139, "text": "available at https github.com"}]
[{"st": 27, "ed": 29, "text": "knowledge graph"}, {"st": 36, "ed": 38, "text": "reinforcement learning"}, {"st": 45, "ed": 47, "text": "transition model"}, {"st": 49, "ed": 51, "text": "partial observability"}, {"st": 70, "ed": 74, "text": "deep recurrent neural network"}, {"st": 77, "ed": 79, "text": "monte carlo"}, {"st": 85, "ed": 87, "text": "partial observability"}, {"st": 140, "ed": 142, "text": "off policy"}, {"st": 158, "ed": 160, "text": "policy improvement"}, {"st": 185, "ed": 187, "text": "experiment results"}, {"st": 209, "ed": 211, "text": "baseline methods"}, {"st": 216, "ed": 218, "text": "policy gradient"}]
[{"st": 2, "ed": 4, "text": "deep learning"}, {"st": 13, "ed": 15, "text": "recent results"}, {"st": 17, "ed": 19, "text": "convolutional architectures"}, {"st": 21, "ed": 23, "text": "recurrent networks"}, {"st": 52, "ed": 55, "text": "convolutional and recurrent"}, {"st": 84, "ed": 86, "text": "convolutional architecture"}, {"st": 88, "ed": 90, "text": "recurrent networks"}, {"st": 116, "ed": 118, "text": "recurrent networks"}, {"st": 122, "ed": 124, "text": "convolutional networks"}, {"st": 130, "ed": 132, "text": "starting point"}]
[{"st": 4, "ed": 6, "text": "natural language"}, {"st": 96, "ed": 98, "text": "previously unseen"}, {"st": 136, "ed": 138, "text": "similarity based"}, {"st": 155, "ed": 157, "text": "similarity based"}, {"st": 157, "ed": 159, "text": "method yields"}, {"st": 159, "ed": 161, "text": "a 20"}, {"st": 170, "ed": 172, "text": "statistically significant"}, {"st": 174, "ed": 176, "text": "speech recognition"}, {"st": 181, "ed": 183, "text": "similarity based"}, {"st": 189, "ed": 191, "text": "maximum likelihood"}, {"st": 196, "ed": 199, "text": "word sense disambiguation"}, {"st": 224, "ed": 226, "text": "similarity based"}]
[{"st": 21, "ed": 23, "text": "previous approaches"}, {"st": 83, "ed": 85, "text": "instance based"}, {"st": 92, "ed": 94, "text": "based approach"}]
[{"st": 3, "ed": 5, "text": "unsupervised learning"}, {"st": 9, "ed": 11, "text": "text corpora"}, {"st": 22, "ed": 24, "text": "word pair"}, {"st": 124, "ed": 126, "text": "word pair"}, {"st": 145, "ed": 147, "text": "empirically evaluated"}, {"st": 151, "ed": 153, "text": "multiple choice"}, {"st": 155, "ed": 157, "text": "analogy questions"}]
[{"st": 26, "ed": 28, "text": "language game"}]
[{"st": 48, "ed": 50, "text": "artificial intelligence"}, {"st": 66, "ed": 68, "text": "equivalence classes"}, {"st": 102, "ed": 104, "text": "expert systems"}]
[{"st": 0, "ed": 2, "text": "data mining"}, {"st": 30, "ed": 32, "text": "theoretical analyses"}, {"st": 73, "ed": 75, "text": "unsupervised training"}, {"st": 129, "ed": 131, "text": "statistical analysis"}]
[{"st": 46, "ed": 48, "text": "automatically generated"}, {"st": 80, "ed": 82, "text": "automatically generated"}, {"st": 142, "ed": 144, "text": "feature vector"}, {"st": 163, "ed": 166, "text": "supervised machine learning"}, {"st": 203, "ed": 206, "text": "achieve competitive results"}]
[]
[{"st": 0, "ed": 3, "text": "the paper presents"}, {"st": 11, "ed": 13, "text": "framework called"}, {"st": 20, "ed": 22, "text": "natural language"}, {"st": 107, "ed": 109, "text": "natural language"}]
[{"st": 56, "ed": 58, "text": "sampling based"}]
[{"st": 43, "ed": 45, "text": "additional information"}, {"st": 47, "ed": 49, "text": "real world"}, {"st": 79, "ed": 81, "text": "similarity measure"}, {"st": 115, "ed": 117, "text": "cross validation"}]
[{"st": 1, "ed": 3, "text": "embedding models"}, {"st": 24, "ed": 26, "text": "embedding model"}, {"st": 28, "ed": 30, "text": "relation extraction"}, {"st": 43, "ed": 45, "text": "key idea"}, {"st": 50, "ed": 52, "text": "hand crafted"}, {"st": 93, "ed": 95, "text": "relation extraction"}, {"st": 115, "ed": 117, "text": "relation extraction"}, {"st": 122, "ed": 124, "text": "relation classification"}, {"st": 132, "ed": 134, "text": "log linear"}, {"st": 136, "ed": 139, "text": "hand crafted features"}]
[{"st": 4, "ed": 6, "text": "character level"}, {"st": 6, "ed": 8, "text": "encoder decoder"}, {"st": 11, "ed": 13, "text": "successfully applied"}, {"st": 14, "ed": 16, "text": "question answering"}, {"st": 28, "ed": 30, "text": "question answering"}, {"st": 63, "ed": 65, "text": "character level"}, {"st": 68, "ed": 70, "text": "fewer parameters"}, {"st": 73, "ed": 75, "text": "word level"}]
[{"st": 1, "ed": 3, "text": "machine learning"}, {"st": 12, "ed": 14, "text": "fixed length"}, {"st": 26, "ed": 28, "text": "fixed length"}, {"st": 36, "ed": 39, "text": "bag of words"}, {"st": 80, "ed": 82, "text": "fixed length"}, {"st": 82, "ed": 84, "text": "feature representations"}, {"st": 126, "ed": 129, "text": "bag of words"}, {"st": 130, "ed": 132, "text": "empirical results"}, {"st": 134, "ed": 136, "text": "paragraph vectors"}, {"st": 137, "ed": 140, "text": "bag of words"}, {"st": 160, "ed": 162, "text": "text classification"}, {"st": 163, "ed": 165, "text": "sentiment analysis"}]
[{"st": 17, "ed": 19, "text": "individual words"}, {"st": 211, "ed": 213, "text": "unsupervised learning"}, {"st": 224, "ed": 226, "text": "supervised learning"}]
[{"st": 11, "ed": 16, "text": "long short term memory lstm"}, {"st": 20, "ed": 23, "text": "recurrent neural network"}, {"st": 56, "ed": 58, "text": "natural language"}, {"st": 107, "ed": 109, "text": "sentiment classification"}]
[{"st": 0, "ed": 2, "text": "prior knowledge"}, {"st": 10, "ed": 12, "text": "natural language"}, {"st": 28, "ed": 30, "text": "proposed approach"}, {"st": 51, "ed": 53, "text": "regularization terms"}, {"st": 60, "ed": 63, "text": "conduct extensive experiments"}]
[{"st": 4, "ed": 7, "text": "deep neural networks"}, {"st": 8, "ed": 10, "text": "natural language"}, {"st": 58, "ed": 60, "text": "neural networks"}, {"st": 122, "ed": 124, "text": "maximum likelihood"}, {"st": 135, "ed": 137, "text": "weak supervision"}, {"st": 162, "ed": 164, "text": "previous approaches"}, {"st": 166, "ed": 169, "text": "end to end"}, {"st": 174, "ed": 176, "text": "feature engineering"}, {"st": 177, "ed": 179, "text": "domain specific"}]
[{"st": 7, "ed": 9, "text": "distributional semantics"}, {"st": 10, "ed": 12, "text": "individual words"}, {"st": 17, "ed": 19, "text": "word pairs"}, {"st": 48, "ed": 50, "text": "pairwise similarities"}, {"st": 89, "ed": 91, "text": "main contribution"}, {"st": 115, "ed": 117, "text": "word pairs"}, {"st": 134, "ed": 136, "text": "multiple choice"}]
[{"st": 32, "ed": 34, "text": "supreme court"}, {"st": 43, "ed": 45, "text": "political science"}, {"st": 89, "ed": 91, "text": "explicitly model"}]
[{"st": 17, "ed": 19, "text": "common sense"}, {"st": 27, "ed": 29, "text": "prior knowledge"}, {"st": 62, "ed": 64, "text": "co occurrence"}, {"st": 90, "ed": 92, "text": "learning framework"}]
[{"st": 22, "ed": 24, "text": "social network"}, {"st": 45, "ed": 47, "text": "new york"}, {"st": 52, "ed": 54, "text": "probabilistic model"}, {"st": 67, "ed": 69, "text": "social network"}, {"st": 104, "ed": 106, "text": "new york"}, {"st": 110, "ed": 112, "text": "distant supervision"}, {"st": 112, "ed": 114, "text": "semi supervised"}, {"st": 162, "ed": 164, "text": "significantly improves"}, {"st": 169, "ed": 171, "text": "relation extraction"}]
[{"st": 45, "ed": 48, "text": "recurrent neural network"}, {"st": 79, "ed": 81, "text": "microsoft research"}]
[{"st": 5, "ed": 8, "text": "convolutional neural network"}, {"st": 10, "ed": 12, "text": "recently achieved"}, {"st": 22, "ed": 24, "text": "word vectors"}, {"st": 33, "ed": 35, "text": "deep learning"}, {"st": 43, "ed": 46, "text": "convolutional neural network"}, {"st": 67, "ed": 69, "text": "classification tasks"}]
[{"st": 5, "ed": 7, "text": "deep learning"}, {"st": 12, "ed": 14, "text": "attention mechanism"}, {"st": 15, "ed": 17, "text": "question answering"}, {"st": 24, "ed": 27, "text": "neural machine translation"}, {"st": 47, "ed": 50, "text": "neural machine translation"}]
[{"st": 7, "ed": 9, "text": "reinforcement learning"}, {"st": 10, "ed": 13, "text": "deep neural networks"}, {"st": 22, "ed": 24, "text": "natural language"}, {"st": 27, "ed": 29, "text": "text based"}, {"st": 46, "ed": 48, "text": "embedding vectors"}]
[{"st": 3, "ed": 5, "text": "representation learning"}, {"st": 13, "ed": 15, "text": "large scale"}, {"st": 26, "ed": 28, "text": "knowledge graph"}, {"st": 39, "ed": 41, "text": "latent representations"}, {"st": 44, "ed": 46, "text": "latent variable"}, {"st": 66, "ed": 68, "text": "embedding models"}, {"st": 85, "ed": 87, "text": "embedding models"}, {"st": 113, "ed": 115, "text": "episodic memory"}, {"st": 117, "ed": 120, "text": "short term memory"}, {"st": 131, "ed": 133, "text": "sensory input"}, {"st": 140, "ed": 142, "text": "episodic memory"}]
[{"st": 40, "ed": 42, "text": "embedding model"}, {"st": 64, "ed": 66, "text": "natural language"}]
[{"st": 9, "ed": 11, "text": "learning algorithm"}]
[{"st": 76, "ed": 78, "text": "self organization"}]
[{"st": 1, "ed": 3, "text": "dialog state"}, {"st": 19, "ed": 21, "text": "spoken language"}, {"st": 32, "ed": 34, "text": "speech acts"}, {"st": 55, "ed": 57, "text": "speech act"}]
[{"st": 4, "ed": 7, "text": "end to end"}, {"st": 9, "ed": 11, "text": "task oriented"}, {"st": 31, "ed": 33, "text": "relational database"}, {"st": 34, "ed": 36, "text": "jointly learn"}, {"st": 55, "ed": 57, "text": "reinforcement learning"}, {"st": 58, "ed": 60, "text": "supervised learning"}, {"st": 62, "ed": 64, "text": "faster learning"}, {"st": 71, "ed": 73, "text": "a 20"}, {"st": 82, "ed": 84, "text": "proposed method"}, {"st": 92, "ed": 94, "text": "distributed representation"}]
[{"st": 14, "ed": 16, "text": "reinforcement learning"}, {"st": 19, "ed": 21, "text": "natural language"}, {"st": 46, "ed": 49, "text": "deep reinforcement learning"}, {"st": 78, "ed": 80, "text": "bi directional"}]
[{"st": 8, "ed": 10, "text": "dialogue systems"}, {"st": 28, "ed": 30, "text": "current approaches"}, {"st": 48, "ed": 50, "text": "spoken language"}, {"st": 58, "ed": 60, "text": "training data"}, {"st": 62, "ed": 64, "text": "hand crafted"}, {"st": 91, "ed": 93, "text": "recent advances"}, {"st": 100, "ed": 102, "text": "pre trained"}, {"st": 102, "ed": 104, "text": "word vectors"}, {"st": 109, "ed": 111, "text": "distributed representations"}, {"st": 141, "ed": 143, "text": "hand crafted"}]
[{"st": 10, "ed": 12, "text": "computational models"}, {"st": 67, "ed": 69, "text": "computational models"}, {"st": 107, "ed": 109, "text": "art language"}, {"st": 143, "ed": 145, "text": "natural language"}]
[{"st": 12, "ed": 14, "text": "don t"}, {"st": 53, "ed": 55, "text": "training data"}, {"st": 84, "ed": 86, "text": "infinite set"}, {"st": 111, "ed": 113, "text": "training data"}]
[{"st": 3, "ed": 5, "text": "representation learning"}, {"st": 14, "ed": 16, "text": "natural language"}, {"st": 47, "ed": 50, "text": "first order logic"}, {"st": 70, "ed": 72, "text": "highly efficient"}, {"st": 78, "ed": 80, "text": "distributed representations"}, {"st": 121, "ed": 123, "text": "embedding space"}, {"st": 158, "ed": 160, "text": "matrix factorization"}]
[{"st": 43, "ed": 45, "text": "weak supervision"}, {"st": 47, "ed": 49, "text": "joint learning"}]
[{"st": 51, "ed": 53, "text": "machine learning"}, {"st": 54, "ed": 56, "text": "wearable technology"}, {"st": 66, "ed": 68, "text": "machine learning"}, {"st": 70, "ed": 72, "text": "learning algorithms"}, {"st": 107, "ed": 109, "text": "reverse engineering"}, {"st": 161, "ed": 163, "text": "computational models"}, {"st": 235, "ed": 237, "text": "real data"}]
[{"st": 4, "ed": 6, "text": "discriminative model"}, {"st": 23, "ed": 25, "text": "explicit feature"}, {"st": 29, "ed": 31, "text": "word representations"}, {"st": 54, "ed": 56, "text": "log linear"}, {"st": 59, "ed": 61, "text": "retrieval performance"}, {"st": 84, "ed": 86, "text": "statistically significant"}, {"st": 92, "ed": 94, "text": "generative models"}, {"st": 101, "ed": 103, "text": "supervised methods"}, {"st": 152, "ed": 154, "text": "discriminative model"}]
[{"st": 0, "ed": 2, "text": "artificial intelligence"}, {"st": 3, "ed": 5, "text": "machine learning"}, {"st": 57, "ed": 59, "text": "machine learning"}, {"st": 102, "ed": 105, "text": "statistical machine learning"}, {"st": 109, "ed": 111, "text": "word embedding"}, {"st": 158, "ed": 160, "text": "status quo"}, {"st": 177, "ed": 179, "text": "machine learning"}, {"st": 204, "ed": 206, "text": "word embedding"}, {"st": 211, "ed": 213, "text": "word embedding"}, {"st": 227, "ed": 229, "text": "machine learning"}]
[{"st": 5, "ed": 8, "text": "end to end"}, {"st": 10, "ed": 12, "text": "speech recognition"}, {"st": 14, "ed": 16, "text": "convolutional network"}, {"st": 43, "ed": 45, "text": "automatic segmentation"}, {"st": 64, "ed": 66, "text": "competitive results"}, {"st": 67, "ed": 70, "text": "word error rate"}, {"st": 78, "ed": 80, "text": "promising results"}]
[{"st": 90, "ed": 92, "text": "real life"}, {"st": 92, "ed": 94, "text": "large scale"}, {"st": 109, "ed": 111, "text": "labelled data"}, {"st": 137, "ed": 139, "text": "labelled data"}]
[{"st": 2, "ed": 5, "text": "difficult to train"}, {"st": 7, "ed": 9, "text": "task oriented"}, {"st": 22, "ed": 24, "text": "dialogue systems"}, {"st": 58, "ed": 60, "text": "source domain"}, {"st": 74, "ed": 76, "text": "transfer learning"}, {"st": 91, "ed": 93, "text": "task oriented"}, {"st": 95, "ed": 97, "text": "transfer learning"}, {"st": 116, "ed": 118, "text": "source domain"}, {"st": 132, "ed": 134, "text": "negative transfer"}, {"st": 139, "ed": 142, "text": "source and target"}, {"st": 163, "ed": 165, "text": "real world"}]
[{"st": 2, "ed": 4, "text": "statistical power"}, {"st": 5, "ed": 7, "text": "neural networks"}, {"st": 90, "ed": 92, "text": "directly optimize"}, {"st": 97, "ed": 99, "text": "structured prediction"}, {"st": 103, "ed": 105, "text": "weak supervision"}, {"st": 117, "ed": 119, "text": "maximum likelihood"}, {"st": 135, "ed": 138, "text": "question answer pairs"}, {"st": 142, "ed": 144, "text": "feature engineering"}, {"st": 145, "ed": 147, "text": "domain specific"}]
[{"st": 37, "ed": 40, "text": "end to end"}, {"st": 41, "ed": 43, "text": "deep learning"}, {"st": 59, "ed": 61, "text": "prior methods"}, {"st": 95, "ed": 97, "text": "sentence representations"}, {"st": 117, "ed": 119, "text": "pre training"}]
[{"st": 1, "ed": 3, "text": "neural network"}, {"st": 5, "ed": 7, "text": "document classification"}, {"st": 8, "ed": 10, "text": "social media"}, {"st": 32, "ed": 34, "text": "social media"}, {"st": 39, "ed": 41, "text": "neural network"}, {"st": 58, "ed": 60, "text": "social media"}, {"st": 151, "ed": 153, "text": "significantly outperforms"}, {"st": 161, "ed": 163, "text": "deep learning"}]
[{"st": 0, "ed": 2, "text": "recent studies"}, {"st": 23, "ed": 25, "text": "multi step"}, {"st": 33, "ed": 35, "text": "previous works"}, {"st": 46, "ed": 48, "text": "random walk"}, {"st": 50, "ed": 52, "text": "multi step"}, {"st": 70, "ed": 72, "text": "prohibitively expensive"}, {"st": 134, "ed": 136, "text": "multi step"}]
[{"st": 2, "ed": 4, "text": "dialog state"}, {"st": 10, "ed": 12, "text": "cross language"}, {"st": 12, "ed": 14, "text": "dialog state"}, {"st": 81, "ed": 85, "text": "convolutional neural networks cnn"}, {"st": 92, "ed": 94, "text": "chinese language"}, {"st": 129, "ed": 131, "text": "machine learning"}, {"st": 135, "ed": 137, "text": "prior knowledge"}, {"st": 153, "ed": 155, "text": "cross language"}]
[{"st": 1, "ed": 3, "text": "machine learning"}, {"st": 3, "ed": 5, "text": "successfully applied"}, {"st": 33, "ed": 35, "text": "image classification"}]
[{"st": 7, "ed": 9, "text": "speech perception"}, {"st": 23, "ed": 25, "text": "multi layer"}, {"st": 70, "ed": 72, "text": "trained model"}]
[{"st": 10, "ed": 14, "text": "generative adversarial networks gans"}, {"st": 18, "ed": 20, "text": "natural language"}, {"st": 31, "ed": 33, "text": "back propagation"}, {"st": 34, "ed": 37, "text": "discrete random variables"}, {"st": 44, "ed": 46, "text": "gan training"}, {"st": 53, "ed": 55, "text": "maximum likelihood"}, {"st": 62, "ed": 64, "text": "directly optimizing"}, {"st": 72, "ed": 74, "text": "low variance"}]
[{"st": 0, "ed": 2, "text": "keyword spotting"}, {"st": 18, "ed": 20, "text": "false alarm"}, {"st": 41, "ed": 44, "text": "recurrent neural networks"}, {"st": 48, "ed": 50, "text": "large scale"}, {"st": 54, "ed": 56, "text": "speech recognition"}, {"st": 62, "ed": 64, "text": "convolutional layers"}, {"st": 65, "ed": 67, "text": "recurrent layers"}, {"st": 98, "ed": 100, "text": "low latency"}]
[{"st": 35, "ed": 37, "text": "task oriented"}, {"st": 42, "ed": 44, "text": "reinforcement learning"}, {"st": 47, "ed": 49, "text": "empirical study"}, {"st": 81, "ed": 83, "text": "experiments demonstrate"}, {"st": 85, "ed": 87, "text": "reinforcement learning"}]
[{"st": 6, "ed": 8, "text": "complex tasks"}, {"st": 68, "ed": 71, "text": "markov decision processes"}, {"st": 76, "ed": 79, "text": "deep reinforcement learning"}, {"st": 100, "ed": 102, "text": "dialogue policy"}, {"st": 110, "ed": 112, "text": "low level"}, {"st": 112, "ed": 114, "text": "dialogue policy"}, {"st": 150, "ed": 153, "text": "simulated and real"}, {"st": 160, "ed": 162, "text": "significant improvements"}]
[{"st": 1, "ed": 3, "text": "evaluation metrics"}, {"st": 54, "ed": 56, "text": "reinforcement learning"}, {"st": 68, "ed": 70, "text": "training objective"}, {"st": 97, "ed": 99, "text": "reinforcement learning"}, {"st": 101, "ed": 103, "text": "computationally expensive"}]
[]
[{"st": 9, "ed": 11, "text": "natural language"}, {"st": 21, "ed": 23, "text": "embedding model"}, {"st": 34, "ed": 36, "text": "attention mechanism"}, {"st": 75, "ed": 77, "text": "benchmark datasets"}]
[{"st": 7, "ed": 9, "text": "natural language"}, {"st": 28, "ed": 30, "text": "natural language"}, {"st": 56, "ed": 58, "text": "programming language"}]
[{"st": 3, "ed": 5, "text": "sequential data"}, {"st": 7, "ed": 9, "text": "natural language"}, {"st": 106, "ed": 108, "text": "hidden states"}, {"st": 154, "ed": 156, "text": "most probable"}, {"st": 182, "ed": 184, "text": "baseline methods"}]
[{"st": 0, "ed": 2, "text": "large scale"}, {"st": 2, "ed": 4, "text": "multi relational"}, {"st": 12, "ed": 14, "text": "latent representations"}, {"st": 15, "ed": 18, "text": "entities and relations"}, {"st": 55, "ed": 57, "text": "latent representations"}, {"st": 73, "ed": 75, "text": "learning objective"}, {"st": 89, "ed": 91, "text": "significantly outperformed"}, {"st": 96, "ed": 98, "text": "baseline methods"}, {"st": 114, "ed": 116, "text": "multi relational"}]
[{"st": 29, "ed": 32, "text": "question answer pairs"}, {"st": 52, "ed": 54, "text": "natural language"}, {"st": 96, "ed": 98, "text": "100 000"}]
[{"st": 3, "ed": 5, "text": "large scale"}, {"st": 20, "ed": 22, "text": "temporal information"}, {"st": 71, "ed": 73, "text": "point process"}, {"st": 93, "ed": 95, "text": "significantly improved"}, {"st": 103, "ed": 105, "text": "large scale"}, {"st": 105, "ed": 107, "text": "real world"}, {"st": 129, "ed": 131, "text": "multi relational"}]
[{"st": 6, "ed": 9, "text": "attention based neural"}, {"st": 9, "ed": 11, "text": "machine translation"}, {"st": 20, "ed": 22, "text": "training set"}, {"st": 24, "ed": 26, "text": "sentence pairs"}, {"st": 30, "ed": 32, "text": "proposed approach"}, {"st": 48, "ed": 50, "text": "search engine"}, {"st": 58, "ed": 60, "text": "sentence pairs"}, {"st": 62, "ed": 64, "text": "training set"}, {"st": 78, "ed": 80, "text": "score based"}, {"st": 94, "ed": 96, "text": "translation memory"}, {"st": 104, "ed": 106, "text": "source sentence"}, {"st": 111, "ed": 113, "text": "sentence pairs"}, {"st": 117, "ed": 119, "text": "empirical evaluation"}, {"st": 133, "ed": 135, "text": "proposed approach"}, {"st": 135, "ed": 137, "text": "significantly outperforms"}, {"st": 149, "ed": 151, "text": "sentence pairs"}]
[{"st": 0, "ed": 2, "text": "word embeddings"}, {"st": 22, "ed": 24, "text": "word embeddings"}, {"st": 31, "ed": 33, "text": "social science"}, {"st": 41, "ed": 43, "text": "big data"}, {"st": 52, "ed": 54, "text": "probabilistic model"}, {"st": 55, "ed": 57, "text": "word embedding"}, {"st": 67, "ed": 69, "text": "key insight"}, {"st": 115, "ed": 117, "text": "word embeddings"}, {"st": 138, "ed": 140, "text": "skip gram"}, {"st": 159, "ed": 161, "text": "social science"}, {"st": 161, "ed": 163, "text": "case studies"}]
[{"st": 0, "ed": 2, "text": "topic models"}, {"st": 8, "ed": 10, "text": "generative models"}, {"st": 17, "ed": 19, "text": "closed form"}, {"st": 38, "ed": 41, "text": "fast and accurate"}, {"st": 85, "ed": 87, "text": "recurrent network"}, {"st": 95, "ed": 98, "text": "number of topics"}, {"st": 119, "ed": 122, "text": "effectiveness and efficiency"}]
[{"st": 3, "ed": 5, "text": "relation extraction"}, {"st": 19, "ed": 21, "text": "natural language"}, {"st": 35, "ed": 37, "text": "relation extraction"}, {"st": 48, "ed": 51, "text": "large training sets"}, {"st": 62, "ed": 64, "text": "distant supervision"}, {"st": 68, "ed": 71, "text": "zero shot learning"}, {"st": 88, "ed": 90, "text": "labeled training"}, {"st": 116, "ed": 118, "text": "zero shot"}]
[{"st": 3, "ed": 6, "text": "entities and relations"}, {"st": 29, "ed": 31, "text": "extraction task"}, {"st": 44, "ed": 47, "text": "end to end"}, {"st": 57, "ed": 60, "text": "entities and relations"}, {"st": 62, "ed": 64, "text": "conduct experiments"}, {"st": 70, "ed": 72, "text": "distant supervision"}, {"st": 81, "ed": 83, "text": "based methods"}, {"st": 92, "ed": 94, "text": "joint learning"}, {"st": 99, "ed": 102, "text": "end to end"}]
[{"st": 5, "ed": 7, "text": "natural language"}, {"st": 13, "ed": 15, "text": "semantically meaningful"}, {"st": 33, "ed": 35, "text": "task oriented"}, {"st": 40, "ed": 43, "text": "end to end"}, {"st": 44, "ed": 46, "text": "neural architecture"}, {"st": 47, "ed": 49, "text": "task oriented"}, {"st": 65, "ed": 67, "text": "raw pixels"}, {"st": 72, "ed": 74, "text": "natural language"}, {"st": 82, "ed": 85, "text": "image and text"}, {"st": 89, "ed": 91, "text": "attention mechanism"}, {"st": 98, "ed": 100, "text": "natural language"}, {"st": 105, "ed": 107, "text": "imitation learning"}, {"st": 138, "ed": 140, "text": "game engine"}, {"st": 145, "ed": 147, "text": "task oriented"}]
[{"st": 9, "ed": 11, "text": "joint inference"}, {"st": 74, "ed": 76, "text": "reinforcement learning"}]
[{"st": 0, "ed": 2, "text": "computational models"}, {"st": 29, "ed": 31, "text": "social media"}, {"st": 70, "ed": 75, "text": "long short term memory lstm"}, {"st": 92, "ed": 94, "text": "lstm network"}, {"st": 99, "ed": 101, "text": "lstm networks"}, {"st": 102, "ed": 104, "text": "sentence level"}, {"st": 126, "ed": 128, "text": "qualitative analysis"}, {"st": 129, "ed": 131, "text": "attention weights"}, {"st": 134, "ed": 136, "text": "lstm models"}]
[{"st": 0, "ed": 2, "text": "computer vision"}, {"st": 14, "ed": 16, "text": "supervised training"}, {"st": 19, "ed": 21, "text": "natural language"}, {"st": 32, "ed": 34, "text": "deep models"}, {"st": 44, "ed": 46, "text": "deep lstm"}, {"st": 56, "ed": 58, "text": "machine translation"}, {"st": 71, "ed": 73, "text": "improves performance"}, {"st": 83, "ed": 85, "text": "wide variety"}, {"st": 89, "ed": 91, "text": "sentiment analysis"}, {"st": 99, "ed": 101, "text": "question answering"}, {"st": 104, "ed": 106, "text": "fine grained"}, {"st": 106, "ed": 108, "text": "sentiment analysis"}, {"st": 111, "ed": 113, "text": "improves performance"}, {"st": 115, "ed": 117, "text": "baseline models"}]
[{"st": 8, "ed": 10, "text": "natural language"}, {"st": 14, "ed": 16, "text": "reinforcement learning"}, {"st": 24, "ed": 26, "text": "deep rl"}, {"st": 91, "ed": 93, "text": "multi task"}]
[{"st": 107, "ed": 109, "text": "natural language"}, {"st": 144, "ed": 146, "text": "neural architecture"}, {"st": 148, "ed": 151, "text": "end to end"}]
[{"st": 10, "ed": 12, "text": "natural language"}, {"st": 19, "ed": 21, "text": "bidirectional lstm"}, {"st": 26, "ed": 28, "text": "fine tuning"}, {"st": 88, "ed": 90, "text": "natural language"}]
[{"st": 0, "ed": 2, "text": "active learning"}, {"st": 29, "ed": 31, "text": "selection methods"}, {"st": 60, "ed": 62, "text": "active learning"}, {"st": 64, "ed": 66, "text": "reinforcement learning"}, {"st": 82, "ed": 84, "text": "active learning"}, {"st": 109, "ed": 111, "text": "cross lingual"}, {"st": 111, "ed": 114, "text": "named entity recognition"}]
[{"st": 5, "ed": 8, "text": "deep reinforcement learning"}, {"st": 13, "ed": 15, "text": "control policies"}, {"st": 16, "ed": 18, "text": "large scale"}, {"st": 18, "ed": 20, "text": "real world"}, {"st": 42, "ed": 45, "text": "deep q network"}, {"st": 53, "ed": 55, "text": "plain text"}, {"st": 67, "ed": 69, "text": "large scale"}, {"st": 94, "ed": 97, "text": "real time bidding"}]
[{"st": 16, "ed": 18, "text": "automatic evaluation"}, {"st": 35, "ed": 37, "text": "automatic evaluation"}, {"st": 46, "ed": 48, "text": "rapid prototyping"}, {"st": 108, "ed": 110, "text": "a level"}]
[{"st": 10, "ed": 12, "text": "machine reading"}, {"st": 24, "ed": 26, "text": "large scale"}, {"st": 34, "ed": 36, "text": "machine reading"}, {"st": 72, "ed": 74, "text": "machine reading"}, {"st": 104, "ed": 106, "text": "strong baseline"}, {"st": 113, "ed": 115, "text": "reinforcement learning"}]
[{"st": 1, "ed": 5, "text": "neural machine translation nmt"}, {"st": 7, "ed": 9, "text": "encoder decoder"}, {"st": 11, "ed": 14, "text": "achieved great success"}, {"st": 110, "ed": 113, "text": "encoder decoder architecture"}]
[{"st": 0, "ed": 2, "text": "knowledge graph"}, {"st": 12, "ed": 14, "text": "question answering"}, {"st": 38, "ed": 40, "text": "qa systems"}, {"st": 50, "ed": 53, "text": "question answer pairs"}, {"st": 74, "ed": 76, "text": "non trivial"}, {"st": 99, "ed": 101, "text": "knowledge graph"}, {"st": 115, "ed": 117, "text": "deep learning"}, {"st": 120, "ed": 123, "text": "end to end"}, {"st": 124, "ed": 126, "text": "learning algorithm"}, {"st": 139, "ed": 141, "text": "method achieves"}, {"st": 149, "ed": 151, "text": "benchmark dataset"}, {"st": 161, "ed": 163, "text": "benchmark datasets"}, {"st": 181, "ed": 183, "text": "method yields"}, {"st": 184, "ed": 186, "text": "promising results"}]
[{"st": 28, "ed": 30, "text": "document classification"}, {"st": 84, "ed": 86, "text": "document classification"}, {"st": 92, "ed": 94, "text": "multi class"}, {"st": 98, "ed": 100, "text": "hierarchical classification"}, {"st": 106, "ed": 108, "text": "deep learning"}, {"st": 109, "ed": 111, "text": "text classification"}, {"st": 117, "ed": 119, "text": "deep learning"}]
[{"st": 4, "ed": 6, "text": "semantically meaningful"}, {"st": 11, "ed": 13, "text": "machine translation"}, {"st": 13, "ed": 15, "text": "dialogue systems"}, {"st": 15, "ed": 17, "text": "image captioning"}, {"st": 22, "ed": 24, "text": "policy gradient"}, {"st": 24, "ed": 27, "text": "generative adversarial nets"}, {"st": 31, "ed": 33, "text": "discriminative model"}, {"st": 39, "ed": 41, "text": "generative model"}, {"st": 43, "ed": 45, "text": "reinforcement learning"}, {"st": 47, "ed": 50, "text": "shown promising results"}, {"st": 107, "ed": 109, "text": "framework called"}, {"st": 129, "ed": 131, "text": "extracted features"}, {"st": 158, "ed": 160, "text": "extracted features"}, {"st": 167, "ed": 169, "text": "latent vector"}, {"st": 179, "ed": 181, "text": "extensive experiments"}, {"st": 182, "ed": 184, "text": "synthetic data"}, {"st": 186, "ed": 188, "text": "real world"}, {"st": 190, "ed": 192, "text": "turing test"}, {"st": 196, "ed": 198, "text": "highly effective"}, {"st": 200, "ed": 202, "text": "text generation"}, {"st": 208, "ed": 210, "text": "short text"}]
[{"st": 3, "ed": 5, "text": "multi modal"}, {"st": 8, "ed": 10, "text": "interactive learning"}, {"st": 31, "ed": 33, "text": "type theory"}, {"st": 85, "ed": 87, "text": "learning rates"}, {"st": 101, "ed": 103, "text": "learning agent"}, {"st": 142, "ed": 144, "text": "dialogue policy"}]
[{"st": 13, "ed": 15, "text": "interactive learning"}, {"st": 79, "ed": 81, "text": "text based"}, {"st": 120, "ed": 122, "text": "n gram"}, {"st": 167, "ed": 169, "text": "reinforcement learning"}, {"st": 187, "ed": 189, "text": "comparable performance"}, {"st": 191, "ed": 193, "text": "rule based"}]
[{"st": 7, "ed": 11, "text": "neural machine translation nmt"}, {"st": 12, "ed": 14, "text": "standard benchmarks"}, {"st": 18, "ed": 20, "text": "parallel corpora"}, {"st": 43, "ed": 46, "text": "semi supervised learning"}, {"st": 53, "ed": 55, "text": "cross lingual"}, {"st": 80, "ed": 82, "text": "unsupervised manner"}, {"st": 106, "ed": 108, "text": "encoder decoder"}, {"st": 136, "ed": 138, "text": "bleu points"}, {"st": 156, "ed": 158, "text": "parallel corpora"}, {"st": 167, "ed": 169, "text": "100 000"}, {"st": 178, "ed": 180, "text": "open source"}]
[{"st": 7, "ed": 10, "text": "advantage actor critic"}, {"st": 13, "ed": 15, "text": "significantly improves"}, {"st": 18, "ed": 20, "text": "dialogue policy"}, {"st": 28, "ed": 31, "text": "generative adversarial networks"}, {"st": 59, "ed": 62, "text": "advantage actor critic"}]
[{"st": 3, "ed": 5, "text": "human intelligence"}, {"st": 80, "ed": 82, "text": "open ended"}]
[{"st": 16, "ed": 18, "text": "current approaches"}, {"st": 36, "ed": 38, "text": "variational autoencoder"}, {"st": 45, "ed": 47, "text": "neural networks"}, {"st": 50, "ed": 53, "text": "recurrent neural networks"}, {"st": 63, "ed": 65, "text": "significantly improves"}, {"st": 82, "ed": 84, "text": "context sensitive"}]
[{"st": 27, "ed": 29, "text": "existing approaches"}, {"st": 32, "ed": 34, "text": "encoder decoder"}, {"st": 61, "ed": 63, "text": "proposed method"}, {"st": 65, "ed": 67, "text": "strong baseline"}]
[{"st": 3, "ed": 5, "text": "structured prediction"}, {"st": 60, "ed": 62, "text": "theoretically analyze"}, {"st": 74, "ed": 76, "text": "analysis shows"}, {"st": 87, "ed": 89, "text": "empirical evaluations"}, {"st": 92, "ed": 94, "text": "proposed method"}, {"st": 114, "ed": 116, "text": "proposed method"}, {"st": 116, "ed": 118, "text": "substantially improves"}, {"st": 123, "ed": 125, "text": "neural network"}, {"st": 129, "ed": 131, "text": "error rate"}, {"st": 140, "ed": 142, "text": "proposed method"}]
[{"st": 12, "ed": 14, "text": "low dimensional"}, {"st": 20, "ed": 22, "text": "word embedding"}, {"st": 32, "ed": 34, "text": "word embedding"}, {"st": 88, "ed": 90, "text": "context specific"}, {"st": 106, "ed": 108, "text": "posterior distributions"}, {"st": 112, "ed": 114, "text": "context dependent"}, {"st": 117, "ed": 119, "text": "potential applications"}, {"st": 152, "ed": 155, "text": "achieve competitive results"}]
[{"st": 2, "ed": 4, "text": "goal oriented"}, {"st": 11, "ed": 13, "text": "supervised learning"}, {"st": 17, "ed": 19, "text": "reinforcement learning"}, {"st": 36, "ed": 38, "text": "encoder decoder"}, {"st": 100, "ed": 102, "text": "goal oriented"}, {"st": 114, "ed": 116, "text": "reward function"}, {"st": 122, "ed": 124, "text": "off policy"}, {"st": 124, "ed": 126, "text": "policy gradient"}]
[{"st": 13, "ed": 15, "text": "open source"}, {"st": 26, "ed": 28, "text": "machine learning"}, {"st": 76, "ed": 80, "text": "available at https github.com"}]
[{"st": 16, "ed": 18, "text": "sentiment analysis"}, {"st": 29, "ed": 31, "text": "prediction models"}]
[{"st": 0, "ed": 2, "text": "social media"}, {"st": 49, "ed": 51, "text": "multi task"}, {"st": 51, "ed": 53, "text": "neural network"}, {"st": 85, "ed": 87, "text": "proposed framework"}, {"st": 107, "ed": 109, "text": "social media"}, {"st": 113, "ed": 115, "text": "attention mechanism"}, {"st": 157, "ed": 159, "text": "proposed framework"}, {"st": 160, "ed": 162, "text": "real world"}]
[{"st": 83, "ed": 86, "text": "multi class classification"}, {"st": 89, "ed": 91, "text": "labeled data"}, {"st": 105, "ed": 107, "text": "existing approaches"}, {"st": 125, "ed": 127, "text": "machine learning"}, {"st": 132, "ed": 134, "text": "class imbalance"}, {"st": 136, "ed": 138, "text": "labeled instances"}, {"st": 151, "ed": 153, "text": "machine learning"}, {"st": 155, "ed": 157, "text": "engineered features"}, {"st": 161, "ed": 163, "text": "deep learning"}]
[{"st": 5, "ed": 8, "text": "shown promising results"}, {"st": 13, "ed": 15, "text": "latent variable"}, {"st": 73, "ed": 75, "text": "latent representations"}, {"st": 83, "ed": 85, "text": "latent variables"}, {"st": 89, "ed": 91, "text": "gaussian noise"}, {"st": 92, "ed": 94, "text": "multi layer"}, {"st": 126, "ed": 128, "text": "substantial improvement"}]
[{"st": 6, "ed": 9, "text": "simple and efficient"}, {"st": 12, "ed": 14, "text": "sentence representations"}, {"st": 28, "ed": 30, "text": "sentence representations"}, {"st": 71, "ed": 73, "text": "efficiently learn"}, {"st": 93, "ed": 95, "text": "sentence representations"}, {"st": 100, "ed": 103, "text": "unsupervised and supervised"}, {"st": 103, "ed": 105, "text": "representation learning"}]
[{"st": 7, "ed": 10, "text": "online learning algorithm"}, {"st": 19, "ed": 21, "text": "higher accuracy"}, {"st": 31, "ed": 34, "text": "online learning algorithm"}, {"st": 72, "ed": 74, "text": "computational complexity"}, {"st": 109, "ed": 111, "text": "higher accuracy"}, {"st": 130, "ed": 132, "text": "proposed algorithm"}, {"st": 133, "ed": 135, "text": "online algorithm"}, {"st": 144, "ed": 146, "text": "proposed algorithm"}, {"st": 171, "ed": 173, "text": "proposed algorithm"}, {"st": 185, "ed": 187, "text": "training data"}]
[{"st": 1, "ed": 3, "text": "complex questions"}, {"st": 5, "ed": 7, "text": "time consuming"}, {"st": 30, "ed": 32, "text": "complex questions"}, {"st": 70, "ed": 72, "text": "complex questions"}, {"st": 80, "ed": 82, "text": "search engine"}, {"st": 91, "ed": 93, "text": "complex questions"}, {"st": 122, "ed": 124, "text": "complex questions"}, {"st": 142, "ed": 144, "text": "empirically demonstrate"}, {"st": 147, "ed": 149, "text": "improves performance"}]
[{"st": 11, "ed": 13, "text": "text generation"}, {"st": 18, "ed": 22, "text": "recurrent neural network language"}, {"st": 26, "ed": 28, "text": "maximum likelihood"}, {"st": 29, "ed": 31, "text": "training scheme"}, {"st": 43, "ed": 45, "text": "recently proposed"}, {"st": 47, "ed": 49, "text": "text generation"}, {"st": 51, "ed": 53, "text": "reinforcement learning"}, {"st": 57, "ed": 60, "text": "generative adversarial nets"}, {"st": 96, "ed": 98, "text": "text generation"}, {"st": 107, "ed": 109, "text": "empirical results"}]
[{"st": 3, "ed": 5, "text": "pattern recognition"}, {"st": 14, "ed": 16, "text": "classification error"}, {"st": 27, "ed": 29, "text": "ground truth"}, {"st": 41, "ed": 43, "text": "speech processing"}, {"st": 46, "ed": 48, "text": "large scale"}, {"st": 48, "ed": 50, "text": "practical applications"}, {"st": 67, "ed": 69, "text": "semi supervised"}, {"st": 71, "ed": 73, "text": "likelihood based"}, {"st": 77, "ed": 79, "text": "unlabeled data"}, {"st": 87, "ed": 89, "text": "automatically generate"}, {"st": 107, "ed": 109, "text": "robust statistics"}, {"st": 117, "ed": 119, "text": "likelihood ratio"}, {"st": 142, "ed": 145, "text": "automatic speech recognition"}, {"st": 154, "ed": 156, "text": "speech data"}]
[{"st": 62, "ed": 64, "text": "weighted sum"}, {"st": 99, "ed": 101, "text": "image denoising"}, {"st": 118, "ed": 121, "text": "times faster than"}, {"st": 144, "ed": 146, "text": "weighted sum"}]
[{"st": 55, "ed": 57, "text": "content based"}, {"st": 109, "ed": 111, "text": "topic model"}]
[]
[{"st": 8, "ed": 10, "text": "social media"}, {"st": 21, "ed": 24, "text": "effective and efficient"}, {"st": 24, "ed": 26, "text": "topic modeling"}, {"st": 28, "ed": 31, "text": "latent dirichlet allocation"}, {"st": 60, "ed": 62, "text": "transfer learning"}, {"st": 73, "ed": 75, "text": "yahoo news"}, {"st": 79, "ed": 81, "text": "topic modeling"}, {"st": 118, "ed": 120, "text": "large scale"}]
[{"st": 13, "ed": 15, "text": "large margin"}, {"st": 22, "ed": 24, "text": "submodular function"}, {"st": 81, "ed": 83, "text": "large margin"}, {"st": 83, "ed": 85, "text": "structured prediction"}, {"st": 94, "ed": 96, "text": "submodular optimization"}, {"st": 101, "ed": 103, "text": "submodular function"}, {"st": 114, "ed": 116, "text": "document summarization"}, {"st": 134, "ed": 136, "text": "document summarization"}]
[{"st": 7, "ed": 9, "text": "word representations"}, {"st": 13, "ed": 15, "text": "sparse coding"}, {"st": 26, "ed": 28, "text": "efficient learning"}, {"st": 36, "ed": 38, "text": "significantly faster"}, {"st": 39, "ed": 41, "text": "previous approaches"}, {"st": 47, "ed": 49, "text": "sparse coding"}, {"st": 60, "ed": 62, "text": "benchmark tasks"}, {"st": 69, "ed": 71, "text": "sentiment analysis"}, {"st": 74, "ed": 76, "text": "method outperforms"}, {"st": 86, "ed": 88, "text": "word representations"}]
[{"st": 21, "ed": 23, "text": "natural language"}, {"st": 45, "ed": 47, "text": "low dimensional"}, {"st": 71, "ed": 73, "text": "neural network"}, {"st": 90, "ed": 92, "text": "low level"}, {"st": 118, "ed": 120, "text": "feature engineering"}, {"st": 128, "ed": 130, "text": "recent advances"}, {"st": 136, "ed": 138, "text": "computer vision"}, {"st": 155, "ed": 157, "text": "learning process"}]
[{"st": 3, "ed": 5, "text": "authorship attribution"}, {"st": 75, "ed": 77, "text": "transition probabilities"}, {"st": 114, "ed": 116, "text": "analysis shows"}]
[{"st": 4, "ed": 6, "text": "topic model"}, {"st": 11, "ed": 13, "text": "probabilistic model"}, {"st": 58, "ed": 60, "text": "real world"}, {"st": 76, "ed": 78, "text": "topic models"}, {"st": 82, "ed": 84, "text": "parallel corpora"}, {"st": 90, "ed": 92, "text": "topic model"}]
[{"st": 0, "ed": 2, "text": "document clustering"}, {"st": 3, "ed": 5, "text": "topic modeling"}, {"st": 7, "ed": 9, "text": "closely related"}, {"st": 16, "ed": 18, "text": "topic modeling"}, {"st": 34, "ed": 36, "text": "document clustering"}, {"st": 40, "ed": 42, "text": "topic models"}, {"st": 66, "ed": 68, "text": "topic model"}, {"st": 71, "ed": 73, "text": "document clustering"}, {"st": 74, "ed": 76, "text": "topic modeling"}, {"st": 78, "ed": 80, "text": "unified framework"}, {"st": 111, "ed": 113, "text": "topic model"}, {"st": 134, "ed": 136, "text": "variational inference"}, {"st": 141, "ed": 143, "text": "hidden variables"}]
[{"st": 1, "ed": 3, "text": "topic models"}, {"st": 13, "ed": 16, "text": "representations of words"}, {"st": 36, "ed": 38, "text": "topic models"}, {"st": 78, "ed": 81, "text": "hierarchical dirichlet process"}, {"st": 84, "ed": 86, "text": "topic model"}, {"st": 89, "ed": 91, "text": "efficient inference"}, {"st": 108, "ed": 110, "text": "word embeddings"}, {"st": 117, "ed": 119, "text": "experiments demonstrate"}, {"st": 121, "ed": 123, "text": "method outperforms"}, {"st": 133, "ed": 135, "text": "text corpora"}]
[{"st": 0, "ed": 2, "text": "topic models"}, {"st": 13, "ed": 15, "text": "topic modeling"}, {"st": 22, "ed": 24, "text": "inference algorithms"}, {"st": 26, "ed": 29, "text": "latent dirichlet allocation"}, {"st": 37, "ed": 39, "text": "topic modeling"}, {"st": 42, "ed": 44, "text": "optimization problem"}, {"st": 48, "ed": 50, "text": "objective function"}, {"st": 80, "ed": 82, "text": "topic modeling"}, {"st": 94, "ed": 96, "text": "lda based"}, {"st": 96, "ed": 98, "text": "topic modeling"}]
[{"st": 5, "ed": 7, "text": "typically assume"}, {"st": 31, "ed": 34, "text": "bag of words"}, {"st": 56, "ed": 59, "text": "bag of words"}, {"st": 89, "ed": 91, "text": "latent structure"}, {"st": 192, "ed": 194, "text": "topic models"}, {"st": 197, "ed": 199, "text": "prediction tasks"}]
[{"st": 0, "ed": 2, "text": "latent topic"}, {"st": 5, "ed": 7, "text": "successfully applied"}, {"st": 60, "ed": 62, "text": "co occurrence"}, {"st": 72, "ed": 75, "text": "word co occurrence"}, {"st": 89, "ed": 91, "text": "generative model"}, {"st": 162, "ed": 164, "text": "link prediction"}]
[{"st": 8, "ed": 10, "text": "regular expression"}, {"st": 23, "ed": 25, "text": "regular expression"}, {"st": 54, "ed": 56, "text": "regular expressions"}, {"st": 63, "ed": 65, "text": "training data"}, {"st": 71, "ed": 73, "text": "regular expressions"}, {"st": 90, "ed": 92, "text": "structured output"}, {"st": 96, "ed": 98, "text": "loss function"}, {"st": 104, "ed": 106, "text": "optimization problem"}, {"st": 111, "ed": 113, "text": "case study"}]
[{"st": 5, "ed": 7, "text": "word representations"}, {"st": 13, "ed": 15, "text": "word level"}, {"st": 31, "ed": 33, "text": "word embeddings"}, {"st": 48, "ed": 50, "text": "word representations"}, {"st": 54, "ed": 56, "text": "word level"}, {"st": 86, "ed": 88, "text": "document classification"}, {"st": 90, "ed": 92, "text": "labeled data"}, {"st": 119, "ed": 122, "text": "method compares favorably"}, {"st": 124, "ed": 126, "text": "previously proposed"}, {"st": 129, "ed": 131, "text": "word level"}]
[{"st": 4, "ed": 6, "text": "topic model"}, {"st": 14, "ed": 17, "text": "latent dirichlet allocation"}, {"st": 107, "ed": 109, "text": "sample size"}, {"st": 138, "ed": 140, "text": "parameter values"}, {"st": 143, "ed": 145, "text": "total number"}, {"st": 155, "ed": 157, "text": "text corpora"}, {"st": 159, "ed": 161, "text": "image dataset"}, {"st": 164, "ed": 166, "text": "model achieves"}, {"st": 174, "ed": 176, "text": "ground truth"}, {"st": 176, "ed": 178, "text": "class labels"}]
[{"st": 0, "ed": 2, "text": "cross language"}, {"st": 7, "ed": 9, "text": "training data"}, {"st": 28, "ed": 30, "text": "word level"}, {"st": 45, "ed": 47, "text": "based methods"}, {"st": 48, "ed": 50, "text": "cross language"}, {"st": 53, "ed": 55, "text": "word representations"}, {"st": 65, "ed": 67, "text": "word level"}, {"st": 77, "ed": 80, "text": "bag of words"}, {"st": 132, "ed": 134, "text": "significant improvement"}, {"st": 149, "ed": 151, "text": "cross language"}, {"st": 175, "ed": 177, "text": "experiments demonstrate"}]
[{"st": 67, "ed": 69, "text": "neural networks"}, {"st": 99, "ed": 101, "text": "distributed representations"}, {"st": 102, "ed": 105, "text": "words and phrases"}]
[{"st": 18, "ed": 20, "text": "statistical models"}, {"st": 22, "ed": 24, "text": "multivariate gaussian"}, {"st": 24, "ed": 26, "text": "mixture models"}, {"st": 29, "ed": 31, "text": "small scale"}, {"st": 45, "ed": 48, "text": "real world data"}, {"st": 51, "ed": 53, "text": "speech processing"}, {"st": 92, "ed": 94, "text": "training data"}, {"st": 145, "ed": 147, "text": "proposed method"}, {"st": 151, "ed": 153, "text": "real world"}, {"st": 158, "ed": 160, "text": "speaker recognition"}, {"st": 165, "ed": 167, "text": "proposed method"}, {"st": 168, "ed": 170, "text": "significantly improve"}]
[{"st": 0, "ed": 2, "text": "cluster analysis"}, {"st": 17, "ed": 19, "text": "cluster analysis"}, {"st": 21, "ed": 23, "text": "text mining"}, {"st": 50, "ed": 52, "text": "world cup"}, {"st": 57, "ed": 59, "text": "real world"}, {"st": 59, "ed": 61, "text": "text data"}, {"st": 116, "ed": 118, "text": "cluster analysis"}, {"st": 131, "ed": 133, "text": "k means"}, {"st": 136, "ed": 138, "text": "clustering algorithm"}, {"st": 139, "ed": 144, "text": "non negative matrix factorization nmf"}]
[{"st": 13, "ed": 16, "text": "johns hopkins university"}, {"st": 22, "ed": 24, "text": "summer camp"}, {"st": 33, "ed": 35, "text": "machine translation"}, {"st": 146, "ed": 149, "text": "statistical machine translation"}, {"st": 173, "ed": 175, "text": "target language"}, {"st": 237, "ed": 239, "text": "significantly outperformed"}]
[{"st": 1, "ed": 3, "text": "word embeddings"}, {"st": 23, "ed": 25, "text": "co occurrence"}, {"st": 28, "ed": 30, "text": "hand tuned"}, {"st": 39, "ed": 41, "text": "generative model"}, {"st": 46, "ed": 48, "text": "log linear"}, {"st": 48, "ed": 50, "text": "topic model"}, {"st": 64, "ed": 66, "text": "closed form"}, {"st": 73, "ed": 75, "text": "theoretical justification"}, {"st": 94, "ed": 96, "text": "low dimensional"}, {"st": 123, "ed": 125, "text": "generative model"}, {"st": 134, "ed": 136, "text": "word vectors"}]
[{"st": 0, "ed": 3, "text": "low dimensional representations"}, {"st": 28, "ed": 30, "text": "context dependent"}, {"st": 38, "ed": 40, "text": "latent variable"}, {"st": 50, "ed": 52, "text": "word representations"}, {"st": 56, "ed": 58, "text": "inference procedure"}, {"st": 76, "ed": 78, "text": "efficient inference"}, {"st": 85, "ed": 87, "text": "learning algorithm"}, {"st": 101, "ed": 104, "text": "method of moments"}, {"st": 116, "ed": 118, "text": "word embeddings"}, {"st": 130, "ed": 132, "text": "kalman filter"}, {"st": 156, "ed": 160, "text": "recurrent neural network language"}]
[{"st": 3, "ed": 5, "text": "natural language"}, {"st": 83, "ed": 86, "text": "latent semantic analysis"}, {"st": 95, "ed": 98, "text": "latent dirichlet allocation"}, {"st": 128, "ed": 130, "text": "web application"}, {"st": 135, "ed": 137, "text": "training set"}]
[{"st": 2, "ed": 4, "text": "machine learning"}, {"st": 68, "ed": 70, "text": "based optimization"}, {"st": 78, "ed": 80, "text": "linear models"}, {"st": 92, "ed": 94, "text": "latent variable"}, {"st": 96, "ed": 98, "text": "neural networks"}, {"st": 103, "ed": 105, "text": "sentiment analysis"}]
[{"st": 77, "ed": 79, "text": "rule based"}, {"st": 87, "ed": 89, "text": "mechanical turk"}, {"st": 99, "ed": 101, "text": "training data"}, {"st": 109, "ed": 111, "text": "multi class"}]
[{"st": 1, "ed": 3, "text": "inference problems"}, {"st": 4, "ed": 6, "text": "structured prediction"}, {"st": 13, "ed": 15, "text": "dependency structure"}, {"st": 24, "ed": 26, "text": "mean field"}, {"st": 28, "ed": 30, "text": "variational inference"}, {"st": 40, "ed": 42, "text": "linear programming"}, {"st": 48, "ed": 51, "text": "semi supervised learning"}, {"st": 74, "ed": 76, "text": "latent variables"}, {"st": 81, "ed": 83, "text": "fast inference"}, {"st": 92, "ed": 94, "text": "generating function"}, {"st": 132, "ed": 134, "text": "handwriting recognition"}, {"st": 141, "ed": 143, "text": "inference procedure"}, {"st": 149, "ed": 151, "text": "highly scalable"}, {"st": 154, "ed": 156, "text": "challenging problem"}, {"st": 161, "ed": 163, "text": "graphical model"}]
[{"st": 2, "ed": 4, "text": "topic models"}, {"st": 25, "ed": 27, "text": "document clustering"}, {"st": 27, "ed": 29, "text": "link prediction"}, {"st": 37, "ed": 39, "text": "topic models"}, {"st": 60, "ed": 62, "text": "real world"}, {"st": 75, "ed": 77, "text": "topic model"}, {"st": 85, "ed": 87, "text": "probability distributions"}, {"st": 89, "ed": 91, "text": "generative model"}, {"st": 128, "ed": 130, "text": "network structure"}, {"st": 191, "ed": 194, "text": "markov random field"}, {"st": 204, "ed": 206, "text": "posterior inference"}, {"st": 221, "ed": 225, "text": "synthetic and real world"}]
[{"st": 2, "ed": 4, "text": "speech recognition"}, {"st": 18, "ed": 20, "text": "active learning"}, {"st": 32, "ed": 34, "text": "speech recognition"}, {"st": 38, "ed": 40, "text": "likelihood based"}, {"st": 40, "ed": 42, "text": "active learning"}, {"st": 51, "ed": 53, "text": "active learning"}, {"st": 70, "ed": 72, "text": "active learning"}, {"st": 73, "ed": 76, "text": "end to end"}]
[{"st": 1, "ed": 3, "text": "sentiment analysis"}, {"st": 17, "ed": 19, "text": "social media"}, {"st": 20, "ed": 22, "text": "sentiment analysis"}, {"st": 39, "ed": 41, "text": "sentiment analysis"}, {"st": 92, "ed": 94, "text": "sentiment analysis"}, {"st": 97, "ed": 99, "text": "starting point"}, {"st": 102, "ed": 104, "text": "sentiment analysis"}, {"st": 149, "ed": 151, "text": "approach outperforms"}]
[{"st": 0, "ed": 2, "text": "text documents"}, {"st": 9, "ed": 11, "text": "abstract concepts"}, {"st": 19, "ed": 21, "text": "machine learning"}, {"st": 32, "ed": 34, "text": "abstract concepts"}, {"st": 89, "ed": 91, "text": "classification decision"}, {"st": 93, "ed": 95, "text": "individual words"}, {"st": 96, "ed": 98, "text": "layer wise"}, {"st": 102, "ed": 104, "text": "recently developed"}, {"st": 121, "ed": 125, "text": "convolutional neural network cnn"}, {"st": 127, "ed": 130, "text": "bag of words"}, {"st": 156, "ed": 158, "text": "individual words"}, {"st": 169, "ed": 171, "text": "relevant information"}, {"st": 172, "ed": 174, "text": "text documents"}, {"st": 227, "ed": 229, "text": "classification accuracy"}, {"st": 233, "ed": 235, "text": "higher level"}]
[{"st": 1, "ed": 3, "text": "topic models"}, {"st": 30, "ed": 32, "text": "variational inference"}, {"st": 35, "ed": 37, "text": "mean field"}, {"st": 67, "ed": 69, "text": "gibbs sampling"}, {"st": 85, "ed": 87, "text": "sampling algorithm"}, {"st": 92, "ed": 94, "text": "conditional distribution"}, {"st": 104, "ed": 106, "text": "empirical results"}, {"st": 107, "ed": 109, "text": "significant improvements"}, {"st": 110, "ed": 112, "text": "prediction performance"}]
[{"st": 49, "ed": 52, "text": "hidden markov models"}]
[{"st": 1, "ed": 3, "text": "recently introduced"}, {"st": 4, "ed": 6, "text": "skip gram"}, {"st": 57, "ed": 59, "text": "significant speedup"}, {"st": 83, "ed": 85, "text": "word representations"}, {"st": 89, "ed": 91, "text": "word order"}, {"st": 112, "ed": 114, "text": "air canada"}]
[{"st": 1, "ed": 3, "text": "deep architectures"}, {"st": 8, "ed": 10, "text": "neural networks"}, {"st": 12, "ed": 14, "text": "successfully applied"}, {"st": 16, "ed": 18, "text": "natural language"}, {"st": 23, "ed": 26, "text": "recurrent neural networks"}, {"st": 48, "ed": 50, "text": "structural information"}, {"st": 75, "ed": 77, "text": "parse tree"}, {"st": 97, "ed": 99, "text": "preliminary experiments"}]
[{"st": 3, "ed": 5, "text": "low rank"}, {"st": 11, "ed": 13, "text": "n gram"}, {"st": 18, "ed": 20, "text": "low rank"}, {"st": 43, "ed": 45, "text": "n gram"}, {"st": 71, "ed": 73, "text": "approach outperforms"}, {"st": 86, "ed": 88, "text": "large corpora"}, {"st": 97, "ed": 99, "text": "machine translation"}]
[{"st": 10, "ed": 12, "text": "active learning"}, {"st": 14, "ed": 16, "text": "significantly reduce"}, {"st": 62, "ed": 65, "text": "support vector machines"}, {"st": 72, "ed": 74, "text": "imbalanced datasets"}, {"st": 80, "ed": 82, "text": "binary classification"}]
[{"st": 20, "ed": 22, "text": "inference procedures"}, {"st": 50, "ed": 52, "text": "imbalanced data"}, {"st": 61, "ed": 63, "text": "key idea"}, {"st": 97, "ed": 100, "text": "labeled training data"}]
[{"st": 3, "ed": 5, "text": "existing methods"}, {"st": 7, "ed": 9, "text": "active learning"}, {"st": 93, "ed": 95, "text": "proposed method"}]
[{"st": 26, "ed": 28, "text": "target language"}, {"st": 36, "ed": 38, "text": "significantly outperformed"}, {"st": 103, "ed": 105, "text": "word order"}]
[{"st": 4, "ed": 7, "text": "bag of words"}, {"st": 12, "ed": 14, "text": "computationally efficient"}, {"st": 18, "ed": 20, "text": "distributed representations"}, {"st": 24, "ed": 27, "text": "scale to large"}, {"st": 67, "ed": 70, "text": "bag of words"}, {"st": 70, "ed": 72, "text": "cross lingual"}, {"st": 85, "ed": 87, "text": "cross lingual"}, {"st": 107, "ed": 109, "text": "cross lingual"}, {"st": 109, "ed": 111, "text": "document classification"}]
[{"st": 0, "ed": 2, "text": "originally designed"}, {"st": 5, "ed": 7, "text": "topic modeling"}, {"st": 10, "ed": 12, "text": "powerful tool"}, {"st": 14, "ed": 16, "text": "latent structure"}, {"st": 17, "ed": 19, "text": "domains including"}, {"st": 98, "ed": 100, "text": "topic model"}, {"st": 144, "ed": 146, "text": "real world"}]
[{"st": 1, "ed": 3, "text": "machine translation"}, {"st": 45, "ed": 47, "text": "mechanical turk"}]
[{"st": 5, "ed": 7, "text": "machine translation"}, {"st": 31, "ed": 33, "text": "diminishing returns"}, {"st": 40, "ed": 42, "text": "active learning"}, {"st": 56, "ed": 59, "text": "amazon mechanical turk"}]
[{"st": 8, "ed": 11, "text": "optical character recognition"}, {"st": 51, "ed": 53, "text": "rule based"}, {"st": 53, "ed": 55, "text": "feature based"}, {"st": 57, "ed": 59, "text": "model based"}, {"st": 68, "ed": 70, "text": "random forests"}, {"st": 79, "ed": 81, "text": "unsupervised methods"}, {"st": 87, "ed": 89, "text": "random forests"}, {"st": 89, "ed": 91, "text": "typically require"}, {"st": 91, "ed": 93, "text": "training data"}, {"st": 100, "ed": 102, "text": "random forests"}]
[{"st": 11, "ed": 13, "text": "natural language"}, {"st": 61, "ed": 63, "text": "manually annotated"}]
[{"st": 4, "ed": 6, "text": "echo chamber"}, {"st": 9, "ed": 11, "text": "generative model"}, {"st": 12, "ed": 14, "text": "social interaction"}, {"st": 34, "ed": 36, "text": "unlike previous"}, {"st": 46, "ed": 48, "text": "temporal dynamics"}, {"st": 84, "ed": 86, "text": "fully bayesian"}, {"st": 107, "ed": 109, "text": "supreme court"}]
[{"st": 6, "ed": 8, "text": "unified framework"}, {"st": 10, "ed": 12, "text": "multi relational"}, {"st": 19, "ed": 21, "text": "empirical study"}, {"st": 24, "ed": 26, "text": "multi relational"}, {"st": 26, "ed": 28, "text": "embedding models"}, {"st": 56, "ed": 58, "text": "pre trained"}, {"st": 74, "ed": 76, "text": "embedding model"}]
[{"st": 18, "ed": 21, "text": "efficient and effective"}, {"st": 24, "ed": 26, "text": "vector representations"}, {"st": 37, "ed": 39, "text": "skip gram"}, {"st": 83, "ed": 85, "text": "cost functions"}]
[{"st": 45, "ed": 47, "text": "text categorization"}, {"st": 73, "ed": 75, "text": "joint probability"}, {"st": 154, "ed": 158, "text": "extensive set of experiments"}, {"st": 192, "ed": 194, "text": "empirical results"}, {"st": 210, "ed": 214, "text": "part of speech tagging"}, {"st": 218, "ed": 220, "text": "significant improvement"}]
[{"st": 0, "ed": 4, "text": "convolutional neural network cnn"}, {"st": 6, "ed": 8, "text": "neural network"}, {"st": 14, "ed": 16, "text": "internal structure"}, {"st": 31, "ed": 33, "text": "text categorization"}, {"st": 39, "ed": 41, "text": "word order"}, {"st": 42, "ed": 44, "text": "text data"}, {"st": 50, "ed": 52, "text": "low dimensional"}, {"st": 52, "ed": 54, "text": "word vectors"}, {"st": 67, "ed": 69, "text": "text data"}, {"st": 126, "ed": 128, "text": "experiments demonstrate"}]
[{"st": 115, "ed": 117, "text": "previous approaches"}]
[{"st": 4, "ed": 7, "text": "recurrent neural network"}, {"st": 23, "ed": 25, "text": "fine grained"}, {"st": 44, "ed": 46, "text": "special cases"}, {"st": 65, "ed": 68, "text": "recurrent neural networks"}, {"st": 76, "ed": 78, "text": "fine grained"}, {"st": 78, "ed": 80, "text": "sentiment analysis"}, {"st": 84, "ed": 86, "text": "comparable results"}, {"st": 88, "ed": 90, "text": "deep models"}, {"st": 92, "ed": 94, "text": "recently published"}]
[{"st": 4, "ed": 7, "text": "multi label classification"}, {"st": 56, "ed": 58, "text": "co occurrence"}, {"st": 66, "ed": 68, "text": "proposed method"}, {"st": 115, "ed": 117, "text": "learned representations"}]
[{"st": 8, "ed": 10, "text": "natural language"}, {"st": 31, "ed": 33, "text": "domain specific"}]
[{"st": 2, "ed": 4, "text": "collaborative filtering"}, {"st": 28, "ed": 30, "text": "content based"}, {"st": 49, "ed": 51, "text": "large scale"}, {"st": 51, "ed": 53, "text": "content based"}, {"st": 53, "ed": 55, "text": "collaborative filtering"}, {"st": 69, "ed": 72, "text": "latent dirichlet allocation"}, {"st": 74, "ed": 76, "text": "deep belief"}, {"st": 81, "ed": 84, "text": "low dimensional latent"}, {"st": 104, "ed": 106, "text": "digital media"}, {"st": 122, "ed": 124, "text": "deep belief"}, {"st": 127, "ed": 129, "text": "topic modeling"}]
[{"st": 5, "ed": 7, "text": "semi supervised"}, {"st": 9, "ed": 13, "text": "convolutional neural networks cnns"}, {"st": 18, "ed": 20, "text": "previous approaches"}, {"st": 23, "ed": 25, "text": "word embeddings"}, {"st": 34, "ed": 36, "text": "unlabeled data"}, {"st": 56, "ed": 59, "text": "semi supervised learning"}, {"st": 85, "ed": 87, "text": "previous approaches"}, {"st": 88, "ed": 90, "text": "sentiment classification"}]
[{"st": 2, "ed": 4, "text": "natural language"}, {"st": 7, "ed": 9, "text": "active learning"}, {"st": 36, "ed": 38, "text": "theoretical analysis"}, {"st": 40, "ed": 42, "text": "active learning"}, {"st": 73, "ed": 75, "text": "trained models"}, {"st": 80, "ed": 82, "text": "f measure"}, {"st": 112, "ed": 114, "text": "previously unseen"}, {"st": 124, "ed": 126, "text": "low variance"}, {"st": 174, "ed": 176, "text": "f measure"}]
[{"st": 29, "ed": 31, "text": "low dimensional"}, {"st": 45, "ed": 47, "text": "word embedding"}, {"st": 78, "ed": 80, "text": "word representations"}, {"st": 86, "ed": 88, "text": "attention mechanism"}, {"st": 125, "ed": 127, "text": "word embedding"}, {"st": 140, "ed": 142, "text": "large corpora"}, {"st": 151, "ed": 153, "text": "training set"}, {"st": 170, "ed": 172, "text": "existing methods"}]
[{"st": 13, "ed": 15, "text": "log linear"}, {"st": 36, "ed": 38, "text": "machine learning"}, {"st": 45, "ed": 47, "text": "recently proposed"}, {"st": 55, "ed": 57, "text": "regularization term"}, {"st": 79, "ed": 81, "text": "empirical evidence"}, {"st": 90, "ed": 92, "text": "theoretical understanding"}, {"st": 109, "ed": 111, "text": "generalization bounds"}, {"st": 122, "ed": 125, "text": "loss in accuracy"}, {"st": 132, "ed": 134, "text": "input distributions"}, {"st": 148, "ed": 150, "text": "theoretical results"}]
[{"st": 7, "ed": 9, "text": "topic model"}, {"st": 22, "ed": 24, "text": "contrastive divergence"}, {"st": 62, "ed": 64, "text": "great learning"}]
[{"st": 2, "ed": 4, "text": "inner product"}, {"st": 13, "ed": 15, "text": "wide applicability"}, {"st": 16, "ed": 18, "text": "recommendation systems"}, {"st": 29, "ed": 32, "text": "locality sensitive hashing"}, {"st": 44, "ed": 46, "text": "recent literature"}, {"st": 73, "ed": 76, "text": "k means clustering"}, {"st": 84, "ed": 86, "text": "k means"}, {"st": 95, "ed": 97, "text": "cosine similarity"}, {"st": 111, "ed": 113, "text": "large vocabulary"}, {"st": 113, "ed": 115, "text": "word embeddings"}, {"st": 119, "ed": 121, "text": "approach yields"}, {"st": 138, "ed": 140, "text": "tree based"}]
[{"st": 23, "ed": 25, "text": "plain text"}, {"st": 31, "ed": 33, "text": "previous works"}, {"st": 37, "ed": 39, "text": "unstructured text"}, {"st": 50, "ed": 52, "text": "unstructured text"}, {"st": 89, "ed": 91, "text": "topic model"}, {"st": 125, "ed": 127, "text": "text mining"}, {"st": 132, "ed": 134, "text": "variational inference"}, {"st": 137, "ed": 139, "text": "em algorithm"}, {"st": 148, "ed": 150, "text": "large scale"}, {"st": 157, "ed": 159, "text": "distributed computing"}, {"st": 162, "ed": 164, "text": "large scale"}]
[{"st": 0, "ed": 2, "text": "hierarchical latent"}, {"st": 6, "ed": 8, "text": "recently proposed"}, {"st": 20, "ed": 22, "text": "lda based"}, {"st": 53, "ed": 57, "text": "expectation maximization em algorithm"}, {"st": 58, "ed": 60, "text": "parameter estimation"}, {"st": 88, "ed": 90, "text": "recent advances"}, {"st": 116, "ed": 118, "text": "lda based"}]
[{"st": 6, "ed": 8, "text": "text mining"}, {"st": 17, "ed": 19, "text": "decision support"}, {"st": 32, "ed": 34, "text": "machine learning"}, {"st": 36, "ed": 38, "text": "recent research"}, {"st": 41, "ed": 43, "text": "machine learning"}, {"st": 65, "ed": 67, "text": "deep learning"}, {"st": 76, "ed": 78, "text": "neural network"}, {"st": 84, "ed": 86, "text": "deep learning"}, {"st": 90, "ed": 92, "text": "outperform traditional"}, {"st": 105, "ed": 107, "text": "deep learning"}, {"st": 133, "ed": 135, "text": "deep learning"}, {"st": 140, "ed": 142, "text": "random forests"}, {"st": 146, "ed": 148, "text": "machine learning"}]
[{"st": 2, "ed": 4, "text": "word embedding"}, {"st": 10, "ed": 12, "text": "embedding models"}, {"st": 13, "ed": 15, "text": "matrix factorization"}, {"st": 28, "ed": 30, "text": "based methods"}, {"st": 33, "ed": 37, "text": "singular value decomposition svd"}, {"st": 51, "ed": 53, "text": "latent factors"}, {"st": 62, "ed": 64, "text": "word embedding"}, {"st": 66, "ed": 68, "text": "generative models"}, {"st": 74, "ed": 76, "text": "latent factors"}, {"st": 80, "ed": 82, "text": "word embedding"}, {"st": 97, "ed": 99, "text": "latent factor"}, {"st": 106, "ed": 108, "text": "low rank"}, {"st": 109, "ed": 111, "text": "positive semidefinite"}, {"st": 142, "ed": 144, "text": "benchmark datasets"}]
[{"st": 4, "ed": 6, "text": "sufficient conditions"}, {"st": 17, "ed": 19, "text": "latent factors"}, {"st": 31, "ed": 33, "text": "latent factors"}, {"st": 44, "ed": 46, "text": "topic models"}, {"st": 50, "ed": 52, "text": "latent factor"}, {"st": 78, "ed": 80, "text": "key insight"}, {"st": 91, "ed": 93, "text": "convex hull"}, {"st": 102, "ed": 105, "text": "word co occurrence"}, {"st": 116, "ed": 118, "text": "sample complexity"}, {"st": 124, "ed": 126, "text": "random projections"}, {"st": 132, "ed": 135, "text": "word co occurrence"}, {"st": 138, "ed": 140, "text": "random projections"}]
[{"st": 0, "ed": 2, "text": "word representations"}, {"st": 7, "ed": 9, "text": "latent variables"}, {"st": 36, "ed": 38, "text": "unsupervised learning"}, {"st": 50, "ed": 52, "text": "observed variables"}, {"st": 90, "ed": 92, "text": "word representations"}, {"st": 95, "ed": 98, "text": "named entity recognition"}, {"st": 123, "ed": 125, "text": "representation learning"}]
[{"st": 1, "ed": 3, "text": "vector representations"}, {"st": 23, "ed": 25, "text": "theoretical understanding"}, {"st": 26, "ed": 28, "text": "word embeddings"}, {"st": 65, "ed": 67, "text": "existing algorithms"}, {"st": 74, "ed": 76, "text": "co occurrence"}, {"st": 80, "ed": 82, "text": "random walks"}, {"st": 102, "ed": 104, "text": "random walks"}, {"st": 136, "ed": 138, "text": "dimensionality reduction"}, {"st": 142, "ed": 144, "text": "tasks including"}]
[{"st": 7, "ed": 9, "text": "structured prediction"}, {"st": 13, "ed": 16, "text": "support vector machines"}, {"st": 47, "ed": 49, "text": "structured learning"}]
[{"st": 0, "ed": 3, "text": "neural language models"}, {"st": 5, "ed": 7, "text": "powerful tool"}, {"st": 60, "ed": 62, "text": "prior knowledge"}, {"st": 73, "ed": 75, "text": "generative model"}, {"st": 96, "ed": 98, "text": "co occurrence"}, {"st": 99, "ed": 101, "text": "distributional semantics"}, {"st": 132, "ed": 134, "text": "link prediction"}]
[{"st": 0, "ed": 2, "text": "hyper parameters"}, {"st": 8, "ed": 11, "text": "learning and inference"}, {"st": 13, "ed": 16, "text": "latent dirichlet allocation"}, {"st": 24, "ed": 26, "text": "latent variables"}, {"st": 26, "ed": 28, "text": "learning process"}, {"st": 29, "ed": 31, "text": "hyper parameters"}, {"st": 46, "ed": 49, "text": "latent dirichlet allocation"}, {"st": 60, "ed": 62, "text": "hyper parameters"}, {"st": 64, "ed": 66, "text": "gibbs sampling"}, {"st": 107, "ed": 109, "text": "existing approaches"}, {"st": 115, "ed": 117, "text": "fixed point"}, {"st": 146, "ed": 148, "text": "topic models"}, {"st": 165, "ed": 167, "text": "binary classification"}]
[{"st": 52, "ed": 54, "text": "current methods"}, {"st": 65, "ed": 67, "text": "alternating optimization"}, {"st": 96, "ed": 98, "text": "emergency department"}, {"st": 112, "ed": 114, "text": "real world"}]
[{"st": 0, "ed": 2, "text": "text documents"}, {"st": 5, "ed": 7, "text": "multiple levels"}, {"st": 9, "ed": 11, "text": "individual words"}, {"st": 56, "ed": 58, "text": "empirically evaluate"}, {"st": 61, "ed": 63, "text": "multi level"}, {"st": 63, "ed": 67, "text": "recurrent neural network language"}, {"st": 76, "ed": 78, "text": "contextual information"}, {"st": 87, "ed": 89, "text": "word level"}, {"st": 89, "ed": 93, "text": "recurrent neural network language"}]
[{"st": 0, "ed": 3, "text": "deep neural networks"}, {"st": 4, "ed": 6, "text": "achieved impressive"}, {"st": 6, "ed": 8, "text": "supervised classification"}, {"st": 11, "ed": 13, "text": "tasks including"}, {"st": 13, "ed": 15, "text": "image recognition"}, {"st": 15, "ed": 17, "text": "speech recognition"}, {"st": 32, "ed": 34, "text": "question answering"}, {"st": 67, "ed": 69, "text": "neural networks"}, {"st": 86, "ed": 89, "text": "end to end"}, {"st": 90, "ed": 92, "text": "neural network"}, {"st": 131, "ed": 133, "text": "weak supervision"}, {"st": 192, "ed": 194, "text": "trained jointly"}, {"st": 213, "ed": 215, "text": "random noise"}, {"st": 227, "ed": 229, "text": "recurrent networks"}, {"st": 232, "ed": 234, "text": "perform poorly"}]
[{"st": 6, "ed": 8, "text": "word embeddings"}, {"st": 15, "ed": 17, "text": "skip gram"}, {"st": 23, "ed": 26, "text": "bag of words"}, {"st": 67, "ed": 70, "text": "qualitatively and quantitatively"}]
[{"st": 0, "ed": 2, "text": "recent advances"}, {"st": 4, "ed": 6, "text": "variational inference"}, {"st": 12, "ed": 14, "text": "latent variable"}, {"st": 22, "ed": 24, "text": "variational inference"}, {"st": 34, "ed": 36, "text": "variational methods"}, {"st": 45, "ed": 47, "text": "latent variables"}, {"st": 51, "ed": 53, "text": "inference network"}, {"st": 95, "ed": 98, "text": "bag of words"}, {"st": 98, "ed": 100, "text": "generative model"}, {"st": 123, "ed": 125, "text": "attention mechanism"}, {"st": 137, "ed": 139, "text": "question answering"}]
[{"st": 39, "ed": 42, "text": "multi task learning"}, {"st": 64, "ed": 66, "text": "machine translation"}, {"st": 90, "ed": 92, "text": "image caption"}, {"st": 128, "ed": 130, "text": "image caption"}, {"st": 144, "ed": 146, "text": "bleu points"}, {"st": 180, "ed": 182, "text": "unsupervised learning"}, {"st": 185, "ed": 187, "text": "skip thought"}]
[{"st": 7, "ed": 9, "text": "supervised learning"}, {"st": 18, "ed": 20, "text": "complex tasks"}, {"st": 44, "ed": 46, "text": "chain rule"}, {"st": 50, "ed": 52, "text": "joint probability"}, {"st": 107, "ed": 109, "text": "random variables"}, {"st": 193, "ed": 195, "text": "empirical evidence"}, {"st": 226, "ed": 228, "text": "joint probability"}]
[{"st": 0, "ed": 2, "text": "word embeddings"}, {"st": 39, "ed": 41, "text": "word embedding"}, {"st": 58, "ed": 60, "text": "embedding methods"}, {"st": 67, "ed": 69, "text": "random walk"}, {"st": 86, "ed": 88, "text": "word sense"}, {"st": 130, "ed": 132, "text": "based approaches"}]
[{"st": 8, "ed": 12, "text": "deep neural network dnn"}, {"st": 12, "ed": 14, "text": "class conditional"}, {"st": 14, "ed": 16, "text": "posterior probabilities"}, {"st": 20, "ed": 22, "text": "low dimensional"}, {"st": 37, "ed": 39, "text": "sparse representation"}, {"st": 83, "ed": 85, "text": "low rank"}, {"st": 88, "ed": 90, "text": "low dimensional"}, {"st": 115, "ed": 118, "text": "continuous speech recognition"}, {"st": 121, "ed": 123, "text": "dnn hmm"}, {"st": 123, "ed": 126, "text": "hidden markov model"}, {"st": 139, "ed": 143, "text": "word error rate wer"}]
[{"st": 5, "ed": 7, "text": "activation function"}, {"st": 47, "ed": 50, "text": "convex loss function"}, {"st": 76, "ed": 78, "text": "empirical results"}, {"st": 79, "ed": 82, "text": "multi label classification"}, {"st": 85, "ed": 87, "text": "attention based"}, {"st": 87, "ed": 89, "text": "neural networks"}, {"st": 90, "ed": 92, "text": "natural language"}, {"st": 99, "ed": 101, "text": "similar performance"}]
[{"st": 3, "ed": 6, "text": "convolutional neural network"}, {"st": 13, "ed": 15, "text": "text categorization"}, {"st": 24, "ed": 26, "text": "special case"}, {"st": 34, "ed": 36, "text": "linear model"}, {"st": 62, "ed": 65, "text": "short term memory"}, {"st": 91, "ed": 94, "text": "effective and efficient"}, {"st": 102, "ed": 106, "text": "supervised and semi supervised"}]
[{"st": 1, "ed": 3, "text": "feature selection"}, {"st": 6, "ed": 8, "text": "text categorization"}, {"st": 18, "ed": 20, "text": "learning process"}, {"st": 31, "ed": 33, "text": "feature selection"}, {"st": 57, "ed": 60, "text": "kullback leibler divergence"}, {"st": 77, "ed": 79, "text": "type ii"}, {"st": 103, "ed": 105, "text": "multi class"}, {"st": 115, "ed": 117, "text": "feature selection"}, {"st": 131, "ed": 133, "text": "promising results"}, {"st": 134, "ed": 136, "text": "extensive experiments"}]
[{"st": 1, "ed": 3, "text": "topic models"}, {"st": 6, "ed": 8, "text": "latent topic"}, {"st": 16, "ed": 18, "text": "response variable"}, {"st": 28, "ed": 30, "text": "variational approximation"}, {"st": 31, "ed": 33, "text": "monte carlo"}, {"st": 39, "ed": 41, "text": "local minimum"}, {"st": 42, "ed": 44, "text": "spectral methods"}, {"st": 50, "ed": 52, "text": "topic models"}, {"st": 54, "ed": 57, "text": "latent dirichlet allocation"}, {"st": 68, "ed": 70, "text": "spectral methods"}, {"st": 139, "ed": 141, "text": "sample complexity"}, {"st": 148, "ed": 151, "text": "a sufficient condition"}, {"st": 157, "ed": 163, "text": "experiments on synthetic and real world"}, {"st": 182, "ed": 184, "text": "large scale"}, {"st": 211, "ed": 213, "text": "spectral methods"}]
[{"st": 1, "ed": 3, "text": "disease surveillance"}, {"st": 8, "ed": 10, "text": "wide variety"}, {"st": 62, "ed": 64, "text": "word embeddings"}, {"st": 72, "ed": 74, "text": "word embeddings"}]
[{"st": 12, "ed": 14, "text": "task specific"}, {"st": 19, "ed": 22, "text": "hand crafted features"}, {"st": 34, "ed": 36, "text": "network architecture"}, {"st": 42, "ed": 44, "text": "character level"}, {"st": 50, "ed": 52, "text": "bidirectional lstm"}, {"st": 59, "ed": 62, "text": "end to end"}, {"st": 64, "ed": 66, "text": "feature engineering"}, {"st": 68, "ed": 70, "text": "pre processing"}, {"st": 110, "ed": 113, "text": "named entity recognition"}]
[{"st": 10, "ed": 12, "text": "medieval latin"}, {"st": 12, "ed": 16, "text": "part of speech tagging"}, {"st": 87, "ed": 89, "text": "context aware"}, {"st": 144, "ed": 146, "text": "neural network"}]
[{"st": 8, "ed": 10, "text": "word embeddings"}, {"st": 63, "ed": 65, "text": "word representations"}]
[{"st": 7, "ed": 9, "text": "n gram"}, {"st": 10, "ed": 12, "text": "neural network"}, {"st": 27, "ed": 29, "text": "random field"}, {"st": 33, "ed": 35, "text": "recently introduced"}, {"st": 70, "ed": 72, "text": "relative error"}, {"st": 82, "ed": 84, "text": "speech recognition"}, {"st": 86, "ed": 88, "text": "log linear"}]
[{"st": 0, "ed": 2, "text": "large scale"}, {"st": 3, "ed": 5, "text": "meta analysis"}, {"st": 56, "ed": 58, "text": "meta analysis"}, {"st": 71, "ed": 74, "text": "latent dirichlet allocation"}, {"st": 88, "ed": 90, "text": "text corpus"}, {"st": 123, "ed": 125, "text": "high dimensional"}, {"st": 136, "ed": 138, "text": "machine learning"}]
[{"st": 0, "ed": 2, "text": "recent advances"}, {"st": 11, "ed": 13, "text": "network architectures"}, {"st": 14, "ed": 16, "text": "attention mechanism"}, {"st": 16, "ed": 18, "text": "learning algorithms"}, {"st": 33, "ed": 35, "text": "speech recognition"}, {"st": 79, "ed": 81, "text": "hidden layers"}, {"st": 83, "ed": 86, "text": "deep neural network"}, {"st": 107, "ed": 109, "text": "extensively evaluate"}, {"st": 111, "ed": 114, "text": "attention based neural"}, {"st": 114, "ed": 116, "text": "machine translation"}]
[{"st": 31, "ed": 33, "text": "hierarchical latent"}, {"st": 67, "ed": 69, "text": "latent variables"}, {"st": 77, "ed": 80, "text": "word co occurrence"}, {"st": 84, "ed": 86, "text": "higher levels"}, {"st": 87, "ed": 89, "text": "co occurrence"}, {"st": 96, "ed": 98, "text": "latent variable"}, {"st": 115, "ed": 117, "text": "latent variables"}, {"st": 126, "ed": 129, "text": "word co occurrence"}, {"st": 148, "ed": 151, "text": "word co occurrence"}, {"st": 159, "ed": 161, "text": "lda based"}, {"st": 161, "ed": 163, "text": "topic models"}, {"st": 170, "ed": 172, "text": "generation process"}]
[{"st": 1, "ed": 3, "text": "structured prediction"}, {"st": 4, "ed": 6, "text": "bandit feedback"}, {"st": 53, "ed": 55, "text": "convex objectives"}, {"st": 56, "ed": 58, "text": "structured prediction"}, {"st": 74, "ed": 76, "text": "natural language"}, {"st": 79, "ed": 81, "text": "output spaces"}, {"st": 83, "ed": 85, "text": "convergence speed"}, {"st": 94, "ed": 96, "text": "task performance"}, {"st": 119, "ed": 121, "text": "convex objective"}]
[{"st": 1, "ed": 4, "text": "deep neural networks"}, {"st": 8, "ed": 10, "text": "application domains"}, {"st": 36, "ed": 40, "text": "recurrent neural networks rnns"}, {"st": 46, "ed": 48, "text": "speech recognition"}, {"st": 62, "ed": 65, "text": "hidden markov model"}, {"st": 117, "ed": 119, "text": "jointly trained"}, {"st": 129, "ed": 131, "text": "complementary information"}]
[{"st": 0, "ed": 3, "text": "deep neural networks"}, {"st": 6, "ed": 8, "text": "successfully applied"}, {"st": 10, "ed": 12, "text": "wide variety"}, {"st": 29, "ed": 31, "text": "feature extraction"}, {"st": 66, "ed": 68, "text": "based regularization"}, {"st": 87, "ed": 89, "text": "feature vectors"}, {"st": 96, "ed": 98, "text": "network outputs"}, {"st": 115, "ed": 117, "text": "empirical evidence"}, {"st": 133, "ed": 135, "text": "hidden layers"}, {"st": 147, "ed": 149, "text": "feature extraction"}, {"st": 150, "ed": 154, "text": "hidden markov model hmm"}, {"st": 176, "ed": 178, "text": "large vocabulary"}, {"st": 178, "ed": 181, "text": "continuous speech recognition"}, {"st": 189, "ed": 193, "text": "word error rate wer"}]
[{"st": 0, "ed": 2, "text": "machine learning"}, {"st": 7, "ed": 9, "text": "statistical properties"}, {"st": 15, "ed": 17, "text": "input data"}, {"st": 30, "ed": 32, "text": "learning algorithm"}, {"st": 48, "ed": 50, "text": "word embedding"}, {"st": 72, "ed": 74, "text": "multiple datasets"}, {"st": 122, "ed": 124, "text": "training examples"}, {"st": 128, "ed": 130, "text": "geometric properties"}]
[{"st": 75, "ed": 77, "text": "poor performance"}, {"st": 85, "ed": 87, "text": "topic model"}]
[{"st": 11, "ed": 13, "text": "natural language"}, {"st": 16, "ed": 18, "text": "generative model"}, {"st": 113, "ed": 115, "text": "background knowledge"}, {"st": 119, "ed": 121, "text": "unlike previous"}, {"st": 139, "ed": 141, "text": "prior probability"}, {"st": 171, "ed": 173, "text": "hierarchical dirichlet"}, {"st": 182, "ed": 184, "text": "efficient inference"}]
[{"st": 28, "ed": 30, "text": "time consuming"}, {"st": 50, "ed": 52, "text": "latent topics"}, {"st": 62, "ed": 64, "text": "latent factors"}]
[{"st": 5, "ed": 7, "text": "neural network"}, {"st": 8, "ed": 10, "text": "natural language"}, {"st": 21, "ed": 23, "text": "update rule"}, {"st": 70, "ed": 72, "text": "natural language"}, {"st": 73, "ed": 75, "text": "natural language"}, {"st": 76, "ed": 78, "text": "question answering"}, {"st": 78, "ed": 80, "text": "sentence classification"}, {"st": 81, "ed": 83, "text": "sentiment analysis"}, {"st": 84, "ed": 86, "text": "machine translation"}, {"st": 103, "ed": 105, "text": "shared memory"}, {"st": 111, "ed": 114, "text": "neural machine translation"}, {"st": 116, "ed": 118, "text": "attention based"}]
[{"st": 0, "ed": 4, "text": "recurrent neural networks rnns"}, {"st": 26, "ed": 28, "text": "explicitly model"}, {"st": 103, "ed": 105, "text": "attention mechanism"}, {"st": 139, "ed": 141, "text": "natural language"}, {"st": 146, "ed": 148, "text": "sentence classification"}]
[{"st": 14, "ed": 16, "text": "prediction problems"}, {"st": 21, "ed": 24, "text": "recurrent neural networks"}, {"st": 53, "ed": 55, "text": "imitation learning"}, {"st": 65, "ed": 67, "text": "search space"}, {"st": 74, "ed": 76, "text": "training procedure"}, {"st": 87, "ed": 89, "text": "highly correlated"}]
[{"st": 20, "ed": 22, "text": "rasch model"}, {"st": 28, "ed": 30, "text": "probabilistic model"}, {"st": 75, "ed": 77, "text": "rasch model"}, {"st": 140, "ed": 142, "text": "rasch model"}]
[{"st": 7, "ed": 9, "text": "word level"}, {"st": 9, "ed": 13, "text": "convolutional neural networks cnn"}, {"st": 24, "ed": 26, "text": "training data"}, {"st": 34, "ed": 36, "text": "character level"}, {"st": 50, "ed": 52, "text": "word level"}, {"st": 55, "ed": 57, "text": "error rates"}, {"st": 59, "ed": 61, "text": "error rates"}, {"st": 79, "ed": 81, "text": "pre processing"}, {"st": 87, "ed": 89, "text": "word level"}, {"st": 101, "ed": 103, "text": "character level"}, {"st": 107, "ed": 109, "text": "word level"}]
[{"st": 4, "ed": 6, "text": "application domains"}, {"st": 35, "ed": 37, "text": "latent factor"}, {"st": 82, "ed": 84, "text": "topic models"}, {"st": 87, "ed": 89, "text": "word embeddings"}, {"st": 100, "ed": 104, "text": "deep recurrent neural networks"}, {"st": 111, "ed": 113, "text": "latent vector"}, {"st": 114, "ed": 116, "text": "gated recurrent"}, {"st": 118, "ed": 122, "text": "trained end to end"}, {"st": 124, "ed": 126, "text": "collaborative filtering"}, {"st": 138, "ed": 140, "text": "significantly higher"}, {"st": 142, "ed": 144, "text": "cold start"}, {"st": 164, "ed": 167, "text": "multi task learning"}, {"st": 170, "ed": 172, "text": "encoder network"}, {"st": 187, "ed": 189, "text": "collaborative filtering"}]
[{"st": 15, "ed": 17, "text": "probability distributions"}, {"st": 30, "ed": 32, "text": "nonparametric bayesian"}, {"st": 54, "ed": 56, "text": "latent variable"}, {"st": 75, "ed": 77, "text": "topic models"}, {"st": 79, "ed": 81, "text": "computer science"}, {"st": 87, "ed": 89, "text": "nonparametric bayesian"}, {"st": 89, "ed": 91, "text": "topic model"}, {"st": 123, "ed": 125, "text": "parametric models"}, {"st": 127, "ed": 130, "text": "goodness of fit"}, {"st": 131, "ed": 133, "text": "real world"}]
[{"st": 6, "ed": 9, "text": "semi supervised learning"}, {"st": 18, "ed": 21, "text": "automatic speech recognition"}, {"st": 23, "ed": 25, "text": "deep neural"}, {"st": 35, "ed": 37, "text": "fine tuning"}, {"st": 39, "ed": 42, "text": "takes advantage of"}, {"st": 45, "ed": 47, "text": "labelled data"}, {"st": 80, "ed": 82, "text": "method outperforms"}, {"st": 83, "ed": 85, "text": "supervised training"}, {"st": 90, "ed": 92, "text": "labelled data"}, {"st": 95, "ed": 97, "text": "error rates"}, {"st": 103, "ed": 108, "text": "graph based semi supervised learning"}]
[{"st": 14, "ed": 16, "text": "output units"}, {"st": 22, "ed": 24, "text": "input sequence"}, {"st": 46, "ed": 49, "text": "wall street journal"}, {"st": 49, "ed": 51, "text": "speech recognition"}, {"st": 70, "ed": 72, "text": "convolutional network"}]
[{"st": 2, "ed": 4, "text": "multi class"}, {"st": 10, "ed": 12, "text": "hierarchical structure"}, {"st": 28, "ed": 30, "text": "predictive power"}, {"st": 57, "ed": 59, "text": "feature vectors"}, {"st": 69, "ed": 71, "text": "representation learning"}, {"st": 73, "ed": 75, "text": "input data"}, {"st": 134, "ed": 136, "text": "empirically validate"}, {"st": 143, "ed": 145, "text": "text classification"}, {"st": 154, "ed": 156, "text": "compare favorably"}]
[{"st": 4, "ed": 6, "text": "segmental models"}, {"st": 11, "ed": 13, "text": "achieve competitive"}, {"st": 13, "ed": 15, "text": "speech recognition"}, {"st": 20, "ed": 22, "text": "deep neural"}, {"st": 25, "ed": 27, "text": "segmental models"}, {"st": 40, "ed": 42, "text": "segmental models"}, {"st": 45, "ed": 49, "text": "trained end to end"}, {"st": 78, "ed": 80, "text": "linear model"}, {"st": 94, "ed": 96, "text": "neural network"}, {"st": 97, "ed": 99, "text": "segmental models"}, {"st": 116, "ed": 118, "text": "weight training"}, {"st": 118, "ed": 121, "text": "end to end"}, {"st": 121, "ed": 123, "text": "joint training"}, {"st": 132, "ed": 135, "text": "end to end"}, {"st": 135, "ed": 137, "text": "fine tuning"}, {"st": 143, "ed": 145, "text": "segmental models"}, {"st": 145, "ed": 149, "text": "trained end to end"}, {"st": 150, "ed": 152, "text": "hinge loss"}, {"st": 152, "ed": 154, "text": "log loss"}, {"st": 155, "ed": 157, "text": "hinge loss"}, {"st": 186, "ed": 188, "text": "log loss"}, {"st": 192, "ed": 194, "text": "strong performance"}, {"st": 196, "ed": 198, "text": "ground truth"}, {"st": 214, "ed": 217, "text": "end to end"}]
[{"st": 0, "ed": 2, "text": "vector representations"}, {"st": 73, "ed": 75, "text": "key feature"}, {"st": 93, "ed": 95, "text": "low rank"}, {"st": 126, "ed": 128, "text": "dimensional subspace"}, {"st": 136, "ed": 138, "text": "clustering algorithm"}, {"st": 177, "ed": 179, "text": "unsupervised manner"}, {"st": 192, "ed": 194, "text": "word sense"}, {"st": 199, "ed": 201, "text": "empirical studies"}]
[{"st": 0, "ed": 3, "text": "recurrent neural networks"}, {"st": 82, "ed": 84, "text": "theoretical framework"}]
[{"st": 6, "ed": 8, "text": "group lasso"}, {"st": 16, "ed": 19, "text": "deep neural network"}, {"st": 20, "ed": 22, "text": "hidden layers"}, {"st": 25, "ed": 27, "text": "dnn hmm"}, {"st": 45, "ed": 47, "text": "weight vectors"}, {"st": 51, "ed": 53, "text": "weight vectors"}, {"st": 61, "ed": 63, "text": "hidden layer"}, {"st": 75, "ed": 77, "text": "experiment results"}, {"st": 80, "ed": 82, "text": "dnn training"}, {"st": 92, "ed": 94, "text": "hidden layer"}]
[{"st": 22, "ed": 25, "text": "positive and negative"}, {"st": 45, "ed": 47, "text": "classification performance"}, {"st": 116, "ed": 118, "text": "based approaches"}, {"st": 146, "ed": 148, "text": "labeled data"}, {"st": 150, "ed": 152, "text": "co occurrence"}, {"st": 175, "ed": 177, "text": "higher accuracy"}]
[{"st": 22, "ed": 24, "text": "natural language"}, {"st": 32, "ed": 34, "text": "extraction task"}, {"st": 59, "ed": 61, "text": "handcrafted features"}, {"st": 62, "ed": 64, "text": "domain specific"}, {"st": 83, "ed": 86, "text": "recurrent neural network"}, {"st": 87, "ed": 89, "text": "bidirectional lstm"}, {"st": 114, "ed": 116, "text": "proposed framework"}, {"st": 118, "ed": 120, "text": "recent methods"}]
[{"st": 9, "ed": 11, "text": "cross lingual"}, {"st": 27, "ed": 29, "text": "task specific"}, {"st": 29, "ed": 31, "text": "cross lingual"}, {"st": 37, "ed": 39, "text": "important features"}, {"st": 57, "ed": 59, "text": "source language"}, {"st": 94, "ed": 96, "text": "cross lingual"}, {"st": 99, "ed": 101, "text": "distributed representation"}, {"st": 125, "ed": 127, "text": "conduct experiments"}, {"st": 131, "ed": 133, "text": "cross lingual"}, {"st": 133, "ed": 135, "text": "sentiment analysis"}, {"st": 139, "ed": 141, "text": "source language"}, {"st": 153, "ed": 155, "text": "parallel corpora"}, {"st": 174, "ed": 176, "text": "cross lingual"}]
[{"st": 2, "ed": 4, "text": "natural language"}, {"st": 10, "ed": 12, "text": "challenging task"}, {"st": 18, "ed": 20, "text": "multi step"}, {"st": 28, "ed": 30, "text": "natural language"}, {"st": 54, "ed": 56, "text": "weakly supervised"}, {"st": 56, "ed": 59, "text": "end to end"}, {"st": 59, "ed": 61, "text": "neural network"}, {"st": 68, "ed": 70, "text": "real world"}, {"st": 74, "ed": 76, "text": "objective function"}, {"st": 80, "ed": 82, "text": "neural network"}, {"st": 93, "ed": 95, "text": "natural language"}, {"st": 95, "ed": 97, "text": "question answering"}, {"st": 101, "ed": 105, "text": "trained end to end"}, {"st": 106, "ed": 108, "text": "weak supervision"}, {"st": 109, "ed": 112, "text": "question answer pairs"}, {"st": 116, "ed": 118, "text": "domain specific"}, {"st": 127, "ed": 129, "text": "previous approaches"}, {"st": 187, "ed": 189, "text": "natural language"}]
[{"st": 2, "ed": 5, "text": "text to speech"}, {"st": 37, "ed": 41, "text": "deep recurrent neural network"}, {"st": 84, "ed": 86, "text": "quality assessment"}, {"st": 109, "ed": 111, "text": "parameter space"}]
[{"st": 2, "ed": 4, "text": "word embedding"}, {"st": 62, "ed": 64, "text": "text corpora"}, {"st": 90, "ed": 92, "text": "low dimensional"}, {"st": 96, "ed": 98, "text": "network embedding"}, {"st": 132, "ed": 136, "text": "real world data sets"}, {"st": 141, "ed": 143, "text": "word embeddings"}, {"st": 158, "ed": 160, "text": "tasks including"}, {"st": 160, "ed": 162, "text": "text classification"}]
[{"st": 13, "ed": 15, "text": "conditional probability"}, {"st": 31, "ed": 33, "text": "neural network"}, {"st": 37, "ed": 39, "text": "bi directional"}, {"st": 47, "ed": 49, "text": "character level"}, {"st": 55, "ed": 57, "text": "bidirectional lstm"}, {"st": 63, "ed": 65, "text": "neural network"}, {"st": 74, "ed": 76, "text": "log linear"}, {"st": 96, "ed": 98, "text": "s matrix"}, {"st": 116, "ed": 119, "text": "end to end"}, {"st": 120, "ed": 122, "text": "training procedure"}]
[{"st": 0, "ed": 4, "text": "convolutional neural networks cnns"}, {"st": 19, "ed": 22, "text": "automatic speech recognition"}, {"st": 25, "ed": 27, "text": "speech recognition"}, {"st": 31, "ed": 34, "text": "hidden markov models"}, {"st": 34, "ed": 37, "text": "gaussian mixture models"}, {"st": 55, "ed": 59, "text": "recurrent neural networks rnns"}, {"st": 72, "ed": 75, "text": "end to end"}, {"st": 75, "ed": 77, "text": "speech recognition"}, {"st": 85, "ed": 87, "text": "computationally expensive"}, {"st": 109, "ed": 112, "text": "end to end"}, {"st": 134, "ed": 136, "text": "phoneme recognition"}, {"st": 146, "ed": 148, "text": "computationally efficient"}, {"st": 166, "ed": 168, "text": "temporal correlations"}]
[{"st": 2, "ed": 4, "text": "large scale"}, {"st": 4, "ed": 6, "text": "kernel methods"}, {"st": 10, "ed": 12, "text": "speech recognition"}, {"st": 17, "ed": 20, "text": "deep neural networks"}, {"st": 27, "ed": 29, "text": "speech recognition"}, {"st": 36, "ed": 38, "text": "benchmark tasks"}, {"st": 46, "ed": 48, "text": "frame level"}, {"st": 48, "ed": 50, "text": "performance metrics"}, {"st": 51, "ed": 53, "text": "cross entropy"}, {"st": 61, "ed": 63, "text": "error rate"}, {"st": 68, "ed": 70, "text": "kernel methods"}, {"st": 72, "ed": 74, "text": "large datasets"}, {"st": 77, "ed": 79, "text": "random fourier"}, {"st": 108, "ed": 110, "text": "random features"}, {"st": 153, "ed": 155, "text": "frame level"}, {"st": 161, "ed": 163, "text": "recognition performance"}, {"st": 195, "ed": 197, "text": "recognition performance"}]
[{"st": 7, "ed": 9, "text": "dna sequence"}, {"st": 58, "ed": 60, "text": "machine learning"}, {"st": 78, "ed": 80, "text": "distributed representations"}, {"st": 92, "ed": 94, "text": "word embedding"}, {"st": 108, "ed": 110, "text": "provide evidence"}, {"st": 134, "ed": 136, "text": "cosine similarity"}]
[{"st": 50, "ed": 52, "text": "natural language"}, {"st": 75, "ed": 79, "text": "deep recurrent neural network"}, {"st": 89, "ed": 91, "text": "hong kong"}, {"st": 105, "ed": 107, "text": "deep network"}, {"st": 121, "ed": 123, "text": "outperforms previous"}]
[{"st": 14, "ed": 16, "text": "social media"}, {"st": 64, "ed": 66, "text": "arabic language"}, {"st": 79, "ed": 81, "text": "training data"}]
[{"st": 0, "ed": 2, "text": "topic models"}, {"st": 11, "ed": 13, "text": "latent structure"}, {"st": 30, "ed": 33, "text": "probabilistic topic models"}, {"st": 95, "ed": 97, "text": "topic modeling"}, {"st": 105, "ed": 107, "text": "topic models"}, {"st": 135, "ed": 137, "text": "topic modeling"}, {"st": 155, "ed": 157, "text": "matrix factorization"}, {"st": 158, "ed": 160, "text": "topic modeling"}, {"st": 165, "ed": 167, "text": "ensemble learning"}, {"st": 174, "ed": 176, "text": "text corpora"}, {"st": 191, "ed": 193, "text": "significantly reduce"}]
[{"st": 7, "ed": 10, "text": "semi supervised learning"}, {"st": 22, "ed": 24, "text": "supervised classification"}, {"st": 40, "ed": 42, "text": "domain specific"}, {"st": 60, "ed": 62, "text": "bayesian optimization"}, {"st": 72, "ed": 74, "text": "text classification"}, {"st": 86, "ed": 88, "text": "classification benchmarks"}, {"st": 95, "ed": 97, "text": "relation extraction"}]
[{"st": 12, "ed": 15, "text": "neural machine translation"}, {"st": 42, "ed": 44, "text": "powerful tool"}, {"st": 53, "ed": 55, "text": "sequential data"}, {"st": 79, "ed": 81, "text": "neural networks"}, {"st": 82, "ed": 84, "text": "natural language"}]
[{"st": 9, "ed": 11, "text": "lstm models"}, {"st": 18, "ed": 20, "text": "rnn based"}, {"st": 20, "ed": 22, "text": "generative models"}, {"st": 27, "ed": 30, "text": "bag of words"}, {"st": 46, "ed": 48, "text": "error rates"}, {"st": 58, "ed": 60, "text": "generative models"}, {"st": 63, "ed": 65, "text": "error rate"}, {"st": 81, "ed": 83, "text": "linear classification"}, {"st": 88, "ed": 90, "text": "conditional independence"}, {"st": 98, "ed": 100, "text": "rnn based"}, {"st": 101, "ed": 103, "text": "classification models"}, {"st": 123, "ed": 125, "text": "zero shot"}, {"st": 126, "ed": 128, "text": "continual learning"}, {"st": 132, "ed": 134, "text": "generative models"}]
[{"st": 4, "ed": 6, "text": "encoder decoder"}, {"st": 6, "ed": 10, "text": "deep neural network architecture"}, {"st": 33, "ed": 35, "text": "source language"}, {"st": 42, "ed": 44, "text": "ground truth"}, {"st": 44, "ed": 46, "text": "source language"}, {"st": 66, "ed": 68, "text": "speech recognition"}, {"st": 84, "ed": 86, "text": "attention based"}, {"st": 90, "ed": 94, "text": "trained end to end"}, {"st": 118, "ed": 120, "text": "speech recognition"}, {"st": 121, "ed": 123, "text": "machine translation"}, {"st": 126, "ed": 128, "text": "bleu points"}, {"st": 142, "ed": 144, "text": "training data"}, {"st": 148, "ed": 150, "text": "multi task"}, {"st": 162, "ed": 164, "text": "encoder network"}, {"st": 165, "ed": 167, "text": "improve performance"}]
[{"st": 2, "ed": 4, "text": "word embedding"}, {"st": 12, "ed": 15, "text": "word co occurrence"}, {"st": 18, "ed": 20, "text": "low rank"}, {"st": 32, "ed": 34, "text": "numerical methods"}, {"st": 36, "ed": 38, "text": "higher order"}, {"st": 38, "ed": 41, "text": "word co occurrence"}, {"st": 50, "ed": 52, "text": "word embeddings"}, {"st": 64, "ed": 66, "text": "main contributions"}, {"st": 120, "ed": 122, "text": "doesn t"}, {"st": 130, "ed": 132, "text": "word embedding"}, {"st": 154, "ed": 156, "text": "word embedding"}, {"st": 160, "ed": 162, "text": "tensor based"}, {"st": 162, "ed": 164, "text": "methods outperform"}, {"st": 166, "ed": 168, "text": "based methods"}, {"st": 178, "ed": 180, "text": "word embeddings"}, {"st": 206, "ed": 208, "text": "word embeddings"}, {"st": 212, "ed": 214, "text": "source code"}, {"st": 215, "ed": 217, "text": "pre trained"}]
[]
[{"st": 5, "ed": 7, "text": "generative process"}, {"st": 10, "ed": 12, "text": "latent representation"}, {"st": 16, "ed": 18, "text": "unsupervised fashion"}, {"st": 32, "ed": 34, "text": "generative models"}, {"st": 36, "ed": 38, "text": "variational autoencoders"}, {"st": 58, "ed": 60, "text": "generative process"}, {"st": 65, "ed": 67, "text": "latent space"}]
[{"st": 3, "ed": 6, "text": "neural machine translation"}, {"st": 54, "ed": 57, "text": "attention based neural"}, {"st": 57, "ed": 59, "text": "machine translation"}, {"st": 66, "ed": 69, "text": "compares favorably with"}, {"st": 73, "ed": 75, "text": "specifically designed"}]
[{"st": 1, "ed": 3, "text": "structured prediction"}, {"st": 5, "ed": 7, "text": "stochastic optimization"}, {"st": 53, "ed": 55, "text": "learning problems"}, {"st": 56, "ed": 58, "text": "attention based"}, {"st": 71, "ed": 73, "text": "learning algorithms"}, {"st": 85, "ed": 88, "text": "neural machine translation"}, {"st": 96, "ed": 98, "text": "bleu points"}, {"st": 99, "ed": 101, "text": "domain adaptation"}]
[{"st": 7, "ed": 9, "text": "learning paradigm"}, {"st": 10, "ed": 14, "text": "neural machine translation nmt"}, {"st": 26, "ed": 28, "text": "previous works"}, {"st": 53, "ed": 57, "text": "generative adversarial networks gans"}, {"st": 60, "ed": 62, "text": "adversarial training"}, {"st": 88, "ed": 92, "text": "convolutional neural network cnn"}, {"st": 132, "ed": 134, "text": "policy gradient"}]
[{"st": 59, "ed": 61, "text": "performance improvements"}, {"st": 69, "ed": 72, "text": "publicly available datasets"}, {"st": 99, "ed": 101, "text": "large data"}]
[{"st": 11, "ed": 13, "text": "difficult task"}, {"st": 14, "ed": 16, "text": "machine learning"}, {"st": 30, "ed": 33, "text": "positive or negative"}, {"st": 54, "ed": 56, "text": "representation learning"}, {"st": 70, "ed": 72, "text": "machine learning"}, {"st": 75, "ed": 77, "text": "classifier performance"}, {"st": 80, "ed": 82, "text": "unlabeled data"}, {"st": 84, "ed": 86, "text": "deep convolutional"}, {"st": 86, "ed": 89, "text": "generative adversarial network"}, {"st": 96, "ed": 98, "text": "extensive experiments"}, {"st": 129, "ed": 131, "text": "unlabeled data"}, {"st": 134, "ed": 136, "text": "improves performance"}, {"st": 141, "ed": 143, "text": "fully supervised"}, {"st": 143, "ed": 145, "text": "baseline approaches"}]
[{"st": 3, "ed": 5, "text": "max pooling"}, {"st": 6, "ed": 8, "text": "loss function"}, {"st": 10, "ed": 15, "text": "long short term memory lstm"}, {"st": 19, "ed": 21, "text": "keyword spotting"}, {"st": 30, "ed": 32, "text": "max pooling"}, {"st": 42, "ed": 45, "text": "cross entropy loss"}, {"st": 57, "ed": 59, "text": "keyword spotting"}, {"st": 65, "ed": 67, "text": "lstm models"}, {"st": 69, "ed": 72, "text": "cross entropy loss"}, {"st": 73, "ed": 75, "text": "max pooling"}, {"st": 78, "ed": 81, "text": "cross entropy loss"}, {"st": 83, "ed": 85, "text": "feed forward"}, {"st": 85, "ed": 88, "text": "deep neural network"}, {"st": 92, "ed": 94, "text": "max pooling"}, {"st": 105, "ed": 108, "text": "cross entropy loss"}, {"st": 112, "ed": 114, "text": "max pooling"}, {"st": 120, "ed": 122, "text": "cross entropy"}, {"st": 122, "ed": 124, "text": "pre trained"}, {"st": 137, "ed": 139, "text": "feed forward"}]
[{"st": 42, "ed": 44, "text": "big data"}, {"st": 59, "ed": 61, "text": "large scale"}, {"st": 70, "ed": 72, "text": "random forests"}, {"st": 74, "ed": 76, "text": "n grams"}, {"st": 85, "ed": 89, "text": "recurrent neural network rnn"}, {"st": 101, "ed": 106, "text": "national center for health statistics"}, {"st": 119, "ed": 121, "text": "deep model"}, {"st": 124, "ed": 127, "text": "short term memory"}, {"st": 131, "ed": 133, "text": "n gram"}, {"st": 151, "ed": 153, "text": "ad hoc"}]
[{"st": 6, "ed": 8, "text": "natural language"}, {"st": 33, "ed": 35, "text": "natural language"}, {"st": 80, "ed": 82, "text": "recent advances"}, {"st": 86, "ed": 88, "text": "neural networks"}, {"st": 90, "ed": 92, "text": "deep learning"}, {"st": 102, "ed": 105, "text": "end to end"}, {"st": 108, "ed": 110, "text": "traditional approaches"}, {"st": 122, "ed": 124, "text": "generative model"}, {"st": 135, "ed": 137, "text": "natural language"}, {"st": 138, "ed": 142, "text": "deep recurrent neural networks"}, {"st": 157, "ed": 159, "text": "feature engineering"}, {"st": 162, "ed": 164, "text": "distributed representation"}, {"st": 186, "ed": 188, "text": "natural language"}, {"st": 213, "ed": 215, "text": "wide variety"}, {"st": 239, "ed": 241, "text": "natural language"}]
[{"st": 2, "ed": 4, "text": "maximum likelihood"}, {"st": 6, "ed": 9, "text": "simple and effective"}, {"st": 9, "ed": 11, "text": "learning framework"}, {"st": 12, "ed": 14, "text": "directly optimize"}, {"st": 16, "ed": 18, "text": "reward function"}, {"st": 19, "ed": 21, "text": "structured prediction"}, {"st": 33, "ed": 35, "text": "task specific"}, {"st": 38, "ed": 40, "text": "maximum likelihood"}, {"st": 78, "ed": 80, "text": "theoretical properties"}, {"st": 134, "ed": 136, "text": "decision boundary"}, {"st": 139, "ed": 141, "text": "decision rule"}, {"st": 176, "ed": 178, "text": "synthetic data"}, {"st": 179, "ed": 181, "text": "multi class"}, {"st": 185, "ed": 187, "text": "real data"}, {"st": 188, "ed": 190, "text": "image captioning"}, {"st": 212, "ed": 214, "text": "structured prediction"}, {"st": 220, "ed": 223, "text": "named entity recognition"}, {"st": 229, "ed": 231, "text": "machine translation"}, {"st": 236, "ed": 238, "text": "maximum likelihood"}]
[{"st": 1, "ed": 3, "text": "neural networks"}, {"st": 8, "ed": 10, "text": "attention mechanism"}, {"st": 70, "ed": 72, "text": "recently proposed"}, {"st": 92, "ed": 94, "text": "attention mechanisms"}, {"st": 106, "ed": 108, "text": "efficient algorithms"}, {"st": 117, "ed": 119, "text": "attention mechanisms"}, {"st": 124, "ed": 126, "text": "neural network"}, {"st": 144, "ed": 146, "text": "attention mechanisms"}, {"st": 148, "ed": 150, "text": "large scale"}, {"st": 151, "ed": 153, "text": "textual entailment"}, {"st": 153, "ed": 155, "text": "machine translation"}, {"st": 159, "ed": 161, "text": "attention mechanisms"}, {"st": 163, "ed": 165, "text": "without sacrificing"}, {"st": 168, "ed": 170, "text": "textual entailment"}, {"st": 176, "ed": 178, "text": "attention mechanisms"}]
[{"st": 1, "ed": 3, "text": "neural network"}, {"st": 44, "ed": 47, "text": "static and dynamic"}, {"st": 121, "ed": 123, "text": "computationally efficient"}]
[{"st": 0, "ed": 4, "text": "recurrent neural networks rnns"}, {"st": 13, "ed": 15, "text": "sequential data"}, {"st": 27, "ed": 29, "text": "hidden layers"}, {"st": 61, "ed": 63, "text": "hidden layers"}, {"st": 87, "ed": 89, "text": "hidden layer"}, {"st": 104, "ed": 106, "text": "rnn architecture"}, {"st": 152, "ed": 154, "text": "hidden layers"}, {"st": 167, "ed": 172, "text": "long short term memory lstm"}, {"st": 174, "ed": 178, "text": "gated recurrent unit gru"}, {"st": 197, "ed": 200, "text": "short term memory"}, {"st": 200, "ed": 202, "text": "recurrent neural"}, {"st": 206, "ed": 209, "text": "gated recurrent unit"}, {"st": 209, "ed": 211, "text": "recurrent neural"}, {"st": 222, "ed": 224, "text": "rnn models"}, {"st": 229, "ed": 231, "text": "conducted experiments"}, {"st": 236, "ed": 238, "text": "word level"}, {"st": 239, "ed": 241, "text": "character level"}, {"st": 250, "ed": 252, "text": "significantly improved"}]
[{"st": 6, "ed": 9, "text": "ability to learn"}, {"st": 10, "ed": 12, "text": "word embeddings"}, {"st": 26, "ed": 29, "text": "neural language models"}, {"st": 80, "ed": 83, "text": "bag of words"}, {"st": 89, "ed": 91, "text": "training objective"}, {"st": 159, "ed": 161, "text": "word embeddings"}, {"st": 167, "ed": 170, "text": "named entity recognition"}]
[{"st": 1, "ed": 5, "text": "generative adversarial network gan"}, {"st": 6, "ed": 9, "text": "achieved great success"}, {"st": 12, "ed": 14, "text": "real valued"}, {"st": 23, "ed": 25, "text": "discrete data"}, {"st": 47, "ed": 50, "text": "short term memory"}, {"st": 55, "ed": 57, "text": "convolutional network"}, {"st": 73, "ed": 75, "text": "latent feature"}, {"st": 77, "ed": 80, "text": "real and synthetic"}, {"st": 88, "ed": 90, "text": "adversarial training"}, {"st": 102, "ed": 104, "text": "quantitative evaluation"}, {"st": 110, "ed": 112, "text": "generate realistic"}]
[{"st": 0, "ed": 2, "text": "topic models"}, {"st": 20, "ed": 22, "text": "topic models"}, {"st": 42, "ed": 44, "text": "topic model"}, {"st": 61, "ed": 63, "text": "topic models"}, {"st": 80, "ed": 82, "text": "semi supervised"}, {"st": 82, "ed": 84, "text": "method called"}, {"st": 86, "ed": 90, "text": "non negative matrix factorization"}, {"st": 140, "ed": 143, "text": "non convex optimization"}, {"st": 149, "ed": 151, "text": "iterative algorithm"}, {"st": 202, "ed": 204, "text": "latent structure"}, {"st": 214, "ed": 216, "text": "proposed approach"}, {"st": 221, "ed": 223, "text": "achieves higher"}, {"st": 234, "ed": 237, "text": "latent dirichlet allocation"}]
[{"st": 0, "ed": 2, "text": "generative models"}, {"st": 38, "ed": 40, "text": "deep learning"}, {"st": 46, "ed": 48, "text": "discriminative models"}, {"st": 77, "ed": 81, "text": "trained end to end"}, {"st": 110, "ed": 112, "text": "input sequence"}, {"st": 195, "ed": 197, "text": "generative models"}, {"st": 213, "ed": 215, "text": "main contributions"}]
[{"st": 5, "ed": 7, "text": "artificially intelligent"}, {"st": 81, "ed": 83, "text": "challenging problem"}, {"st": 84, "ed": 86, "text": "artificial intelligence"}, {"st": 120, "ed": 122, "text": "unsupervised learning"}, {"st": 126, "ed": 128, "text": "prior knowledge"}, {"st": 129, "ed": 131, "text": "agent learns"}, {"st": 181, "ed": 183, "text": "agent learns"}, {"st": 209, "ed": 211, "text": "natural language"}]
[{"st": 87, "ed": 89, "text": "qualitative analysis"}, {"st": 107, "ed": 109, "text": "empirically demonstrate"}]
[{"st": 1, "ed": 3, "text": "topic modeling"}, {"st": 15, "ed": 18, "text": "high computational cost"}, {"st": 48, "ed": 50, "text": "efficient inference"}, {"st": 52, "ed": 55, "text": "low dimensional embedding"}, {"st": 72, "ed": 74, "text": "variational inference"}, {"st": 84, "ed": 86, "text": "extensive experiments"}, {"st": 101, "ed": 104, "text": "orders of magnitude"}, {"st": 109, "ed": 111, "text": "without sacrificing"}, {"st": 120, "ed": 122, "text": "document classification"}]
[{"st": 2, "ed": 4, "text": "random field"}, {"st": 86, "ed": 88, "text": "neural networks"}, {"st": 106, "ed": 108, "text": "word embedding"}, {"st": 109, "ed": 111, "text": "feature learning"}, {"st": 129, "ed": 131, "text": "efficient inference"}, {"st": 144, "ed": 149, "text": "deep convolutional neural networks cnns"}, {"st": 186, "ed": 188, "text": "speech recognition"}]
[{"st": 6, "ed": 9, "text": "statistical machine translation"}, {"st": 50, "ed": 52, "text": "most probable"}, {"st": 61, "ed": 63, "text": "output space"}, {"st": 117, "ed": 119, "text": "bleu points"}]
[{"st": 0, "ed": 3, "text": "recurrent neural networks"}, {"st": 11, "ed": 13, "text": "text analysis"}, {"st": 25, "ed": 27, "text": "recently proposed"}, {"st": 28, "ed": 30, "text": "variational dropout"}, {"st": 38, "ed": 42, "text": "feed forward neural network"}, {"st": 66, "ed": 68, "text": "variational dropout"}, {"st": 76, "ed": 78, "text": "sentiment analysis"}]
[{"st": 9, "ed": 11, "text": "structured data"}, {"st": 28, "ed": 30, "text": "pre trained"}, {"st": 37, "ed": 39, "text": "knowledge graph"}, {"st": 39, "ed": 41, "text": "typically assume"}, {"st": 61, "ed": 64, "text": "entities and relations"}, {"st": 92, "ed": 94, "text": "explicitly modeling"}, {"st": 105, "ed": 107, "text": "penalty functions"}, {"st": 110, "ed": 112, "text": "knowledge graph"}, {"st": 125, "ed": 127, "text": "significantly outperforms"}]
[{"st": 4, "ed": 6, "text": "topic model"}, {"st": 82, "ed": 85, "text": "bag of words"}, {"st": 90, "ed": 93, "text": "variational auto encoder"}, {"st": 117, "ed": 119, "text": "feed forward"}, {"st": 132, "ed": 134, "text": "preliminary experiments"}]
[{"st": 4, "ed": 8, "text": "markov chain monte carlo"}, {"st": 21, "ed": 23, "text": "directly applied"}, {"st": 25, "ed": 27, "text": "topic models"}, {"st": 99, "ed": 101, "text": "experiments confirm"}, {"st": 106, "ed": 108, "text": "prediction performance"}, {"st": 111, "ed": 113, "text": "parallel algorithm"}]
[{"st": 0, "ed": 2, "text": "word embeddings"}, {"st": 5, "ed": 7, "text": "individual words"}, {"st": 23, "ed": 25, "text": "natural language"}, {"st": 37, "ed": 39, "text": "word embeddings"}, {"st": 43, "ed": 45, "text": "large corpora"}, {"st": 46, "ed": 48, "text": "text documents"}, {"st": 58, "ed": 60, "text": "word embeddings"}, {"st": 61, "ed": 63, "text": "sentiment analysis"}, {"st": 66, "ed": 68, "text": "sentiment analysis"}, {"st": 79, "ed": 81, "text": "vector representations"}, {"st": 100, "ed": 102, "text": "cost function"}, {"st": 106, "ed": 108, "text": "word embeddings"}, {"st": 128, "ed": 130, "text": "word embeddings"}, {"st": 138, "ed": 142, "text": "real world data sets"}, {"st": 156, "ed": 158, "text": "word embeddings"}, {"st": 159, "ed": 161, "text": "sentiment analysis"}]
[{"st": 1, "ed": 3, "text": "latent representations"}, {"st": 14, "ed": 16, "text": "natural language"}, {"st": 18, "ed": 22, "text": "recurrent neural networks rnns"}, {"st": 36, "ed": 38, "text": "rnn based"}, {"st": 72, "ed": 74, "text": "proposed method"}, {"st": 76, "ed": 79, "text": "easy to implement"}, {"st": 85, "ed": 87, "text": "building block"}, {"st": 107, "ed": 109, "text": "quantitative evaluation"}, {"st": 110, "ed": 112, "text": "semi supervised"}, {"st": 112, "ed": 114, "text": "text classification"}]
[{"st": 0, "ed": 2, "text": "unseen data"}, {"st": 6, "ed": 8, "text": "performance degradation"}, {"st": 12, "ed": 15, "text": "supervised machine learning"}, {"st": 25, "ed": 27, "text": "machine learning"}, {"st": 32, "ed": 34, "text": "supervised manner"}, {"st": 46, "ed": 48, "text": "unseen data"}, {"st": 75, "ed": 77, "text": "unseen data"}, {"st": 91, "ed": 94, "text": "training and testing"}, {"st": 119, "ed": 121, "text": "unseen data"}, {"st": 142, "ed": 144, "text": "fully connected"}, {"st": 193, "ed": 195, "text": "hidden layer"}, {"st": 224, "ed": 226, "text": "performance gains"}, {"st": 227, "ed": 229, "text": "speech recognition"}, {"st": 232, "ed": 234, "text": "standard benchmark"}, {"st": 234, "ed": 236, "text": "speech recognition"}, {"st": 240, "ed": 242, "text": "proposed approach"}, {"st": 245, "ed": 247, "text": "performance degradation"}, {"st": 249, "ed": 251, "text": "unseen data"}]
[{"st": 1, "ed": 3, "text": "latent variable"}, {"st": 10, "ed": 12, "text": "sentence representations"}, {"st": 15, "ed": 18, "text": "generative and discriminative"}, {"st": 25, "ed": 27, "text": "latent variable"}, {"st": 55, "ed": 57, "text": "unsupervised manner"}, {"st": 60, "ed": 62, "text": "predictive performance"}, {"st": 68, "ed": 71, "text": "short term memory"}, {"st": 91, "ed": 93, "text": "significantly outperforms"}, {"st": 101, "ed": 103, "text": "semi supervised"}]
[{"st": 0, "ed": 4, "text": "convolutional neural networks cnns"}, {"st": 10, "ed": 12, "text": "building block"}, {"st": 13, "ed": 15, "text": "natural language"}, {"st": 61, "ed": 63, "text": "natural language"}, {"st": 81, "ed": 83, "text": "question answer"}, {"st": 83, "ed": 85, "text": "sentence pairs"}, {"st": 89, "ed": 91, "text": "question answering"}]
[{"st": 0, "ed": 2, "text": "word embeddings"}, {"st": 10, "ed": 12, "text": "exponential family"}, {"st": 25, "ed": 27, "text": "exponential family"}, {"st": 111, "ed": 113, "text": "empirical studies"}]
[{"st": 3, "ed": 5, "text": "web service"}, {"st": 45, "ed": 47, "text": "web service"}]
[{"st": 10, "ed": 12, "text": "machine learning"}, {"st": 13, "ed": 15, "text": "natural language"}, {"st": 120, "ed": 122, "text": "individual words"}, {"st": 137, "ed": 139, "text": "sentence level"}]
[{"st": 1, "ed": 4, "text": "propose and evaluate"}, {"st": 12, "ed": 14, "text": "matrix multiplications"}, {"st": 18, "ed": 20, "text": "fully connected"}, {"st": 21, "ed": 23, "text": "recurrent layers"}, {"st": 24, "ed": 26, "text": "neural networks"}, {"st": 28, "ed": 30, "text": "large vocabulary"}, {"st": 30, "ed": 33, "text": "continuous speech recognition"}, {"st": 42, "ed": 44, "text": "trace norm"}, {"st": 44, "ed": 46, "text": "regularization technique"}, {"st": 48, "ed": 50, "text": "low rank"}, {"st": 58, "ed": 60, "text": "low rank"}, {"st": 104, "ed": 106, "text": "batch sizes"}, {"st": 111, "ed": 113, "text": "speed ups"}, {"st": 134, "ed": 136, "text": "neural networks"}, {"st": 138, "ed": 140, "text": "fully connected"}]
[{"st": 13, "ed": 15, "text": "skip gram"}, {"st": 26, "ed": 28, "text": "negative samples"}, {"st": 48, "ed": 50, "text": "skip gram"}, {"st": 63, "ed": 66, "text": "stochastic gradient descent"}, {"st": 76, "ed": 78, "text": "negative samples"}, {"st": 80, "ed": 82, "text": "inner product"}, {"st": 99, "ed": 101, "text": "convergence rate"}, {"st": 109, "ed": 111, "text": "sampling algorithm"}, {"st": 115, "ed": 117, "text": "negative samples"}, {"st": 128, "ed": 130, "text": "multi dimensional"}, {"st": 161, "ed": 163, "text": "fine grained"}, {"st": 165, "ed": 167, "text": "significant improvement"}]
[{"st": 1, "ed": 3, "text": "deep learning"}, {"st": 81, "ed": 83, "text": "recurrent networks"}, {"st": 87, "ed": 89, "text": "prior knowledge"}, {"st": 104, "ed": 106, "text": "natural language"}]
[{"st": 7, "ed": 9, "text": "loss function"}, {"st": 11, "ed": 14, "text": "end to end"}, {"st": 31, "ed": 34, "text": "end to end"}, {"st": 41, "ed": 43, "text": "loss function"}, {"st": 85, "ed": 87, "text": "loss function"}, {"st": 117, "ed": 119, "text": "domain adaptation"}]
[{"st": 0, "ed": 2, "text": "speech recognition"}, {"st": 6, "ed": 8, "text": "recognition performance"}, {"st": 33, "ed": 35, "text": "speech data"}, {"st": 67, "ed": 69, "text": "speech data"}, {"st": 86, "ed": 89, "text": "ability to learn"}, {"st": 106, "ed": 108, "text": "reinforcement learning"}, {"st": 110, "ed": 112, "text": "speech recognition"}, {"st": 116, "ed": 118, "text": "policy gradient"}, {"st": 133, "ed": 135, "text": "reinforcement learning"}, {"st": 137, "ed": 139, "text": "proposed framework"}, {"st": 156, "ed": 158, "text": "proposed method"}, {"st": 160, "ed": 162, "text": "recognition performance"}]
[{"st": 11, "ed": 13, "text": "natural language"}, {"st": 27, "ed": 29, "text": "word embeddings"}, {"st": 29, "ed": 31, "text": "paragraph vectors"}, {"st": 36, "ed": 38, "text": "fixed length"}, {"st": 61, "ed": 63, "text": "neural network"}, {"st": 64, "ed": 66, "text": "paragraph vectors"}, {"st": 70, "ed": 72, "text": "generative model"}, {"st": 73, "ed": 75, "text": "maximum likelihood"}, {"st": 96, "ed": 98, "text": "bayesian posterior"}, {"st": 105, "ed": 107, "text": "paragraph vectors"}, {"st": 119, "ed": 121, "text": "improves performance"}, {"st": 122, "ed": 124, "text": "supervised learning"}, {"st": 127, "ed": 129, "text": "sentiment analysis"}]
[{"st": 10, "ed": 12, "text": "existing approaches"}, {"st": 18, "ed": 21, "text": "recurrent neural networks"}, {"st": 35, "ed": 39, "text": "convolutional neural network cnn"}, {"st": 72, "ed": 74, "text": "receptive field"}, {"st": 88, "ed": 90, "text": "unsupervised learning"}, {"st": 100, "ed": 102, "text": "empirical results"}, {"st": 109, "ed": 111, "text": "approach produces"}]
[{"st": 8, "ed": 10, "text": "weak learners"}, {"st": 17, "ed": 19, "text": "meta learning"}, {"st": 28, "ed": 30, "text": "machine learning"}, {"st": 37, "ed": 39, "text": "neural network"}, {"st": 43, "ed": 45, "text": "loss function"}, {"st": 53, "ed": 55, "text": "loss function"}, {"st": 59, "ed": 61, "text": "training phase"}, {"st": 65, "ed": 67, "text": "optimization method"}, {"st": 77, "ed": 79, "text": "text classification"}]
[{"st": 0, "ed": 3, "text": "recurrent neural network"}, {"st": 9, "ed": 12, "text": "short term memory"}, {"st": 23, "ed": 25, "text": "outperform traditional"}, {"st": 25, "ed": 27, "text": "n gram"}, {"st": 29, "ed": 31, "text": "speech recognition"}, {"st": 40, "ed": 42, "text": "n gram"}, {"st": 53, "ed": 55, "text": "recent research"}, {"st": 77, "ed": 79, "text": "speech recognition"}, {"st": 96, "ed": 98, "text": "speech recognition"}, {"st": 105, "ed": 109, "text": "word error rate wer"}, {"st": 121, "ed": 123, "text": "n gram"}]
[{"st": 8, "ed": 10, "text": "web based"}, {"st": 10, "ed": 12, "text": "machine translation"}, {"st": 66, "ed": 68, "text": "recently proposed"}]
[{"st": 7, "ed": 9, "text": "embedding model"}, {"st": 12, "ed": 14, "text": "latent topic"}, {"st": 20, "ed": 22, "text": "distance function"}, {"st": 32, "ed": 34, "text": "variational autoencoder"}, {"st": 48, "ed": 50, "text": "distance function"}, {"st": 57, "ed": 59, "text": "mahalanobis distance"}, {"st": 109, "ed": 111, "text": "gaussian distribution"}, {"st": 116, "ed": 118, "text": "topic model"}, {"st": 120, "ed": 122, "text": "word embedding"}, {"st": 172, "ed": 174, "text": "embedding vectors"}, {"st": 187, "ed": 189, "text": "word vectors"}]
[{"st": 0, "ed": 2, "text": "deep learning"}, {"st": 4, "ed": 6, "text": "recently achieved"}, {"st": 7, "ed": 9, "text": "empirical success"}, {"st": 10, "ed": 12, "text": "machine translation"}, {"st": 13, "ed": 15, "text": "response generation"}, {"st": 18, "ed": 20, "text": "text generation"}, {"st": 31, "ed": 34, "text": "end to end"}, {"st": 34, "ed": 36, "text": "neural network"}, {"st": 64, "ed": 66, "text": "significantly fewer"}, {"st": 80, "ed": 82, "text": "text generation"}, {"st": 126, "ed": 128, "text": "text generation"}, {"st": 135, "ed": 137, "text": "real world"}]
[{"st": 1, "ed": 3, "text": "distributed representations"}, {"st": 14, "ed": 16, "text": "natural language"}, {"st": 20, "ed": 22, "text": "successfully applied"}, {"st": 26, "ed": 28, "text": "recommender systems"}, {"st": 36, "ed": 38, "text": "content based"}, {"st": 42, "ed": 44, "text": "learned representations"}, {"st": 46, "ed": 48, "text": "generative model"}, {"st": 52, "ed": 54, "text": "method works"}, {"st": 75, "ed": 77, "text": "generative model"}, {"st": 97, "ed": 100, "text": "gaussian mixture model"}, {"st": 116, "ed": 119, "text": "latent semantic analysis"}, {"st": 134, "ed": 136, "text": "standard benchmark"}, {"st": 150, "ed": 152, "text": "learned representations"}]
[{"st": 6, "ed": 8, "text": "open source"}, {"st": 13, "ed": 17, "text": "neural machine translation nmt"}, {"st": 48, "ed": 51, "text": "training and inference"}, {"st": 56, "ed": 58, "text": "encoder decoder"}, {"st": 60, "ed": 63, "text": "recurrent neural networks"}, {"st": 67, "ed": 69, "text": "fully convolutional"}, {"st": 80, "ed": 82, "text": "regularization techniques"}, {"st": 128, "ed": 130, "text": "machine translation"}, {"st": 144, "ed": 146, "text": "architectures including"}, {"st": 175, "ed": 177, "text": "free software"}]
[{"st": 4, "ed": 6, "text": "neural architectures"}, {"st": 7, "ed": 9, "text": "expert knowledge"}, {"st": 23, "ed": 27, "text": "recurrent neural network rnn"}, {"st": 30, "ed": 32, "text": "existing methods"}, {"st": 42, "ed": 45, "text": "domain specific language"}, {"st": 74, "ed": 77, "text": "gated recurrent unit"}, {"st": 79, "ed": 82, "text": "short term memory"}, {"st": 104, "ed": 106, "text": "random search"}, {"st": 111, "ed": 113, "text": "reinforcement learning"}, {"st": 127, "ed": 129, "text": "machine translation"}, {"st": 150, "ed": 152, "text": "rnn architectures"}]
[{"st": 0, "ed": 3, "text": "recurrent neural networks"}, {"st": 17, "ed": 19, "text": "image processing"}, {"st": 40, "ed": 42, "text": "deep learning"}, {"st": 53, "ed": 56, "text": "recurrent neural networks"}, {"st": 73, "ed": 75, "text": "existing literature"}, {"st": 97, "ed": 99, "text": "character level"}, {"st": 99, "ed": 102, "text": "recurrent neural networks"}, {"st": 125, "ed": 127, "text": "neural network"}, {"st": 129, "ed": 131, "text": "parameter settings"}, {"st": 144, "ed": 146, "text": "sampling scheme"}, {"st": 183, "ed": 185, "text": "hidden states"}]
[{"st": 0, "ed": 2, "text": "natural language"}, {"st": 24, "ed": 26, "text": "powerful tool"}, {"st": 33, "ed": 35, "text": "word level"}, {"st": 87, "ed": 90, "text": "k nearest neighbor"}, {"st": 109, "ed": 111, "text": "real world"}, {"st": 112, "ed": 114, "text": "inference problem"}]
[{"st": 4, "ed": 6, "text": "machine learning"}, {"st": 15, "ed": 17, "text": "naive bayes"}, {"st": 20, "ed": 23, "text": "recurrent neural networks"}, {"st": 26, "ed": 28, "text": "text classification"}, {"st": 45, "ed": 47, "text": "multi label"}, {"st": 64, "ed": 66, "text": "gated recurrent"}, {"st": 68, "ed": 70, "text": "neural networks"}, {"st": 78, "ed": 80, "text": "approach achieves"}]
[{"st": 0, "ed": 2, "text": "topic modeling"}, {"st": 5, "ed": 7, "text": "compact representation"}, {"st": 29, "ed": 31, "text": "topic modeling"}, {"st": 61, "ed": 63, "text": "topic model"}, {"st": 65, "ed": 68, "text": "probabilistic graphical model"}, {"st": 107, "ed": 109, "text": "variational inference"}, {"st": 126, "ed": 128, "text": "significant improvements"}, {"st": 130, "ed": 132, "text": "topic models"}, {"st": 142, "ed": 145, "text": "ability to capture"}]
[{"st": 12, "ed": 15, "text": "ability to learn"}, {"st": 81, "ed": 83, "text": "final prediction"}, {"st": 90, "ed": 92, "text": "sentiment analysis"}, {"st": 108, "ed": 111, "text": "words and phrases"}, {"st": 129, "ed": 131, "text": "level labels"}, {"st": 143, "ed": 146, "text": "positive and negative"}]
[{"st": 0, "ed": 2, "text": "transfer learning"}, {"st": 4, "ed": 6, "text": "computer vision"}, {"st": 7, "ed": 9, "text": "existing approaches"}, {"st": 13, "ed": 15, "text": "task specific"}, {"st": 22, "ed": 24, "text": "fine tuned"}, {"st": 29, "ed": 31, "text": "transfer learning"}, {"st": 48, "ed": 50, "text": "fine tuning"}, {"st": 54, "ed": 56, "text": "art language"}, {"st": 59, "ed": 61, "text": "significantly outperforms"}, {"st": 68, "ed": 70, "text": "text classification"}, {"st": 83, "ed": 85, "text": "open source"}]
[{"st": 2, "ed": 5, "text": "convolutional neural network"}, {"st": 10, "ed": 12, "text": "deep learning"}, {"st": 14, "ed": 16, "text": "sentence classification"}, {"st": 19, "ed": 21, "text": "sentiment analysis"}, {"st": 25, "ed": 27, "text": "neural networks"}, {"st": 51, "ed": 53, "text": "image classification"}]
[{"st": 5, "ed": 8, "text": "support vector machine"}, {"st": 8, "ed": 10, "text": "active learning"}, {"st": 14, "ed": 16, "text": "imbalanced datasets"}, {"st": 72, "ed": 74, "text": "text classification"}, {"st": 83, "ed": 85, "text": "consistently outperform"}]
[{"st": 0, "ed": 2, "text": "deep learning"}, {"st": 7, "ed": 9, "text": "machine learning"}, {"st": 12, "ed": 14, "text": "multiple layers"}, {"st": 34, "ed": 36, "text": "deep learning"}, {"st": 39, "ed": 41, "text": "application domains"}, {"st": 41, "ed": 43, "text": "deep learning"}, {"st": 48, "ed": 50, "text": "sentiment analysis"}, {"st": 60, "ed": 62, "text": "deep learning"}]
[{"st": 2, "ed": 4, "text": "active learning"}, {"st": 5, "ed": 7, "text": "batch sizes"}, {"st": 30, "ed": 32, "text": "batch sizes"}, {"st": 41, "ed": 43, "text": "batch sizes"}, {"st": 48, "ed": 50, "text": "learning curve"}, {"st": 54, "ed": 56, "text": "open question"}, {"st": 57, "ed": 59, "text": "batch size"}, {"st": 69, "ed": 71, "text": "batch sizes"}, {"st": 129, "ed": 131, "text": "batch sizes"}]
[{"st": 16, "ed": 19, "text": "convolutional neural networks"}]
[{"st": 1, "ed": 3, "text": "variational inference"}, {"st": 5, "ed": 7, "text": "instance specific"}, {"st": 21, "ed": 24, "text": "deep generative models"}, {"st": 26, "ed": 28, "text": "variational autoencoders"}, {"st": 52, "ed": 54, "text": "variational parameters"}, {"st": 56, "ed": 59, "text": "stochastic variational inference"}, {"st": 73, "ed": 75, "text": "inference network"}, {"st": 76, "ed": 78, "text": "generative model"}, {"st": 80, "ed": 84, "text": "trained end to end"}, {"st": 85, "ed": 87, "text": "gradient based"}, {"st": 97, "ed": 99, "text": "generative models"}, {"st": 117, "ed": 119, "text": "approach outperforms"}, {"st": 126, "ed": 129, "text": "text and image"}]
[{"st": 0, "ed": 2, "text": "structured prediction"}, {"st": 44, "ed": 46, "text": "map inference"}, {"st": 112, "ed": 115, "text": "deep neural networks"}, {"st": 127, "ed": 129, "text": "natural language"}, {"st": 131, "ed": 133, "text": "competitive accuracy"}, {"st": 137, "ed": 140, "text": "ability to capture"}, {"st": 140, "ed": 142, "text": "natural language"}]
[{"st": 1, "ed": 3, "text": "text classification"}, {"st": 15, "ed": 17, "text": "training data"}, {"st": 32, "ed": 34, "text": "annotated data"}, {"st": 49, "ed": 51, "text": "text classification"}, {"st": 54, "ed": 56, "text": "real world"}, {"st": 61, "ed": 63, "text": "provide theoretical"}, {"st": 94, "ed": 96, "text": "theoretically sound"}, {"st": 99, "ed": 101, "text": "adversarial networks"}, {"st": 119, "ed": 121, "text": "multiple domains"}, {"st": 144, "ed": 146, "text": "significantly outperform"}, {"st": 147, "ed": 149, "text": "prior art"}]
[{"st": 3, "ed": 5, "text": "text documents"}, {"st": 34, "ed": 36, "text": "error prone"}, {"st": 62, "ed": 64, "text": "convolutional network"}, {"st": 80, "ed": 83, "text": "convolutional neural network"}, {"st": 86, "ed": 88, "text": "attention mechanism"}, {"st": 141, "ed": 143, "text": "attention mechanism"}]
[{"st": 0, "ed": 3, "text": "mild cognitive impairment"}, {"st": 58, "ed": 60, "text": "supervised learning"}, {"st": 66, "ed": 68, "text": "clinical trial"}, {"st": 104, "ed": 106, "text": "reinforcement learning"}, {"st": 158, "ed": 160, "text": "reinforcement learning"}, {"st": 185, "ed": 187, "text": "significantly outperform"}, {"st": 191, "ed": 193, "text": "supervised learning"}]
[{"st": 14, "ed": 17, "text": "hidden markov model"}, {"st": 22, "ed": 24, "text": "expectation maximization"}, {"st": 48, "ed": 50, "text": "sample size"}, {"st": 54, "ed": 56, "text": "accurately estimate"}, {"st": 62, "ed": 64, "text": "higher order"}, {"st": 81, "ed": 83, "text": "co occurrence"}, {"st": 122, "ed": 124, "text": "higher quality"}, {"st": 145, "ed": 148, "text": "bag of words"}]
[{"st": 23, "ed": 25, "text": "latent variable"}, {"st": 27, "ed": 29, "text": "denoising autoencoders"}, {"st": 39, "ed": 41, "text": "extensively evaluate"}, {"st": 45, "ed": 47, "text": "machine translation"}, {"st": 53, "ed": 55, "text": "image caption"}]
[{"st": 15, "ed": 17, "text": "unstructured text"}, {"st": 39, "ed": 42, "text": "word sense disambiguation"}, {"st": 50, "ed": 52, "text": "natural language"}, {"st": 102, "ed": 105, "text": "short term memory"}]
[{"st": 2, "ed": 4, "text": "machine learning"}, {"st": 5, "ed": 7, "text": "low dimensional"}, {"st": 45, "ed": 47, "text": "inner product"}, {"st": 56, "ed": 58, "text": "final output"}, {"st": 84, "ed": 86, "text": "likelihood ratio"}, {"st": 118, "ed": 120, "text": "heavy tailed"}, {"st": 140, "ed": 142, "text": "proposed method"}]
[{"st": 0, "ed": 2, "text": "structured prediction"}, {"st": 9, "ed": 11, "text": "neural network"}, {"st": 35, "ed": 37, "text": "structured output"}, {"st": 41, "ed": 43, "text": "continuous variables"}, {"st": 61, "ed": 63, "text": "neural network"}, {"st": 70, "ed": 72, "text": "inference network"}, {"st": 84, "ed": 86, "text": "large margin"}, {"st": 89, "ed": 91, "text": "joint training"}, {"st": 100, "ed": 103, "text": "multi label classification"}, {"st": 105, "ed": 107, "text": "speed ups"}, {"st": 113, "ed": 115, "text": "et al"}, {"st": 129, "ed": 131, "text": "performs comparably"}, {"st": 132, "ed": 134, "text": "exact inference"}, {"st": 144, "ed": 146, "text": "improved accuracy"}, {"st": 183, "ed": 185, "text": "dynamic programming"}, {"st": 190, "ed": 193, "text": "conditional random fields"}, {"st": 199, "ed": 201, "text": "fast inference"}]
[{"st": 5, "ed": 7, "text": "deep learning"}, {"st": 13, "ed": 16, "text": "neural machine translation"}, {"st": 19, "ed": 21, "text": "reference implementation"}]
[{"st": 75, "ed": 77, "text": "clinical trials"}, {"st": 94, "ed": 96, "text": "word embeddings"}, {"st": 120, "ed": 123, "text": "deep neural networks"}, {"st": 124, "ed": 126, "text": "pre trained"}, {"st": 126, "ed": 128, "text": "word embedding"}, {"st": 151, "ed": 153, "text": "f measure"}, {"st": 163, "ed": 165, "text": "validation set"}, {"st": 170, "ed": 172, "text": "trained model"}, {"st": 187, "ed": 189, "text": "clinical practice"}, {"st": 201, "ed": 203, "text": "word embedding"}, {"st": 233, "ed": 235, "text": "representation learning"}, {"st": 236, "ed": 238, "text": "neural networks"}, {"st": 249, "ed": 251, "text": "clinical trial"}]
[{"st": 0, "ed": 2, "text": "keyword spotting"}, {"st": 11, "ed": 13, "text": "user interactions"}, {"st": 29, "ed": 31, "text": "neural networks"}, {"st": 47, "ed": 49, "text": "speech processing"}, {"st": 70, "ed": 72, "text": "limited memory"}, {"st": 78, "ed": 80, "text": "neural network"}, {"st": 92, "ed": 94, "text": "neural network"}, {"st": 102, "ed": 104, "text": "resource constrained"}, {"st": 108, "ed": 110, "text": "neural network"}, {"st": 112, "ed": 114, "text": "keyword spotting"}, {"st": 134, "ed": 136, "text": "neural network"}, {"st": 147, "ed": 149, "text": "without sacrificing"}, {"st": 156, "ed": 159, "text": "convolutional neural network"}, {"st": 166, "ed": 168, "text": "neural network"}]
[{"st": 13, "ed": 15, "text": "multi modal"}, {"st": 19, "ed": 21, "text": "natural language"}, {"st": 22, "ed": 24, "text": "unlike previous"}, {"st": 34, "ed": 36, "text": "embedding space"}, {"st": 90, "ed": 92, "text": "extensive experimental"}, {"st": 92, "ed": 94, "text": "evaluation shows"}, {"st": 113, "ed": 115, "text": "significantly improves"}]
[{"st": 12, "ed": 14, "text": "answer questions"}, {"st": 39, "ed": 42, "text": "short term memory"}, {"st": 49, "ed": 53, "text": "convolutional neural network cnn"}, {"st": 56, "ed": 58, "text": "visual representation"}, {"st": 91, "ed": 93, "text": "question answering"}, {"st": 114, "ed": 117, "text": "question answer pairs"}]
[{"st": 50, "ed": 52, "text": "common sense"}, {"st": 69, "ed": 71, "text": "approach produces"}, {"st": 140, "ed": 143, "text": "vision and language"}, {"st": 143, "ed": 146, "text": "approach significantly outperforms"}]
[{"st": 5, "ed": 7, "text": "machine translation"}, {"st": 9, "ed": 11, "text": "encoder decoder"}, {"st": 27, "ed": 30, "text": "convolutional and recurrent"}, {"st": 30, "ed": 32, "text": "neural networks"}, {"st": 70, "ed": 72, "text": "image representations"}, {"st": 79, "ed": 82, "text": "convolutional neural networks"}, {"st": 86, "ed": 89, "text": "recurrent neural networks"}]
[{"st": 2, "ed": 4, "text": "artificial intelligence"}, {"st": 9, "ed": 11, "text": "answer questions"}, {"st": 12, "ed": 14, "text": "visual data"}, {"st": 28, "ed": 31, "text": "visual question answering"}, {"st": 43, "ed": 45, "text": "answer questions"}, {"st": 50, "ed": 52, "text": "multiple sources"}, {"st": 71, "ed": 73, "text": "visual reasoning"}, {"st": 100, "ed": 102, "text": "visual reasoning"}]
[{"st": 5, "ed": 7, "text": "image text"}, {"st": 7, "ed": 9, "text": "multi modal"}, {"st": 26, "ed": 28, "text": "multi modal"}, {"st": 32, "ed": 34, "text": "image text"}, {"st": 38, "ed": 40, "text": "multi modal"}, {"st": 50, "ed": 52, "text": "previous methods"}, {"st": 53, "ed": 55, "text": "image text"}, {"st": 58, "ed": 60, "text": "multi modal"}, {"st": 67, "ed": 69, "text": "multi modal"}, {"st": 74, "ed": 76, "text": "image text"}, {"st": 86, "ed": 89, "text": "image and text"}, {"st": 90, "ed": 92, "text": "multi modal"}, {"st": 92, "ed": 94, "text": "feature space"}, {"st": 99, "ed": 101, "text": "image text"}, {"st": 107, "ed": 109, "text": "multi modal"}, {"st": 125, "ed": 128, "text": "end to end"}, {"st": 135, "ed": 137, "text": "multi modal"}]
[{"st": 5, "ed": 7, "text": "high dimensional"}, {"st": 13, "ed": 15, "text": "linear transformation"}, {"st": 26, "ed": 29, "text": "automatic speech recognition"}, {"st": 54, "ed": 56, "text": "low dimensional"}, {"st": 97, "ed": 99, "text": "generative framework"}]
[{"st": 7, "ed": 10, "text": "recurrent neural network"}, {"st": 11, "ed": 13, "text": "rnn model"}, {"st": 57, "ed": 61, "text": "deep recurrent neural network"}, {"st": 65, "ed": 68, "text": "deep convolutional network"}, {"st": 98, "ed": 100, "text": "benchmark datasets"}, {"st": 123, "ed": 125, "text": "rnn model"}, {"st": 129, "ed": 131, "text": "retrieval tasks"}, {"st": 139, "ed": 141, "text": "performance improvement"}, {"st": 149, "ed": 151, "text": "directly optimize"}, {"st": 153, "ed": 155, "text": "objective function"}]
[{"st": 2, "ed": 4, "text": "recent advances"}, {"st": 8, "ed": 10, "text": "machine translation"}, {"st": 13, "ed": 15, "text": "encoder decoder"}, {"st": 22, "ed": 24, "text": "embedding space"}, {"st": 36, "ed": 38, "text": "distributed representations"}, {"st": 46, "ed": 48, "text": "image text"}, {"st": 48, "ed": 50, "text": "embedding models"}, {"st": 142, "ed": 144, "text": "embedding space"}]
[{"st": 43, "ed": 45, "text": "feature vector"}]
[{"st": 7, "ed": 10, "text": "recurrent neural network"}, {"st": 11, "ed": 13, "text": "rnn model"}, {"st": 51, "ed": 55, "text": "deep recurrent neural network"}, {"st": 59, "ed": 62, "text": "deep convolutional network"}, {"st": 92, "ed": 94, "text": "benchmark datasets"}, {"st": 102, "ed": 104, "text": "ms coco"}, {"st": 120, "ed": 122, "text": "rnn model"}, {"st": 123, "ed": 125, "text": "retrieval tasks"}, {"st": 133, "ed": 135, "text": "performance improvement"}, {"st": 143, "ed": 145, "text": "directly optimize"}, {"st": 147, "ed": 149, "text": "objective function"}]
[{"st": 3, "ed": 5, "text": "skip gram"}, {"st": 13, "ed": 15, "text": "visual information"}, {"st": 18, "ed": 20, "text": "skip gram"}, {"st": 28, "ed": 30, "text": "word representations"}, {"st": 52, "ed": 54, "text": "visual representations"}, {"st": 61, "ed": 63, "text": "natural images"}, {"st": 68, "ed": 70, "text": "visual features"}, {"st": 88, "ed": 90, "text": "visual information"}, {"st": 104, "ed": 106, "text": "zero shot"}]
[{"st": 10, "ed": 12, "text": "visual concepts"}, {"st": 29, "ed": 31, "text": "visual features"}, {"st": 69, "ed": 71, "text": "image captioning"}, {"st": 85, "ed": 87, "text": "weight sharing"}, {"st": 91, "ed": 93, "text": "improves performance"}, {"st": 94, "ed": 96, "text": "image captioning"}, {"st": 113, "ed": 115, "text": "prevent overfitting"}, {"st": 141, "ed": 143, "text": "visual concepts"}, {"st": 150, "ed": 152, "text": "previously learned"}]
[{"st": 0, "ed": 3, "text": "recurrent neural networks"}, {"st": 17, "ed": 19, "text": "recent results"}, {"st": 20, "ed": 22, "text": "machine translation"}, {"st": 71, "ed": 74, "text": "training and inference"}, {"st": 88, "ed": 90, "text": "curriculum learning"}, {"st": 95, "ed": 97, "text": "training process"}, {"st": 122, "ed": 124, "text": "sequence prediction"}, {"st": 128, "ed": 130, "text": "approach yields"}, {"st": 144, "ed": 146, "text": "image captioning"}]
[{"st": 45, "ed": 47, "text": "method outperforms"}, {"st": 47, "ed": 49, "text": "previous methods"}, {"st": 55, "ed": 58, "text": "taking into account"}, {"st": 70, "ed": 72, "text": "recent successes"}, {"st": 73, "ed": 75, "text": "deep learning"}, {"st": 77, "ed": 79, "text": "image captioning"}, {"st": 81, "ed": 83, "text": "image captioning"}, {"st": 99, "ed": 101, "text": "large scale"}, {"st": 122, "ed": 124, "text": "https github.com"}]
[{"st": 9, "ed": 11, "text": "visual content"}, {"st": 13, "ed": 15, "text": "challenging problem"}, {"st": 23, "ed": 25, "text": "image text"}, {"st": 31, "ed": 33, "text": "ground truth"}, {"st": 66, "ed": 68, "text": "attention mechanism"}, {"st": 85, "ed": 87, "text": "recurrent network"}, {"st": 125, "ed": 127, "text": "directly applied"}, {"st": 170, "ed": 172, "text": "large margin"}]
[{"st": 11, "ed": 13, "text": "visual recognition"}, {"st": 54, "ed": 56, "text": "training data"}, {"st": 165, "ed": 167, "text": "large scale"}]
[{"st": 2, "ed": 4, "text": "compositional structure"}, {"st": 12, "ed": 15, "text": "vision and language"}, {"st": 50, "ed": 52, "text": "computer vision"}, {"st": 54, "ed": 56, "text": "multi modal"}, {"st": 63, "ed": 67, "text": "visual question answering vqa"}, {"st": 147, "ed": 149, "text": "low level"}, {"st": 182, "ed": 184, "text": "fine grained"}, {"st": 228, "ed": 230, "text": "proposed approach"}]
[{"st": 3, "ed": 5, "text": "question answering"}, {"st": 10, "ed": 14, "text": "convolutional neural network cnn"}, {"st": 42, "ed": 46, "text": "gated recurrent unit gru"}, {"st": 54, "ed": 56, "text": "fully connected"}, {"st": 84, "ed": 86, "text": "fully connected"}, {"st": 119, "ed": 121, "text": "hash function"}, {"st": 146, "ed": 150, "text": "trained end to end"}, {"st": 151, "ed": 153, "text": "back propagation"}, {"st": 160, "ed": 162, "text": "pre trained"}, {"st": 166, "ed": 168, "text": "proposed algorithm"}]
[{"st": 17, "ed": 19, "text": "neural network"}, {"st": 20, "ed": 22, "text": "multiple layers"}, {"st": 34, "ed": 36, "text": "large margin"}, {"st": 39, "ed": 41, "text": "cross view"}, {"st": 52, "ed": 54, "text": "metric learning"}, {"st": 55, "ed": 57, "text": "extensive experiments"}, {"st": 62, "ed": 64, "text": "significant improvements"}, {"st": 76, "ed": 78, "text": "method achieves"}]
[{"st": 1, "ed": 3, "text": "textual entailment"}, {"st": 4, "ed": 6, "text": "image captioning"}, {"st": 10, "ed": 12, "text": "special cases"}, {"st": 15, "ed": 17, "text": "visual semantic"}, {"st": 29, "ed": 31, "text": "explicitly modeling"}, {"st": 61, "ed": 63, "text": "tasks involving"}, {"st": 72, "ed": 74, "text": "improve performance"}, {"st": 75, "ed": 77, "text": "current approaches"}, {"st": 81, "ed": 83, "text": "image caption"}]
[{"st": 4, "ed": 6, "text": "zero shot"}, {"st": 6, "ed": 8, "text": "event detection"}, {"st": 10, "ed": 12, "text": "multi modal"}, {"st": 44, "ed": 46, "text": "zero shot"}, {"st": 46, "ed": 48, "text": "event detection"}, {"st": 55, "ed": 57, "text": "distributional semantics"}, {"st": 149, "ed": 151, "text": "event detection"}]
[{"st": 89, "ed": 91, "text": "computational models"}, {"st": 172, "ed": 175, "text": "quantitatively and qualitatively"}]
[{"st": 6, "ed": 8, "text": "visual concepts"}, {"st": 15, "ed": 17, "text": "visual object"}, {"st": 31, "ed": 33, "text": "machine learning"}, {"st": 42, "ed": 44, "text": "learning process"}, {"st": 65, "ed": 67, "text": "textual description"}, {"st": 92, "ed": 94, "text": "domain transfer"}, {"st": 104, "ed": 106, "text": "constrained optimization"}, {"st": 115, "ed": 117, "text": "transfer function"}, {"st": 159, "ed": 163, "text": "reproducing kernel hilbert space"}, {"st": 163, "ed": 165, "text": "kernel functions"}, {"st": 177, "ed": 179, "text": "kernel function"}, {"st": 180, "ed": 182, "text": "unstructured text"}, {"st": 186, "ed": 188, "text": "distributional semantics"}, {"st": 214, "ed": 216, "text": "fine grained"}]
[{"st": 6, "ed": 8, "text": "natural language"}, {"st": 45, "ed": 47, "text": "image retrieval"}, {"st": 68, "ed": 70, "text": "image annotation"}, {"st": 79, "ed": 81, "text": "objective function"}, {"st": 85, "ed": 89, "text": "convolutional neural networks cnn"}, {"st": 90, "ed": 92, "text": "multi layer"}, {"st": 99, "ed": 102, "text": "image and sentence"}, {"st": 155, "ed": 158, "text": "image and sentence"}, {"st": 177, "ed": 179, "text": "visual input"}, {"st": 210, "ed": 212, "text": "n grams"}, {"st": 228, "ed": 230, "text": "deep network"}, {"st": 239, "ed": 241, "text": "comparable performance"}, {"st": 266, "ed": 268, "text": "negative samples"}, {"st": 281, "ed": 283, "text": "bi directional"}]
[{"st": 1, "ed": 3, "text": "natural language"}, {"st": 17, "ed": 21, "text": "convolutional neural network cnn"}, {"st": 23, "ed": 25, "text": "image features"}, {"st": 27, "ed": 31, "text": "recurrent neural network rnn"}, {"st": 51, "ed": 53, "text": "image features"}]
[{"st": 8, "ed": 10, "text": "audio visual"}, {"st": 25, "ed": 27, "text": "feature level"}, {"st": 42, "ed": 44, "text": "audio visual"}, {"st": 50, "ed": 53, "text": "short term memory"}, {"st": 53, "ed": 56, "text": "recurrent neural network"}, {"st": 56, "ed": 58, "text": "lstm rnn"}, {"st": 66, "ed": 68, "text": "soft attention"}, {"st": 78, "ed": 80, "text": "embedding vectors"}, {"st": 106, "ed": 108, "text": "soft attention"}, {"st": 110, "ed": 112, "text": "experiment results"}, {"st": 117, "ed": 119, "text": "qualitative analysis"}]
[{"st": 24, "ed": 28, "text": "feed forward neural networks"}, {"st": 36, "ed": 38, "text": "tabula rasa"}, {"st": 57, "ed": 59, "text": "preliminary experiments"}, {"st": 60, "ed": 62, "text": "promising results"}]
[{"st": 7, "ed": 10, "text": "encoder decoder framework"}, {"st": 24, "ed": 26, "text": "encoder decoder"}, {"st": 50, "ed": 52, "text": "attention mechanism"}, {"st": 55, "ed": 57, "text": "hidden states"}, {"st": 76, "ed": 78, "text": "attention mechanism"}, {"st": 89, "ed": 91, "text": "special case"}, {"st": 106, "ed": 108, "text": "encoder decoder"}, {"st": 113, "ed": 115, "text": "image captioning"}, {"st": 116, "ed": 118, "text": "source code"}]
[{"st": 37, "ed": 39, "text": "relevant information"}, {"st": 80, "ed": 82, "text": "cold start"}, {"st": 94, "ed": 96, "text": "object detection"}, {"st": 111, "ed": 113, "text": "substantial improvements"}, {"st": 115, "ed": 117, "text": "detection task"}]
[{"st": 0, "ed": 2, "text": "attention mechanisms"}, {"st": 7, "ed": 9, "text": "deep learning"}, {"st": 13, "ed": 15, "text": "natural language"}, {"st": 29, "ed": 31, "text": "attention maps"}, {"st": 56, "ed": 58, "text": "image captioning"}, {"st": 63, "ed": 65, "text": "quantitative evaluation"}, {"st": 72, "ed": 74, "text": "attention maps"}, {"st": 104, "ed": 106, "text": "attention maps"}, {"st": 145, "ed": 147, "text": "attention maps"}]
[{"st": 0, "ed": 4, "text": "visual question answering vqa"}, {"st": 9, "ed": 11, "text": "natural language"}, {"st": 30, "ed": 32, "text": "vqa models"}, {"st": 123, "ed": 125, "text": "lstm rnns"}, {"st": 136, "ed": 138, "text": "strong baselines"}, {"st": 149, "ed": 151, "text": "vqa models"}]
[{"st": 14, "ed": 16, "text": "previous works"}, {"st": 24, "ed": 26, "text": "visual data"}, {"st": 35, "ed": 37, "text": "probabilistic model"}, {"st": 48, "ed": 50, "text": "hand crafted"}, {"st": 50, "ed": 52, "text": "feature engineering"}, {"st": 54, "ed": 57, "text": "end to end"}, {"st": 60, "ed": 62, "text": "distributed representations"}, {"st": 111, "ed": 113, "text": "outperforms previous"}]
[{"st": 4, "ed": 6, "text": "natural language"}, {"st": 30, "ed": 32, "text": "natural language"}, {"st": 75, "ed": 77, "text": "aggregate data"}, {"st": 79, "ed": 81, "text": "motion capture"}, {"st": 119, "ed": 121, "text": "natural language"}, {"st": 129, "ed": 131, "text": "web based"}, {"st": 241, "ed": 243, "text": "natural language"}]
[{"st": 15, "ed": 17, "text": "traditional approaches"}, {"st": 26, "ed": 28, "text": "visual features"}, {"st": 36, "ed": 39, "text": "end to end"}, {"st": 53, "ed": 57, "text": "trained end to end"}, {"st": 63, "ed": 65, "text": "sentence level"}, {"st": 120, "ed": 122, "text": "recurrent network"}, {"st": 124, "ed": 127, "text": "connectionist temporal classification"}, {"st": 143, "ed": 146, "text": "end to end"}, {"st": 146, "ed": 148, "text": "sentence level"}, {"st": 151, "ed": 153, "text": "simultaneously learns"}, {"st": 154, "ed": 156, "text": "visual features"}, {"st": 169, "ed": 171, "text": "sentence level"}, {"st": 183, "ed": 185, "text": "word level"}]
[{"st": 10, "ed": 12, "text": "audio visual"}, {"st": 12, "ed": 15, "text": "automatic speech recognition"}, {"st": 19, "ed": 23, "text": "deep recurrent neural network"}, {"st": 55, "ed": 58, "text": "linear dimensionality reduction"}, {"st": 60, "ed": 62, "text": "visual features"}, {"st": 69, "ed": 71, "text": "visual features"}, {"st": 112, "ed": 114, "text": "significant improvement"}, {"st": 116, "ed": 118, "text": "error rate"}]
[{"st": 3, "ed": 6, "text": "optical character recognition"}, {"st": 24, "ed": 26, "text": "post processing"}, {"st": 26, "ed": 28, "text": "significantly improve"}]
[{"st": 8, "ed": 10, "text": "image captioning"}, {"st": 35, "ed": 40, "text": "long short term memory lstm"}, {"st": 45, "ed": 47, "text": "weight matrix"}, {"st": 72, "ed": 74, "text": "image caption"}, {"st": 115, "ed": 117, "text": "benchmark datasets"}, {"st": 126, "ed": 128, "text": "proposed method"}, {"st": 128, "ed": 130, "text": "significantly outperforms"}]
[{"st": 10, "ed": 12, "text": "word embeddings"}, {"st": 23, "ed": 25, "text": "large scale"}, {"st": 62, "ed": 64, "text": "ms coco"}, {"st": 66, "ed": 68, "text": "large scale"}, {"st": 68, "ed": 70, "text": "image dataset"}, {"st": 86, "ed": 88, "text": "word embeddings"}, {"st": 136, "ed": 140, "text": "recurrent neural networks rnns"}, {"st": 142, "ed": 145, "text": "text and image"}, {"st": 155, "ed": 157, "text": "visual information"}, {"st": 159, "ed": 161, "text": "word embeddings"}, {"st": 163, "ed": 165, "text": "weight sharing"}]
[{"st": 7, "ed": 9, "text": "starting point"}, {"st": 27, "ed": 29, "text": "machine learning"}, {"st": 50, "ed": 52, "text": "deep learning"}, {"st": 56, "ed": 58, "text": "input images"}, {"st": 137, "ed": 139, "text": "image processing"}, {"st": 152, "ed": 155, "text": "convolutional neural network"}, {"st": 191, "ed": 193, "text": "great potential"}, {"st": 194, "ed": 196, "text": "deep learning"}]
[{"st": 3, "ed": 5, "text": "existing methods"}, {"st": 24, "ed": 27, "text": "taking advantage of"}, {"st": 31, "ed": 33, "text": "unsupervised learning"}, {"st": 34, "ed": 37, "text": "deep neural networks"}, {"st": 40, "ed": 43, "text": "end to end"}, {"st": 43, "ed": 45, "text": "learning framework"}, {"st": 52, "ed": 54, "text": "multi modal"}, {"st": 58, "ed": 60, "text": "proposed method"}, {"st": 61, "ed": 63, "text": "representation learning"}, {"st": 65, "ed": 67, "text": "auto encoders"}, {"st": 69, "ed": 71, "text": "cross domain"}, {"st": 74, "ed": 77, "text": "maximum mean discrepancy"}, {"st": 104, "ed": 107, "text": "labeled and unlabeled"}, {"st": 128, "ed": 130, "text": "applications including"}, {"st": 132, "ed": 134, "text": "few shot"}, {"st": 134, "ed": 136, "text": "image recognition"}]
[{"st": 0, "ed": 2, "text": "existing methods"}, {"st": 3, "ed": 5, "text": "visual reasoning"}, {"st": 17, "ed": 19, "text": "explicitly modeling"}, {"st": 55, "ed": 57, "text": "visual reasoning"}, {"st": 99, "ed": 101, "text": "neural networks"}, {"st": 116, "ed": 118, "text": "visual reasoning"}, {"st": 123, "ed": 125, "text": "significantly outperforms"}, {"st": 125, "ed": 127, "text": "strong baselines"}]
[{"st": 19, "ed": 21, "text": "co occurrence"}, {"st": 24, "ed": 26, "text": "text corpora"}, {"st": 42, "ed": 44, "text": "visual information"}, {"st": 56, "ed": 58, "text": "conceptually simple"}, {"st": 63, "ed": 65, "text": "outperforms previous"}, {"st": 89, "ed": 92, "text": "orders of magnitude"}]
[{"st": 47, "ed": 49, "text": "unlike previous"}, {"st": 78, "ed": 80, "text": "reinforcement learning"}, {"st": 89, "ed": 91, "text": "gumbel softmax"}, {"st": 158, "ed": 160, "text": "natural language"}, {"st": 167, "ed": 169, "text": "prior information"}, {"st": 170, "ed": 172, "text": "natural language"}]
[{"st": 10, "ed": 12, "text": "visual concepts"}, {"st": 14, "ed": 16, "text": "low level"}, {"st": 16, "ed": 18, "text": "visual processing"}, {"st": 26, "ed": 28, "text": "computational models"}, {"st": 30, "ed": 32, "text": "vision tasks"}, {"st": 64, "ed": 66, "text": "visual processing"}, {"st": 73, "ed": 75, "text": "batch normalization"}, {"st": 79, "ed": 81, "text": "residual network"}, {"st": 94, "ed": 96, "text": "significantly improves"}, {"st": 96, "ed": 98, "text": "strong baselines"}, {"st": 100, "ed": 103, "text": "visual question answering"}, {"st": 116, "ed": 118, "text": "visual processing"}]
[{"st": 7, "ed": 9, "text": "visual semantic"}, {"st": 11, "ed": 13, "text": "cross modal"}, {"st": 22, "ed": 24, "text": "structured prediction"}, {"st": 25, "ed": 27, "text": "ranking loss"}, {"st": 38, "ed": 40, "text": "loss functions"}, {"st": 43, "ed": 45, "text": "multi modal"}, {"st": 49, "ed": 51, "text": "fine tuning"}, {"st": 58, "ed": 60, "text": "significant gains"}, {"st": 71, "ed": 73, "text": "ms coco"}, {"st": 85, "ed": 87, "text": "ms coco"}, {"st": 88, "ed": 90, "text": "approach outperforms"}, {"st": 103, "ed": 105, "text": "image retrieval"}]
[{"st": 4, "ed": 6, "text": "labeled datasets"}, {"st": 17, "ed": 19, "text": "vision language"}, {"st": 28, "ed": 32, "text": "visual question answering vqa"}, {"st": 84, "ed": 87, "text": "vision and language"}, {"st": 187, "ed": 189, "text": "multiple choice"}, {"st": 194, "ed": 196, "text": "multilayer perceptrons"}]
[{"st": 10, "ed": 12, "text": "image captioning"}, {"st": 19, "ed": 21, "text": "real world"}, {"st": 40, "ed": 42, "text": "text descriptions"}, {"st": 50, "ed": 52, "text": "ground truth"}, {"st": 128, "ed": 130, "text": "image content"}, {"st": 147, "ed": 149, "text": "training data"}, {"st": 162, "ed": 164, "text": "image representations"}, {"st": 166, "ed": 168, "text": "ground truth"}, {"st": 183, "ed": 185, "text": "training scheme"}]
[{"st": 3, "ed": 5, "text": "method called"}, {"st": 17, "ed": 19, "text": "training process"}, {"st": 24, "ed": 26, "text": "proposed method"}, {"st": 42, "ed": 44, "text": "loss function"}, {"st": 49, "ed": 51, "text": "loss function"}, {"st": 74, "ed": 76, "text": "trained model"}, {"st": 79, "ed": 81, "text": "higher accuracy"}, {"st": 83, "ed": 85, "text": "faster convergence"}, {"st": 97, "ed": 99, "text": "proposed method"}, {"st": 109, "ed": 111, "text": "proposed method"}, {"st": 111, "ed": 113, "text": "achieves comparable"}, {"st": 125, "ed": 127, "text": "source code"}, {"st": 131, "ed": 133, "text": "https github.com"}]
[{"st": 7, "ed": 9, "text": "image editing"}, {"st": 19, "ed": 21, "text": "natural language"}, {"st": 53, "ed": 55, "text": "image segmentation"}, {"st": 75, "ed": 77, "text": "step size"}, {"st": 100, "ed": 102, "text": "additional information"}, {"st": 128, "ed": 131, "text": "end to end"}, {"st": 150, "ed": 152, "text": "image segmentation"}]
[{"st": 3, "ed": 5, "text": "interactive learning"}, {"st": 28, "ed": 32, "text": "visual question answering vqa"}, {"st": 110, "ed": 112, "text": "generated data"}, {"st": 140, "ed": 142, "text": "vqa models"}]
[{"st": 6, "ed": 8, "text": "unsupervised learning"}, {"st": 11, "ed": 13, "text": "embedding space"}, {"st": 34, "ed": 36, "text": "handwritten digits"}, {"st": 90, "ed": 92, "text": "loss function"}, {"st": 94, "ed": 96, "text": "regularization term"}, {"st": 98, "ed": 100, "text": "variational autoencoders"}, {"st": 104, "ed": 106, "text": "posterior distributions"}, {"st": 116, "ed": 118, "text": "regularization term"}]
[{"st": 4, "ed": 7, "text": "image and text"}, {"st": 12, "ed": 14, "text": "computer vision"}, {"st": 15, "ed": 17, "text": "natural language"}, {"st": 38, "ed": 41, "text": "takes advantage of"}, {"st": 41, "ed": 43, "text": "recent advances"}, {"st": 44, "ed": 47, "text": "generative adversarial networks"}]
[{"st": 4, "ed": 6, "text": "sequential data"}, {"st": 21, "ed": 25, "text": "dynamic time warping dtw"}, {"st": 26, "ed": 29, "text": "conditional random fields"}, {"st": 62, "ed": 65, "text": "end to end"}, {"st": 72, "ed": 75, "text": "end to end"}, {"st": 75, "ed": 77, "text": "neural architecture"}, {"st": 89, "ed": 92, "text": "short term memory"}, {"st": 103, "ed": 105, "text": "tasks including"}, {"st": 120, "ed": 122, "text": "extensive experiments"}, {"st": 123, "ed": 126, "text": "synthetic and real"}]
[{"st": 13, "ed": 15, "text": "machine learning"}, {"st": 55, "ed": 57, "text": "machine learning"}, {"st": 86, "ed": 88, "text": "message passing"}]
[{"st": 84, "ed": 86, "text": "quantum information"}]
[{"st": 13, "ed": 15, "text": "error prone"}, {"st": 92, "ed": 95, "text": "deep neural networks"}, {"st": 110, "ed": 112, "text": "convolutional networks"}, {"st": 121, "ed": 123, "text": "higher dimensional"}, {"st": 123, "ed": 125, "text": "input data"}, {"st": 188, "ed": 190, "text": "image data"}, {"st": 222, "ed": 224, "text": "similar accuracy"}, {"st": 276, "ed": 278, "text": "training set"}, {"st": 281, "ed": 283, "text": "labeled images"}]
[{"st": 27, "ed": 29, "text": "low dimensional"}, {"st": 36, "ed": 38, "text": "encoder decoder"}, {"st": 89, "ed": 91, "text": "point clouds"}, {"st": 105, "ed": 107, "text": "point wise"}, {"st": 145, "ed": 147, "text": "low dimensional"}, {"st": 162, "ed": 164, "text": "gradient based"}, {"st": 188, "ed": 190, "text": "real world"}, {"st": 212, "ed": 214, "text": "baseline methods"}]
[{"st": 137, "ed": 139, "text": "case study"}, {"st": 145, "ed": 147, "text": "magnetic resonance"}, {"st": 166, "ed": 168, "text": "k means"}]
[{"st": 16, "ed": 18, "text": "network architecture"}, {"st": 25, "ed": 27, "text": "visual cues"}, {"st": 63, "ed": 66, "text": "spatial and temporal"}, {"st": 80, "ed": 82, "text": "approach achieves"}, {"st": 87, "ed": 89, "text": "classification performance"}, {"st": 95, "ed": 97, "text": "rgb d"}, {"st": 105, "ed": 107, "text": "spatio temporal"}]
[{"st": 3, "ed": 6, "text": "world health organization"}, {"st": 6, "ed": 8, "text": "breast cancer"}, {"st": 38, "ed": 40, "text": "image segmentation"}, {"st": 46, "ed": 48, "text": "image analysis"}, {"st": 50, "ed": 53, "text": "taking into account"}, {"st": 72, "ed": 74, "text": "highly dependent"}, {"st": 87, "ed": 89, "text": "semi supervised"}, {"st": 103, "ed": 105, "text": "image segmentation"}, {"st": 152, "ed": 154, "text": "proposed method"}, {"st": 156, "ed": 158, "text": "significant reduction"}, {"st": 189, "ed": 191, "text": "selection process"}, {"st": 194, "ed": 196, "text": "simulated annealing"}, {"st": 196, "ed": 198, "text": "optimization method"}, {"st": 205, "ed": 207, "text": "proposed approach"}, {"st": 256, "ed": 258, "text": "breast cancer"}]
[{"st": 10, "ed": 12, "text": "multi class"}, {"st": 153, "ed": 156, "text": "block coordinate descent"}, {"st": 170, "ed": 172, "text": "classification accuracy"}, {"st": 173, "ed": 175, "text": "faster convergence"}]
[{"st": 2, "ed": 4, "text": "time series"}, {"st": 9, "ed": 11, "text": "machine learning"}, {"st": 16, "ed": 18, "text": "computational biology"}, {"st": 26, "ed": 28, "text": "motion capture"}, {"st": 46, "ed": 48, "text": "gaussian process"}, {"st": 55, "ed": 57, "text": "variational approximations"}, {"st": 58, "ed": 60, "text": "gaussian process"}, {"st": 60, "ed": 62, "text": "latent variable"}, {"st": 67, "ed": 69, "text": "dimensionality reduction"}, {"st": 89, "ed": 91, "text": "latent space"}, {"st": 102, "ed": 104, "text": "motion capture"}]
[{"st": 6, "ed": 9, "text": "probabilistic topic models"}, {"st": 12, "ed": 14, "text": "non trivial"}, {"st": 35, "ed": 37, "text": "fast inference"}, {"st": 42, "ed": 44, "text": "latent representations"}, {"st": 57, "ed": 60, "text": "probabilistic topic models"}, {"st": 80, "ed": 83, "text": "linear convergence rate"}, {"st": 89, "ed": 91, "text": "prior knowledge"}, {"st": 129, "ed": 131, "text": "topic models"}]
[{"st": 49, "ed": 52, "text": "convolutional neural networks"}, {"st": 54, "ed": 56, "text": "visual representations"}, {"st": 64, "ed": 66, "text": "learned features"}, {"st": 86, "ed": 89, "text": "unsupervised feature learning"}, {"st": 89, "ed": 92, "text": "approach significantly outperforms"}, {"st": 92, "ed": 94, "text": "previous approaches"}, {"st": 95, "ed": 97, "text": "visual recognition"}, {"st": 118, "ed": 120, "text": "autonomous driving"}, {"st": 122, "ed": 124, "text": "large scale"}, {"st": 124, "ed": 126, "text": "scene recognition"}]
[{"st": 5, "ed": 7, "text": "text descriptions"}, {"st": 9, "ed": 11, "text": "challenging problem"}, {"st": 12, "ed": 14, "text": "computer vision"}, {"st": 53, "ed": 56, "text": "generative adversarial networks"}, {"st": 60, "ed": 62, "text": "photo realistic"}, {"st": 104, "ed": 106, "text": "low resolution"}, {"st": 116, "ed": 118, "text": "text descriptions"}, {"st": 126, "ed": 128, "text": "photo realistic"}, {"st": 161, "ed": 163, "text": "conditional gan"}, {"st": 178, "ed": 180, "text": "extensive experiments"}, {"st": 188, "ed": 190, "text": "benchmark datasets"}, {"st": 193, "ed": 195, "text": "proposed method"}, {"st": 196, "ed": 198, "text": "significant improvements"}, {"st": 200, "ed": 202, "text": "photo realistic"}]
[{"st": 1, "ed": 4, "text": "image super resolution"}, {"st": 16, "ed": 18, "text": "low resolution"}, {"st": 37, "ed": 41, "text": "signal to noise ratio"}, {"st": 51, "ed": 53, "text": "human perception"}, {"st": 91, "ed": 93, "text": "texture synthesis"}, {"st": 113, "ed": 115, "text": "ground truth"}, {"st": 120, "ed": 122, "text": "feed forward"}, {"st": 122, "ed": 124, "text": "fully convolutional"}, {"st": 124, "ed": 126, "text": "neural networks"}, {"st": 128, "ed": 130, "text": "adversarial training"}, {"st": 137, "ed": 139, "text": "image quality"}, {"st": 143, "ed": 145, "text": "extensive experiments"}, {"st": 164, "ed": 167, "text": "quantitative and qualitative"}]
[{"st": 3, "ed": 5, "text": "computer vision"}, {"st": 8, "ed": 10, "text": "inverse problem"}, {"st": 11, "ed": 13, "text": "computer graphics"}, {"st": 31, "ed": 33, "text": "vision tasks"}, {"st": 58, "ed": 60, "text": "generative models"}, {"st": 67, "ed": 69, "text": "real world"}, {"st": 99, "ed": 101, "text": "latent variables"}, {"st": 119, "ed": 121, "text": "computer graphics"}, {"st": 121, "ed": 123, "text": "originally designed"}, {"st": 145, "ed": 147, "text": "probabilistic programming"}, {"st": 147, "ed": 149, "text": "computer graphics"}, {"st": 150, "ed": 152, "text": "approximate bayesian"}, {"st": 209, "ed": 211, "text": "real world"}]
[{"st": 4, "ed": 6, "text": "vision problems"}, {"st": 10, "ed": 12, "text": "generative models"}, {"st": 14, "ed": 16, "text": "computer graphics"}, {"st": 25, "ed": 27, "text": "natural images"}, {"st": 30, "ed": 32, "text": "low dimensional"}, {"st": 56, "ed": 58, "text": "generative models"}, {"st": 103, "ed": 105, "text": "real world"}, {"st": 106, "ed": 108, "text": "vision problems"}, {"st": 109, "ed": 111, "text": "approximate inference"}, {"st": 112, "ed": 114, "text": "generative models"}, {"st": 157, "ed": 159, "text": "feature space"}, {"st": 162, "ed": 164, "text": "mid level"}, {"st": 164, "ed": 166, "text": "image representations"}, {"st": 171, "ed": 173, "text": "inference algorithm"}, {"st": 183, "ed": 185, "text": "monte carlo"}, {"st": 192, "ed": 194, "text": "training data"}, {"st": 204, "ed": 207, "text": "human pose estimation"}, {"st": 215, "ed": 218, "text": "quantitative and qualitative"}, {"st": 218, "ed": 220, "text": "performance improvements"}]
[{"st": 0, "ed": 4, "text": "functional magnetic resonance imaging"}, {"st": 8, "ed": 10, "text": "non invasive"}, {"st": 30, "ed": 32, "text": "human brain"}, {"st": 52, "ed": 54, "text": "human brain"}, {"st": 62, "ed": 64, "text": "signal processing"}, {"st": 76, "ed": 78, "text": "multi dimensional"}, {"st": 91, "ed": 94, "text": "blind source separation"}, {"st": 108, "ed": 111, "text": "independent component analysis"}, {"st": 191, "ed": 193, "text": "human brain"}]
[{"st": 19, "ed": 21, "text": "ensemble based"}, {"st": 36, "ed": 38, "text": "multiple classifiers"}, {"st": 86, "ed": 88, "text": "diabetic retinopathy"}, {"st": 93, "ed": 95, "text": "proposed method"}]
[{"st": 0, "ed": 2, "text": "cross domain"}, {"st": 2, "ed": 4, "text": "visual data"}, {"st": 9, "ed": 11, "text": "fundamental problems"}, {"st": 13, "ed": 15, "text": "real world"}, {"st": 15, "ed": 17, "text": "vision tasks"}, {"st": 26, "ed": 28, "text": "conventional approaches"}, {"st": 43, "ed": 45, "text": "common space"}, {"st": 65, "ed": 67, "text": "pairwise similarity"}, {"st": 70, "ed": 72, "text": "existing models"}, {"st": 85, "ed": 87, "text": "mahalanobis distance"}, {"st": 88, "ed": 90, "text": "cosine similarity"}, {"st": 99, "ed": 101, "text": "similarity measure"}, {"st": 102, "ed": 104, "text": "feature representation"}, {"st": 114, "ed": 116, "text": "similarity measure"}, {"st": 119, "ed": 121, "text": "deep architecture"}, {"st": 123, "ed": 126, "text": "end to end"}, {"st": 131, "ed": 133, "text": "extensively evaluate"}, {"st": 140, "ed": 142, "text": "cross domain"}, {"st": 144, "ed": 147, "text": "person re identification"}, {"st": 151, "ed": 153, "text": "face verification"}, {"st": 160, "ed": 163, "text": "images and videos"}]
[{"st": 0, "ed": 4, "text": "robust principal component analysis"}, {"st": 11, "ed": 13, "text": "low rank"}, {"st": 16, "ed": 18, "text": "data mining"}, {"st": 19, "ed": 21, "text": "machine learning"}, {"st": 29, "ed": 31, "text": "low rank"}, {"st": 97, "ed": 99, "text": "low rank"}, {"st": 160, "ed": 162, "text": "extensive experiments"}, {"st": 213, "ed": 216, "text": "times faster than"}]
[{"st": 5, "ed": 7, "text": "cross modal"}, {"st": 7, "ed": 10, "text": "convolutional neural networks"}, {"st": 14, "ed": 16, "text": "biologically inspired"}, {"st": 32, "ed": 34, "text": "larger scale"}, {"st": 34, "ed": 36, "text": "network topology"}, {"st": 44, "ed": 46, "text": "weight sharing"}, {"st": 48, "ed": 50, "text": "hidden layers"}, {"st": 61, "ed": 63, "text": "neural network"}, {"st": 96, "ed": 98, "text": "input data"}, {"st": 130, "ed": 132, "text": "input data"}, {"st": 133, "ed": 135, "text": "domain knowledge"}, {"st": 136, "ed": 138, "text": "unsupervised methods"}, {"st": 176, "ed": 178, "text": "cross modal"}, {"st": 181, "ed": 186, "text": "cifar 10 and cifar 100"}, {"st": 192, "ed": 194, "text": "training data"}, {"st": 208, "ed": 210, "text": "significantly outperform"}, {"st": 221, "ed": 223, "text": "dataset size"}]
[{"st": 8, "ed": 10, "text": "lifelong learning"}, {"st": 41, "ed": 43, "text": "previous tasks"}, {"st": 171, "ed": 173, "text": "image classification"}, {"st": 174, "ed": 176, "text": "video prediction"}]
[{"st": 4, "ed": 6, "text": "object detection"}, {"st": 16, "ed": 18, "text": "object instances"}, {"st": 20, "ed": 22, "text": "recognition systems"}, {"st": 58, "ed": 60, "text": "data model"}, {"st": 78, "ed": 80, "text": "proposed framework"}, {"st": 107, "ed": 109, "text": "mutual information"}]
[{"st": 32, "ed": 34, "text": "generative model"}, {"st": 147, "ed": 150, "text": "received much attention"}]
[{"st": 20, "ed": 22, "text": "social interaction"}, {"st": 41, "ed": 44, "text": "deep q network"}, {"st": 57, "ed": 60, "text": "trial and error"}, {"st": 89, "ed": 92, "text": "end to end"}]
[{"st": 8, "ed": 10, "text": "social interaction"}, {"st": 47, "ed": 49, "text": "social interaction"}, {"st": 93, "ed": 96, "text": "end to end"}, {"st": 96, "ed": 98, "text": "reinforcement learning"}]
[]
[{"st": 0, "ed": 3, "text": "convolutional neural networks"}, {"st": 8, "ed": 10, "text": "great importance"}, {"st": 11, "ed": 14, "text": "the past decade"}, {"st": 32, "ed": 34, "text": "computer vision"}, {"st": 39, "ed": 41, "text": "deep networks"}, {"st": 63, "ed": 65, "text": "pre trained"}, {"st": 94, "ed": 96, "text": "prediction performance"}, {"st": 128, "ed": 130, "text": "improve performance"}, {"st": 132, "ed": 134, "text": "existing approaches"}, {"st": 143, "ed": 145, "text": "pascal voc"}, {"st": 147, "ed": 149, "text": "supervised classification"}, {"st": 151, "ed": 154, "text": "mnist and cifar10"}]
[{"st": 4, "ed": 6, "text": "alzheimer disease"}, {"st": 31, "ed": 35, "text": "deep convolutional neural network"}, {"st": 37, "ed": 39, "text": "predict future"}, {"st": 42, "ed": 45, "text": "mild cognitive impairment"}, {"st": 76, "ed": 78, "text": "feature extraction"}, {"st": 103, "ed": 105, "text": "feature based"}, {"st": 113, "ed": 115, "text": "cnn based"}, {"st": 117, "ed": 119, "text": "significantly higher"}, {"st": 150, "ed": 152, "text": "deep learning"}]
[{"st": 19, "ed": 21, "text": "gesture recognition"}, {"st": 27, "ed": 29, "text": "machine learning"}, {"st": 73, "ed": 75, "text": "real world"}]
[{"st": 21, "ed": 23, "text": "training data"}, {"st": 49, "ed": 52, "text": "generative adversarial network"}, {"st": 58, "ed": 60, "text": "adversarial training"}, {"st": 93, "ed": 96, "text": "end to end"}, {"st": 134, "ed": 136, "text": "neural network"}]
[{"st": 25, "ed": 27, "text": "deep learning"}, {"st": 32, "ed": 34, "text": "recent years"}, {"st": 46, "ed": 48, "text": "neural network"}, {"st": 82, "ed": 84, "text": "score based"}, {"st": 108, "ed": 110, "text": "distance based"}, {"st": 125, "ed": 127, "text": "classification error"}, {"st": 141, "ed": 143, "text": "significant improvement"}]
[{"st": 1, "ed": 5, "text": "generative adversarial networks gans"}, {"st": 7, "ed": 9, "text": "remarkable success"}, {"st": 27, "ed": 30, "text": "generative adversarial networks"}, {"st": 36, "ed": 38, "text": "photo realistic"}, {"st": 45, "ed": 48, "text": "generative adversarial network"}, {"st": 75, "ed": 77, "text": "low resolution"}, {"st": 87, "ed": 89, "text": "text descriptions"}, {"st": 97, "ed": 99, "text": "photo realistic"}, {"st": 103, "ed": 105, "text": "multi stage"}, {"st": 105, "ed": 108, "text": "generative adversarial network"}, {"st": 136, "ed": 138, "text": "multiple scales"}, {"st": 166, "ed": 168, "text": "extensive experiments"}, {"st": 173, "ed": 176, "text": "generative adversarial networks"}, {"st": 176, "ed": 178, "text": "significantly outperform"}, {"st": 186, "ed": 188, "text": "photo realistic"}]
[{"st": 30, "ed": 32, "text": "learning systems"}, {"st": 32, "ed": 34, "text": "lifelong learning"}, {"st": 92, "ed": 94, "text": "neural network"}, {"st": 189, "ed": 191, "text": "object recognition"}, {"st": 195, "ed": 197, "text": "challenging problem"}]
[{"st": 82, "ed": 84, "text": "deep learning"}, {"st": 115, "ed": 118, "text": "takes into account"}]
[{"st": 13, "ed": 15, "text": "deep networks"}, {"st": 26, "ed": 28, "text": "neural network"}]
[{"st": 1, "ed": 3, "text": "recent years"}, {"st": 3, "ed": 7, "text": "convolutional neural networks cnns"}, {"st": 13, "ed": 15, "text": "computer vision"}, {"st": 18, "ed": 20, "text": "object recognition"}, {"st": 28, "ed": 30, "text": "catastrophic forgetting"}, {"st": 31, "ed": 34, "text": "hyper parameter tuning"}, {"st": 35, "ed": 37, "text": "incremental learning"}, {"st": 49, "ed": 52, "text": "deep neural network"}, {"st": 55, "ed": 57, "text": "multiple levels"}, {"st": 96, "ed": 101, "text": "cifar 10 and cifar 100"}, {"st": 108, "ed": 110, "text": "fine tuning"}, {"st": 130, "ed": 135, "text": "cifar 10 and cifar 100"}]
[{"st": 4, "ed": 7, "text": "artificial intelligence ai"}, {"st": 8, "ed": 10, "text": "computer vision"}, {"st": 31, "ed": 34, "text": "optical coherence tomography"}, {"st": 60, "ed": 63, "text": "optical coherence tomography"}, {"st": 80, "ed": 82, "text": "deep learning"}, {"st": 150, "ed": 152, "text": "clinical trials"}]
[{"st": 33, "ed": 35, "text": "adversarial attacks"}, {"st": 54, "ed": 56, "text": "deep architectures"}, {"st": 109, "ed": 111, "text": "convergence guarantees"}, {"st": 135, "ed": 137, "text": "standard datasets"}, {"st": 155, "ed": 157, "text": "continuous optimization"}]
[{"st": 5, "ed": 7, "text": "vision tasks"}, {"st": 77, "ed": 79, "text": "sparse coding"}, {"st": 124, "ed": 126, "text": "computational models"}]
[{"st": 3, "ed": 5, "text": "recent approaches"}, {"st": 19, "ed": 21, "text": "input images"}, {"st": 37, "ed": 39, "text": "partial observability"}, {"st": 42, "ed": 44, "text": "path planning"}, {"st": 69, "ed": 71, "text": "object categories"}, {"st": 95, "ed": 97, "text": "raw image"}, {"st": 136, "ed": 138, "text": "object detection"}]
[{"st": 8, "ed": 10, "text": "vision problems"}, {"st": 45, "ed": 47, "text": "low level"}, {"st": 47, "ed": 49, "text": "vision tasks"}, {"st": 64, "ed": 66, "text": "supervised tasks"}, {"st": 76, "ed": 78, "text": "network layers"}, {"st": 96, "ed": 98, "text": "fully connected"}, {"st": 99, "ed": 101, "text": "convolutional layers"}, {"st": 118, "ed": 121, "text": "end to end"}, {"st": 123, "ed": 125, "text": "back propagation"}, {"st": 172, "ed": 174, "text": "performance improvements"}]
[{"st": 6, "ed": 8, "text": "unsupervised learning"}, {"st": 35, "ed": 37, "text": "post processing"}, {"st": 152, "ed": 154, "text": "discriminative features"}, {"st": 156, "ed": 158, "text": "traffic sign"}, {"st": 189, "ed": 191, "text": "multiple labels"}, {"st": 193, "ed": 195, "text": "higher accuracy"}]
[{"st": 6, "ed": 9, "text": "convolutional neural networks"}, {"st": 49, "ed": 52, "text": "convolutional neural network"}, {"st": 122, "ed": 124, "text": "temporal data"}]
[{"st": 5, "ed": 7, "text": "lifelong learning"}, {"st": 23, "ed": 25, "text": "vision systems"}, {"st": 30, "ed": 32, "text": "catastrophic forgetting"}, {"st": 62, "ed": 64, "text": "previous tasks"}, {"st": 122, "ed": 124, "text": "previous tasks"}, {"st": 160, "ed": 162, "text": "image classification"}]
[{"st": 71, "ed": 73, "text": "gesture recognition"}, {"st": 83, "ed": 85, "text": "input method"}, {"st": 94, "ed": 96, "text": "machine learning"}, {"st": 111, "ed": 113, "text": "computationally intensive"}, {"st": 119, "ed": 121, "text": "computer vision"}, {"st": 132, "ed": 134, "text": "computer vision"}]
[{"st": 0, "ed": 2, "text": "deep learning"}, {"st": 83, "ed": 85, "text": "unsupervised training"}]
[{"st": 3, "ed": 6, "text": "end to end"}, {"st": 11, "ed": 13, "text": "breast cancer"}, {"st": 39, "ed": 41, "text": "image level"}, {"st": 115, "ed": 117, "text": "compare favorably"}, {"st": 159, "ed": 161, "text": "https github.com"}]
[{"st": 49, "ed": 52, "text": "federal aviation administration"}, {"st": 69, "ed": 71, "text": "multi dimensional"}, {"st": 72, "ed": 74, "text": "time series"}, {"st": 117, "ed": 119, "text": "time series"}, {"st": 142, "ed": 144, "text": "current methods"}, {"st": 151, "ed": 153, "text": "time series"}, {"st": 167, "ed": 171, "text": "multiple instance learning mil"}, {"st": 172, "ed": 176, "text": "deep recurrent neural networks"}, {"st": 183, "ed": 186, "text": "ability to learn"}, {"st": 187, "ed": 189, "text": "weakly supervised"}, {"st": 217, "ed": 219, "text": "proposed algorithm"}, {"st": 229, "ed": 231, "text": "real world"}, {"st": 238, "ed": 240, "text": "commercial airline"}]
[{"st": 7, "ed": 9, "text": "image classification"}, {"st": 13, "ed": 15, "text": "neural networks"}, {"st": 52, "ed": 55, "text": "kullback leibler divergence"}, {"st": 67, "ed": 69, "text": "deep network"}, {"st": 81, "ed": 83, "text": "deep learning"}]
[{"st": 0, "ed": 3, "text": "person re identification"}, {"st": 12, "ed": 14, "text": "cross view"}, {"st": 15, "ed": 17, "text": "training data"}, {"st": 24, "ed": 26, "text": "invariant features"}, {"st": 46, "ed": 48, "text": "image generation"}, {"st": 63, "ed": 67, "text": "generative adversarial network gan"}, {"st": 146, "ed": 148, "text": "training data"}, {"st": 150, "ed": 152, "text": "fine tuning"}, {"st": 161, "ed": 163, "text": "extensive experiments"}, {"st": 200, "ed": 202, "text": "fine tuning"}, {"st": 205, "ed": 207, "text": "existing models"}, {"st": 207, "ed": 209, "text": "fine tuned"}]
[{"st": 0, "ed": 2, "text": "recent advances"}, {"st": 4, "ed": 6, "text": "super resolution"}, {"st": 9, "ed": 12, "text": "convolutional neural networks"}, {"st": 14, "ed": 16, "text": "motion compensation"}, {"st": 23, "ed": 25, "text": "low resolution"}, {"st": 58, "ed": 60, "text": "sliding window"}, {"st": 77, "ed": 79, "text": "super resolution"}, {"st": 94, "ed": 96, "text": "multiple times"}, {"st": 98, "ed": 100, "text": "computational cost"}, {"st": 129, "ed": 132, "text": "end to end"}, {"st": 136, "ed": 138, "text": "super resolution"}, {"st": 161, "ed": 163, "text": "computational cost"}, {"st": 178, "ed": 180, "text": "proposed method"}, {"st": 200, "ed": 202, "text": "previous methods"}, {"st": 212, "ed": 214, "text": "proposed framework"}, {"st": 217, "ed": 219, "text": "significantly outperform"}]
[{"st": 0, "ed": 3, "text": "convolutional neural networks"}, {"st": 26, "ed": 28, "text": "deep models"}, {"st": 92, "ed": 94, "text": "training set"}, {"st": 121, "ed": 123, "text": "convolutional network"}]
[{"st": 65, "ed": 67, "text": "fuzzy logic"}, {"st": 81, "ed": 83, "text": "open ended"}, {"st": 90, "ed": 92, "text": "dynamical systems"}]
[{"st": 13, "ed": 15, "text": "recent successes"}, {"st": 16, "ed": 19, "text": "deep neural networks"}, {"st": 25, "ed": 28, "text": "end to end"}, {"st": 28, "ed": 30, "text": "deep network"}, {"st": 37, "ed": 40, "text": "local and global"}, {"st": 43, "ed": 45, "text": "deep architecture"}, {"st": 46, "ed": 49, "text": "convolutional neural networks"}, {"st": 67, "ed": 69, "text": "amino acid"}, {"st": 75, "ed": 77, "text": "neural network"}, {"st": 79, "ed": 82, "text": "gated recurrent unit"}, {"st": 88, "ed": 91, "text": "multi task learning"}, {"st": 99, "ed": 101, "text": "amino acid"}, {"st": 106, "ed": 108, "text": "deep network"}]
[{"st": 14, "ed": 16, "text": "reinforcement learning"}, {"st": 56, "ed": 58, "text": "reinforcement learning"}, {"st": 68, "ed": 71, "text": "states and actions"}, {"st": 73, "ed": 75, "text": "optimal policy"}, {"st": 100, "ed": 103, "text": "restricted boltzmann machines"}, {"st": 163, "ed": 165, "text": "reinforcement learning"}]
[{"st": 33, "ed": 35, "text": "genetic algorithms"}, {"st": 50, "ed": 52, "text": "efficient learning"}, {"st": 109, "ed": 111, "text": "fitness function"}, {"st": 139, "ed": 141, "text": "total number"}, {"st": 155, "ed": 158, "text": "statistical hypothesis testing"}, {"st": 162, "ed": 164, "text": "null hypothesis"}, {"st": 190, "ed": 192, "text": "evolutionary algorithm"}, {"st": 194, "ed": 196, "text": "non trivial"}]
[{"st": 2, "ed": 4, "text": "neural networks"}, {"st": 16, "ed": 18, "text": "successful applications"}, {"st": 48, "ed": 51, "text": "recurrent neural networks"}, {"st": 90, "ed": 92, "text": "reinforcement learning"}, {"st": 95, "ed": 97, "text": "policy search"}, {"st": 101, "ed": 103, "text": "neural network"}, {"st": 138, "ed": 140, "text": "future directions"}, {"st": 146, "ed": 148, "text": "deep learning"}]
[{"st": 38, "ed": 40, "text": "wide variety"}, {"st": 77, "ed": 79, "text": "contingency plan"}, {"st": 95, "ed": 98, "text": "trial and error"}, {"st": 168, "ed": 171, "text": "trial and error"}, {"st": 171, "ed": 173, "text": "learning algorithm"}, {"st": 212, "ed": 214, "text": "robotic arm"}]
[{"st": 35, "ed": 37, "text": "distributional semantics"}, {"st": 38, "ed": 40, "text": "deep learning"}, {"st": 41, "ed": 43, "text": "neural networks"}, {"st": 57, "ed": 59, "text": "unsupervised training"}, {"st": 73, "ed": 75, "text": "predictive model"}, {"st": 98, "ed": 100, "text": "text descriptions"}]
[{"st": 20, "ed": 22, "text": "challenging problem"}, {"st": 23, "ed": 25, "text": "highly accurate"}, {"st": 76, "ed": 78, "text": "numerical experiments"}, {"st": 94, "ed": 97, "text": "recurrent neural networks"}, {"st": 111, "ed": 113, "text": "artificial intelligence"}]
[{"st": 4, "ed": 6, "text": "weight update"}, {"st": 8, "ed": 10, "text": "higher order"}, {"st": 25, "ed": 27, "text": "neural networks"}, {"st": 51, "ed": 53, "text": "spectral radius"}, {"st": 55, "ed": 57, "text": "weight update"}, {"st": 75, "ed": 77, "text": "weight update"}, {"st": 91, "ed": 93, "text": "neural architecture"}, {"st": 119, "ed": 121, "text": "proposed approach"}, {"st": 127, "ed": 129, "text": "neural architecture"}]
[{"st": 0, "ed": 2, "text": "recent theoretical"}, {"st": 25, "ed": 27, "text": "free energy"}, {"st": 28, "ed": 30, "text": "reinforcement learning"}, {"st": 54, "ed": 56, "text": "free energy"}, {"st": 70, "ed": 72, "text": "reinforcement learning"}, {"st": 103, "ed": 105, "text": "reinforcement learning"}]
[{"st": 10, "ed": 12, "text": "real world"}, {"st": 23, "ed": 25, "text": "collective intelligence"}, {"st": 38, "ed": 40, "text": "large scale"}, {"st": 82, "ed": 84, "text": "machine learning"}, {"st": 87, "ed": 89, "text": "deep learning"}, {"st": 119, "ed": 121, "text": "based approach"}, {"st": 134, "ed": 136, "text": "proposed approach"}, {"st": 161, "ed": 163, "text": "extensive simulations"}, {"st": 189, "ed": 191, "text": "proposed approach"}, {"st": 196, "ed": 198, "text": "real world"}, {"st": 204, "ed": 206, "text": "computationally expensive"}]
[{"st": 15, "ed": 17, "text": "supervised learning"}, {"st": 23, "ed": 25, "text": "multi modal"}]
[{"st": 8, "ed": 10, "text": "machine learning"}, {"st": 15, "ed": 17, "text": "software engineering"}, {"st": 21, "ed": 23, "text": "project management"}, {"st": 39, "ed": 41, "text": "machine learning"}, {"st": 55, "ed": 57, "text": "gain insight"}, {"st": 68, "ed": 70, "text": "machine learning"}, {"st": 91, "ed": 93, "text": "empirical study"}, {"st": 99, "ed": 101, "text": "machine learning"}, {"st": 107, "ed": 109, "text": "empirical study"}, {"st": 133, "ed": 135, "text": "machine learning"}, {"st": 137, "ed": 139, "text": "specific performance"}, {"st": 149, "ed": 151, "text": "domain knowledge"}, {"st": 168, "ed": 170, "text": "machine learning"}, {"st": 228, "ed": 231, "text": "strengths and weaknesses"}, {"st": 234, "ed": 236, "text": "machine learning"}, {"st": 251, "ed": 253, "text": "software engineering"}]
[{"st": 9, "ed": 11, "text": "neural network"}, {"st": 19, "ed": 21, "text": "machine learning"}, {"st": 29, "ed": 33, "text": "feed forward neural networks"}, {"st": 42, "ed": 45, "text": "supervised and unsupervised"}, {"st": 60, "ed": 62, "text": "pre training"}, {"st": 68, "ed": 70, "text": "network parameters"}, {"st": 129, "ed": 131, "text": "fast approximate"}, {"st": 154, "ed": 156, "text": "standard backpropagation"}, {"st": 163, "ed": 165, "text": "prevent overfitting"}, {"st": 212, "ed": 214, "text": "gamma ray"}]
[{"st": 6, "ed": 9, "text": "non convex optimization"}, {"st": 32, "ed": 34, "text": "theoretical results"}, {"st": 37, "ed": 40, "text": "non convex optimization"}, {"st": 57, "ed": 60, "text": "stochastic gradient descent"}, {"st": 95, "ed": 97, "text": "recently introduced"}, {"st": 98, "ed": 100, "text": "convex optimization"}, {"st": 159, "ed": 161, "text": "empirical risk"}, {"st": 164, "ed": 167, "text": "convex loss functions"}]
[{"st": 2, "ed": 5, "text": "partial differential equations"}, {"st": 75, "ed": 77, "text": "transaction costs"}, {"st": 100, "ed": 102, "text": "computational effort"}, {"st": 106, "ed": 108, "text": "grows exponentially"}, {"st": 161, "ed": 163, "text": "differential equations"}, {"st": 188, "ed": 191, "text": "deep neural nets"}, {"st": 194, "ed": 197, "text": "stochastic gradient descent"}, {"st": 200, "ed": 202, "text": "numerical results"}]
[{"st": 1, "ed": 3, "text": "recent years"}, {"st": 16, "ed": 18, "text": "brain function"}, {"st": 36, "ed": 38, "text": "ground truth"}, {"st": 49, "ed": 51, "text": "previously proposed"}, {"st": 55, "ed": 58, "text": "independent component analysis"}, {"st": 60, "ed": 63, "text": "sparse dictionary learning"}, {"st": 71, "ed": 74, "text": "blind source separation"}, {"st": 87, "ed": 89, "text": "time series"}, {"st": 128, "ed": 132, "text": "convolutional neural network cnn"}, {"st": 141, "ed": 143, "text": "low level"}, {"st": 147, "ed": 149, "text": "time series"}, {"st": 162, "ed": 164, "text": "deep convolutional"}, {"st": 174, "ed": 176, "text": "hierarchical structure"}, {"st": 241, "ed": 244, "text": "efficient and scalable"}, {"st": 246, "ed": 248, "text": "big data"}, {"st": 261, "ed": 263, "text": "big data"}]
[{"st": 1, "ed": 3, "text": "training error"}, {"st": 5, "ed": 8, "text": "deep neural networks"}, {"st": 16, "ed": 18, "text": "residual networks"}, {"st": 33, "ed": 35, "text": "lyapunov stability"}, {"st": 44, "ed": 46, "text": "step size"}, {"st": 73, "ed": 75, "text": "residual networks"}]
[{"st": 0, "ed": 2, "text": "deep learning"}, {"st": 13, "ed": 15, "text": "significant computational"}, {"st": 18, "ed": 20, "text": "processing units"}, {"st": 39, "ed": 41, "text": "framework called"}, {"st": 65, "ed": 67, "text": "neural network"}, {"st": 67, "ed": 69, "text": "framework called"}, {"st": 81, "ed": 84, "text": "times faster than"}, {"st": 86, "ed": 88, "text": "javascript library"}, {"st": 89, "ed": 93, "text": "deep convolutional neural networks"}, {"st": 93, "ed": 95, "text": "deep cnns"}, {"st": 111, "ed": 113, "text": "deep learning"}, {"st": 114, "ed": 116, "text": "deep cnns"}, {"st": 118, "ed": 120, "text": "web browsers"}]
[{"st": 22, "ed": 24, "text": "existing methods"}, {"st": 52, "ed": 55, "text": "trial and error"}, {"st": 68, "ed": 71, "text": "short term memory"}, {"st": 72, "ed": 74, "text": "neural networks"}, {"st": 96, "ed": 98, "text": "outperform existing"}, {"st": 149, "ed": 151, "text": "approach outperforms"}]
[{"st": 6, "ed": 8, "text": "sensory input"}, {"st": 16, "ed": 18, "text": "recent years"}, {"st": 21, "ed": 23, "text": "learning rules"}, {"st": 28, "ed": 30, "text": "neural network"}, {"st": 36, "ed": 38, "text": "natural images"}, {"st": 47, "ed": 49, "text": "deep architectures"}, {"st": 75, "ed": 77, "text": "hidden nodes"}, {"st": 106, "ed": 108, "text": "machine learning"}, {"st": 112, "ed": 114, "text": "hidden nodes"}]
[{"st": 1, "ed": 3, "text": "deep networks"}, {"st": 5, "ed": 7, "text": "time consuming"}, {"st": 11, "ed": 13, "text": "object recognition"}, {"st": 41, "ed": 43, "text": "batch processing"}, {"st": 63, "ed": 65, "text": "deep learning"}, {"st": 73, "ed": 75, "text": "deep networks"}, {"st": 95, "ed": 97, "text": "deep learning"}, {"st": 101, "ed": 103, "text": "multi dimensional"}, {"st": 111, "ed": 114, "text": "stochastic gradient descent"}, {"st": 137, "ed": 139, "text": "parameter tuning"}]
[{"st": 2, "ed": 4, "text": "machine learning"}, {"st": 48, "ed": 51, "text": "each data point"}, {"st": 96, "ed": 98, "text": "chain rule"}, {"st": 106, "ed": 108, "text": "large scale"}, {"st": 117, "ed": 119, "text": "faster training"}, {"st": 217, "ed": 219, "text": "load balancing"}, {"st": 219, "ed": 221, "text": "fault tolerance"}, {"st": 222, "ed": 224, "text": "streaming data"}, {"st": 250, "ed": 252, "text": "fast approximate"}, {"st": 275, "ed": 277, "text": "training set"}]
[{"st": 1, "ed": 3, "text": "inverse problems"}, {"st": 4, "ed": 6, "text": "iterative algorithms"}, {"st": 52, "ed": 54, "text": "faster convergence"}, {"st": 64, "ed": 66, "text": "computational cost"}, {"st": 86, "ed": 88, "text": "low dimensional"}, {"st": 102, "ed": 104, "text": "faster convergence"}, {"st": 124, "ed": 126, "text": "recent advances"}, {"st": 127, "ed": 129, "text": "sparse recovery"}, {"st": 129, "ed": 131, "text": "compressed sensing"}, {"st": 151, "ed": 153, "text": "neural networks"}]
[{"st": 3, "ed": 5, "text": "associative memory"}, {"st": 31, "ed": 33, "text": "associative memory"}, {"st": 34, "ed": 36, "text": "neural networks"}, {"st": 43, "ed": 45, "text": "associative memory"}, {"st": 74, "ed": 76, "text": "pattern recognition"}, {"st": 86, "ed": 88, "text": "deep learning"}, {"st": 96, "ed": 99, "text": "feedforward neural networks"}, {"st": 101, "ed": 103, "text": "hidden layer"}, {"st": 105, "ed": 107, "text": "activation functions"}, {"st": 122, "ed": 124, "text": "activation functions"}, {"st": 126, "ed": 129, "text": "rectified linear units"}, {"st": 147, "ed": 149, "text": "associative memory"}, {"st": 154, "ed": 156, "text": "neural networks"}, {"st": 158, "ed": 160, "text": "activation functions"}, {"st": 194, "ed": 196, "text": "handwritten digits"}]
[{"st": 3, "ed": 5, "text": "domain adaptation"}, {"st": 101, "ed": 103, "text": "theoretical results"}, {"st": 105, "ed": 107, "text": "training procedure"}, {"st": 109, "ed": 111, "text": "adversarial networks"}]
[{"st": 14, "ed": 16, "text": "online fashion"}, {"st": 24, "ed": 26, "text": "e commerce"}, {"st": 88, "ed": 90, "text": "handcrafted features"}, {"st": 103, "ed": 105, "text": "domains including"}, {"st": 115, "ed": 117, "text": "handcrafted features"}, {"st": 136, "ed": 138, "text": "feature representations"}, {"st": 174, "ed": 176, "text": "significant improvement"}]
[{"st": 15, "ed": 18, "text": "modern machine learning"}, {"st": 27, "ed": 29, "text": "machine learning"}, {"st": 55, "ed": 58, "text": "stochastic gradient descent"}, {"st": 63, "ed": 65, "text": "machine learning"}, {"st": 104, "ed": 106, "text": "update rule"}, {"st": 109, "ed": 111, "text": "linear combination"}, {"st": 119, "ed": 121, "text": "current approaches"}, {"st": 134, "ed": 136, "text": "update rule"}, {"st": 152, "ed": 154, "text": "update rule"}, {"st": 162, "ed": 164, "text": "d dimensional"}, {"st": 172, "ed": 175, "text": "o n 2"}]
[{"st": 0, "ed": 3, "text": "dance dance revolution"}, {"st": 106, "ed": 109, "text": "convolutional neural networks"}, {"st": 113, "ed": 115, "text": "low level"}, {"st": 132, "ed": 134, "text": "generative model"}, {"st": 137, "ed": 139, "text": "n gram"}]
[{"st": 5, "ed": 7, "text": "cerebral cortex"}, {"st": 11, "ed": 13, "text": "multiple sources"}, {"st": 21, "ed": 24, "text": "input and output"}, {"st": 147, "ed": 149, "text": "spatio temporal"}]
[{"st": 8, "ed": 11, "text": "partial differential equations"}, {"st": 15, "ed": 17, "text": "differential equations"}, {"st": 29, "ed": 31, "text": "reinforcement learning"}, {"st": 46, "ed": 48, "text": "loss function"}, {"st": 71, "ed": 73, "text": "neural network"}, {"st": 80, "ed": 82, "text": "numerical results"}]
[{"st": 9, "ed": 11, "text": "neural network"}, {"st": 14, "ed": 16, "text": "local minima"}, {"st": 37, "ed": 39, "text": "local minima"}]
[{"st": 5, "ed": 8, "text": "non convex optimization"}, {"st": 13, "ed": 15, "text": "stationary point"}, {"st": 19, "ed": 21, "text": "local minimum"}, {"st": 78, "ed": 80, "text": "local minima"}]
[{"st": 5, "ed": 7, "text": "neural network"}, {"st": 8, "ed": 10, "text": "hidden neurons"}, {"st": 35, "ed": 37, "text": "local learning"}, {"st": 49, "ed": 51, "text": "hidden layers"}, {"st": 126, "ed": 128, "text": "network architecture"}]
[{"st": 4, "ed": 6, "text": "neural architectures"}, {"st": 46, "ed": 48, "text": "algebraic topology"}, {"st": 99, "ed": 101, "text": "empirical analysis"}, {"st": 109, "ed": 111, "text": "neural networks"}, {"st": 133, "ed": 135, "text": "fully connected"}]
[{"st": 27, "ed": 30, "text": "latent semantic analysis"}, {"st": 33, "ed": 35, "text": "difficult task"}, {"st": 41, "ed": 43, "text": "multiple choice"}, {"st": 72, "ed": 74, "text": "fine tune"}, {"st": 80, "ed": 82, "text": "multiple choice"}, {"st": 124, "ed": 126, "text": "training data"}, {"st": 143, "ed": 145, "text": "multiple choice"}]
[{"st": 90, "ed": 92, "text": "efficient inference"}, {"st": 134, "ed": 136, "text": "disjoint sets"}, {"st": 192, "ed": 194, "text": "training set"}]
[{"st": 1, "ed": 3, "text": "prediction problems"}, {"st": 5, "ed": 7, "text": "imitation learning"}, {"st": 27, "ed": 29, "text": "poor performance"}, {"st": 36, "ed": 38, "text": "recent approaches"}, {"st": 52, "ed": 54, "text": "non stationary"}, {"st": 71, "ed": 73, "text": "iterative algorithm"}, {"st": 77, "ed": 79, "text": "deterministic policy"}, {"st": 90, "ed": 92, "text": "online learning"}, {"st": 129, "ed": 131, "text": "approach outperforms"}, {"st": 131, "ed": 133, "text": "previous approaches"}, {"st": 136, "ed": 138, "text": "imitation learning"}]
[{"st": 53, "ed": 55, "text": "predictive performance"}, {"st": 67, "ed": 69, "text": "latent variable"}, {"st": 91, "ed": 94, "text": "spike and slab"}, {"st": 96, "ed": 99, "text": "spike and slab"}, {"st": 131, "ed": 134, "text": "spike and slab"}, {"st": 134, "ed": 136, "text": "bayesian methods"}, {"st": 169, "ed": 171, "text": "previously unseen"}]
[{"st": 24, "ed": 26, "text": "performance guarantees"}, {"st": 26, "ed": 28, "text": "existing methods"}, {"st": 46, "ed": 48, "text": "iterative method"}, {"st": 72, "ed": 75, "text": "online learning algorithm"}, {"st": 81, "ed": 83, "text": "near optimal"}, {"st": 89, "ed": 91, "text": "training error"}, {"st": 103, "ed": 106, "text": "discrete and continuous"}]
[{"st": 3, "ed": 5, "text": "structural dynamics"}, {"st": 7, "ed": 9, "text": "large scale"}, {"st": 25, "ed": 27, "text": "connectivity patterns"}, {"st": 41, "ed": 43, "text": "automatically learn"}, {"st": 44, "ed": 46, "text": "structural dynamics"}, {"st": 126, "ed": 128, "text": "non stationary"}, {"st": 157, "ed": 159, "text": "connectivity patterns"}, {"st": 161, "ed": 163, "text": "non stationary"}, {"st": 170, "ed": 172, "text": "experiments demonstrate"}]
[{"st": 11, "ed": 13, "text": "reinforcement learning"}, {"st": 31, "ed": 33, "text": "prior distribution"}, {"st": 66, "ed": 68, "text": "posterior distribution"}, {"st": 97, "ed": 99, "text": "existing methods"}, {"st": 106, "ed": 108, "text": "unlike existing"}, {"st": 124, "ed": 126, "text": "sample complexity"}, {"st": 127, "ed": 129, "text": "empirically demonstrate"}]
[{"st": 6, "ed": 8, "text": "bayesian network"}, {"st": 20, "ed": 22, "text": "bayesian network"}, {"st": 28, "ed": 30, "text": "posterior probabilities"}, {"st": 46, "ed": 48, "text": "empirical results"}, {"st": 53, "ed": 57, "text": "real and synthetic data"}, {"st": 62, "ed": 64, "text": "method outperforms"}, {"st": 65, "ed": 67, "text": "model selection"}]
[{"st": 80, "ed": 82, "text": "observational data"}]
[{"st": 10, "ed": 12, "text": "learning rates"}, {"st": 13, "ed": 16, "text": "stochastic gradient descent"}, {"st": 28, "ed": 30, "text": "learning rates"}, {"st": 37, "ed": 39, "text": "learning rates"}, {"st": 43, "ed": 45, "text": "non stationary"}, {"st": 71, "ed": 73, "text": "loss functions"}, {"st": 91, "ed": 93, "text": "finite difference"}, {"st": 102, "ed": 104, "text": "linear complexity"}, {"st": 106, "ed": 108, "text": "hyper parameter"}]
[{"st": 0, "ed": 2, "text": "recently developed"}, {"st": 11, "ed": 13, "text": "probability density"}, {"st": 17, "ed": 19, "text": "low dimensional"}, {"st": 36, "ed": 38, "text": "em algorithm"}, {"st": 58, "ed": 60, "text": "low dimensional"}, {"st": 76, "ed": 78, "text": "joint probability"}, {"st": 93, "ed": 95, "text": "discrete variables"}, {"st": 96, "ed": 98, "text": "continuous variables"}, {"st": 114, "ed": 117, "text": "networks from data"}]
[{"st": 8, "ed": 10, "text": "optimization problem"}, {"st": 33, "ed": 35, "text": "optimization problem"}, {"st": 42, "ed": 44, "text": "search space"}, {"st": 45, "ed": 47, "text": "extremely large"}, {"st": 95, "ed": 97, "text": "faster learning"}, {"st": 103, "ed": 105, "text": "iterative algorithm"}, {"st": 149, "ed": 152, "text": "synthetic and real"}, {"st": 160, "ed": 162, "text": "significantly faster"}]
[{"st": 6, "ed": 8, "text": "variational approximation"}, {"st": 10, "ed": 12, "text": "logistic function"}, {"st": 14, "ed": 16, "text": "approximate inference"}, {"st": 29, "ed": 31, "text": "logistic function"}, {"st": 36, "ed": 38, "text": "exact inference"}, {"st": 43, "ed": 45, "text": "variational parameters"}, {"st": 91, "ed": 93, "text": "significant speedup"}, {"st": 96, "ed": 98, "text": "discrete variables"}]
[{"st": 0, "ed": 2, "text": "recent advances"}, {"st": 4, "ed": 6, "text": "reinforcement learning"}, {"st": 31, "ed": 33, "text": "multi agent"}, {"st": 101, "ed": 103, "text": "parametric models"}, {"st": 109, "ed": 111, "text": "domain knowledge"}, {"st": 117, "ed": 119, "text": "fine grained"}, {"st": 120, "ed": 122, "text": "compact representation"}, {"st": 128, "ed": 130, "text": "empirical evaluation"}, {"st": 133, "ed": 135, "text": "approach outperforms"}, {"st": 136, "ed": 138, "text": "multi agent"}, {"st": 138, "ed": 140, "text": "reinforcement learning"}]
[{"st": 2, "ed": 4, "text": "bayesian network"}, {"st": 9, "ed": 11, "text": "np hard"}, {"st": 24, "ed": 26, "text": "network structures"}, {"st": 41, "ed": 43, "text": "structure learning"}, {"st": 47, "ed": 49, "text": "local learning"}, {"st": 88, "ed": 90, "text": "score based"}, {"st": 90, "ed": 92, "text": "local learning"}, {"st": 101, "ed": 103, "text": "theoretically sound"}, {"st": 117, "ed": 119, "text": "empirical results"}, {"st": 128, "ed": 130, "text": "constraint based"}, {"st": 140, "ed": 142, "text": "network structure"}]
[{"st": 3, "ed": 5, "text": "recently proposed"}, {"st": 58, "ed": 60, "text": "fixed point"}, {"st": 63, "ed": 65, "text": "convex optimization"}, {"st": 80, "ed": 82, "text": "fixed point"}, {"st": 86, "ed": 88, "text": "fixed point"}, {"st": 102, "ed": 104, "text": "fixed points"}, {"st": 116, "ed": 118, "text": "em algorithm"}, {"st": 119, "ed": 122, "text": "takes advantage of"}]
[{"st": 2, "ed": 4, "text": "theoretical bounds"}, {"st": 16, "ed": 19, "text": "sequential decision making"}, {"st": 20, "ed": 22, "text": "unlike previous"}, {"st": 42, "ed": 44, "text": "decision maker"}, {"st": 85, "ed": 87, "text": "provide theoretical"}]
[{"st": 9, "ed": 11, "text": "receptive fields"}, {"st": 13, "ed": 15, "text": "deep learning"}, {"st": 37, "ed": 39, "text": "natural image"}, {"st": 69, "ed": 71, "text": "deep learning"}, {"st": 77, "ed": 79, "text": "learning paradigm"}, {"st": 84, "ed": 87, "text": "restricted boltzmann machine"}]
[{"st": 14, "ed": 16, "text": "complex data"}, {"st": 31, "ed": 33, "text": "latent factors"}, {"st": 56, "ed": 58, "text": "scales linearly"}]
[{"st": 12, "ed": 14, "text": "pairwise comparisons"}, {"st": 37, "ed": 39, "text": "similarity matrix"}, {"st": 40, "ed": 42, "text": "pairwise comparisons"}, {"st": 67, "ed": 69, "text": "pairwise comparisons"}, {"st": 88, "ed": 90, "text": "pairwise comparisons"}, {"st": 139, "ed": 141, "text": "semi supervised"}, {"st": 146, "ed": 149, "text": "synthetic and real"}]
[{"st": 4, "ed": 6, "text": "pac bayes"}, {"st": 10, "ed": 12, "text": "generalization performance"}, {"st": 13, "ed": 15, "text": "multi view"}, {"st": 109, "ed": 111, "text": "unlabeled data"}, {"st": 120, "ed": 122, "text": "semi supervised"}, {"st": 122, "ed": 124, "text": "multi view"}, {"st": 130, "ed": 132, "text": "multi view"}, {"st": 132, "ed": 134, "text": "pac bayes"}, {"st": 145, "ed": 147, "text": "pac bayes"}, {"st": 154, "ed": 156, "text": "multi view"}]
[{"st": 25, "ed": 28, "text": "generative and discriminative"}, {"st": 33, "ed": 35, "text": "kernel learning"}, {"st": 51, "ed": 53, "text": "base kernels"}, {"st": 76, "ed": 78, "text": "empirical results"}, {"st": 82, "ed": 84, "text": "significantly improve"}, {"st": 90, "ed": 92, "text": "learning algorithm"}]
[{"st": 0, "ed": 2, "text": "hidden variables"}, {"st": 26, "ed": 28, "text": "machine learning"}, {"st": 36, "ed": 38, "text": "graphical model"}, {"st": 53, "ed": 56, "text": "directed acyclic graph"}, {"st": 119, "ed": 122, "text": "undirected graphical models"}, {"st": 131, "ed": 133, "text": "log linear"}]
[{"st": 22, "ed": 24, "text": "context dependent"}, {"st": 64, "ed": 66, "text": "outlier detection"}, {"st": 95, "ed": 97, "text": "multi dimensional"}, {"st": 97, "ed": 99, "text": "classification problem"}, {"st": 104, "ed": 106, "text": "output space"}, {"st": 136, "ed": 138, "text": "multi dimensional"}, {"st": 138, "ed": 140, "text": "classification problems"}]
[{"st": 7, "ed": 9, "text": "machine learning"}, {"st": 36, "ed": 38, "text": "large scale"}, {"st": 65, "ed": 67, "text": "structured data"}, {"st": 83, "ed": 85, "text": "hinge loss"}, {"st": 85, "ed": 88, "text": "markov random fields"}, {"st": 88, "ed": 90, "text": "hl mrfs"}, {"st": 95, "ed": 98, "text": "probabilistic graphical model"}, {"st": 113, "ed": 116, "text": "probabilistic graphical models"}, {"st": 117, "ed": 119, "text": "fuzzy logic"}, {"st": 133, "ed": 135, "text": "hl mrfs"}, {"st": 150, "ed": 152, "text": "probabilistic programming"}, {"st": 155, "ed": 157, "text": "hl mrfs"}, {"st": 174, "ed": 176, "text": "most probable"}, {"st": 178, "ed": 180, "text": "map inference"}, {"st": 188, "ed": 190, "text": "convex optimization"}, {"st": 194, "ed": 196, "text": "message passing"}, {"st": 216, "ed": 218, "text": "hl mrfs"}, {"st": 233, "ed": 235, "text": "hl mrfs"}, {"st": 240, "ed": 242, "text": "structured data"}]
[{"st": 29, "ed": 31, "text": "max margin"}, {"st": 39, "ed": 41, "text": "max margin"}, {"st": 71, "ed": 73, "text": "parameter space"}, {"st": 94, "ed": 96, "text": "mixed integer"}, {"st": 96, "ed": 98, "text": "linear programming"}]
[{"st": 0, "ed": 3, "text": "human activity recognition"}, {"st": 5, "ed": 7, "text": "ubiquitous computing"}, {"st": 11, "ed": 13, "text": "deep learning"}, {"st": 23, "ed": 25, "text": "hand crafted"}, {"st": 25, "ed": 27, "text": "feature extraction"}, {"st": 36, "ed": 38, "text": "deep architectures"}, {"st": 77, "ed": 79, "text": "deep convolutional"}, {"st": 165, "ed": 167, "text": "deep learning"}]
[{"st": 5, "ed": 7, "text": "graphical model"}, {"st": 7, "ed": 10, "text": "inference and learning"}, {"st": 41, "ed": 43, "text": "deep architecture"}, {"st": 46, "ed": 50, "text": "sum product networks spns"}, {"st": 89, "ed": 91, "text": "graphical model"}, {"st": 117, "ed": 119, "text": "learning algorithms"}, {"st": 129, "ed": 132, "text": "inference and learning"}, {"st": 158, "ed": 160, "text": "deep networks"}]
[{"st": 110, "ed": 112, "text": "recent literature"}]
[{"st": 0, "ed": 3, "text": "determinantal point processes"}, {"st": 7, "ed": 9, "text": "random matrix"}, {"st": 11, "ed": 13, "text": "quantum physics"}, {"st": 33, "ed": 35, "text": "exact inference"}, {"st": 37, "ed": 39, "text": "marginal probabilities"}, {"st": 44, "ed": 46, "text": "open question"}, {"st": 64, "ed": 66, "text": "feature based"}, {"st": 79, "ed": 81, "text": "efficient learning"}, {"st": 91, "ed": 94, "text": "markov random fields"}, {"st": 129, "ed": 131, "text": "important information"}, {"st": 186, "ed": 188, "text": "document summarization"}]
[{"st": 9, "ed": 13, "text": "multi armed bandit problems"}, {"st": 40, "ed": 42, "text": "main result"}]
[{"st": 10, "ed": 12, "text": "causal relationships"}, {"st": 21, "ed": 23, "text": "causal models"}, {"st": 30, "ed": 32, "text": "generating process"}, {"st": 44, "ed": 46, "text": "existing methods"}, {"st": 71, "ed": 74, "text": "independent component analysis"}, {"st": 112, "ed": 114, "text": "previous methods"}, {"st": 144, "ed": 146, "text": "empirically demonstrate"}]
[{"st": 3, "ed": 5, "text": "reinforcement learning"}, {"st": 24, "ed": 26, "text": "exploration exploitation"}, {"st": 72, "ed": 74, "text": "sample complexity"}, {"st": 83, "ed": 85, "text": "near optimal"}]
[{"st": 1, "ed": 3, "text": "optimal control"}, {"st": 6, "ed": 9, "text": "inverse reinforcement learning"}, {"st": 11, "ed": 14, "text": "problem of recovering"}, {"st": 16, "ed": 18, "text": "reward function"}, {"st": 20, "ed": 23, "text": "markov decision process"}, {"st": 24, "ed": 26, "text": "expert demonstrations"}, {"st": 35, "ed": 37, "text": "optimal control"}, {"st": 66, "ed": 68, "text": "reward function"}, {"st": 79, "ed": 81, "text": "globally optimal"}]
[{"st": 3, "ed": 5, "text": "machine learning"}]
[]
[{"st": 54, "ed": 56, "text": "worst case"}, {"st": 61, "ed": 63, "text": "true distribution"}, {"st": 85, "ed": 87, "text": "conjugate prior"}, {"st": 93, "ed": 95, "text": "sufficient statistics"}, {"st": 98, "ed": 100, "text": "marginal probabilities"}, {"st": 108, "ed": 110, "text": "preliminary experiments"}]
[{"st": 0, "ed": 2, "text": "belief propagation"}, {"st": 9, "ed": 11, "text": "approximate inference"}, {"st": 129, "ed": 133, "text": "synthetic and real world"}]
[{"st": 4, "ed": 7, "text": "probabilistic graphical model"}, {"st": 33, "ed": 35, "text": "active learning"}, {"st": 52, "ed": 54, "text": "resource allocation"}]
[{"st": 26, "ed": 29, "text": "statistical relational learning"}, {"st": 44, "ed": 46, "text": "latent structure"}, {"st": 59, "ed": 61, "text": "latent structure"}, {"st": 93, "ed": 95, "text": "real world"}]
[{"st": 35, "ed": 37, "text": "variational principle"}]
[{"st": 20, "ed": 22, "text": "partially observable"}, {"st": 64, "ed": 66, "text": "action selection"}, {"st": 96, "ed": 98, "text": "accurate estimates"}]
[{"st": 24, "ed": 26, "text": "weak interaction"}]
[{"st": 0, "ed": 2, "text": "traditional approaches"}, {"st": 3, "ed": 5, "text": "bayes net"}, {"st": 5, "ed": 7, "text": "structure learning"}, {"st": 7, "ed": 9, "text": "typically assume"}, {"st": 27, "ed": 29, "text": "real world"}, {"st": 50, "ed": 52, "text": "prior knowledge"}, {"st": 54, "ed": 56, "text": "hierarchical bayesian"}, {"st": 62, "ed": 64, "text": "structure learning"}, {"st": 75, "ed": 77, "text": "generative model"}, {"st": 85, "ed": 87, "text": "bayes net"}, {"st": 103, "ed": 105, "text": "prior probability"}, {"st": 139, "ed": 141, "text": "bayes net"}]
[{"st": 32, "ed": 34, "text": "graphical model"}, {"st": 45, "ed": 47, "text": "latent variables"}, {"st": 58, "ed": 60, "text": "foreign key"}, {"st": 61, "ed": 63, "text": "experiments demonstrate"}, {"st": 74, "ed": 78, "text": "synthetic and real world"}]
[{"st": 3, "ed": 5, "text": "machine learning"}, {"st": 8, "ed": 10, "text": "widely studied"}, {"st": 16, "ed": 18, "text": "cross validation"}, {"st": 22, "ed": 24, "text": "unseen data"}, {"st": 37, "ed": 39, "text": "training data"}, {"st": 78, "ed": 80, "text": "machine learning"}, {"st": 85, "ed": 87, "text": "training set"}, {"st": 103, "ed": 105, "text": "training set"}, {"st": 132, "ed": 135, "text": "bias and variance"}, {"st": 193, "ed": 195, "text": "bias variance"}, {"st": 209, "ed": 211, "text": "machine learning"}, {"st": 234, "ed": 236, "text": "training samples"}, {"st": 247, "ed": 249, "text": "highly correlated"}, {"st": 278, "ed": 280, "text": "accurately estimate"}]
[{"st": 21, "ed": 23, "text": "marginal likelihood"}, {"st": 24, "ed": 26, "text": "incomplete data"}, {"st": 98, "ed": 100, "text": "synthetic data"}, {"st": 103, "ed": 105, "text": "naive bayes"}]
[{"st": 12, "ed": 14, "text": "marginal likelihood"}, {"st": 64, "ed": 66, "text": "hidden variables"}, {"st": 98, "ed": 100, "text": "naive bayes"}]
[{"st": 20, "ed": 22, "text": "random variables"}, {"st": 42, "ed": 44, "text": "non trivial"}, {"st": 63, "ed": 66, "text": "bias and variance"}, {"st": 76, "ed": 78, "text": "cross validation"}, {"st": 80, "ed": 82, "text": "significantly reduced"}, {"st": 90, "ed": 93, "text": "bias and variance"}, {"st": 97, "ed": 99, "text": "cross validation"}]
[{"st": 8, "ed": 10, "text": "recent studies"}, {"st": 17, "ed": 19, "text": "probability distributions"}, {"st": 23, "ed": 27, "text": "reproducing kernel hilbert spaces"}, {"st": 41, "ed": 44, "text": "maximum mean discrepancy"}, {"st": 49, "ed": 51, "text": "kernel based"}, {"st": 54, "ed": 56, "text": "substantial improvements"}, {"st": 58, "ed": 60, "text": "sample sizes"}]
[{"st": 14, "ed": 16, "text": "machine learning"}, {"st": 66, "ed": 68, "text": "feature selection"}]
[{"st": 26, "ed": 28, "text": "key challenge"}, {"st": 36, "ed": 38, "text": "web search"}, {"st": 45, "ed": 47, "text": "search engine"}, {"st": 82, "ed": 84, "text": "accurately estimate"}, {"st": 132, "ed": 134, "text": "causal inference"}, {"st": 137, "ed": 139, "text": "contextual bandit"}, {"st": 177, "ed": 179, "text": "search engine"}, {"st": 192, "ed": 194, "text": "promising results"}, {"st": 197, "ed": 199, "text": "wide applicability"}]
[{"st": 35, "ed": 37, "text": "low dimensional"}, {"st": 46, "ed": 48, "text": "existing methods"}, {"st": 48, "ed": 50, "text": "perform poorly"}, {"st": 69, "ed": 71, "text": "high dimensional"}, {"st": 81, "ed": 83, "text": "low dimensional"}, {"st": 95, "ed": 97, "text": "high dimensional"}, {"st": 110, "ed": 112, "text": "based method"}, {"st": 120, "ed": 122, "text": "proposed approach"}, {"st": 127, "ed": 129, "text": "low dimensional"}, {"st": 139, "ed": 141, "text": "high dimensional"}, {"st": 166, "ed": 168, "text": "experiment results"}, {"st": 171, "ed": 173, "text": "proposed method"}, {"st": 184, "ed": 186, "text": "extremely high"}, {"st": 186, "ed": 188, "text": "dimensional space"}, {"st": 192, "ed": 194, "text": "low dimensional"}, {"st": 198, "ed": 200, "text": "previously proposed"}]
[{"st": 1, "ed": 5, "text": "maximum mean discrepancy mmd"}, {"st": 7, "ed": 9, "text": "recently proposed"}, {"st": 25, "ed": 27, "text": "large scale"}, {"st": 40, "ed": 42, "text": "method called"}, {"st": 64, "ed": 66, "text": "linear combination"}, {"st": 81, "ed": 84, "text": "taking advantage of"}, {"st": 97, "ed": 100, "text": "o n 2"}, {"st": 126, "ed": 128, "text": "basis functions"}, {"st": 165, "ed": 167, "text": "uniform convergence"}, {"st": 226, "ed": 228, "text": "similar accuracy"}]
[{"st": 61, "ed": 63, "text": "error rate"}, {"st": 78, "ed": 80, "text": "optimization problem"}, {"st": 88, "ed": 90, "text": "empirical results"}, {"st": 92, "ed": 96, "text": "simulated and real world"}]
[{"st": 1, "ed": 3, "text": "collaborative filtering"}, {"st": 4, "ed": 6, "text": "content based"}, {"st": 53, "ed": 55, "text": "clustering technique"}, {"st": 60, "ed": 62, "text": "exploration exploitation"}, {"st": 65, "ed": 68, "text": "multi armed bandit"}, {"st": 71, "ed": 74, "text": "takes into account"}, {"st": 121, "ed": 124, "text": "takes advantage of"}, {"st": 134, "ed": 136, "text": "collaborative filtering"}, {"st": 140, "ed": 142, "text": "empirical analysis"}, {"st": 145, "ed": 147, "text": "real world"}, {"st": 152, "ed": 154, "text": "prediction performance"}, {"st": 157, "ed": 160, "text": "click through rate"}]
[{"st": 13, "ed": 15, "text": "machine learning"}, {"st": 79, "ed": 81, "text": "sample complexity"}, {"st": 96, "ed": 98, "text": "budget constraint"}, {"st": 104, "ed": 106, "text": "error bounds"}, {"st": 155, "ed": 157, "text": "budget constraint"}, {"st": 161, "ed": 163, "text": "regret bounds"}]
[{"st": 4, "ed": 6, "text": "anomaly detection"}, {"st": 161, "ed": 163, "text": "main contributions"}, {"st": 170, "ed": 172, "text": "large scale"}, {"st": 172, "ed": 174, "text": "quantitative evaluations"}, {"st": 190, "ed": 192, "text": "anomaly detection"}, {"st": 194, "ed": 196, "text": "real data"}, {"st": 223, "ed": 225, "text": "anomaly detection"}]
[{"st": 5, "ed": 7, "text": "meta analysis"}, {"st": 9, "ed": 11, "text": "anomaly detection"}, {"st": 21, "ed": 23, "text": "anomaly detection"}, {"st": 33, "ed": 35, "text": "anomaly detection"}, {"st": 48, "ed": 50, "text": "real world"}, {"st": 74, "ed": 76, "text": "anomaly detection"}, {"st": 128, "ed": 131, "text": "statistical hypothesis testing"}, {"st": 164, "ed": 167, "text": "strengths and weaknesses"}, {"st": 207, "ed": 209, "text": "anomaly detection"}, {"st": 216, "ed": 218, "text": "anomaly detection"}]
[{"st": 2, "ed": 4, "text": "network structure"}, {"st": 24, "ed": 26, "text": "scale free"}, {"st": 94, "ed": 99, "text": "alternating direction method of multipliers"}, {"st": 104, "ed": 106, "text": "graphical model"}, {"st": 114, "ed": 118, "text": "synthetic and real data"}, {"st": 126, "ed": 129, "text": "scale free network"}, {"st": 143, "ed": 145, "text": "scale free"}]
[{"st": 3, "ed": 6, "text": "problem of recovering"}, {"st": 7, "ed": 10, "text": "low rank tensor"}, {"st": 22, "ed": 26, "text": "signal to noise ratio"}, {"st": 112, "ed": 114, "text": "o sqrt"}, {"st": 117, "ed": 119, "text": "k 1"}, {"st": 136, "ed": 138, "text": "nuclear norm"}, {"st": 141, "ed": 143, "text": "empirically demonstrate"}]
[{"st": 1, "ed": 3, "text": "map inference"}, {"st": 5, "ed": 8, "text": "determinantal point processes"}, {"st": 31, "ed": 33, "text": "real world"}, {"st": 53, "ed": 55, "text": "kernel matrix"}, {"st": 67, "ed": 69, "text": "successfully applied"}, {"st": 81, "ed": 84, "text": "change point detection"}, {"st": 93, "ed": 95, "text": "method named"}, {"st": 102, "ed": 104, "text": "change point"}, {"st": 116, "ed": 118, "text": "change point"}, {"st": 153, "ed": 155, "text": "extensive experiments"}, {"st": 157, "ed": 159, "text": "real world"}]
[{"st": 22, "ed": 25, "text": "markov decision processes"}, {"st": 29, "ed": 31, "text": "transfer learning"}, {"st": 38, "ed": 40, "text": "low dimensional"}, {"st": 67, "ed": 69, "text": "training procedure"}, {"st": 108, "ed": 110, "text": "personalized medicine"}]
[{"st": 1, "ed": 4, "text": "ability to learn"}, {"st": 17, "ed": 19, "text": "neural networks"}, {"st": 33, "ed": 35, "text": "catastrophic forgetting"}, {"st": 100, "ed": 102, "text": "classification tasks"}, {"st": 106, "ed": 108, "text": "hand written"}]
[{"st": 12, "ed": 14, "text": "complex tasks"}, {"st": 31, "ed": 33, "text": "policy search"}, {"st": 119, "ed": 121, "text": "policy search"}, {"st": 128, "ed": 130, "text": "active learning"}, {"st": 131, "ed": 133, "text": "preliminary results"}]
[{"st": 29, "ed": 31, "text": "existing approaches"}, {"st": 35, "ed": 37, "text": "feature engineering"}, {"st": 41, "ed": 43, "text": "network embedding"}, {"st": 65, "ed": 67, "text": "recent studies"}, {"st": 68, "ed": 70, "text": "network embedding"}, {"st": 76, "ed": 78, "text": "embedding methods"}, {"st": 156, "ed": 158, "text": "network embedding"}, {"st": 169, "ed": 171, "text": "latent feature"}, {"st": 177, "ed": 179, "text": "jointly trained"}, {"st": 181, "ed": 183, "text": "task specific"}, {"st": 192, "ed": 194, "text": "network embedding"}, {"st": 216, "ed": 218, "text": "network embedding"}, {"st": 223, "ed": 225, "text": "joint training"}, {"st": 232, "ed": 234, "text": "experiments demonstrate"}, {"st": 239, "ed": 241, "text": "network embedding"}]
[{"st": 27, "ed": 29, "text": "hand engineered"}, {"st": 42, "ed": 45, "text": "stochastic gradient descent"}, {"st": 46, "ed": 50, "text": "multi layer neural networks"}, {"st": 61, "ed": 63, "text": "data stream"}, {"st": 93, "ed": 95, "text": "incremental learning"}, {"st": 103, "ed": 105, "text": "hidden units"}, {"st": 157, "ed": 159, "text": "feature representation"}]
[{"st": 42, "ed": 45, "text": "received much attention"}, {"st": 46, "ed": 48, "text": "named entity"}, {"st": 68, "ed": 70, "text": "named entity"}, {"st": 139, "ed": 142, "text": "simple yet effective"}, {"st": 189, "ed": 191, "text": "active learning"}, {"st": 210, "ed": 212, "text": "active learning"}]
[{"st": 12, "ed": 14, "text": "probabilistic models"}, {"st": 15, "ed": 17, "text": "latent variables"}, {"st": 20, "ed": 22, "text": "approximate inference"}, {"st": 37, "ed": 39, "text": "probabilistic inference"}, {"st": 63, "ed": 65, "text": "approximate inference"}, {"st": 92, "ed": 94, "text": "approximate inference"}]
[{"st": 6, "ed": 10, "text": "recurrent neural network rnn"}, {"st": 15, "ed": 17, "text": "partial observability"}, {"st": 19, "ed": 21, "text": "latent representation"}, {"st": 28, "ed": 30, "text": "supervised learning"}, {"st": 32, "ed": 34, "text": "reinforcement learning"}, {"st": 39, "ed": 41, "text": "policy gradient"}, {"st": 65, "ed": 67, "text": "rnn based"}, {"st": 72, "ed": 74, "text": "key idea"}, {"st": 91, "ed": 93, "text": "experience replay"}, {"st": 114, "ed": 116, "text": "policy gradient"}]
[{"st": 0, "ed": 2, "text": "data science"}, {"st": 17, "ed": 19, "text": "problems involving"}, {"st": 24, "ed": 26, "text": "data science"}, {"st": 38, "ed": 40, "text": "scientific knowledge"}, {"st": 45, "ed": 47, "text": "data science"}, {"st": 74, "ed": 76, "text": "interpretable models"}, {"st": 110, "ed": 112, "text": "quantum chemistry"}, {"st": 147, "ed": 149, "text": "domain knowledge"}]
[{"st": 6, "ed": 8, "text": "unified framework"}, {"st": 18, "ed": 20, "text": "probability distributions"}, {"st": 24, "ed": 26, "text": "optimization problem"}, {"st": 33, "ed": 35, "text": "parameter space"}, {"st": 49, "ed": 51, "text": "natural gradient"}, {"st": 93, "ed": 95, "text": "gaussian distributions"}, {"st": 106, "ed": 108, "text": "exponential families"}, {"st": 112, "ed": 114, "text": "cross entropy"}, {"st": 122, "ed": 124, "text": "theoretical justification"}, {"st": 132, "ed": 134, "text": "step size"}, {"st": 154, "ed": 156, "text": "objective function"}, {"st": 161, "ed": 163, "text": "step sizes"}, {"st": 180, "ed": 182, "text": "step sizes"}]
[{"st": 7, "ed": 10, "text": "directed acyclic graph"}, {"st": 28, "ed": 30, "text": "conditional probability"}, {"st": 48, "ed": 51, "text": "directed acyclic graph"}, {"st": 97, "ed": 99, "text": "marginal likelihood"}, {"st": 109, "ed": 111, "text": "prior distribution"}, {"st": 128, "ed": 132, "text": "markov chain monte carlo"}, {"st": 137, "ed": 139, "text": "hill climbing"}, {"st": 152, "ed": 156, "text": "real and synthetic data"}]
[{"st": 67, "ed": 69, "text": "minimization problems"}, {"st": 114, "ed": 116, "text": "problem instances"}, {"st": 136, "ed": 138, "text": "substantially improve"}]
[{"st": 0, "ed": 2, "text": "gaussian processes"}, {"st": 25, "ed": 27, "text": "gaussian processes"}, {"st": 40, "ed": 42, "text": "exact inference"}, {"st": 63, "ed": 65, "text": "large scale"}, {"st": 71, "ed": 73, "text": "problems including"}, {"st": 83, "ed": 85, "text": "significantly outperforms"}, {"st": 88, "ed": 90, "text": "gaussian process"}, {"st": 111, "ed": 113, "text": "exact inference"}, {"st": 117, "ed": 119, "text": "large scale"}]
[{"st": 3, "ed": 5, "text": "sparse coding"}, {"st": 7, "ed": 9, "text": "activity recognition"}, {"st": 12, "ed": 14, "text": "mobile computing"}, {"st": 17, "ed": 19, "text": "fundamental problems"}, {"st": 21, "ed": 23, "text": "supervised learning"}, {"st": 33, "ed": 35, "text": "feature representation"}, {"st": 44, "ed": 46, "text": "expert knowledge"}, {"st": 70, "ed": 72, "text": "ground truth"}, {"st": 78, "ed": 80, "text": "unlabeled data"}, {"st": 103, "ed": 105, "text": "learning paradigm"}, {"st": 116, "ed": 118, "text": "unlabeled data"}, {"st": 133, "ed": 135, "text": "feature space"}, {"st": 145, "ed": 147, "text": "feature extraction"}, {"st": 152, "ed": 154, "text": "feature representations"}, {"st": 182, "ed": 184, "text": "recognition tasks"}, {"st": 204, "ed": 206, "text": "sparse coding"}, {"st": 207, "ed": 209, "text": "significantly outperforms"}, {"st": 215, "ed": 217, "text": "supervised learning"}, {"st": 233, "ed": 235, "text": "generalization capabilities"}, {"st": 248, "ed": 250, "text": "feature learning"}, {"st": 250, "ed": 252, "text": "approach outperforms"}]
[{"st": 44, "ed": 46, "text": "sampling based"}, {"st": 56, "ed": 58, "text": "likelihood ratio"}, {"st": 73, "ed": 76, "text": "stochastic gradient descent"}, {"st": 98, "ed": 100, "text": "reinforcement learning"}]
[{"st": 10, "ed": 12, "text": "feature selection"}]
[{"st": 0, "ed": 2, "text": "kernel based"}, {"st": 2, "ed": 4, "text": "reinforcement learning"}, {"st": 8, "ed": 10, "text": "reinforcement learning"}, {"st": 24, "ed": 26, "text": "kernel approximation"}, {"st": 61, "ed": 63, "text": "computational cost"}, {"st": 68, "ed": 70, "text": "large scale"}, {"st": 87, "ed": 89, "text": "reinforcement learning"}, {"st": 90, "ed": 92, "text": "kernel based"}, {"st": 102, "ed": 104, "text": "transition matrix"}, {"st": 124, "ed": 126, "text": "transition matrix"}, {"st": 166, "ed": 169, "text": "takes into account"}, {"st": 182, "ed": 184, "text": "computational complexity"}, {"st": 232, "ed": 234, "text": "kernel based"}, {"st": 234, "ed": 236, "text": "reinforcement learning"}, {"st": 242, "ed": 244, "text": "large scale"}, {"st": 247, "ed": 249, "text": "off line"}, {"st": 283, "ed": 285, "text": "extensive empirical"}]
[{"st": 2, "ed": 4, "text": "bayesian nonparametric"}, {"st": 6, "ed": 8, "text": "machine learning"}, {"st": 48, "ed": 50, "text": "based methods"}, {"st": 60, "ed": 62, "text": "variational inference"}, {"st": 123, "ed": 125, "text": "error bounds"}, {"st": 134, "ed": 136, "text": "variational inference"}, {"st": 143, "ed": 145, "text": "nonparametric models"}, {"st": 160, "ed": 162, "text": "variational inference"}, {"st": 168, "ed": 170, "text": "latent structure"}, {"st": 180, "ed": 182, "text": "latent variables"}, {"st": 200, "ed": 203, "text": "nonnegative matrix factorization"}, {"st": 211, "ed": 213, "text": "compare favorably"}, {"st": 215, "ed": 217, "text": "sampling based"}]
[{"st": 23, "ed": 25, "text": "desirable properties"}, {"st": 52, "ed": 54, "text": "gaussian processes"}, {"st": 77, "ed": 79, "text": "highly scalable"}, {"st": 111, "ed": 113, "text": "probabilistic model"}]
[{"st": 50, "ed": 53, "text": "rates of convergence"}, {"st": 73, "ed": 75, "text": "empirical evaluation"}]
[{"st": 3, "ed": 5, "text": "causal relationships"}, {"st": 7, "ed": 9, "text": "observational data"}, {"st": 22, "ed": 24, "text": "causal discovery"}, {"st": 55, "ed": 57, "text": "vice versa"}, {"st": 77, "ed": 79, "text": "selection bias"}, {"st": 81, "ed": 83, "text": "causal discovery"}, {"st": 106, "ed": 108, "text": "additive noise"}, {"st": 113, "ed": 115, "text": "causal inference"}, {"st": 129, "ed": 131, "text": "cause effect"}, {"st": 152, "ed": 154, "text": "ground truth"}, {"st": 166, "ed": 168, "text": "causal discovery"}, {"st": 171, "ed": 173, "text": "real world"}, {"st": 183, "ed": 185, "text": "empirical results"}, {"st": 186, "ed": 189, "text": "real world data"}, {"st": 204, "ed": 206, "text": "observational data"}, {"st": 215, "ed": 217, "text": "statistically significant"}, {"st": 227, "ed": 229, "text": "additive noise"}, {"st": 252, "ed": 254, "text": "real world"}]
[{"st": 18, "ed": 20, "text": "recent results"}, {"st": 23, "ed": 25, "text": "cause effect"}, {"st": 52, "ed": 55, "text": "supervised machine learning"}, {"st": 103, "ed": 105, "text": "supervised learning"}]
[{"st": 37, "ed": 39, "text": "generalization capabilities"}, {"st": 72, "ed": 74, "text": "artificial intelligence"}, {"st": 75, "ed": 77, "text": "recently shown"}, {"st": 82, "ed": 84, "text": "reinforcement learning"}]
[{"st": 1, "ed": 3, "text": "max margin"}, {"st": 9, "ed": 11, "text": "practical applications"}, {"st": 13, "ed": 15, "text": "text categorization"}, {"st": 17, "ed": 19, "text": "social network"}, {"st": 19, "ed": 21, "text": "link prediction"}, {"st": 35, "ed": 37, "text": "max margin"}, {"st": 39, "ed": 41, "text": "monte carlo"}, {"st": 46, "ed": 48, "text": "remains challenging"}, {"st": 53, "ed": 55, "text": "large scale"}, {"st": 65, "ed": 67, "text": "monte carlo"}, {"st": 71, "ed": 74, "text": "easy to implement"}, {"st": 81, "ed": 83, "text": "detailed balance"}, {"st": 117, "ed": 121, "text": "markov chain monte carlo"}, {"st": 123, "ed": 125, "text": "efficiently solve"}, {"st": 126, "ed": 128, "text": "posterior inference"}, {"st": 132, "ed": 134, "text": "max margin"}, {"st": 136, "ed": 139, "text": "extensive experimental results"}]
[{"st": 1, "ed": 4, "text": "efficient and scalable"}, {"st": 56, "ed": 58, "text": "epsilon greedy"}, {"st": 66, "ed": 68, "text": "atari games"}, {"st": 83, "ed": 85, "text": "exploration strategies"}, {"st": 86, "ed": 88, "text": "thompson sampling"}, {"st": 118, "ed": 120, "text": "neural network"}]
[{"st": 5, "ed": 8, "text": "armed bandit problem"}, {"st": 10, "ed": 12, "text": "learning agent"}, {"st": 42, "ed": 44, "text": "real valued"}, {"st": 76, "ed": 78, "text": "sample complexity"}]
[{"st": 12, "ed": 14, "text": "great importance"}, {"st": 28, "ed": 30, "text": "ensemble learning"}, {"st": 74, "ed": 76, "text": "empirical evaluation"}, {"st": 106, "ed": 108, "text": "prediction tasks"}]
[{"st": 8, "ed": 10, "text": "approximate inference"}, {"st": 11, "ed": 14, "text": "undirected graphical models"}, {"st": 55, "ed": 57, "text": "mean field"}, {"st": 119, "ed": 121, "text": "empirical analysis"}]
[{"st": 44, "ed": 46, "text": "real applications"}, {"st": 51, "ed": 53, "text": "rademacher complexity"}, {"st": 67, "ed": 69, "text": "complexity bounds"}, {"st": 75, "ed": 77, "text": "generalization error"}]
[{"st": 10, "ed": 12, "text": "gaussian processes"}, {"st": 32, "ed": 35, "text": "gaussian processes gps"}, {"st": 58, "ed": 61, "text": "learning and inference"}, {"st": 67, "ed": 70, "text": "o n 2"}, {"st": 96, "ed": 98, "text": "multi level"}, {"st": 116, "ed": 118, "text": "significantly faster"}, {"st": 153, "ed": 155, "text": "fast inference"}, {"st": 169, "ed": 171, "text": "input space"}, {"st": 196, "ed": 198, "text": "large scale"}]
[{"st": 142, "ed": 144, "text": "min max"}, {"st": 170, "ed": 172, "text": "statistically significant"}, {"st": 193, "ed": 195, "text": "training examples"}]
[{"st": 7, "ed": 9, "text": "gaussian process"}, {"st": 18, "ed": 20, "text": "lipschitz continuous"}, {"st": 20, "ed": 22, "text": "reward functions"}, {"st": 26, "ed": 28, "text": "active learning"}, {"st": 30, "ed": 32, "text": "bayesian optimization"}, {"st": 55, "ed": 57, "text": "sequential decision"}, {"st": 65, "ed": 67, "text": "exploration exploitation"}, {"st": 89, "ed": 91, "text": "key contribution"}, {"st": 104, "ed": 106, "text": "reward functions"}, {"st": 127, "ed": 129, "text": "asymptotically optimal"}, {"st": 129, "ed": 132, "text": "branch and bound"}, {"st": 141, "ed": 143, "text": "empirically demonstrate"}]
[{"st": 3, "ed": 5, "text": "icu mortality"}, {"st": 16, "ed": 18, "text": "resource allocation"}, {"st": 26, "ed": 28, "text": "machine learning"}, {"st": 65, "ed": 67, "text": "logistic regression"}, {"st": 126, "ed": 128, "text": "existing models"}]
[{"st": 0, "ed": 3, "text": "gaussian processes gps"}, {"st": 9, "ed": 11, "text": "machine learning"}, {"st": 12, "ed": 14, "text": "computer vision"}, {"st": 56, "ed": 58, "text": "gaussian processes"}, {"st": 60, "ed": 62, "text": "higher order"}, {"st": 62, "ed": 64, "text": "probabilistic programming"}, {"st": 90, "ed": 92, "text": "gaussian processes"}, {"st": 97, "ed": 99, "text": "real valued"}, {"st": 119, "ed": 121, "text": "input output"}, {"st": 135, "ed": 137, "text": "gp regression"}, {"st": 139, "ed": 141, "text": "hyper parameter"}, {"st": 147, "ed": 149, "text": "time series"}, {"st": 151, "ed": 153, "text": "fully bayesian"}, {"st": 153, "ed": 155, "text": "structure learning"}]
[{"st": 35, "ed": 37, "text": "bayes optimal"}, {"st": 46, "ed": 50, "text": "partially observable markov decision"}, {"st": 53, "ed": 56, "text": "curse of dimensionality"}, {"st": 73, "ed": 75, "text": "computationally tractable"}, {"st": 75, "ed": 77, "text": "instance specific"}, {"st": 84, "ed": 86, "text": "bayes optimal"}]
[{"st": 2, "ed": 4, "text": "recommender systems"}, {"st": 40, "ed": 42, "text": "low rank"}, {"st": 59, "ed": 61, "text": "nuclear norm"}, {"st": 67, "ed": 69, "text": "rank approximation"}, {"st": 72, "ed": 74, "text": "optimization strategy"}, {"st": 82, "ed": 84, "text": "real datasets"}]
[{"st": 8, "ed": 10, "text": "off policy"}, {"st": 10, "ed": 12, "text": "multi step"}, {"st": 12, "ed": 15, "text": "temporal difference learning"}, {"st": 17, "ed": 19, "text": "off policy"}, {"st": 51, "ed": 53, "text": "off policy"}, {"st": 56, "ed": 58, "text": "policy evaluation"}, {"st": 88, "ed": 90, "text": "off policy"}, {"st": 101, "ed": 103, "text": "continuous state"}]
[{"st": 88, "ed": 90, "text": "distance measures"}]
[{"st": 3, "ed": 5, "text": "artificial intelligence"}, {"st": 36, "ed": 38, "text": "non trivial"}, {"st": 47, "ed": 49, "text": "learning parameters"}, {"st": 63, "ed": 65, "text": "meta learning"}, {"st": 79, "ed": 81, "text": "learning parameters"}, {"st": 93, "ed": 95, "text": "recently developed"}, {"st": 97, "ed": 99, "text": "artificial intelligence"}, {"st": 106, "ed": 108, "text": "meta learning"}, {"st": 109, "ed": 111, "text": "reinforcement learning"}, {"st": 120, "ed": 122, "text": "random walk"}, {"st": 130, "ed": 132, "text": "meta learning"}, {"st": 196, "ed": 198, "text": "reinforcement learning"}, {"st": 218, "ed": 220, "text": "near optimal"}]
[{"st": 0, "ed": 2, "text": "off policy"}, {"st": 2, "ed": 4, "text": "reinforcement learning"}, {"st": 6, "ed": 8, "text": "applications including"}, {"st": 8, "ed": 11, "text": "learning from demonstration"}, {"st": 30, "ed": 32, "text": "policy evaluation"}, {"st": 40, "ed": 42, "text": "reinforcement learning"}, {"st": 45, "ed": 47, "text": "off policy"}, {"st": 50, "ed": 52, "text": "linear complexity"}, {"st": 53, "ed": 56, "text": "temporal difference td"}, {"st": 70, "ed": 72, "text": "policy evaluation"}, {"st": 86, "ed": 88, "text": "empirical comparison"}]
[{"st": 9, "ed": 11, "text": "parametric models"}, {"st": 12, "ed": 14, "text": "machine learning"}, {"st": 16, "ed": 18, "text": "response variable"}, {"st": 22, "ed": 24, "text": "monotonic function"}, {"st": 26, "ed": 28, "text": "linear combination"}, {"st": 42, "ed": 44, "text": "nonlinear function"}, {"st": 59, "ed": 61, "text": "low dimensional"}, {"st": 66, "ed": 68, "text": "efficiently learn"}, {"st": 86, "ed": 88, "text": "computationally efficient"}, {"st": 104, "ed": 106, "text": "group sparsity"}, {"st": 107, "ed": 109, "text": "low rank"}, {"st": 116, "ed": 118, "text": "proposed method"}, {"st": 120, "ed": 122, "text": "predictive performance"}, {"st": 125, "ed": 128, "text": "generalized linear models"}, {"st": 138, "ed": 141, "text": "feedforward neural networks"}]
[{"st": 50, "ed": 52, "text": "real world"}, {"st": 103, "ed": 105, "text": "bandit problem"}, {"st": 128, "ed": 131, "text": "upper confidence bound"}, {"st": 134, "ed": 136, "text": "theoretical analysis"}, {"st": 171, "ed": 174, "text": "synthetic and real"}]
[{"st": 3, "ed": 5, "text": "great promise"}, {"st": 14, "ed": 16, "text": "posterior sampling"}, {"st": 18, "ed": 20, "text": "differential privacy"}, {"st": 77, "ed": 79, "text": "differential privacy"}, {"st": 86, "ed": 88, "text": "posterior inference"}, {"st": 115, "ed": 117, "text": "time series"}]
[{"st": 27, "ed": 29, "text": "social choice"}, {"st": 31, "ed": 33, "text": "natural language"}, {"st": 43, "ed": 45, "text": "rank aggregation"}, {"st": 47, "ed": 49, "text": "ground truth"}, {"st": 84, "ed": 86, "text": "unsupervised manner"}, {"st": 89, "ed": 91, "text": "challenging problem"}, {"st": 94, "ed": 96, "text": "based methods"}, {"st": 124, "ed": 126, "text": "rank aggregation"}, {"st": 168, "ed": 170, "text": "synthetic datasets"}, {"st": 191, "ed": 193, "text": "real datasets"}, {"st": 205, "ed": 207, "text": "significant improvement"}, {"st": 211, "ed": 213, "text": "rank aggregation"}]
[{"st": 0, "ed": 2, "text": "link prediction"}, {"st": 36, "ed": 38, "text": "machine learning"}, {"st": 57, "ed": 59, "text": "previous methods"}, {"st": 62, "ed": 64, "text": "empirical results"}]
[{"st": 2, "ed": 4, "text": "real world"}, {"st": 6, "ed": 8, "text": "reinforcement learning"}, {"st": 10, "ed": 12, "text": "partially observable"}, {"st": 13, "ed": 15, "text": "efficient learning"}, {"st": 22, "ed": 24, "text": "sample complexity"}, {"st": 26, "ed": 28, "text": "partially observable"}, {"st": 44, "ed": 46, "text": "partially observable"}, {"st": 64, "ed": 66, "text": "near optimal"}, {"st": 82, "ed": 84, "text": "recent advances"}, {"st": 85, "ed": 88, "text": "method of moments"}, {"st": 89, "ed": 91, "text": "latent variable"}]
[{"st": 7, "ed": 9, "text": "el nino"}, {"st": 23, "ed": 25, "text": "feature learning"}, {"st": 79, "ed": 81, "text": "el nino"}, {"st": 128, "ed": 130, "text": "low dimensional"}]
[{"st": 1, "ed": 3, "text": "learning rate"}, {"st": 55, "ed": 57, "text": "learning rate"}, {"st": 61, "ed": 63, "text": "key idea"}, {"st": 77, "ed": 79, "text": "neural networks"}, {"st": 83, "ed": 86, "text": "efficient and effective"}]
[{"st": 6, "ed": 8, "text": "key challenge"}, {"st": 9, "ed": 11, "text": "reinforcement learning"}, {"st": 24, "ed": 26, "text": "discrete state"}, {"st": 37, "ed": 39, "text": "deep rl"}, {"st": 51, "ed": 53, "text": "epsilon greedy"}, {"st": 56, "ed": 58, "text": "gaussian noise"}, {"st": 70, "ed": 72, "text": "exploration strategy"}, {"st": 92, "ed": 94, "text": "variational inference"}, {"st": 96, "ed": 98, "text": "neural networks"}, {"st": 101, "ed": 103, "text": "continuous state"}, {"st": 110, "ed": 112, "text": "reward function"}, {"st": 139, "ed": 142, "text": "continuous control tasks"}]
[{"st": 2, "ed": 4, "text": "reinforcement learning"}, {"st": 22, "ed": 24, "text": "safety critical"}, {"st": 49, "ed": 52, "text": "markov decision processes"}, {"st": 92, "ed": 94, "text": "gaussian process"}, {"st": 131, "ed": 134, "text": "states and actions"}, {"st": 149, "ed": 151, "text": "noisy observations"}]
[{"st": 22, "ed": 24, "text": "confidence interval"}, {"st": 37, "ed": 39, "text": "current methods"}, {"st": 43, "ed": 45, "text": "off policy"}, {"st": 48, "ed": 50, "text": "importance sampling"}, {"st": 63, "ed": 65, "text": "model based"}, {"st": 71, "ed": 73, "text": "discrete state"}, {"st": 102, "ed": 104, "text": "off policy"}, {"st": 117, "ed": 119, "text": "confidence bounds"}, {"st": 123, "ed": 125, "text": "limited data"}, {"st": 127, "ed": 130, "text": "continuous and discrete"}, {"st": 154, "ed": 156, "text": "transition function"}, {"st": 171, "ed": 173, "text": "model based"}, {"st": 179, "ed": 181, "text": "empirically evaluate"}, {"st": 192, "ed": 194, "text": "off policy"}, {"st": 194, "ed": 196, "text": "confidence interval"}]
[{"st": 0, "ed": 2, "text": "constraint based"}, {"st": 2, "ed": 4, "text": "causal discovery"}, {"st": 5, "ed": 7, "text": "limited data"}, {"st": 9, "ed": 11, "text": "notoriously difficult"}, {"st": 42, "ed": 44, "text": "existing approaches"}, {"st": 67, "ed": 69, "text": "search space"}, {"st": 73, "ed": 75, "text": "coarse grained"}, {"st": 139, "ed": 141, "text": "synthetic data"}]
[{"st": 39, "ed": 41, "text": "time series"}, {"st": 82, "ed": 84, "text": "prediction models"}, {"st": 86, "ed": 88, "text": "prediction errors"}, {"st": 95, "ed": 98, "text": "short term memory"}, {"st": 100, "ed": 102, "text": "encoder decoder"}, {"st": 104, "ed": 106, "text": "anomaly detection"}, {"st": 113, "ed": 115, "text": "time series"}, {"st": 132, "ed": 134, "text": "time series"}, {"st": 137, "ed": 139, "text": "space shuttle"}, {"st": 143, "ed": 145, "text": "real world"}, {"st": 187, "ed": 189, "text": "time series"}, {"st": 198, "ed": 200, "text": "time series"}]
[{"st": 13, "ed": 15, "text": "probabilistic models"}, {"st": 45, "ed": 47, "text": "local models"}, {"st": 77, "ed": 79, "text": "local models"}, {"st": 126, "ed": 128, "text": "statistically efficient"}, {"st": 132, "ed": 135, "text": "theoretical and empirical"}]
[{"st": 12, "ed": 14, "text": "deep learning"}, {"st": 28, "ed": 30, "text": "machine learning"}, {"st": 35, "ed": 37, "text": "surrogate models"}, {"st": 38, "ed": 40, "text": "gaussian processes"}, {"st": 45, "ed": 47, "text": "validation error"}, {"st": 55, "ed": 57, "text": "accurate estimates"}, {"st": 58, "ed": 60, "text": "sufficient statistics"}, {"st": 86, "ed": 88, "text": "deep learning"}, {"st": 105, "ed": 107, "text": "hyperparameter optimization"}, {"st": 110, "ed": 112, "text": "radial basis"}, {"st": 118, "ed": 120, "text": "mixed integer"}, {"st": 159, "ed": 163, "text": "mnist and cifar 10"}, {"st": 165, "ed": 168, "text": "deep neural networks"}, {"st": 170, "ed": 172, "text": "significantly outperforms"}, {"st": 175, "ed": 177, "text": "bayesian optimization"}, {"st": 193, "ed": 196, "text": "times faster than"}]
[{"st": 12, "ed": 14, "text": "relational learning"}, {"st": 16, "ed": 20, "text": "supervised and semi supervised"}, {"st": 29, "ed": 31, "text": "existing methods"}, {"st": 50, "ed": 52, "text": "wide variety"}, {"st": 53, "ed": 55, "text": "classification problems"}, {"st": 62, "ed": 64, "text": "existing methods"}, {"st": 64, "ed": 66, "text": "perform poorly"}, {"st": 67, "ed": 70, "text": "multi class classification"}, {"st": 87, "ed": 89, "text": "relational learning"}, {"st": 97, "ed": 100, "text": "learning and inference"}, {"st": 114, "ed": 116, "text": "multi class"}, {"st": 119, "ed": 121, "text": "labeled instances"}, {"st": 125, "ed": 127, "text": "experiments demonstrate"}]
[{"st": 2, "ed": 4, "text": "collaborative filtering"}, {"st": 10, "ed": 12, "text": "latent factors"}, {"st": 14, "ed": 16, "text": "user preferences"}, {"st": 22, "ed": 24, "text": "predict future"}, {"st": 26, "ed": 28, "text": "collaborative filtering"}, {"st": 32, "ed": 34, "text": "latent factors"}, {"st": 42, "ed": 44, "text": "user preferences"}, {"st": 61, "ed": 63, "text": "matrix factorization"}, {"st": 68, "ed": 70, "text": "matrix factorization"}, {"st": 75, "ed": 77, "text": "latent factors"}, {"st": 93, "ed": 96, "text": "stochastic variational inference"}, {"st": 123, "ed": 125, "text": "predictive accuracy"}, {"st": 130, "ed": 133, "text": "static and dynamic"}]
[{"st": 1, "ed": 3, "text": "clustering approaches"}, {"st": 8, "ed": 10, "text": "large data"}, {"st": 21, "ed": 23, "text": "key idea"}, {"st": 68, "ed": 71, "text": "multi view data"}, {"st": 73, "ed": 75, "text": "multiple sources"}, {"st": 84, "ed": 86, "text": "clustering approaches"}, {"st": 102, "ed": 104, "text": "clustering approach"}, {"st": 109, "ed": 111, "text": "fuzzy clustering"}, {"st": 122, "ed": 124, "text": "multiple views"}, {"st": 160, "ed": 162, "text": "real world"}, {"st": 162, "ed": 164, "text": "multi view"}, {"st": 176, "ed": 178, "text": "fuzzy clustering"}, {"st": 181, "ed": 183, "text": "clustering accuracy"}, {"st": 185, "ed": 187, "text": "great potential"}, {"st": 191, "ed": 194, "text": "multi view data"}]
[{"st": 6, "ed": 8, "text": "machine learning"}, {"st": 31, "ed": 33, "text": "engineered features"}, {"st": 40, "ed": 42, "text": "engineered features"}]
[{"st": 15, "ed": 17, "text": "collective classification"}, {"st": 28, "ed": 30, "text": "great promise"}, {"st": 39, "ed": 41, "text": "collective classification"}, {"st": 62, "ed": 64, "text": "deep learning"}, {"st": 66, "ed": 68, "text": "collective classification"}, {"st": 69, "ed": 71, "text": "multi relational"}, {"st": 76, "ed": 78, "text": "theoretical properties"}, {"st": 121, "ed": 123, "text": "higher order"}, {"st": 132, "ed": 135, "text": "learning and inference"}, {"st": 154, "ed": 156, "text": "real world"}, {"st": 170, "ed": 172, "text": "film genre"}, {"st": 179, "ed": 181, "text": "higher accuracy"}]
[{"st": 0, "ed": 2, "text": "convolutional networks"}, {"st": 26, "ed": 28, "text": "supervised learning"}, {"st": 34, "ed": 36, "text": "previous attempts"}, {"st": 41, "ed": 43, "text": "unlabeled data"}, {"st": 68, "ed": 70, "text": "unsupervised training"}, {"st": 71, "ed": 73, "text": "convolutional networks"}, {"st": 79, "ed": 81, "text": "spatial regions"}, {"st": 90, "ed": 92, "text": "neural networks"}, {"st": 101, "ed": 103, "text": "back propagation"}]
[{"st": 0, "ed": 2, "text": "probabilistic programming"}, {"st": 17, "ed": 19, "text": "probabilistic program"}, {"st": 31, "ed": 33, "text": "sampling based"}, {"st": 110, "ed": 112, "text": "neural network"}, {"st": 126, "ed": 128, "text": "posterior distribution"}, {"st": 156, "ed": 158, "text": "preliminary results"}, {"st": 169, "ed": 171, "text": "machine learning"}, {"st": 188, "ed": 190, "text": "observed data"}]
[{"st": 0, "ed": 3, "text": "deep neural networks"}, {"st": 4, "ed": 6, "text": "achieved impressive"}, {"st": 9, "ed": 11, "text": "image classification"}, {"st": 19, "ed": 21, "text": "adversarial perturbations"}, {"st": 27, "ed": 29, "text": "input image"}, {"st": 37, "ed": 39, "text": "potential applications"}, {"st": 43, "ed": 46, "text": "end to end"}, {"st": 65, "ed": 67, "text": "feed forward"}, {"st": 67, "ed": 71, "text": "multi layer neural networks"}, {"st": 83, "ed": 85, "text": "image classification"}, {"st": 100, "ed": 102, "text": "lighting conditions"}, {"st": 138, "ed": 140, "text": "exhaustive search"}, {"st": 154, "ed": 156, "text": "method works"}, {"st": 165, "ed": 167, "text": "existing methods"}, {"st": 170, "ed": 172, "text": "adversarial examples"}, {"st": 187, "ed": 189, "text": "adversarial examples"}, {"st": 199, "ed": 201, "text": "fine tune"}, {"st": 221, "ed": 223, "text": "deep learning"}, {"st": 228, "ed": 230, "text": "existing techniques"}, {"st": 233, "ed": 235, "text": "adversarial examples"}]
[{"st": 0, "ed": 2, "text": "decision makers"}, {"st": 53, "ed": 55, "text": "cost effective"}, {"st": 85, "ed": 87, "text": "diagnostic test"}, {"st": 123, "ed": 126, "text": "markov decision process"}, {"st": 133, "ed": 136, "text": "upper confidence bound"}, {"st": 147, "ed": 149, "text": "search space"}, {"st": 153, "ed": 155, "text": "real world"}, {"st": 155, "ed": 157, "text": "observational data"}]
[{"st": 10, "ed": 12, "text": "sequential data"}, {"st": 38, "ed": 40, "text": "closed form"}, {"st": 40, "ed": 42, "text": "kernel functions"}, {"st": 53, "ed": 55, "text": "inductive biases"}, {"st": 57, "ed": 60, "text": "short term memory"}, {"st": 61, "ed": 63, "text": "recurrent networks"}, {"st": 84, "ed": 86, "text": "gaussian process"}, {"st": 86, "ed": 88, "text": "marginal likelihood"}, {"st": 133, "ed": 135, "text": "autonomous driving"}]
[{"st": 11, "ed": 13, "text": "discrete variables"}, {"st": 16, "ed": 18, "text": "joint distribution"}, {"st": 53, "ed": 55, "text": "probability distributions"}, {"st": 73, "ed": 75, "text": "cause effect"}, {"st": 102, "ed": 104, "text": "recent developments"}, {"st": 108, "ed": 110, "text": "causal models"}, {"st": 111, "ed": 113, "text": "continuous variables"}]
[{"st": 0, "ed": 2, "text": "policy gradient"}, {"st": 12, "ed": 14, "text": "reinforcement learning"}, {"st": 30, "ed": 32, "text": "off policy"}, {"st": 43, "ed": 45, "text": "policy gradient"}, {"st": 46, "ed": 48, "text": "off policy"}, {"st": 65, "ed": 67, "text": "fixed points"}, {"st": 70, "ed": 72, "text": "policy gradient"}, {"st": 109, "ed": 111, "text": "policy gradient"}, {"st": 131, "ed": 133, "text": "policy gradient"}, {"st": 146, "ed": 148, "text": "numerical examples"}, {"st": 167, "ed": 169, "text": "atari games"}, {"st": 177, "ed": 180, "text": "advantage actor critic"}]
[{"st": 4, "ed": 7, "text": "deep reinforcement learning"}, {"st": 28, "ed": 30, "text": "previously learned"}, {"st": 39, "ed": 41, "text": "training procedure"}, {"st": 42, "ed": 44, "text": "improved performance"}, {"st": 46, "ed": 48, "text": "approximation error"}, {"st": 90, "ed": 92, "text": "significantly improved"}]
[{"st": 86, "ed": 88, "text": "problem specific"}, {"st": 96, "ed": 98, "text": "real datasets"}]
[{"st": 2, "ed": 4, "text": "exploration exploitation"}, {"st": 6, "ed": 9, "text": "markov decision processes"}, {"st": 20, "ed": 22, "text": "reinforcement learning"}, {"st": 34, "ed": 37, "text": "states and actions"}, {"st": 45, "ed": 48, "text": "low dimensional latent"}, {"st": 54, "ed": 56, "text": "hidden state"}, {"st": 75, "ed": 78, "text": "markov decision processes"}, {"st": 80, "ed": 82, "text": "hidden states"}, {"st": 164, "ed": 166, "text": "cumulative reward"}, {"st": 171, "ed": 173, "text": "optimal policy"}]
[{"st": 5, "ed": 7, "text": "machine learning"}, {"st": 17, "ed": 19, "text": "accurate predictions"}, {"st": 103, "ed": 105, "text": "rule based"}]
[{"st": 0, "ed": 2, "text": "reinforcement learning"}, {"st": 20, "ed": 22, "text": "reinforcement learning"}, {"st": 25, "ed": 28, "text": "deep q networks"}, {"st": 44, "ed": 46, "text": "atari games"}, {"st": 63, "ed": 65, "text": "explicit knowledge"}, {"st": 110, "ed": 112, "text": "recently developed"}, {"st": 112, "ed": 115, "text": "deep neural network"}, {"st": 117, "ed": 119, "text": "frame prediction"}, {"st": 120, "ed": 122, "text": "atari games"}, {"st": 135, "ed": 137, "text": "optimization problem"}, {"st": 148, "ed": 150, "text": "network parameters"}, {"st": 151, "ed": 153, "text": "empirical evaluations"}, {"st": 155, "ed": 157, "text": "atari games"}, {"st": 159, "ed": 161, "text": "cumulative reward"}, {"st": 179, "ed": 181, "text": "reinforcement learning"}]
[{"st": 3, "ed": 5, "text": "open source"}, {"st": 44, "ed": 46, "text": "embedded systems"}, {"st": 59, "ed": 62, "text": "times faster than"}]
[{"st": 11, "ed": 13, "text": "kernel machines"}, {"st": 14, "ed": 17, "text": "deep neural networks"}, {"st": 69, "ed": 71, "text": "linear prediction"}, {"st": 92, "ed": 94, "text": "feature importance"}, {"st": 109, "ed": 111, "text": "kernel machines"}, {"st": 112, "ed": 114, "text": "deep learning"}, {"st": 150, "ed": 152, "text": "feature importance"}, {"st": 153, "ed": 155, "text": "instance based"}, {"st": 155, "ed": 157, "text": "feature importance"}]
[{"st": 16, "ed": 18, "text": "main challenges"}, {"st": 74, "ed": 78, "text": "canonical correlation analysis cca"}, {"st": 179, "ed": 181, "text": "method achieves"}]
[{"st": 3, "ed": 6, "text": "temporal difference td"}, {"st": 21, "ed": 23, "text": "least squares"}, {"st": 24, "ed": 26, "text": "least square"}, {"st": 49, "ed": 51, "text": "learning rate"}, {"st": 73, "ed": 75, "text": "least squares"}, {"st": 102, "ed": 104, "text": "least squares"}, {"st": 114, "ed": 116, "text": "significantly reduce"}, {"st": 147, "ed": 149, "text": "large scale"}]
[{"st": 6, "ed": 8, "text": "online algorithm"}, {"st": 29, "ed": 31, "text": "off policy"}, {"st": 34, "ed": 36, "text": "meta algorithm"}, {"st": 57, "ed": 59, "text": "article presents"}, {"st": 61, "ed": 63, "text": "meta algorithm"}, {"st": 66, "ed": 68, "text": "bandit algorithm"}, {"st": 100, "ed": 102, "text": "theoretical analysis"}, {"st": 115, "ed": 117, "text": "empirically evaluated"}, {"st": 139, "ed": 141, "text": "true online"}]
[{"st": 2, "ed": 4, "text": "gaussian process"}, {"st": 13, "ed": 15, "text": "linear regression"}, {"st": 20, "ed": 22, "text": "surrogate model"}, {"st": 32, "ed": 34, "text": "space complexity"}, {"st": 98, "ed": 100, "text": "approximation algorithms"}, {"st": 161, "ed": 163, "text": "empirical study"}, {"st": 166, "ed": 168, "text": "consistently outperform"}]
[{"st": 1, "ed": 4, "text": "statistical relational learning"}, {"st": 4, "ed": 6, "text": "knowledge graph"}, {"st": 30, "ed": 32, "text": "embedding models"}, {"st": 54, "ed": 56, "text": "complex valued"}, {"st": 63, "ed": 65, "text": "complex valued"}, {"st": 126, "ed": 128, "text": "dot product"}, {"st": 134, "ed": 136, "text": "dot product"}, {"st": 161, "ed": 163, "text": "large data"}, {"st": 180, "ed": 182, "text": "link prediction"}]
[{"st": 5, "ed": 7, "text": "causal structure"}, {"st": 12, "ed": 14, "text": "random variables"}, {"st": 34, "ed": 36, "text": "learning strategy"}, {"st": 64, "ed": 66, "text": "optimal solution"}, {"st": 100, "ed": 102, "text": "proposed algorithm"}, {"st": 105, "ed": 107, "text": "approximation algorithm"}, {"st": 121, "ed": 125, "text": "synthetic and real data"}]
[{"st": 1, "ed": 3, "text": "machine learning"}, {"st": 15, "ed": 17, "text": "machine learning"}, {"st": 54, "ed": 56, "text": "machine learning"}]
[{"st": 5, "ed": 7, "text": "network architecture"}, {"st": 9, "ed": 11, "text": "optimization problems"}, {"st": 24, "ed": 27, "text": "end to end"}, {"st": 39, "ed": 41, "text": "hidden states"}, {"st": 45, "ed": 48, "text": "fully connected layers"}, {"st": 67, "ed": 69, "text": "sensitivity analysis"}, {"st": 92, "ed": 94, "text": "highly efficient"}, {"st": 107, "ed": 109, "text": "primal dual"}, {"st": 109, "ed": 112, "text": "interior point method"}, {"st": 158, "ed": 161, "text": "input and output"}]
[{"st": 82, "ed": 85, "text": "sufficient condition for"}, {"st": 138, "ed": 140, "text": "theoretical findings"}]
[{"st": 2, "ed": 4, "text": "neural network"}, {"st": 9, "ed": 11, "text": "feature learning"}, {"st": 41, "ed": 44, "text": "end to end"}, {"st": 44, "ed": 46, "text": "representation learning"}, {"st": 55, "ed": 57, "text": "transfer learning"}, {"st": 57, "ed": 59, "text": "representation learning"}, {"st": 77, "ed": 79, "text": "supervised methods"}, {"st": 94, "ed": 96, "text": "generative models"}, {"st": 136, "ed": 138, "text": "generative models"}, {"st": 153, "ed": 155, "text": "supervised learning"}, {"st": 162, "ed": 164, "text": "representation learning"}, {"st": 168, "ed": 170, "text": "generative models"}, {"st": 172, "ed": 176, "text": "generative adversarial networks gans"}]
[{"st": 2, "ed": 5, "text": "markov decision process"}, {"st": 8, "ed": 11, "text": "sequential decision making"}, {"st": 29, "ed": 31, "text": "optimal policy"}, {"st": 49, "ed": 51, "text": "large scale"}, {"st": 69, "ed": 71, "text": "temporal difference"}, {"st": 75, "ed": 77, "text": "loss function"}, {"st": 96, "ed": 98, "text": "kalman filter"}, {"st": 118, "ed": 120, "text": "transition probabilities"}, {"st": 127, "ed": 129, "text": "provide theoretical"}, {"st": 140, "ed": 142, "text": "continuous state"}]
[{"st": 9, "ed": 11, "text": "complex nonlinear"}, {"st": 14, "ed": 17, "text": "deep generative models"}, {"st": 67, "ed": 69, "text": "autoregressive models"}, {"st": 76, "ed": 78, "text": "accurate predictions"}, {"st": 110, "ed": 113, "text": "end to end"}, {"st": 113, "ed": 115, "text": "fully differentiable"}, {"st": 120, "ed": 122, "text": "policy optimization"}, {"st": 130, "ed": 132, "text": "sample efficiency"}]
[{"st": 4, "ed": 6, "text": "wide variety"}, {"st": 7, "ed": 9, "text": "applications including"}, {"st": 9, "ed": 11, "text": "question answering"}, {"st": 38, "ed": 40, "text": "convolutional networks"}, {"st": 52, "ed": 54, "text": "link prediction"}, {"st": 81, "ed": 83, "text": "neural networks"}, {"st": 95, "ed": 97, "text": "multi relational"}, {"st": 125, "ed": 127, "text": "link prediction"}, {"st": 132, "ed": 134, "text": "significantly improved"}]
[{"st": 1, "ed": 3, "text": "performance bounds"}, {"st": 4, "ed": 6, "text": "reinforcement learning"}, {"st": 89, "ed": 91, "text": "finite state"}, {"st": 104, "ed": 106, "text": "optimal regret"}]
[{"st": 4, "ed": 7, "text": "stochastic gradient descent"}, {"st": 15, "ed": 17, "text": "linear convergence"}, {"st": 18, "ed": 20, "text": "strongly convex"}, {"st": 58, "ed": 60, "text": "update rule"}, {"st": 93, "ed": 95, "text": "per iteration"}, {"st": 101, "ed": 103, "text": "linear convergence"}, {"st": 104, "ed": 106, "text": "strongly convex"}, {"st": 110, "ed": 112, "text": "mathcal o"}, {"st": 115, "ed": 117, "text": "convergence rate"}, {"st": 119, "ed": 121, "text": "strongly convex"}, {"st": 158, "ed": 160, "text": "machine learning"}, {"st": 163, "ed": 165, "text": "logistic regression"}, {"st": 165, "ed": 167, "text": "ridge regression"}]
[{"st": 12, "ed": 14, "text": "decision making"}, {"st": 29, "ed": 31, "text": "objective function"}, {"st": 36, "ed": 38, "text": "gaussian process"}, {"st": 45, "ed": 47, "text": "automatically learn"}, {"st": 48, "ed": 50, "text": "gaussian process"}, {"st": 51, "ed": 53, "text": "surrogate models"}, {"st": 79, "ed": 81, "text": "parameter values"}, {"st": 108, "ed": 110, "text": "surrogate model"}, {"st": 114, "ed": 116, "text": "objective functions"}, {"st": 155, "ed": 157, "text": "surrogate models"}, {"st": 159, "ed": 161, "text": "decision makers"}, {"st": 163, "ed": 165, "text": "accurately predict"}]
[{"st": 12, "ed": 14, "text": "domain knowledge"}, {"st": 30, "ed": 32, "text": "structured data"}, {"st": 34, "ed": 36, "text": "probabilistic programming"}, {"st": 70, "ed": 72, "text": "sparse matrix"}, {"st": 80, "ed": 82, "text": "nonparametric bayesian"}, {"st": 113, "ed": 115, "text": "probabilistic programming"}, {"st": 133, "ed": 135, "text": "public health"}]
[{"st": 34, "ed": 37, "text": "recurrent neural networks"}, {"st": 125, "ed": 127, "text": "atari games"}]
[{"st": 25, "ed": 28, "text": "stochastic gradient descent"}, {"st": 28, "ed": 30, "text": "vr sgd"}, {"st": 40, "ed": 42, "text": "starting point"}, {"st": 57, "ed": 59, "text": "vr sgd"}, {"st": 80, "ed": 82, "text": "learning rates"}, {"st": 83, "ed": 85, "text": "step sizes"}, {"st": 91, "ed": 93, "text": "vr sgd"}, {"st": 102, "ed": 104, "text": "convergence analysis"}, {"st": 110, "ed": 112, "text": "learning rate"}, {"st": 114, "ed": 116, "text": "vr sgd"}, {"st": 149, "ed": 151, "text": "update rules"}, {"st": 156, "ed": 158, "text": "objective functions"}, {"st": 162, "ed": 164, "text": "vr sgd"}, {"st": 171, "ed": 173, "text": "strongly convex"}, {"st": 178, "ed": 180, "text": "reduction techniques"}, {"st": 188, "ed": 190, "text": "convergence properties"}, {"st": 191, "ed": 193, "text": "vr sgd"}, {"st": 194, "ed": 196, "text": "strongly convex"}, {"st": 200, "ed": 202, "text": "vr sgd"}, {"st": 211, "ed": 213, "text": "convergence guarantees"}, {"st": 214, "ed": 216, "text": "vr sgd"}, {"st": 218, "ed": 220, "text": "strongly convex"}, {"st": 228, "ed": 230, "text": "vr sgd"}]
[{"st": 87, "ed": 89, "text": "past observations"}]
[{"st": 10, "ed": 12, "text": "natural language"}, {"st": 71, "ed": 73, "text": "reinforcement learning"}, {"st": 76, "ed": 78, "text": "marginal likelihood"}, {"st": 84, "ed": 86, "text": "learning algorithm"}, {"st": 131, "ed": 133, "text": "learning algorithm"}, {"st": 141, "ed": 143, "text": "significant gains"}, {"st": 153, "ed": 155, "text": "context dependent"}]
[{"st": 7, "ed": 10, "text": "deep neural networks"}, {"st": 46, "ed": 49, "text": "deep neural networks"}, {"st": 68, "ed": 70, "text": "weight matrices"}, {"st": 73, "ed": 75, "text": "convolutional layers"}, {"st": 118, "ed": 120, "text": "cifar 10"}]
[{"st": 0, "ed": 2, "text": "machine learning"}, {"st": 6, "ed": 8, "text": "multiple domains"}, {"st": 10, "ed": 12, "text": "wide variety"}, {"st": 21, "ed": 23, "text": "natural language"}, {"st": 26, "ed": 28, "text": "computer vision"}, {"st": 32, "ed": 34, "text": "ubiquitous computing"}, {"st": 39, "ed": 41, "text": "machine learning"}, {"st": 63, "ed": 65, "text": "domain specific"}, {"st": 69, "ed": 71, "text": "time consuming"}, {"st": 99, "ed": 101, "text": "world knowledge"}, {"st": 103, "ed": 105, "text": "machine learning"}, {"st": 121, "ed": 123, "text": "world knowledge"}, {"st": 124, "ed": 126, "text": "domain specific"}, {"st": 135, "ed": 137, "text": "world knowledge"}, {"st": 144, "ed": 146, "text": "feature representation"}, {"st": 163, "ed": 165, "text": "future directions"}]
[{"st": 13, "ed": 15, "text": "value iteration"}, {"st": 20, "ed": 23, "text": "deep neural network"}, {"st": 26, "ed": 28, "text": "similarity metric"}, {"st": 34, "ed": 36, "text": "reinforcement learning"}, {"st": 40, "ed": 44, "text": "trained end to end"}, {"st": 55, "ed": 57, "text": "learning objective"}, {"st": 64, "ed": 66, "text": "internal structure"}]
[{"st": 0, "ed": 2, "text": "latent features"}, {"st": 4, "ed": 6, "text": "deep learning"}, {"st": 12, "ed": 14, "text": "powerful tool"}, {"st": 53, "ed": 55, "text": "latent features"}, {"st": 67, "ed": 69, "text": "latent representations"}, {"st": 89, "ed": 91, "text": "latent features"}, {"st": 126, "ed": 128, "text": "latent features"}]
[{"st": 21, "ed": 23, "text": "classification problems"}, {"st": 58, "ed": 60, "text": "proposed method"}, {"st": 101, "ed": 104, "text": "real world data"}, {"st": 123, "ed": 125, "text": "outperform existing"}]
[{"st": 1, "ed": 3, "text": "probabilistic inference"}, {"st": 9, "ed": 11, "text": "examples include"}, {"st": 12, "ed": 14, "text": "monte carlo"}, {"st": 17, "ed": 19, "text": "variational inference"}, {"st": 20, "ed": 22, "text": "machine learning"}, {"st": 23, "ed": 27, "text": "markov chain monte carlo"}, {"st": 42, "ed": 44, "text": "approximate inference"}, {"st": 66, "ed": 68, "text": "approximate inference"}, {"st": 76, "ed": 78, "text": "inference algorithms"}, {"st": 82, "ed": 84, "text": "probabilistic models"}, {"st": 86, "ed": 88, "text": "random variables"}, {"st": 91, "ed": 93, "text": "inference algorithm"}, {"st": 131, "ed": 133, "text": "hidden markov"}, {"st": 134, "ed": 137, "text": "dirichlet process mixture"}, {"st": 152, "ed": 154, "text": "inference algorithms"}, {"st": 160, "ed": 162, "text": "inference algorithms"}]
[{"st": 10, "ed": 12, "text": "reinforcement learning"}, {"st": 13, "ed": 16, "text": "markov decision processes"}, {"st": 25, "ed": 27, "text": "linear programming"}, {"st": 29, "ed": 31, "text": "policy optimization"}, {"st": 59, "ed": 61, "text": "optimization problem"}, {"st": 82, "ed": 84, "text": "reinforcement learning"}, {"st": 100, "ed": 102, "text": "convergence properties"}, {"st": 126, "ed": 128, "text": "optimal policy"}, {"st": 132, "ed": 134, "text": "policy gradient"}, {"st": 157, "ed": 159, "text": "regularization techniques"}, {"st": 165, "ed": 167, "text": "reinforcement learning"}]
[{"st": 32, "ed": 34, "text": "complex models"}, {"st": 44, "ed": 46, "text": "deep learning"}, {"st": 69, "ed": 71, "text": "complex models"}, {"st": 96, "ed": 98, "text": "unified framework"}, {"st": 130, "ed": 132, "text": "feature importance"}, {"st": 135, "ed": 137, "text": "theoretical results"}, {"st": 157, "ed": 159, "text": "existing methods"}, {"st": 162, "ed": 164, "text": "recent methods"}, {"st": 185, "ed": 187, "text": "computational performance"}]
[{"st": 1, "ed": 3, "text": "real world"}, {"st": 3, "ed": 5, "text": "reward function"}, {"st": 28, "ed": 30, "text": "reinforcement learning"}, {"st": 59, "ed": 61, "text": "decision problem"}, {"st": 104, "ed": 107, "text": "inverse reinforcement learning"}, {"st": 108, "ed": 110, "text": "semi supervised"}, {"st": 110, "ed": 112, "text": "reinforcement learning"}]
[{"st": 1, "ed": 3, "text": "moment matching"}, {"st": 7, "ed": 9, "text": "deep generative"}, {"st": 13, "ed": 17, "text": "generative adversarial network gan"}, {"st": 31, "ed": 35, "text": "maximum mean discrepancy mmd"}, {"st": 38, "ed": 40, "text": "theoretical guarantees"}, {"st": 46, "ed": 48, "text": "empirical performance"}, {"st": 66, "ed": 68, "text": "computational efficiency"}, {"st": 87, "ed": 89, "text": "batch size"}, {"st": 107, "ed": 109, "text": "computational efficiency"}, {"st": 112, "ed": 114, "text": "kernel learning"}, {"st": 121, "ed": 123, "text": "gaussian kernel"}, {"st": 147, "ed": 149, "text": "distance measure"}, {"st": 161, "ed": 163, "text": "weak topology"}, {"st": 180, "ed": 182, "text": "benchmark datasets"}, {"st": 183, "ed": 186, "text": "mnist cifar 10"}, {"st": 194, "ed": 196, "text": "significantly outperforms"}]
[{"st": 4, "ed": 6, "text": "collaborative filtering"}, {"st": 6, "ed": 8, "text": "recommender systems"}, {"st": 22, "ed": 24, "text": "collaborative filtering"}, {"st": 69, "ed": 73, "text": "synthetic and real data"}]
[{"st": 34, "ed": 36, "text": "time series"}, {"st": 61, "ed": 63, "text": "point processes"}, {"st": 86, "ed": 88, "text": "time series"}, {"st": 106, "ed": 110, "text": "recurrent neural network rnn"}, {"st": 115, "ed": 117, "text": "time series"}, {"st": 164, "ed": 166, "text": "point process"}, {"st": 190, "ed": 192, "text": "pre defined"}, {"st": 198, "ed": 201, "text": "end to end"}, {"st": 211, "ed": 213, "text": "deep network"}, {"st": 214, "ed": 216, "text": "point process"}]
[{"st": 3, "ed": 5, "text": "structure learning"}, {"st": 7, "ed": 11, "text": "markov random fields mrfs"}, {"st": 25, "ed": 27, "text": "feature space"}, {"st": 39, "ed": 41, "text": "active set"}, {"st": 119, "ed": 121, "text": "significant computational"}, {"st": 123, "ed": 125, "text": "structure learning"}]
[{"st": 2, "ed": 4, "text": "causal inference"}, {"st": 66, "ed": 68, "text": "causal inference"}, {"st": 92, "ed": 94, "text": "significantly improved"}, {"st": 96, "ed": 98, "text": "sample complexity"}, {"st": 104, "ed": 106, "text": "experiment results"}, {"st": 109, "ed": 111, "text": "proposed algorithm"}]
[{"st": 1, "ed": 3, "text": "domain adaptation"}, {"st": 8, "ed": 10, "text": "recent years"}, {"st": 11, "ed": 13, "text": "theoretical results"}, {"st": 30, "ed": 32, "text": "multiple source"}, {"st": 32, "ed": 34, "text": "domain adaptation"}, {"st": 51, "ed": 53, "text": "generalization bound"}, {"st": 54, "ed": 56, "text": "domain adaptation"}, {"st": 59, "ed": 61, "text": "multiple source"}, {"st": 63, "ed": 65, "text": "labeled instances"}, {"st": 82, "ed": 84, "text": "expert knowledge"}, {"st": 103, "ed": 105, "text": "efficient learning"}, {"st": 108, "ed": 110, "text": "neural networks"}, {"st": 118, "ed": 120, "text": "feature representations"}, {"st": 149, "ed": 151, "text": "domain adversarial"}, {"st": 190, "ed": 192, "text": "saddle point"}, {"st": 207, "ed": 210, "text": "conduct extensive experiments"}, {"st": 216, "ed": 218, "text": "real world"}, {"st": 219, "ed": 221, "text": "sentiment analysis"}, {"st": 221, "ed": 223, "text": "digit classification"}]
[{"st": 13, "ed": 16, "text": "deep neural network"}, {"st": 94, "ed": 96, "text": "pac bayesian"}, {"st": 107, "ed": 110, "text": "weights and activations"}, {"st": 145, "ed": 147, "text": "avoid overfitting"}, {"st": 163, "ed": 165, "text": "phase transitions"}, {"st": 176, "ed": 179, "text": "sheds light on"}, {"st": 186, "ed": 188, "text": "loss function"}]
[{"st": 2, "ed": 4, "text": "reinforcement learning"}, {"st": 10, "ed": 12, "text": "real world"}, {"st": 56, "ed": 58, "text": "reward function"}, {"st": 59, "ed": 61, "text": "atari games"}, {"st": 140, "ed": 142, "text": "previously learned"}]
[{"st": 5, "ed": 7, "text": "reinforcement learning"}, {"st": 15, "ed": 17, "text": "continual learning"}, {"st": 22, "ed": 24, "text": "reinforcement learning"}, {"st": 30, "ed": 32, "text": "continual learning"}, {"st": 34, "ed": 36, "text": "incremental learning"}, {"st": 44, "ed": 46, "text": "weight initialization"}, {"st": 57, "ed": 59, "text": "incremental learning"}, {"st": 70, "ed": 72, "text": "strong baseline"}, {"st": 84, "ed": 86, "text": "qualitative analysis"}, {"st": 87, "ed": 89, "text": "reinforcement learning"}, {"st": 93, "ed": 95, "text": "incremental learning"}]
[{"st": 6, "ed": 9, "text": "ability to learn"}, {"st": 51, "ed": 53, "text": "recent results"}, {"st": 56, "ed": 58, "text": "artificial intelligence"}, {"st": 65, "ed": 67, "text": "visual representation"}, {"st": 72, "ed": 76, "text": "deep convolutional neural network"}, {"st": 179, "ed": 181, "text": "visual tasks"}]
[{"st": 3, "ed": 6, "text": "simple and efficient"}, {"st": 21, "ed": 23, "text": "lower dimensional"}, {"st": 31, "ed": 33, "text": "lower dimensional"}, {"st": 53, "ed": 58, "text": "non negative matrix factorization nmf"}, {"st": 61, "ed": 63, "text": "linear classification"}, {"st": 72, "ed": 75, "text": "loss in accuracy"}, {"st": 76, "ed": 78, "text": "real data"}, {"st": 119, "ed": 121, "text": "method works"}, {"st": 130, "ed": 132, "text": "random projections"}, {"st": 138, "ed": 141, "text": "non convex optimization"}]
[{"st": 10, "ed": 12, "text": "maximum likelihood"}, {"st": 15, "ed": 17, "text": "sequence prediction"}, {"st": 39, "ed": 41, "text": "sequence prediction"}, {"st": 43, "ed": 45, "text": "generator network"}, {"st": 57, "ed": 59, "text": "point wise"}, {"st": 59, "ed": 61, "text": "ground truth"}, {"st": 102, "ed": 104, "text": "experiments conducted"}, {"st": 107, "ed": 109, "text": "sequence prediction"}, {"st": 110, "ed": 112, "text": "machine translation"}, {"st": 114, "ed": 116, "text": "text summarization"}, {"st": 123, "ed": 125, "text": "significant improvements"}]
[{"st": 15, "ed": 17, "text": "causal model"}, {"st": 20, "ed": 22, "text": "causal graph"}, {"st": 49, "ed": 51, "text": "active learning"}, {"st": 89, "ed": 91, "text": "significantly improves"}]
[{"st": 0, "ed": 2, "text": "complex systems"}, {"st": 11, "ed": 13, "text": "causal models"}, {"st": 118, "ed": 120, "text": "time series"}, {"st": 142, "ed": 145, "text": "sheds light on"}]
[{"st": 1, "ed": 3, "text": "vision systems"}, {"st": 32, "ed": 34, "text": "judea pearl"}]
[{"st": 41, "ed": 43, "text": "learning framework"}, {"st": 44, "ed": 47, "text": "takes into account"}, {"st": 73, "ed": 75, "text": "ground truth"}, {"st": 80, "ed": 82, "text": "learning framework"}, {"st": 101, "ed": 103, "text": "rotten tomatoes"}, {"st": 117, "ed": 119, "text": "ground truth"}, {"st": 124, "ed": 126, "text": "promising results"}]
[{"st": 10, "ed": 13, "text": "deep reinforcement learning"}, {"st": 27, "ed": 29, "text": "reinforcement learning"}]
[{"st": 6, "ed": 9, "text": "hierarchical dirichlet process"}, {"st": 9, "ed": 12, "text": "hidden markov model"}, {"st": 19, "ed": 21, "text": "prior information"}, {"st": 36, "ed": 38, "text": "similarity function"}, {"st": 44, "ed": 46, "text": "transition probabilities"}, {"st": 47, "ed": 49, "text": "pair wise"}, {"st": 133, "ed": 135, "text": "synthetic datasets"}]
[{"st": 7, "ed": 9, "text": "low dimensional"}, {"st": 17, "ed": 19, "text": "topic models"}, {"st": 29, "ed": 31, "text": "latent variable"}, {"st": 47, "ed": 49, "text": "accurate prediction"}, {"st": 53, "ed": 55, "text": "existing approaches"}, {"st": 88, "ed": 90, "text": "generative models"}, {"st": 99, "ed": 102, "text": "semi supervised learning"}, {"st": 108, "ed": 110, "text": "learning algorithms"}, {"st": 111, "ed": 113, "text": "semi supervised"}, {"st": 115, "ed": 117, "text": "topic models"}, {"st": 118, "ed": 121, "text": "stochastic gradient descent"}, {"st": 134, "ed": 136, "text": "topic models"}, {"st": 142, "ed": 144, "text": "logistic regression"}, {"st": 146, "ed": 148, "text": "sentiment analysis"}, {"st": 149, "ed": 152, "text": "electronic health records"}]
[{"st": 4, "ed": 6, "text": "reinforcement learning"}, {"st": 11, "ed": 13, "text": "feature representation"}, {"st": 34, "ed": 36, "text": "feature representation"}, {"st": 51, "ed": 53, "text": "feature representation"}, {"st": 55, "ed": 57, "text": "reward function"}]
[{"st": 22, "ed": 24, "text": "proposed method"}, {"st": 28, "ed": 30, "text": "activation functions"}, {"st": 38, "ed": 40, "text": "activation functions"}, {"st": 63, "ed": 65, "text": "proposed method"}, {"st": 140, "ed": 142, "text": "proposed method"}, {"st": 146, "ed": 149, "text": "convolutional neural networks"}, {"st": 151, "ed": 153, "text": "object recognition"}, {"st": 153, "ed": 155, "text": "benchmark tasks"}, {"st": 155, "ed": 160, "text": "cifar 10 and cifar 100"}, {"st": 166, "ed": 168, "text": "proposed method"}, {"st": 183, "ed": 185, "text": "weight initialization"}, {"st": 199, "ed": 201, "text": "computational cost"}]
[{"st": 0, "ed": 2, "text": "variational inference"}, {"st": 20, "ed": 22, "text": "variational inference"}, {"st": 53, "ed": 55, "text": "variational inference"}, {"st": 57, "ed": 59, "text": "theoretical properties"}, {"st": 70, "ed": 72, "text": "convergence properties"}, {"st": 86, "ed": 88, "text": "frank wolfe"}, {"st": 93, "ed": 95, "text": "theoretical insights"}, {"st": 97, "ed": 99, "text": "sufficient conditions"}, {"st": 112, "ed": 114, "text": "previous works"}, {"st": 115, "ed": 117, "text": "variational inference"}, {"st": 136, "ed": 138, "text": "probabilistic models"}]
[{"st": 1, "ed": 4, "text": "vanishing gradient problem"}, {"st": 15, "ed": 17, "text": "recent years"}, {"st": 41, "ed": 43, "text": "neural networks"}, {"st": 44, "ed": 46, "text": "activation functions"}, {"st": 61, "ed": 63, "text": "neural network"}, {"st": 77, "ed": 79, "text": "neural networks"}, {"st": 85, "ed": 87, "text": "distance measures"}, {"st": 111, "ed": 114, "text": "stochastic gradient descent"}, {"st": 129, "ed": 131, "text": "efficiently learn"}, {"st": 145, "ed": 147, "text": "neural networks"}, {"st": 152, "ed": 155, "text": "vanishing and exploding"}, {"st": 159, "ed": 161, "text": "neural networks"}]
[{"st": 11, "ed": 13, "text": "latent representations"}, {"st": 20, "ed": 22, "text": "episodic memory"}, {"st": 23, "ed": 25, "text": "semantic memory"}, {"st": 38, "ed": 40, "text": "sensory input"}, {"st": 78, "ed": 80, "text": "sensory input"}, {"st": 114, "ed": 116, "text": "latent representations"}, {"st": 159, "ed": 161, "text": "closely related"}, {"st": 164, "ed": 166, "text": "learning systems"}, {"st": 174, "ed": 176, "text": "episodic memory"}, {"st": 183, "ed": 185, "text": "semantic memory"}]
[{"st": 1, "ed": 3, "text": "distributed stochastic"}, {"st": 5, "ed": 7, "text": "t sne"}, {"st": 14, "ed": 16, "text": "dimensionality reduction"}, {"st": 35, "ed": 37, "text": "t sne"}, {"st": 63, "ed": 65, "text": "t sne"}, {"st": 75, "ed": 77, "text": "t sne"}, {"st": 79, "ed": 81, "text": "empirically validate"}, {"st": 113, "ed": 116, "text": "minimum description length"}]
[{"st": 7, "ed": 9, "text": "machine learning"}, {"st": 26, "ed": 28, "text": "feature vector"}, {"st": 38, "ed": 40, "text": "true label"}, {"st": 72, "ed": 74, "text": "true label"}, {"st": 85, "ed": 87, "text": "true label"}, {"st": 94, "ed": 96, "text": "probabilistic model"}, {"st": 98, "ed": 100, "text": "true label"}, {"st": 120, "ed": 122, "text": "variational bayesian"}, {"st": 124, "ed": 126, "text": "numerical experiments"}, {"st": 132, "ed": 134, "text": "outperforms existing"}, {"st": 142, "ed": 144, "text": "true labels"}]
[{"st": 25, "ed": 27, "text": "based optimization"}, {"st": 39, "ed": 41, "text": "density based"}, {"st": 77, "ed": 79, "text": "generated data"}, {"st": 84, "ed": 86, "text": "feature space"}, {"st": 107, "ed": 109, "text": "feature space"}, {"st": 171, "ed": 173, "text": "efficiently learn"}, {"st": 183, "ed": 185, "text": "extensive experiments"}, {"st": 188, "ed": 190, "text": "real world"}, {"st": 208, "ed": 210, "text": "multi modal"}]
[{"st": 4, "ed": 6, "text": "theoretical justification"}, {"st": 9, "ed": 11, "text": "classification performance"}, {"st": 38, "ed": 40, "text": "approximation error"}, {"st": 52, "ed": 54, "text": "grows exponentially"}, {"st": 98, "ed": 100, "text": "exponentially large"}, {"st": 109, "ed": 111, "text": "exponentially large"}, {"st": 118, "ed": 120, "text": "exponentially large"}, {"st": 129, "ed": 132, "text": "curse of dimensionality"}, {"st": 164, "ed": 167, "text": "single hidden layer"}, {"st": 170, "ed": 172, "text": "grows exponentially"}, {"st": 231, "ed": 233, "text": "multiple layers"}, {"st": 267, "ed": 269, "text": "significantly fewer"}, {"st": 273, "ed": 276, "text": "single hidden layer"}]
[{"st": 12, "ed": 14, "text": "continuous state"}, {"st": 19, "ed": 21, "text": "policy gradient"}, {"st": 51, "ed": 53, "text": "significantly reduces"}, {"st": 69, "ed": 71, "text": "empirical results"}, {"st": 85, "ed": 87, "text": "policy gradient"}, {"st": 91, "ed": 93, "text": "atari games"}, {"st": 102, "ed": 104, "text": "policy search"}]
[{"st": 5, "ed": 7, "text": "causal structure"}, {"st": 38, "ed": 40, "text": "optimization problem"}, {"st": 57, "ed": 59, "text": "objective function"}, {"st": 63, "ed": 65, "text": "greedy algorithm"}, {"st": 68, "ed": 70, "text": "frac 1"}, {"st": 71, "ed": 73, "text": "approximation algorithm"}, {"st": 84, "ed": 86, "text": "greedy algorithm"}, {"st": 90, "ed": 93, "text": "orders of magnitude"}, {"st": 98, "ed": 100, "text": "proposed approach"}, {"st": 101, "ed": 104, "text": "synthetic and real"}]
[{"st": 7, "ed": 9, "text": "decision making"}, {"st": 86, "ed": 88, "text": "promising results"}, {"st": 98, "ed": 100, "text": "theoretical properties"}, {"st": 123, "ed": 126, "text": "achieve competitive results"}]
[{"st": 26, "ed": 28, "text": "unsupervised learning"}, {"st": 32, "ed": 34, "text": "supervised learning"}, {"st": 67, "ed": 70, "text": "number of clusters"}, {"st": 72, "ed": 74, "text": "clustering algorithm"}, {"st": 92, "ed": 94, "text": "improved performance"}, {"st": 116, "ed": 118, "text": "deep networks"}]
[{"st": 3, "ed": 5, "text": "exploration exploitation"}, {"st": 79, "ed": 81, "text": "fixed point"}, {"st": 112, "ed": 114, "text": "standard deviation"}, {"st": 121, "ed": 123, "text": "existing approaches"}, {"st": 138, "ed": 140, "text": "exploration strategy"}, {"st": 141, "ed": 143, "text": "epsilon greedy"}]
[{"st": 7, "ed": 9, "text": "challenging task"}, {"st": 16, "ed": 18, "text": "attention mechanisms"}, {"st": 27, "ed": 30, "text": "high computational cost"}, {"st": 39, "ed": 41, "text": "representation learning"}, {"st": 51, "ed": 53, "text": "associative memory"}, {"st": 62, "ed": 64, "text": "fixed size"}, {"st": 78, "ed": 80, "text": "associative memory"}, {"st": 96, "ed": 98, "text": "rule based"}, {"st": 115, "ed": 117, "text": "update rule"}, {"st": 138, "ed": 140, "text": "associative memory"}, {"st": 154, "ed": 156, "text": "rnn architectures"}]
[{"st": 0, "ed": 2, "text": "feature engineering"}, {"st": 4, "ed": 6, "text": "crucial step"}, {"st": 18, "ed": 20, "text": "feature space"}, {"st": 50, "ed": 52, "text": "domain knowledge"}, {"st": 110, "ed": 112, "text": "highly efficient"}, {"st": 112, "ed": 114, "text": "exploration strategy"}, {"st": 117, "ed": 119, "text": "reinforcement learning"}]
[{"st": 10, "ed": 12, "text": "optimization methods"}, {"st": 16, "ed": 18, "text": "deep learning"}, {"st": 22, "ed": 25, "text": "recurrent neural network"}, {"st": 32, "ed": 35, "text": "domain specific language"}, {"st": 63, "ed": 65, "text": "reinforcement learning"}, {"st": 77, "ed": 79, "text": "cifar 10"}, {"st": 83, "ed": 85, "text": "update rules"}, {"st": 131, "ed": 133, "text": "architectures including"}, {"st": 133, "ed": 135, "text": "imagenet classification"}, {"st": 138, "ed": 141, "text": "neural machine translation"}]
[{"st": 0, "ed": 2, "text": "transfer learning"}, {"st": 5, "ed": 7, "text": "reinforcement learning"}, {"st": 24, "ed": 26, "text": "learning process"}, {"st": 28, "ed": 30, "text": "great importance"}, {"st": 36, "ed": 38, "text": "theoretical analysis"}, {"st": 66, "ed": 70, "text": "multi armed bandit problem"}, {"st": 78, "ed": 81, "text": "provide theoretical guarantees"}, {"st": 84, "ed": 86, "text": "selection process"}, {"st": 95, "ed": 97, "text": "conduct experiments"}, {"st": 118, "ed": 120, "text": "transfer learning"}]
[{"st": 6, "ed": 8, "text": "representation learning"}, {"st": 39, "ed": 41, "text": "low dimensional"}, {"st": 70, "ed": 72, "text": "low dimensional"}, {"st": 120, "ed": 122, "text": "maximum likelihood"}, {"st": 189, "ed": 191, "text": "natural language"}]
[{"st": 4, "ed": 6, "text": "data management"}, {"st": 15, "ed": 17, "text": "machine learning"}, {"st": 23, "ed": 25, "text": "multi dimensional"}, {"st": 42, "ed": 44, "text": "streaming data"}, {"st": 135, "ed": 137, "text": "baseline approaches"}, {"st": 142, "ed": 144, "text": "maximum likelihood"}, {"st": 151, "ed": 153, "text": "prediction errors"}]
[{"st": 2, "ed": 4, "text": "neural networks"}, {"st": 47, "ed": 49, "text": "hand crafted"}, {"st": 60, "ed": 62, "text": "structure learning"}, {"st": 65, "ed": 67, "text": "fully automated"}, {"st": 75, "ed": 77, "text": "structure learning"}, {"st": 103, "ed": 105, "text": "training examples"}]
[{"st": 9, "ed": 11, "text": "variational autoencoder"}, {"st": 13, "ed": 15, "text": "prior distribution"}, {"st": 17, "ed": 19, "text": "latent variables"}]
[{"st": 8, "ed": 10, "text": "exploration strategy"}, {"st": 13, "ed": 16, "text": "multi armed bandits"}, {"st": 90, "ed": 92, "text": "simulated annealing"}, {"st": 106, "ed": 108, "text": "optimal regret"}]
[{"st": 10, "ed": 12, "text": "representation learning"}, {"st": 20, "ed": 22, "text": "deep learning"}, {"st": 56, "ed": 58, "text": "machine learning"}, {"st": 62, "ed": 64, "text": "random forest"}, {"st": 70, "ed": 72, "text": "experiments demonstrate"}, {"st": 86, "ed": 88, "text": "training set"}, {"st": 92, "ed": 94, "text": "hidden layer"}, {"st": 94, "ed": 96, "text": "feature map"}]
[{"st": 7, "ed": 10, "text": "probabilistic graphical model"}, {"st": 27, "ed": 29, "text": "massive data"}, {"st": 56, "ed": 58, "text": "worst case"}, {"st": 110, "ed": 112, "text": "exponential family"}, {"st": 127, "ed": 129, "text": "worst case"}, {"st": 153, "ed": 155, "text": "theoretical results"}, {"st": 156, "ed": 158, "text": "empirically evaluated"}, {"st": 163, "ed": 165, "text": "real data"}]
[{"st": 0, "ed": 3, "text": "deep neural networks"}, {"st": 8, "ed": 10, "text": "wide variety"}, {"st": 17, "ed": 19, "text": "neural network"}, {"st": 45, "ed": 48, "text": "deep neural networks"}, {"st": 51, "ed": 53, "text": "floating point"}, {"st": 147, "ed": 149, "text": "wide variety"}, {"st": 150, "ed": 152, "text": "models including"}, {"st": 153, "ed": 155, "text": "neural networks"}, {"st": 155, "ed": 158, "text": "recurrent neural networks"}, {"st": 166, "ed": 168, "text": "large scale"}, {"st": 189, "ed": 191, "text": "deep learning"}]
[{"st": 34, "ed": 36, "text": "objective function"}, {"st": 46, "ed": 48, "text": "reinforcement learning"}, {"st": 48, "ed": 50, "text": "objective function"}, {"st": 71, "ed": 73, "text": "reinforcement learning"}]
[{"st": 1, "ed": 3, "text": "decision making"}, {"st": 14, "ed": 16, "text": "machine learning"}, {"st": 18, "ed": 21, "text": "classification and regression"}, {"st": 78, "ed": 80, "text": "machine learning"}, {"st": 96, "ed": 98, "text": "least squares"}, {"st": 122, "ed": 124, "text": "proposed algorithm"}, {"st": 137, "ed": 139, "text": "classification tasks"}, {"st": 140, "ed": 142, "text": "proposed method"}, {"st": 156, "ed": 158, "text": "reverse discrimination"}, {"st": 171, "ed": 173, "text": "proposed algorithm"}]
[{"st": 0, "ed": 2, "text": "policy evaluation"}, {"st": 14, "ed": 16, "text": "reinforcement learning"}, {"st": 24, "ed": 26, "text": "policy iteration"}, {"st": 34, "ed": 36, "text": "policy gradient"}, {"st": 58, "ed": 60, "text": "policy evaluation"}, {"st": 62, "ed": 65, "text": "takes advantage of"}, {"st": 80, "ed": 82, "text": "sample efficiency"}, {"st": 83, "ed": 85, "text": "higher accuracy"}, {"st": 91, "ed": 93, "text": "proposed method"}, {"st": 95, "ed": 97, "text": "least squares"}, {"st": 97, "ed": 99, "text": "policy iteration"}, {"st": 110, "ed": 112, "text": "basis functions"}, {"st": 114, "ed": 116, "text": "standard benchmarks"}]
[{"st": 31, "ed": 33, "text": "transfer knowledge"}, {"st": 60, "ed": 62, "text": "gain insight"}, {"st": 87, "ed": 89, "text": "side information"}, {"st": 179, "ed": 181, "text": "confidence intervals"}, {"st": 193, "ed": 195, "text": "additive models"}, {"st": 216, "ed": 218, "text": "lending club"}]
[{"st": 8, "ed": 10, "text": "machine learning"}, {"st": 26, "ed": 29, "text": "stochastic gradient descent"}, {"st": 44, "ed": 47, "text": "deep neural networks"}, {"st": 51, "ed": 54, "text": "labeled training data"}, {"st": 101, "ed": 103, "text": "learning rate"}, {"st": 108, "ed": 110, "text": "batch size"}, {"st": 136, "ed": 139, "text": "stochastic gradient descent"}, {"st": 141, "ed": 144, "text": "stochastic differential equation"}, {"st": 163, "ed": 165, "text": "learning rate"}, {"st": 167, "ed": 169, "text": "training set"}, {"st": 178, "ed": 180, "text": "batch size"}, {"st": 185, "ed": 187, "text": "learning rate"}, {"st": 192, "ed": 194, "text": "training set"}]
[{"st": 3, "ed": 5, "text": "neural networks"}, {"st": 5, "ed": 9, "text": "recurrent neural networks rnns"}, {"st": 18, "ed": 21, "text": "weights and activations"}]
[{"st": 21, "ed": 23, "text": "grows exponentially"}, {"st": 32, "ed": 34, "text": "variational inference"}, {"st": 38, "ed": 40, "text": "efficient inference"}, {"st": 58, "ed": 60, "text": "discriminative learning"}, {"st": 63, "ed": 65, "text": "monte carlo"}, {"st": 84, "ed": 86, "text": "labeled data"}, {"st": 139, "ed": 141, "text": "excellent performance"}, {"st": 145, "ed": 147, "text": "network topologies"}]
[{"st": 20, "ed": 22, "text": "feature representation"}, {"st": 33, "ed": 35, "text": "machine learning"}, {"st": 39, "ed": 41, "text": "anomaly detection"}, {"st": 41, "ed": 43, "text": "link prediction"}, {"st": 47, "ed": 49, "text": "existing techniques"}, {"st": 50, "ed": 52, "text": "random walks"}, {"st": 71, "ed": 73, "text": "examples include"}, {"st": 75, "ed": 77, "text": "embedding methods"}, {"st": 86, "ed": 88, "text": "deep learning"}, {"st": 92, "ed": 94, "text": "random walk"}, {"st": 136, "ed": 138, "text": "feature vector"}, {"st": 173, "ed": 175, "text": "representation learning"}, {"st": 181, "ed": 183, "text": "random walk"}, {"st": 228, "ed": 230, "text": "existing methods"}, {"st": 237, "ed": 239, "text": "previous methods"}]
[{"st": 1, "ed": 3, "text": "reinforcement learning"}, {"st": 39, "ed": 41, "text": "reinforcement learning"}, {"st": 64, "ed": 66, "text": "reinforcement learning"}, {"st": 122, "ed": 124, "text": "existing results"}, {"st": 135, "ed": 137, "text": "reinforcement learning"}, {"st": 157, "ed": 159, "text": "significantly outperforms"}]
[{"st": 0, "ed": 3, "text": "multi label classification"}, {"st": 18, "ed": 20, "text": "similarity based"}, {"st": 22, "ed": 24, "text": "multi label"}, {"st": 31, "ed": 33, "text": "similarity based"}, {"st": 49, "ed": 52, "text": "multi label classification"}, {"st": 57, "ed": 59, "text": "compare favorably"}, {"st": 61, "ed": 63, "text": "wide variety"}, {"st": 64, "ed": 66, "text": "existing algorithms"}]
[{"st": 71, "ed": 74, "text": "number of clusters"}, {"st": 87, "ed": 90, "text": "number of clusters"}]
[{"st": 0, "ed": 4, "text": "generative adversarial network gan"}, {"st": 21, "ed": 23, "text": "higher dimensional"}, {"st": 30, "ed": 32, "text": "computational complexity"}, {"st": 44, "ed": 46, "text": "generative adversarial"}, {"st": 63, "ed": 67, "text": "number of model parameters"}, {"st": 88, "ed": 90, "text": "alternating optimization"}, {"st": 101, "ed": 103, "text": "achieve high"}]
[{"st": 0, "ed": 3, "text": "extreme learning machine"}, {"st": 7, "ed": 10, "text": "single hidden layer"}, {"st": 25, "ed": 27, "text": "hidden layer"}, {"st": 28, "ed": 30, "text": "randomly generated"}, {"st": 34, "ed": 36, "text": "output layer"}, {"st": 58, "ed": 61, "text": "extreme learning machine"}, {"st": 63, "ed": 66, "text": "extreme learning machine"}, {"st": 71, "ed": 73, "text": "rough set"}, {"st": 150, "ed": 152, "text": "classification task"}]
[{"st": 13, "ed": 15, "text": "transcription factor"}, {"st": 37, "ed": 40, "text": "multi label classification"}, {"st": 77, "ed": 79, "text": "deep architecture"}, {"st": 111, "ed": 113, "text": "few shot"}, {"st": 148, "ed": 150, "text": "significantly outperforms"}, {"st": 164, "ed": 166, "text": "deep learning"}, {"st": 177, "ed": 179, "text": "large scale"}]
[{"st": 1, "ed": 3, "text": "meta learning"}, {"st": 27, "ed": 29, "text": "previous tasks"}, {"st": 63, "ed": 65, "text": "meta learning"}, {"st": 69, "ed": 71, "text": "generalization error"}, {"st": 77, "ed": 79, "text": "pac bayes"}, {"st": 84, "ed": 86, "text": "takes place"}, {"st": 108, "ed": 110, "text": "prior knowledge"}, {"st": 130, "ed": 132, "text": "objective function"}, {"st": 150, "ed": 152, "text": "improved performance"}, {"st": 154, "ed": 156, "text": "meta learning"}, {"st": 163, "ed": 165, "text": "prior information"}]
[{"st": 0, "ed": 3, "text": "temporal difference td"}, {"st": 23, "ed": 25, "text": "q sigma"}, {"st": 36, "ed": 38, "text": "q sigma"}, {"st": 42, "ed": 44, "text": "multi step"}, {"st": 45, "ed": 47, "text": "q sigma"}, {"st": 54, "ed": 56, "text": "q sigma"}, {"st": 60, "ed": 62, "text": "q sigma"}, {"st": 70, "ed": 72, "text": "q sigma"}, {"st": 86, "ed": 88, "text": "q sigma"}]
[{"st": 0, "ed": 4, "text": "generative adversarial networks gans"}, {"st": 90, "ed": 92, "text": "classification problem"}, {"st": 94, "ed": 97, "text": "convex loss function"}, {"st": 115, "ed": 117, "text": "classification problem"}, {"st": 133, "ed": 135, "text": "classification problem"}, {"st": 144, "ed": 146, "text": "convex function"}, {"st": 182, "ed": 184, "text": "min max"}, {"st": 189, "ed": 191, "text": "logistic loss"}, {"st": 212, "ed": 215, "text": "convex loss function"}, {"st": 245, "ed": 247, "text": "kernel based"}]
[{"st": 5, "ed": 7, "text": "machine learning"}, {"st": 15, "ed": 17, "text": "predictive analytics"}, {"st": 23, "ed": 25, "text": "artificial intelligence"}, {"st": 30, "ed": 32, "text": "piecewise linear"}, {"st": 46, "ed": 48, "text": "competitive accuracy"}, {"st": 61, "ed": 63, "text": "rule based"}, {"st": 65, "ed": 67, "text": "sparse linear"}, {"st": 81, "ed": 84, "text": "high computational cost"}, {"st": 122, "ed": 124, "text": "sparse linear"}, {"st": 126, "ed": 128, "text": "distributed memory"}, {"st": 148, "ed": 152, "text": "number of training samples"}, {"st": 173, "ed": 175, "text": "prediction accuracy"}]
[{"st": 2, "ed": 4, "text": "continuous functions"}, {"st": 9, "ed": 11, "text": "real world"}, {"st": 13, "ed": 15, "text": "map inference"}, {"st": 16, "ed": 19, "text": "determinantal point processes"}, {"st": 21, "ed": 23, "text": "mean field"}, {"st": 37, "ed": 39, "text": "convex functions"}, {"st": 63, "ed": 65, "text": "continuous functions"}, {"st": 75, "ed": 77, "text": "geometric properties"}, {"st": 87, "ed": 89, "text": "stationary points"}, {"st": 90, "ed": 92, "text": "global optimum"}, {"st": 102, "ed": 104, "text": "optimization algorithms"}, {"st": 126, "ed": 128, "text": "existing methods"}, {"st": 131, "ed": 133, "text": "stationary points"}, {"st": 138, "ed": 140, "text": "recent progress"}, {"st": 150, "ed": 152, "text": "frank wolfe"}, {"st": 175, "ed": 177, "text": "continuous functions"}, {"st": 185, "ed": 187, "text": "theoretical findings"}, {"st": 190, "ed": 194, "text": "synthetic and real world"}]
[{"st": 0, "ed": 4, "text": "recurrent neural networks rnns"}, {"st": 16, "ed": 18, "text": "speech recognition"}, {"st": 18, "ed": 20, "text": "machine translation"}, {"st": 34, "ed": 36, "text": "deep learning"}, {"st": 106, "ed": 108, "text": "group lasso"}, {"st": 162, "ed": 165, "text": "loss in accuracy"}]
[{"st": 4, "ed": 6, "text": "unsupervised clustering"}, {"st": 19, "ed": 21, "text": "projection matrix"}, {"st": 45, "ed": 47, "text": "alternating minimization"}, {"st": 63, "ed": 65, "text": "k means"}, {"st": 84, "ed": 86, "text": "ell 1"}, {"st": 105, "ed": 107, "text": "alternating minimization"}, {"st": 137, "ed": 139, "text": "significantly improves"}, {"st": 143, "ed": 145, "text": "k means"}, {"st": 145, "ed": 147, "text": "spectral clustering"}]
[{"st": 0, "ed": 2, "text": "embedding methods"}, {"st": 4, "ed": 6, "text": "word embedding"}, {"st": 16, "ed": 18, "text": "embedding methods"}, {"st": 32, "ed": 34, "text": "linear transformation"}, {"st": 47, "ed": 49, "text": "approach yields"}, {"st": 75, "ed": 77, "text": "d dimensional"}, {"st": 78, "ed": 80, "text": "encoding scheme"}, {"st": 95, "ed": 97, "text": "d dimensional"}, {"st": 126, "ed": 128, "text": "semantically meaningful"}, {"st": 133, "ed": 135, "text": "discrete optimization"}, {"st": 153, "ed": 155, "text": "significantly improved"}]
[{"st": 3, "ed": 7, "text": "multi armed bandit problems"}, {"st": 12, "ed": 14, "text": "decision maker"}, {"st": 45, "ed": 47, "text": "random graph"}, {"st": 61, "ed": 63, "text": "thompson sampling"}, {"st": 78, "ed": 80, "text": "sampling based"}, {"st": 98, "ed": 100, "text": "regret bound"}, {"st": 122, "ed": 124, "text": "random graph"}, {"st": 129, "ed": 131, "text": "regret bound"}, {"st": 169, "ed": 171, "text": "random graph"}, {"st": 183, "ed": 185, "text": "outperform existing"}, {"st": 189, "ed": 192, "text": "upper confidence bound"}, {"st": 192, "ed": 194, "text": "epsilon greedy"}]
[{"st": 7, "ed": 9, "text": "deep learning"}, {"st": 14, "ed": 16, "text": "existing works"}, {"st": 26, "ed": 28, "text": "open source"}, {"st": 61, "ed": 63, "text": "deep learning"}, {"st": 91, "ed": 93, "text": "source code"}, {"st": 109, "ed": 111, "text": "automatically generated"}, {"st": 170, "ed": 172, "text": "proposed framework"}]
[{"st": 9, "ed": 12, "text": "entities and relations"}, {"st": 42, "ed": 44, "text": "existing methods"}, {"st": 51, "ed": 53, "text": "scoring function"}, {"st": 61, "ed": 63, "text": "scoring functions"}, {"st": 89, "ed": 91, "text": "neural network"}, {"st": 92, "ed": 94, "text": "score function"}, {"st": 112, "ed": 115, "text": "standard benchmark datasets"}]
[{"st": 3, "ed": 5, "text": "reinforcement learning"}, {"st": 30, "ed": 32, "text": "side information"}, {"st": 89, "ed": 92, "text": "markov decision processes"}, {"st": 143, "ed": 145, "text": "linear combinations"}, {"st": 147, "ed": 149, "text": "finite set"}, {"st": 159, "ed": 161, "text": "learning algorithm"}]
[{"st": 8, "ed": 10, "text": "classification results"}, {"st": 26, "ed": 28, "text": "existing works"}, {"st": 28, "ed": 30, "text": "generating adversarial"}, {"st": 39, "ed": 41, "text": "existing methods"}, {"st": 143, "ed": 145, "text": "extensive experiments"}]
[{"st": 0, "ed": 2, "text": "recommender systems"}, {"st": 19, "ed": 21, "text": "ranking based"}, {"st": 112, "ed": 114, "text": "web application"}, {"st": 185, "ed": 187, "text": "contextual information"}, {"st": 248, "ed": 250, "text": "recommendation systems"}, {"st": 265, "ed": 267, "text": "real world"}]
[{"st": 6, "ed": 8, "text": "reinforcement learning"}, {"st": 34, "ed": 36, "text": "limited data"}, {"st": 59, "ed": 62, "text": "deep neural networks"}]
[{"st": 2, "ed": 4, "text": "covariate shift"}, {"st": 7, "ed": 10, "text": "training and test"}, {"st": 22, "ed": 25, "text": "training and test"}, {"st": 54, "ed": 56, "text": "covariate shift"}, {"st": 73, "ed": 75, "text": "accurately estimate"}, {"st": 88, "ed": 90, "text": "linear subspaces"}, {"st": 104, "ed": 107, "text": "simulated and real"}]
[{"st": 11, "ed": 13, "text": "latent variable"}, {"st": 13, "ed": 16, "text": "probabilistic graphical models"}, {"st": 20, "ed": 22, "text": "predictive state"}, {"st": 23, "ed": 25, "text": "instrumental variable"}, {"st": 27, "ed": 31, "text": "reproducing kernel hilbert space"}, {"st": 37, "ed": 39, "text": "learning framework"}, {"st": 42, "ed": 44, "text": "latent variable"}, {"st": 48, "ed": 50, "text": "latent variable"}, {"st": 57, "ed": 59, "text": "parameter learning"}, {"st": 64, "ed": 66, "text": "supervised learning"}, {"st": 76, "ed": 78, "text": "belief propagation"}, {"st": 81, "ed": 83, "text": "junction tree"}, {"st": 110, "ed": 112, "text": "real world"}, {"st": 114, "ed": 116, "text": "computational biology"}, {"st": 117, "ed": 119, "text": "computer vision"}]
[{"st": 0, "ed": 3, "text": "deep reinforcement learning"}, {"st": 18, "ed": 20, "text": "human perception"}, {"st": 26, "ed": 28, "text": "low level"}, {"st": 28, "ed": 30, "text": "sensory input"}, {"st": 81, "ed": 83, "text": "low level"}, {"st": 83, "ed": 85, "text": "sensory input"}, {"st": 94, "ed": 96, "text": "proposed method"}, {"st": 99, "ed": 101, "text": "low level"}, {"st": 160, "ed": 163, "text": "deep q network"}, {"st": 166, "ed": 168, "text": "attention model"}, {"st": 181, "ed": 183, "text": "multi agent"}, {"st": 186, "ed": 188, "text": "multi agent"}, {"st": 195, "ed": 197, "text": "faster learning"}]
[{"st": 0, "ed": 2, "text": "learning algorithms"}, {"st": 4, "ed": 6, "text": "generative models"}, {"st": 26, "ed": 28, "text": "wasserstein distance"}, {"st": 33, "ed": 36, "text": "maximum mean discrepancy"}, {"st": 63, "ed": 65, "text": "global convergence"}, {"st": 69, "ed": 71, "text": "wasserstein distance"}]
[{"st": 48, "ed": 50, "text": "instance based"}, {"st": 74, "ed": 76, "text": "proposed method"}, {"st": 114, "ed": 116, "text": "proposed approach"}]
[{"st": 5, "ed": 7, "text": "bayesian network"}, {"st": 9, "ed": 11, "text": "observational data"}, {"st": 65, "ed": 67, "text": "causal relationships"}, {"st": 104, "ed": 106, "text": "causal relationships"}, {"st": 121, "ed": 123, "text": "training set"}, {"st": 137, "ed": 139, "text": "training set"}, {"st": 166, "ed": 168, "text": "simulated data"}, {"st": 171, "ed": 173, "text": "proposed approach"}, {"st": 190, "ed": 193, "text": "precision and recall"}]
[{"st": 0, "ed": 3, "text": "deep neural networks"}, {"st": 9, "ed": 11, "text": "training set"}, {"st": 19, "ed": 21, "text": "early stopping"}, {"st": 31, "ed": 33, "text": "adversarial examples"}, {"st": 37, "ed": 40, "text": "deep neural networks"}, {"st": 58, "ed": 61, "text": "deep neural networks"}, {"st": 86, "ed": 89, "text": "deep neural network"}, {"st": 107, "ed": 109, "text": "adversarial samples"}, {"st": 121, "ed": 123, "text": "adversarial samples"}, {"st": 126, "ed": 130, "text": "deep convolutional neural network"}, {"st": 140, "ed": 142, "text": "adversarial attacks"}, {"st": 151, "ed": 153, "text": "adversarial training"}]
[{"st": 1, "ed": 3, "text": "covariate shift"}, {"st": 4, "ed": 6, "text": "source data"}, {"st": 12, "ed": 14, "text": "input space"}, {"st": 26, "ed": 28, "text": "machine learning"}, {"st": 77, "ed": 79, "text": "source data"}, {"st": 119, "ed": 121, "text": "kernel methods"}, {"st": 134, "ed": 136, "text": "competing methods"}]
[{"st": 1, "ed": 3, "text": "probability distributions"}, {"st": 7, "ed": 9, "text": "neural networks"}, {"st": 54, "ed": 56, "text": "prior knowledge"}, {"st": 60, "ed": 62, "text": "structural information"}, {"st": 80, "ed": 82, "text": "matrix variate"}, {"st": 95, "ed": 98, "text": "sequential decision making"}, {"st": 98, "ed": 100, "text": "problems including"}, {"st": 100, "ed": 102, "text": "contextual bandits"}, {"st": 108, "ed": 111, "text": "synthetic and real"}]
[{"st": 1, "ed": 3, "text": "deep learning"}, {"st": 12, "ed": 14, "text": "deep learning"}, {"st": 41, "ed": 43, "text": "deep network"}, {"st": 66, "ed": 68, "text": "speech recognition"}, {"st": 68, "ed": 70, "text": "image recognition"}, {"st": 85, "ed": 87, "text": "deep learning"}, {"st": 90, "ed": 92, "text": "deep learning"}]
[{"st": 2, "ed": 6, "text": "deep reinforcement learning rl"}, {"st": 15, "ed": 17, "text": "decision making"}, {"st": 31, "ed": 33, "text": "sample complexity"}, {"st": 35, "ed": 37, "text": "convergence properties"}, {"st": 55, "ed": 57, "text": "real world"}, {"st": 67, "ed": 69, "text": "off policy"}, {"st": 71, "ed": 73, "text": "deep rl"}, {"st": 77, "ed": 79, "text": "maximum entropy"}, {"st": 79, "ed": 81, "text": "reinforcement learning"}, {"st": 90, "ed": 92, "text": "expected reward"}, {"st": 109, "ed": 111, "text": "deep rl"}, {"st": 125, "ed": 127, "text": "off policy"}, {"st": 136, "ed": 138, "text": "method achieves"}, {"st": 147, "ed": 149, "text": "continuous control"}, {"st": 149, "ed": 151, "text": "benchmark tasks"}, {"st": 156, "ed": 158, "text": "off policy"}, {"st": 167, "ed": 169, "text": "off policy"}, {"st": 177, "ed": 179, "text": "similar performance"}]
[{"st": 7, "ed": 9, "text": "point process"}, {"st": 12, "ed": 14, "text": "efficient learning"}, {"st": 17, "ed": 19, "text": "multi dimensional"}, {"st": 21, "ed": 23, "text": "point process"}, {"st": 44, "ed": 46, "text": "point processes"}, {"st": 99, "ed": 104, "text": "alternating direction method of multipliers"}, {"st": 121, "ed": 123, "text": "logistic regression"}, {"st": 144, "ed": 146, "text": "empirical results"}, {"st": 147, "ed": 149, "text": "real world"}, {"st": 160, "ed": 162, "text": "source code"}]
[{"st": 0, "ed": 2, "text": "deep learning"}, {"st": 33, "ed": 36, "text": "deep neural networks"}, {"st": 38, "ed": 40, "text": "decision making"}, {"st": 58, "ed": 60, "text": "deep learning"}, {"st": 73, "ed": 75, "text": "deep learning"}, {"st": 98, "ed": 100, "text": "neural networks"}, {"st": 103, "ed": 105, "text": "deep learning"}, {"st": 105, "ed": 107, "text": "visual analytics"}, {"st": 128, "ed": 130, "text": "visual analytics"}, {"st": 131, "ed": 133, "text": "deep learning"}, {"st": 171, "ed": 173, "text": "deep learning"}, {"st": 173, "ed": 175, "text": "visual analytics"}, {"st": 183, "ed": 185, "text": "open research"}, {"st": 195, "ed": 197, "text": "visual analytics"}, {"st": 198, "ed": 200, "text": "deep learning"}]
[{"st": 14, "ed": 17, "text": "extreme learning machine"}, {"st": 20, "ed": 23, "text": "feedforward neural networks"}, {"st": 31, "ed": 33, "text": "regularization technique"}, {"st": 35, "ed": 37, "text": "learning process"}, {"st": 60, "ed": 62, "text": "randomly chosen"}, {"st": 96, "ed": 98, "text": "hidden nodes"}, {"st": 115, "ed": 117, "text": "fully connected"}, {"st": 154, "ed": 156, "text": "least square"}, {"st": 159, "ed": 161, "text": "numerical simulations"}, {"st": 163, "ed": 165, "text": "benchmark problems"}]
[{"st": 1, "ed": 4, "text": "recurrent neural network"}, {"st": 8, "ed": 10, "text": "powerful tool"}, {"st": 12, "ed": 14, "text": "sequential data"}, {"st": 34, "ed": 36, "text": "external memory"}, {"st": 70, "ed": 72, "text": "proposed method"}, {"st": 97, "ed": 99, "text": "content based"}, {"st": 124, "ed": 126, "text": "prior knowledge"}]
[{"st": 43, "ed": 45, "text": "machine learning"}, {"st": 47, "ed": 50, "text": "deep neural networks"}, {"st": 66, "ed": 68, "text": "technique called"}, {"st": 72, "ed": 77, "text": "deep convolutional neural networks cnn"}, {"st": 87, "ed": 89, "text": "hidden layer"}, {"st": 91, "ed": 93, "text": "deep cnn"}, {"st": 102, "ed": 104, "text": "clustering algorithm"}, {"st": 114, "ed": 116, "text": "training data"}, {"st": 117, "ed": 119, "text": "random forest"}, {"st": 121, "ed": 123, "text": "learning algorithms"}, {"st": 149, "ed": 151, "text": "method achieves"}, {"st": 158, "ed": 160, "text": "without sacrificing"}, {"st": 166, "ed": 168, "text": "deep cnn"}, {"st": 177, "ed": 179, "text": "deep cnn"}]
[{"st": 3, "ed": 5, "text": "multi step"}, {"st": 6, "ed": 8, "text": "learning algorithm"}, {"st": 9, "ed": 11, "text": "q sigma"}, {"st": 37, "ed": 39, "text": "multi step"}, {"st": 39, "ed": 42, "text": "temporal difference learning"}, {"st": 43, "ed": 45, "text": "q sigma"}, {"st": 61, "ed": 63, "text": "off line"}, {"st": 84, "ed": 86, "text": "q sigma"}, {"st": 97, "ed": 99, "text": "q sigma"}, {"st": 128, "ed": 130, "text": "error bound"}, {"st": 131, "ed": 133, "text": "q sigma"}, {"st": 134, "ed": 136, "text": "policy evaluation"}, {"st": 140, "ed": 142, "text": "q sigma"}, {"st": 160, "ed": 163, "text": "temporal difference learning"}, {"st": 173, "ed": 175, "text": "q sigma"}, {"st": 181, "ed": 183, "text": "existing algorithms"}, {"st": 189, "ed": 191, "text": "significantly faster"}]
[{"st": 2, "ed": 4, "text": "policy iteration"}, {"st": 7, "ed": 9, "text": "policy improvement"}, {"st": 44, "ed": 46, "text": "policy improvement"}, {"st": 51, "ed": 53, "text": "empirical evidence"}, {"st": 81, "ed": 83, "text": "policy improvement"}, {"st": 99, "ed": 101, "text": "reinforcement learning"}, {"st": 115, "ed": 117, "text": "empirical success"}]
[{"st": 66, "ed": 68, "text": "convolutional neural"}, {"st": 75, "ed": 77, "text": "diabetic retinopathy"}, {"st": 79, "ed": 81, "text": "evaluation demonstrates"}]
[{"st": 0, "ed": 2, "text": "representation learning"}, {"st": 13, "ed": 15, "text": "representation learning"}, {"st": 22, "ed": 24, "text": "representation learning"}, {"st": 25, "ed": 27, "text": "learned features"}, {"st": 76, "ed": 79, "text": "curse of dimensionality"}, {"st": 89, "ed": 91, "text": "improve performance"}, {"st": 95, "ed": 97, "text": "learning algorithms"}, {"st": 113, "ed": 115, "text": "representation learning"}]
[{"st": 3, "ed": 6, "text": "end to end"}, {"st": 27, "ed": 29, "text": "near optimal"}, {"st": 31, "ed": 33, "text": "problem instances"}, {"st": 59, "ed": 61, "text": "policy gradient"}, {"st": 67, "ed": 69, "text": "trained model"}, {"st": 98, "ed": 101, "text": "training and inference"}, {"st": 123, "ed": 125, "text": "approach outperforms"}, {"st": 128, "ed": 130, "text": "medium sized"}, {"st": 142, "ed": 144, "text": "proposed framework"}, {"st": 167, "ed": 169, "text": "combinatorial optimization"}]
[{"st": 0, "ed": 2, "text": "deep learning"}, {"st": 7, "ed": 9, "text": "computer vision"}, {"st": 14, "ed": 16, "text": "domains including"}, {"st": 29, "ed": 31, "text": "risk management"}, {"st": 37, "ed": 39, "text": "uncertainty estimates"}, {"st": 72, "ed": 74, "text": "recent developments"}, {"st": 76, "ed": 78, "text": "deep learning"}, {"st": 85, "ed": 87, "text": "noisy observations"}, {"st": 112, "ed": 114, "text": "uncertainty quantification"}, {"st": 127, "ed": 129, "text": "super resolution"}, {"st": 157, "ed": 159, "text": "gaussian distribution"}, {"st": 162, "ed": 164, "text": "predictive accuracy"}, {"st": 181, "ed": 183, "text": "uncertainty estimates"}]
[{"st": 3, "ed": 6, "text": "evidence lower bound"}, {"st": 30, "ed": 32, "text": "variational autoencoder"}, {"st": 45, "ed": 47, "text": "disentangled representations"}, {"st": 65, "ed": 67, "text": "mutual information"}, {"st": 73, "ed": 76, "text": "quantitative and qualitative"}, {"st": 96, "ed": 98, "text": "latent variables"}]
[{"st": 1, "ed": 3, "text": "embedding methods"}, {"st": 45, "ed": 47, "text": "disentangled representations"}, {"st": 61, "ed": 64, "text": "propose and evaluate"}, {"st": 66, "ed": 68, "text": "loss function"}, {"st": 142, "ed": 145, "text": "few shot learning"}, {"st": 170, "ed": 172, "text": "disentangled representations"}, {"st": 189, "ed": 191, "text": "deep representations"}]
[{"st": 0, "ed": 2, "text": "recent developments"}, {"st": 37, "ed": 40, "text": "convolutional neural network"}]
[{"st": 6, "ed": 8, "text": "online optimization"}, {"st": 11, "ed": 13, "text": "objective functions"}, {"st": 36, "ed": 38, "text": "frank wolfe"}, {"st": 56, "ed": 58, "text": "regret bound"}, {"st": 59, "ed": 61, "text": "o sqrt"}, {"st": 69, "ed": 71, "text": "online optimization"}, {"st": 114, "ed": 116, "text": "regret bound"}, {"st": 117, "ed": 119, "text": "o sqrt"}, {"st": 143, "ed": 145, "text": "submodular functions"}, {"st": 163, "ed": 165, "text": "problem instances"}]
[{"st": 14, "ed": 17, "text": "online learning algorithms"}, {"st": 26, "ed": 28, "text": "sufficient conditions"}, {"st": 34, "ed": 36, "text": "step size"}, {"st": 102, "ed": 104, "text": "linear convergence"}, {"st": 111, "ed": 113, "text": "step size"}, {"st": 114, "ed": 117, "text": "a sufficient condition"}, {"st": 128, "ed": 130, "text": "error bounds"}, {"st": 130, "ed": 133, "text": "under mild conditions"}, {"st": 138, "ed": 140, "text": "loss function"}, {"st": 163, "ed": 165, "text": "strong convexity"}]
[{"st": 4, "ed": 6, "text": "computationally efficient"}, {"st": 21, "ed": 23, "text": "statistical models"}, {"st": 32, "ed": 34, "text": "heavy tailed"}, {"st": 62, "ed": 64, "text": "risk minimization"}, {"st": 73, "ed": 75, "text": "linear regression"}, {"st": 75, "ed": 77, "text": "logistic regression"}, {"st": 95, "ed": 97, "text": "computationally tractable"}, {"st": 110, "ed": 112, "text": "empirical performance"}, {"st": 117, "ed": 120, "text": "synthetic and real"}]
[{"st": 1, "ed": 3, "text": "multi objective"}, {"st": 13, "ed": 15, "text": "optimal solution"}, {"st": 19, "ed": 21, "text": "optimal policy"}, {"st": 51, "ed": 53, "text": "infinite set"}, {"st": 69, "ed": 71, "text": "gaussian processes"}, {"st": 72, "ed": 74, "text": "pairwise comparisons"}, {"st": 81, "ed": 83, "text": "multi objective"}, {"st": 83, "ed": 85, "text": "decision support"}, {"st": 99, "ed": 101, "text": "main contribution"}, {"st": 168, "ed": 170, "text": "decision support"}, {"st": 173, "ed": 175, "text": "real world"}]
[{"st": 1, "ed": 3, "text": "ensemble methods"}, {"st": 82, "ed": 84, "text": "base classifiers"}, {"st": 149, "ed": 151, "text": "cross validation"}, {"st": 167, "ed": 169, "text": "empirical analysis"}, {"st": 177, "ed": 179, "text": "neural networks"}, {"st": 187, "ed": 189, "text": "random forest"}]
[{"st": 8, "ed": 10, "text": "representation learning"}, {"st": 10, "ed": 12, "text": "link prediction"}, {"st": 13, "ed": 15, "text": "semi supervised"}, {"st": 28, "ed": 30, "text": "joint representation"}, {"st": 42, "ed": 45, "text": "multi task learning"}, {"st": 46, "ed": 48, "text": "link prediction"}, {"st": 66, "ed": 70, "text": "trained end to end"}, {"st": 78, "ed": 80, "text": "link prediction"}, {"st": 86, "ed": 88, "text": "empirical evaluation"}, {"st": 102, "ed": 104, "text": "significant improvement"}, {"st": 107, "ed": 109, "text": "related methods"}, {"st": 116, "ed": 120, "text": "available at https github.com"}, {"st": 122, "ed": 124, "text": "representation learning"}]
[{"st": 67, "ed": 69, "text": "theoretical bounds"}, {"st": 75, "ed": 77, "text": "observational data"}, {"st": 81, "ed": 83, "text": "domain adaptation"}, {"st": 92, "ed": 94, "text": "optimal policies"}, {"st": 95, "ed": 97, "text": "domain adversarial"}, {"st": 104, "ed": 106, "text": "domain adversarial"}, {"st": 118, "ed": 120, "text": "breast cancer"}]
[{"st": 78, "ed": 80, "text": "salient features"}, {"st": 100, "ed": 102, "text": "efficiently learn"}]
[{"st": 0, "ed": 4, "text": "nonnegative matrix factorization nmf"}, {"st": 5, "ed": 8, "text": "attracted much attention"}, {"st": 52, "ed": 54, "text": "distributed memory"}, {"st": 65, "ed": 68, "text": "block coordinate descent"}, {"st": 76, "ed": 79, "text": "block coordinate descent"}, {"st": 80, "ed": 82, "text": "closed form"}, {"st": 82, "ed": 84, "text": "update rules"}, {"st": 108, "ed": 110, "text": "per iteration"}, {"st": 114, "ed": 117, "text": "efficiency and scalability"}, {"st": 119, "ed": 121, "text": "proposed algorithm"}]
[{"st": 8, "ed": 10, "text": "reinforcement learning"}, {"st": 12, "ed": 14, "text": "finite horizon"}, {"st": 18, "ed": 20, "text": "agent learns"}, {"st": 32, "ed": 34, "text": "main contribution"}, {"st": 50, "ed": 52, "text": "regret bound"}, {"st": 54, "ed": 56, "text": "o sqrt"}, {"st": 97, "ed": 99, "text": "regret bound"}, {"st": 148, "ed": 150, "text": "non trivial"}, {"st": 153, "ed": 155, "text": "transition model"}, {"st": 156, "ed": 158, "text": "transition model"}, {"st": 191, "ed": 193, "text": "minimax optimal"}, {"st": 262, "ed": 264, "text": "asymptotically optimal"}]
[{"st": 51, "ed": 53, "text": "machine learning"}, {"st": 98, "ed": 100, "text": "machine learning"}, {"st": 127, "ed": 129, "text": "generalization ability"}, {"st": 130, "ed": 132, "text": "machine learning"}, {"st": 136, "ed": 138, "text": "benchmark datasets"}]
[{"st": 32, "ed": 34, "text": "image processing"}, {"st": 34, "ed": 36, "text": "pattern recognition"}, {"st": 40, "ed": 42, "text": "recent years"}, {"st": 59, "ed": 61, "text": "neural networks"}, {"st": 92, "ed": 94, "text": "complex data"}, {"st": 122, "ed": 124, "text": "least square"}, {"st": 146, "ed": 148, "text": "least squares"}, {"st": 149, "ed": 151, "text": "l 0"}, {"st": 157, "ed": 159, "text": "l 2"}, {"st": 188, "ed": 190, "text": "least square"}]
[{"st": 47, "ed": 49, "text": "low level"}, {"st": 55, "ed": 57, "text": "expert demonstrations"}, {"st": 78, "ed": 80, "text": "imitation learning"}, {"st": 82, "ed": 84, "text": "reinforcement learning"}, {"st": 100, "ed": 102, "text": "empirically demonstrate"}, {"st": 107, "ed": 109, "text": "significantly faster"}, {"st": 120, "ed": 122, "text": "sample efficient"}, {"st": 128, "ed": 130, "text": "provide theoretical"}]
[{"st": 11, "ed": 13, "text": "neural networks"}, {"st": 17, "ed": 19, "text": "local minima"}, {"st": 21, "ed": 23, "text": "similar performance"}, {"st": 63, "ed": 65, "text": "neural networks"}, {"st": 74, "ed": 76, "text": "training error"}, {"st": 80, "ed": 82, "text": "local minima"}, {"st": 85, "ed": 87, "text": "hinge loss"}, {"st": 105, "ed": 107, "text": "surrogate loss"}, {"st": 125, "ed": 127, "text": "loss function"}, {"st": 130, "ed": 132, "text": "quadratic loss"}, {"st": 133, "ed": 135, "text": "logistic loss"}]
[{"st": 3, "ed": 5, "text": "theoretical insights"}, {"st": 27, "ed": 29, "text": "training data"}, {"st": 42, "ed": 44, "text": "local search"}, {"st": 49, "ed": 51, "text": "globally optimal"}, {"st": 69, "ed": 71, "text": "sample size"}, {"st": 74, "ed": 76, "text": "rademacher complexity"}, {"st": 79, "ed": 81, "text": "weight decay"}, {"st": 106, "ed": 108, "text": "loss function"}]
[{"st": 5, "ed": 7, "text": "traffic flow"}, {"st": 20, "ed": 22, "text": "multi step"}, {"st": 22, "ed": 24, "text": "traffic flow"}, {"st": 80, "ed": 82, "text": "multi step"}, {"st": 82, "ed": 84, "text": "traffic flow"}, {"st": 84, "ed": 86, "text": "prediction performance"}, {"st": 89, "ed": 91, "text": "multi output"}, {"st": 124, "ed": 126, "text": "conditional generative"}, {"st": 136, "ed": 138, "text": "prediction performance"}, {"st": 140, "ed": 142, "text": "multi output"}, {"st": 147, "ed": 149, "text": "real world"}, {"st": 149, "ed": 151, "text": "traffic flow"}, {"st": 159, "ed": 161, "text": "multi step"}, {"st": 161, "ed": 163, "text": "traffic flow"}, {"st": 167, "ed": 169, "text": "multi output"}]
[{"st": 0, "ed": 2, "text": "predictive models"}, {"st": 4, "ed": 6, "text": "historical data"}, {"st": 53, "ed": 55, "text": "existing methods"}, {"st": 80, "ed": 82, "text": "existing methods"}, {"st": 85, "ed": 87, "text": "anti discrimination"}, {"st": 90, "ed": 92, "text": "classification problems"}, {"st": 95, "ed": 97, "text": "directly applied"}, {"st": 129, "ed": 131, "text": "causal graph"}, {"st": 154, "ed": 156, "text": "causal graph"}]
[{"st": 5, "ed": 7, "text": "multi objective"}, {"st": 7, "ed": 10, "text": "deep reinforcement learning"}, {"st": 50, "ed": 52, "text": "proposed approach"}, {"st": 62, "ed": 64, "text": "proposed framework"}, {"st": 71, "ed": 74, "text": "deep reinforcement learning"}]
[{"st": 0, "ed": 2, "text": "network quantization"}, {"st": 8, "ed": 11, "text": "deep neural networks"}, {"st": 15, "ed": 17, "text": "network quantization"}, {"st": 37, "ed": 39, "text": "network quantization"}, {"st": 42, "ed": 44, "text": "network quantization"}, {"st": 50, "ed": 52, "text": "multi level"}, {"st": 52, "ed": 54, "text": "network quantization"}, {"st": 68, "ed": 70, "text": "network quantization"}, {"st": 139, "ed": 141, "text": "extensive experiments"}, {"st": 148, "ed": 150, "text": "neural networks"}, {"st": 152, "ed": 154, "text": "vgg 16"}]
[{"st": 4, "ed": 7, "text": "provide theoretical guarantees"}, {"st": 17, "ed": 19, "text": "special case"}, {"st": 21, "ed": 23, "text": "reinforcement learning"}, {"st": 83, "ed": 86, "text": "easy to implement"}, {"st": 91, "ed": 93, "text": "computational resources"}, {"st": 104, "ed": 106, "text": "nearest neighbor"}, {"st": 113, "ed": 115, "text": "reinforcement learning"}]
[{"st": 1, "ed": 3, "text": "reinforcement learning"}, {"st": 18, "ed": 20, "text": "previous studies"}, {"st": 26, "ed": 28, "text": "worst case"}, {"st": 65, "ed": 67, "text": "optimal policy"}]
[{"st": 7, "ed": 10, "text": "deep reinforcement learning"}, {"st": 10, "ed": 12, "text": "neural network"}, {"st": 53, "ed": 55, "text": "imitation learning"}, {"st": 57, "ed": 60, "text": "temporal difference td"}, {"st": 60, "ed": 62, "text": "reinforcement learning"}, {"st": 103, "ed": 105, "text": "significantly improves"}]
[{"st": 0, "ed": 2, "text": "reinforcement learning"}, {"st": 22, "ed": 24, "text": "real world"}, {"st": 37, "ed": 39, "text": "reinforcement learning"}, {"st": 62, "ed": 64, "text": "learning task"}, {"st": 67, "ed": 69, "text": "robotic arm"}, {"st": 150, "ed": 152, "text": "reinforcement learning"}, {"st": 156, "ed": 158, "text": "real world"}]
[{"st": 59, "ed": 61, "text": "partially observed"}, {"st": 61, "ed": 64, "text": "markov decision processes"}, {"st": 87, "ed": 89, "text": "finite horizon"}, {"st": 89, "ed": 91, "text": "regret bounds"}]
[{"st": 6, "ed": 8, "text": "feature selection"}, {"st": 8, "ed": 10, "text": "learning algorithms"}, {"st": 41, "ed": 43, "text": "performance guarantees"}, {"st": 55, "ed": 57, "text": "small sample"}, {"st": 73, "ed": 75, "text": "theoretical bounds"}, {"st": 118, "ed": 120, "text": "pac bayes"}, {"st": 147, "ed": 149, "text": "dna microarray"}, {"st": 183, "ed": 185, "text": "classification accuracy"}]
[{"st": 0, "ed": 2, "text": "low rank"}, {"st": 11, "ed": 13, "text": "machine learning"}, {"st": 15, "ed": 17, "text": "large scale"}, {"st": 43, "ed": 45, "text": "low rank"}, {"st": 48, "ed": 50, "text": "sampling based"}, {"st": 91, "ed": 93, "text": "accurately estimate"}, {"st": 129, "ed": 131, "text": "matrix approximation"}, {"st": 139, "ed": 142, "text": "extensive experimental results"}, {"st": 143, "ed": 146, "text": "synthetic and real"}, {"st": 150, "ed": 152, "text": "worst case"}, {"st": 152, "ed": 154, "text": "theoretical analysis"}, {"st": 163, "ed": 165, "text": "proposed algorithm"}, {"st": 166, "ed": 168, "text": "low rank"}, {"st": 197, "ed": 199, "text": "sampling based"}, {"st": 199, "ed": 201, "text": "matrix approximation"}]
[{"st": 2, "ed": 5, "text": "unsupervised pre training"}, {"st": 6, "ed": 8, "text": "deep architectures"}, {"st": 15, "ed": 17, "text": "denoising autoencoders"}, {"st": 22, "ed": 24, "text": "pre processing"}, {"st": 31, "ed": 33, "text": "significant improvements"}, {"st": 57, "ed": 60, "text": "layer by layer"}, {"st": 130, "ed": 133, "text": "deep neural networks"}, {"st": 138, "ed": 140, "text": "deep learning"}]
[{"st": 8, "ed": 10, "text": "online setting"}, {"st": 43, "ed": 45, "text": "cost sensitive"}, {"st": 64, "ed": 66, "text": "classification rules"}, {"st": 82, "ed": 84, "text": "optimal regret"}]
[{"st": 0, "ed": 2, "text": "multi step"}, {"st": 10, "ed": 12, "text": "time series"}, {"st": 52, "ed": 54, "text": "multi step"}, {"st": 72, "ed": 74, "text": "large scale"}, {"st": 113, "ed": 115, "text": "multi step"}, {"st": 132, "ed": 134, "text": "multiple output"}]
[{"st": 31, "ed": 33, "text": "previously proposed"}, {"st": 79, "ed": 81, "text": "variational bayesian"}, {"st": 95, "ed": 97, "text": "object tracking"}]
[{"st": 22, "ed": 26, "text": "expectation maximization em algorithm"}, {"st": 34, "ed": 36, "text": "log likelihood"}, {"st": 43, "ed": 45, "text": "log likelihood"}, {"st": 52, "ed": 54, "text": "log likelihood"}, {"st": 80, "ed": 82, "text": "maximum entropy"}, {"st": 141, "ed": 143, "text": "log likelihood"}, {"st": 194, "ed": 197, "text": "easy to implement"}, {"st": 214, "ed": 216, "text": "bayesian network"}]
[{"st": 10, "ed": 12, "text": "active learning"}, {"st": 40, "ed": 42, "text": "linear classifiers"}, {"st": 44, "ed": 46, "text": "squared loss"}, {"st": 55, "ed": 57, "text": "weighted average"}, {"st": 60, "ed": 62, "text": "recent results"}, {"st": 83, "ed": 85, "text": "empirical comparison"}, {"st": 95, "ed": 97, "text": "previously proposed"}, {"st": 104, "ed": 106, "text": "empirical performance"}]
[{"st": 17, "ed": 19, "text": "mixture components"}, {"st": 27, "ed": 29, "text": "observed variables"}, {"st": 35, "ed": 37, "text": "graph structure"}, {"st": 47, "ed": 49, "text": "mixture components"}, {"st": 55, "ed": 57, "text": "mixture model"}, {"st": 66, "ed": 68, "text": "graphical model"}, {"st": 87, "ed": 89, "text": "mixture components"}, {"st": 129, "ed": 131, "text": "maximum likelihood"}, {"st": 180, "ed": 182, "text": "observed variables"}, {"st": 192, "ed": 194, "text": "mixture components"}]
[{"st": 8, "ed": 10, "text": "hidden state"}, {"st": 29, "ed": 31, "text": "generative framework"}, {"st": 63, "ed": 65, "text": "online learning"}]
[{"st": 83, "ed": 85, "text": "prior knowledge"}, {"st": 112, "ed": 114, "text": "sample complexity"}, {"st": 115, "ed": 117, "text": "reinforcement learning"}, {"st": 129, "ed": 131, "text": "countably infinite"}]
[{"st": 58, "ed": 60, "text": "sample size"}, {"st": 80, "ed": 82, "text": "causal discovery"}, {"st": 135, "ed": 137, "text": "sample sizes"}]
[{"st": 27, "ed": 30, "text": "dynamic bayesian networks"}, {"st": 50, "ed": 52, "text": "discrete state"}, {"st": 61, "ed": 63, "text": "continuous state"}, {"st": 66, "ed": 68, "text": "differential equations"}, {"st": 86, "ed": 89, "text": "dynamic bayesian networks"}, {"st": 92, "ed": 94, "text": "compact representations"}, {"st": 105, "ed": 107, "text": "globally optimal"}]
[{"st": 3, "ed": 5, "text": "probabilistic model"}, {"st": 35, "ed": 37, "text": "efficient inference"}, {"st": 48, "ed": 50, "text": "em algorithm"}]
[{"st": 0, "ed": 2, "text": "monte carlo"}, {"st": 49, "ed": 51, "text": "bayes optimal"}, {"st": 134, "ed": 136, "text": "off policy"}]
[{"st": 4, "ed": 6, "text": "fixed points"}, {"st": 7, "ed": 10, "text": "loopy belief propagation"}, {"st": 13, "ed": 15, "text": "stationary points"}, {"st": 24, "ed": 27, "text": "bethe free energy"}, {"st": 83, "ed": 85, "text": "linear functions"}, {"st": 122, "ed": 124, "text": "stationary point"}, {"st": 141, "ed": 143, "text": "linear combination"}, {"st": 145, "ed": 147, "text": "log partition"}, {"st": 156, "ed": 158, "text": "linear combination"}]
[{"st": 0, "ed": 2, "text": "learning algorithms"}, {"st": 45, "ed": 47, "text": "time consuming"}, {"st": 50, "ed": 52, "text": "ground truth"}, {"st": 56, "ed": 59, "text": "semi supervised learning"}, {"st": 65, "ed": 67, "text": "unlabeled data"}, {"st": 78, "ed": 80, "text": "semi supervised"}, {"st": 91, "ed": 93, "text": "labeled data"}, {"st": 112, "ed": 114, "text": "proposed approach"}, {"st": 126, "ed": 128, "text": "true label"}, {"st": 135, "ed": 138, "text": "labeled and unlabeled"}, {"st": 172, "ed": 174, "text": "unlabeled data"}]
[{"st": 0, "ed": 3, "text": "multi task learning"}, {"st": 5, "ed": 7, "text": "learning paradigm"}, {"st": 12, "ed": 14, "text": "generalization performance"}, {"st": 16, "ed": 18, "text": "learning task"}, {"st": 67, "ed": 69, "text": "method called"}, {"st": 69, "ed": 71, "text": "multi task"}, {"st": 95, "ed": 97, "text": "objective function"}, {"st": 130, "ed": 133, "text": "multi task learning"}, {"st": 154, "ed": 157, "text": "multi task learning"}, {"st": 158, "ed": 160, "text": "experiments conducted"}, {"st": 168, "ed": 171, "text": "benchmark data sets"}]
[{"st": 11, "ed": 13, "text": "hierarchical structure"}, {"st": 23, "ed": 25, "text": "layer wise"}, {"st": 36, "ed": 38, "text": "learning algorithm"}, {"st": 45, "ed": 47, "text": "activation functions"}, {"st": 67, "ed": 69, "text": "real data"}, {"st": 89, "ed": 91, "text": "generative model"}]
[{"st": 6, "ed": 8, "text": "increasingly important"}, {"st": 30, "ed": 33, "text": "statistical relational learning"}, {"st": 165, "ed": 167, "text": "competing approaches"}]
[{"st": 22, "ed": 24, "text": "training data"}, {"st": 45, "ed": 47, "text": "gibbs sampling"}, {"st": 55, "ed": 57, "text": "search space"}, {"st": 65, "ed": 67, "text": "em algorithm"}, {"st": 125, "ed": 127, "text": "em algorithm"}, {"st": 130, "ed": 132, "text": "brain tumor"}, {"st": 136, "ed": 139, "text": "advantages and disadvantages"}]
[{"st": 0, "ed": 2, "text": "joint distributions"}, {"st": 17, "ed": 19, "text": "conditional distributions"}, {"st": 34, "ed": 36, "text": "computationally expensive"}, {"st": 43, "ed": 45, "text": "continuous variables"}, {"st": 46, "ed": 48, "text": "complex nonlinear"}, {"st": 57, "ed": 59, "text": "joint distribution"}, {"st": 66, "ed": 68, "text": "previous research"}, {"st": 101, "ed": 103, "text": "wide variety"}, {"st": 166, "ed": 170, "text": "bayesian network structure learning"}, {"st": 191, "ed": 193, "text": "joint probability"}, {"st": 197, "ed": 199, "text": "continuous variables"}]
[{"st": 2, "ed": 4, "text": "supervised learning"}, {"st": 67, "ed": 69, "text": "probabilistic model"}, {"st": 98, "ed": 100, "text": "undirected models"}, {"st": 118, "ed": 120, "text": "undirected models"}, {"st": 156, "ed": 158, "text": "probabilistic inference"}, {"st": 163, "ed": 165, "text": "collective classification"}, {"st": 176, "ed": 178, "text": "classification task"}, {"st": 183, "ed": 185, "text": "significantly improved"}]
[{"st": 26, "ed": 28, "text": "iterative algorithms"}, {"st": 54, "ed": 56, "text": "models including"}, {"st": 56, "ed": 59, "text": "undirected graphical models"}, {"st": 64, "ed": 66, "text": "chain graphs"}, {"st": 80, "ed": 82, "text": "undirected models"}, {"st": 96, "ed": 98, "text": "numerical simulations"}]
[{"st": 6, "ed": 8, "text": "approximate inference"}, {"st": 9, "ed": 11, "text": "probabilistic programs"}, {"st": 27, "ed": 29, "text": "probabilistic program"}, {"st": 55, "ed": 57, "text": "mean field"}, {"st": 57, "ed": 59, "text": "probabilistic programs"}]
[{"st": 11, "ed": 13, "text": "bayesian network"}, {"st": 25, "ed": 27, "text": "conditional independence"}, {"st": 66, "ed": 68, "text": "cross entropy"}, {"st": 71, "ed": 73, "text": "conditional independence"}, {"st": 86, "ed": 89, "text": "goodness of fit"}]
[{"st": 5, "ed": 7, "text": "probabilistic models"}, {"st": 27, "ed": 29, "text": "hidden variables"}, {"st": 72, "ed": 74, "text": "score based"}, {"st": 122, "ed": 125, "text": "synthetic and real"}, {"st": 135, "ed": 137, "text": "hidden variables"}]
[{"st": 14, "ed": 16, "text": "joint distribution"}, {"st": 49, "ed": 51, "text": "document classification"}, {"st": 131, "ed": 133, "text": "iterative algorithms"}]
[{"st": 15, "ed": 17, "text": "improved performance"}, {"st": 20, "ed": 22, "text": "previous research"}, {"st": 28, "ed": 30, "text": "conditional probability"}, {"st": 33, "ed": 35, "text": "neural networks"}, {"st": 45, "ed": 47, "text": "decision trees"}, {"st": 121, "ed": 123, "text": "generated data"}, {"st": 166, "ed": 168, "text": "adventure game"}]
[{"st": 1, "ed": 3, "text": "search space"}, {"st": 4, "ed": 6, "text": "bayesian network"}, {"st": 63, "ed": 65, "text": "equivalence classes"}, {"st": 72, "ed": 74, "text": "approach produces"}, {"st": 120, "ed": 122, "text": "equivalence class"}, {"st": 129, "ed": 131, "text": "approach produces"}, {"st": 146, "ed": 148, "text": "empirical results"}, {"st": 155, "ed": 159, "text": "markov chain monte carlo"}]
[{"st": 54, "ed": 56, "text": "maximum likelihood"}, {"st": 77, "ed": 79, "text": "combinatorial optimization"}, {"st": 86, "ed": 88, "text": "maximum likelihood"}, {"st": 106, "ed": 108, "text": "integer programming"}, {"st": 109, "ed": 111, "text": "approximation algorithms"}, {"st": 113, "ed": 115, "text": "performance guarantees"}, {"st": 158, "ed": 160, "text": "np hard"}]
[{"st": 5, "ed": 7, "text": "reinforcement learning"}, {"st": 59, "ed": 61, "text": "bias variance"}, {"st": 111, "ed": 113, "text": "policy gradient"}, {"st": 119, "ed": 121, "text": "experiments demonstrate"}]
[{"st": 1, "ed": 4, "text": "continuous speech recognition"}, {"st": 12, "ed": 14, "text": "real world"}, {"st": 19, "ed": 21, "text": "large vocabulary"}, {"st": 42, "ed": 45, "text": "hidden markov models"}, {"st": 55, "ed": 57, "text": "speech signal"}]
[{"st": 17, "ed": 19, "text": "conditional independence"}, {"st": 21, "ed": 23, "text": "random variables"}, {"st": 47, "ed": 49, "text": "class conditional"}, {"st": 49, "ed": 51, "text": "network structures"}, {"st": 59, "ed": 61, "text": "posterior probability"}, {"st": 72, "ed": 74, "text": "structure learning"}, {"st": 86, "ed": 88, "text": "speech recognition"}, {"st": 103, "ed": 105, "text": "maximum likelihood"}, {"st": 114, "ed": 117, "text": "dynamic bayesian networks"}]
[{"st": 74, "ed": 76, "text": "posterior probability"}, {"st": 98, "ed": 100, "text": "efficiently compute"}, {"st": 163, "ed": 167, "text": "markov chain monte carlo"}, {"st": 201, "ed": 203, "text": "empirical results"}, {"st": 204, "ed": 207, "text": "synthetic and real"}, {"st": 222, "ed": 224, "text": "network structures"}]
[{"st": 41, "ed": 43, "text": "marginal likelihood"}, {"st": 56, "ed": 58, "text": "closed form"}, {"st": 70, "ed": 72, "text": "computational cost"}, {"st": 97, "ed": 99, "text": "gaussian process"}, {"st": 147, "ed": 149, "text": "gaussian process"}, {"st": 160, "ed": 162, "text": "empirical results"}, {"st": 169, "ed": 171, "text": "real life"}]
[{"st": 12, "ed": 15, "text": "structure and parameters"}, {"st": 50, "ed": 52, "text": "main results"}, {"st": 107, "ed": 109, "text": "exact bayesian"}, {"st": 121, "ed": 123, "text": "latent variable"}]
[{"st": 13, "ed": 15, "text": "high dimensional"}, {"st": 40, "ed": 42, "text": "sampling distribution"}, {"st": 56, "ed": 58, "text": "sampling distribution"}, {"st": 72, "ed": 75, "text": "stochastic gradient descent"}, {"st": 80, "ed": 82, "text": "sampling distribution"}, {"st": 94, "ed": 97, "text": "stochastic gradient descent"}, {"st": 109, "ed": 111, "text": "sampling distribution"}]
[{"st": 24, "ed": 26, "text": "exact inference"}, {"st": 36, "ed": 38, "text": "mean field"}, {"st": 43, "ed": 45, "text": "mean field"}, {"st": 64, "ed": 66, "text": "highly correlated"}, {"st": 78, "ed": 80, "text": "posterior distribution"}, {"st": 110, "ed": 112, "text": "update rules"}, {"st": 124, "ed": 126, "text": "variational approximation"}, {"st": 138, "ed": 140, "text": "mean field"}, {"st": 144, "ed": 147, "text": "quantitative and qualitative"}]
[{"st": 14, "ed": 17, "text": "branch and bound"}, {"st": 20, "ed": 22, "text": "bayesian network"}, {"st": 26, "ed": 29, "text": "minimum description length"}, {"st": 43, "ed": 45, "text": "network structures"}, {"st": 56, "ed": 58, "text": "preliminary experiments"}, {"st": 87, "ed": 89, "text": "heuristic search"}]
[{"st": 7, "ed": 9, "text": "hierarchical clustering"}, {"st": 12, "ed": 14, "text": "objective function"}, {"st": 32, "ed": 34, "text": "feature set"}, {"st": 97, "ed": 99, "text": "document clustering"}, {"st": 105, "ed": 107, "text": "likelihood function"}, {"st": 153, "ed": 155, "text": "marginal likelihood"}, {"st": 162, "ed": 165, "text": "number of clusters"}, {"st": 192, "ed": 194, "text": "synthetic data"}]
[{"st": 1, "ed": 3, "text": "variational approximations"}, {"st": 6, "ed": 8, "text": "mean field"}, {"st": 17, "ed": 19, "text": "mean field"}, {"st": 43, "ed": 45, "text": "mean field"}, {"st": 62, "ed": 64, "text": "mean field"}, {"st": 68, "ed": 70, "text": "junction tree"}, {"st": 91, "ed": 93, "text": "mean field"}, {"st": 124, "ed": 126, "text": "variational approximations"}]
[{"st": 16, "ed": 19, "text": "inverse reinforcement learning"}, {"st": 35, "ed": 37, "text": "active learning"}, {"st": 41, "ed": 43, "text": "learning agent"}, {"st": 55, "ed": 57, "text": "sample efficient"}, {"st": 66, "ed": 69, "text": "inverse reinforcement learning"}, {"st": 78, "ed": 80, "text": "sample complexity"}, {"st": 101, "ed": 103, "text": "sample complexity"}, {"st": 113, "ed": 115, "text": "existing methods"}, {"st": 123, "ed": 126, "text": "multi class classification"}, {"st": 133, "ed": 136, "text": "learning from demonstration"}]
[{"st": 4, "ed": 6, "text": "empirically evaluate"}, {"st": 16, "ed": 18, "text": "naive bayes"}, {"st": 20, "ed": 22, "text": "naive bayes"}, {"st": 24, "ed": 26, "text": "naive bayes"}, {"st": 40, "ed": 42, "text": "conditional independence"}, {"st": 105, "ed": 107, "text": "demonstrate empirically"}, {"st": 126, "ed": 128, "text": "machine learning"}, {"st": 129, "ed": 131, "text": "data mining"}]
[{"st": 1, "ed": 3, "text": "recent years"}, {"st": 6, "ed": 8, "text": "significant progress"}, {"st": 14, "ed": 16, "text": "bayesian networks"}, {"st": 20, "ed": 22, "text": "complex data"}, {"st": 110, "ed": 112, "text": "computationally efficient"}]
[{"st": 15, "ed": 17, "text": "naive bayes"}, {"st": 21, "ed": 23, "text": "finite mixture"}, {"st": 32, "ed": 34, "text": "strong assumptions"}, {"st": 49, "ed": 51, "text": "classification performance"}, {"st": 73, "ed": 75, "text": "finite mixture"}, {"st": 84, "ed": 86, "text": "naive bayes"}, {"st": 94, "ed": 96, "text": "predictive performance"}, {"st": 97, "ed": 99, "text": "real datasets"}, {"st": 105, "ed": 107, "text": "predictive performance"}]
[{"st": 18, "ed": 20, "text": "conditional probability"}, {"st": 21, "ed": 23, "text": "recent research"}, {"st": 39, "ed": 41, "text": "log linear"}, {"st": 44, "ed": 46, "text": "log linear"}, {"st": 64, "ed": 66, "text": "structure learning"}, {"st": 72, "ed": 74, "text": "naive bayes"}, {"st": 83, "ed": 86, "text": "minimum message length"}, {"st": 91, "ed": 93, "text": "structure learning"}]
[{"st": 1, "ed": 3, "text": "recent years"}, {"st": 12, "ed": 14, "text": "bayesian networks"}, {"st": 36, "ed": 38, "text": "incomplete data"}, {"st": 44, "ed": 46, "text": "missing values"}, {"st": 64, "ed": 68, "text": "expectation maximization em algorithm"}, {"st": 133, "ed": 135, "text": "probabilistic models"}]
[{"st": 2, "ed": 4, "text": "computationally efficient"}, {"st": 16, "ed": 18, "text": "graphical model"}, {"st": 77, "ed": 79, "text": "posterior probability"}, {"st": 94, "ed": 97, "text": "synthetic and real"}]
[{"st": 17, "ed": 19, "text": "decision trees"}, {"st": 77, "ed": 79, "text": "estimation problem"}, {"st": 82, "ed": 84, "text": "modified version"}]
[{"st": 2, "ed": 5, "text": "deep neural networks"}, {"st": 14, "ed": 16, "text": "quadratic function"}, {"st": 22, "ed": 24, "text": "deep architectures"}, {"st": 37, "ed": 39, "text": "main goal"}, {"st": 48, "ed": 51, "text": "layer by layer"}, {"st": 76, "ed": 78, "text": "training error"}, {"st": 110, "ed": 112, "text": "deep architecture"}]
[{"st": 0, "ed": 2, "text": "online learning"}, {"st": 6, "ed": 9, "text": "received increasing attention"}, {"st": 17, "ed": 19, "text": "web based"}, {"st": 106, "ed": 108, "text": "ucb algorithm"}, {"st": 113, "ed": 115, "text": "bandit problem"}]
[{"st": 30, "ed": 32, "text": "sample complexity"}, {"st": 38, "ed": 40, "text": "objective function"}, {"st": 59, "ed": 61, "text": "objective function"}, {"st": 62, "ed": 64, "text": "surrogate model"}, {"st": 71, "ed": 73, "text": "objective function"}, {"st": 95, "ed": 97, "text": "surrogate model"}, {"st": 100, "ed": 103, "text": "kullback leibler divergence"}, {"st": 110, "ed": 112, "text": "training distribution"}, {"st": 122, "ed": 124, "text": "proposed approach"}, {"st": 124, "ed": 126, "text": "shows significant"}, {"st": 126, "ed": 128, "text": "performance gains"}, {"st": 135, "ed": 137, "text": "benchmark problems"}, {"st": 147, "ed": 149, "text": "quasi newton"}]
[{"st": 0, "ed": 3, "text": "generalized linear models"}, {"st": 13, "ed": 15, "text": "linear regression"}, {"st": 46, "ed": 48, "text": "local search"}, {"st": 123, "ed": 125, "text": "empirical study"}]
[{"st": 4, "ed": 6, "text": "nonparametric bayesian"}, {"st": 14, "ed": 16, "text": "domain knowledge"}, {"st": 25, "ed": 27, "text": "posterior distributions"}, {"st": 28, "ed": 30, "text": "bayes rule"}, {"st": 60, "ed": 62, "text": "posterior inference"}, {"st": 64, "ed": 66, "text": "regularization term"}, {"st": 71, "ed": 73, "text": "posterior distribution"}, {"st": 87, "ed": 89, "text": "expert knowledge"}, {"st": 118, "ed": 120, "text": "linear operator"}, {"st": 122, "ed": 124, "text": "posterior distributions"}, {"st": 133, "ed": 135, "text": "convex analysis"}, {"st": 152, "ed": 155, "text": "support vector machines"}, {"st": 157, "ed": 159, "text": "multi task"}, {"st": 161, "ed": 164, "text": "support vector machines"}, {"st": 169, "ed": 171, "text": "large margin"}, {"st": 176, "ed": 178, "text": "nonparametric bayesian"}, {"st": 182, "ed": 184, "text": "latent features"}, {"st": 187, "ed": 190, "text": "multi task learning"}, {"st": 193, "ed": 195, "text": "efficient inference"}, {"st": 198, "ed": 200, "text": "empirical studies"}, {"st": 202, "ed": 204, "text": "benchmark datasets"}, {"st": 213, "ed": 215, "text": "large margin"}]
[{"st": 7, "ed": 9, "text": "multi task"}, {"st": 9, "ed": 11, "text": "gaussian processes"}, {"st": 44, "ed": 46, "text": "multi task"}, {"st": 46, "ed": 48, "text": "gaussian processes"}, {"st": 72, "ed": 74, "text": "large scale"}]
[{"st": 82, "ed": 84, "text": "kernel function"}]
[{"st": 16, "ed": 18, "text": "random field"}, {"st": 23, "ed": 26, "text": "a posteriori map"}, {"st": 45, "ed": 47, "text": "most probable"}, {"st": 62, "ed": 64, "text": "message passing"}, {"st": 78, "ed": 80, "text": "recently proposed"}, {"st": 80, "ed": 82, "text": "linear programming"}, {"st": 91, "ed": 94, "text": "orders of magnitude"}]
[{"st": 23, "ed": 25, "text": "feature spaces"}, {"st": 43, "ed": 45, "text": "bayes rule"}, {"st": 57, "ed": 59, "text": "feature space"}, {"st": 65, "ed": 67, "text": "feature space"}, {"st": 72, "ed": 74, "text": "value iteration"}, {"st": 98, "ed": 100, "text": "feature space"}]
[{"st": 6, "ed": 8, "text": "real world"}, {"st": 51, "ed": 53, "text": "existing approaches"}, {"st": 61, "ed": 63, "text": "real world"}, {"st": 84, "ed": 86, "text": "transition function"}]
[{"st": 11, "ed": 14, "text": "joint probability distribution"}, {"st": 17, "ed": 19, "text": "structure learning"}, {"st": 43, "ed": 45, "text": "conditional probability"}, {"st": 54, "ed": 56, "text": "conditional distributions"}, {"st": 64, "ed": 66, "text": "inference algorithms"}, {"st": 74, "ed": 76, "text": "closed form"}, {"st": 118, "ed": 120, "text": "significantly improve"}, {"st": 126, "ed": 128, "text": "standard datasets"}, {"st": 131, "ed": 134, "text": "orders of magnitude"}, {"st": 142, "ed": 144, "text": "conditional distributions"}]
[{"st": 2, "ed": 4, "text": "search algorithms"}, {"st": 8, "ed": 11, "text": "branch and bound"}, {"st": 22, "ed": 24, "text": "bayesian network"}, {"st": 82, "ed": 84, "text": "sparse representation"}, {"st": 95, "ed": 97, "text": "empirical results"}, {"st": 102, "ed": 104, "text": "significantly improved"}, {"st": 105, "ed": 108, "text": "efficiency and scalability"}]
[{"st": 7, "ed": 9, "text": "bayesian network"}, {"st": 24, "ed": 26, "text": "mixed integer"}, {"st": 26, "ed": 28, "text": "linear programming"}, {"st": 30, "ed": 32, "text": "structure learning"}]
[{"st": 0, "ed": 2, "text": "bayesian network"}, {"st": 19, "ed": 21, "text": "naive bayes"}, {"st": 77, "ed": 79, "text": "computational cost"}]
[{"st": 5, "ed": 7, "text": "text classification"}, {"st": 8, "ed": 10, "text": "computational biology"}, {"st": 14, "ed": 17, "text": "multi label classification"}, {"st": 39, "ed": 42, "text": "directed acyclic graph"}]
[{"st": 7, "ed": 9, "text": "online learning"}, {"st": 15, "ed": 17, "text": "learning agent"}, {"st": 47, "ed": 49, "text": "efficient learning"}, {"st": 50, "ed": 52, "text": "large scale"}, {"st": 64, "ed": 66, "text": "learning algorithms"}, {"st": 69, "ed": 71, "text": "thompson sampling"}, {"st": 81, "ed": 83, "text": "computationally efficient"}, {"st": 106, "ed": 108, "text": "statistically efficient"}, {"st": 113, "ed": 115, "text": "regret bounds"}, {"st": 143, "ed": 145, "text": "experiment results"}, {"st": 158, "ed": 160, "text": "significantly outperforms"}]
[{"st": 3, "ed": 5, "text": "reinforcement learning"}, {"st": 19, "ed": 22, "text": "exploration and exploitation"}, {"st": 30, "ed": 32, "text": "bayes optimal"}, {"st": 38, "ed": 40, "text": "search space"}, {"st": 49, "ed": 51, "text": "sample based"}, {"st": 54, "ed": 56, "text": "bayes optimal"}, {"st": 59, "ed": 61, "text": "monte carlo"}, {"st": 80, "ed": 82, "text": "benchmark problems"}, {"st": 88, "ed": 90, "text": "bayes rule"}]
[{"st": 13, "ed": 15, "text": "prior knowledge"}, {"st": 33, "ed": 35, "text": "context specific"}, {"st": 50, "ed": 52, "text": "network parameters"}, {"st": 74, "ed": 76, "text": "prior knowledge"}, {"st": 90, "ed": 92, "text": "true distribution"}, {"st": 96, "ed": 98, "text": "small sample"}]
[{"st": 14, "ed": 16, "text": "network structure"}, {"st": 23, "ed": 25, "text": "np hard"}, {"st": 51, "ed": 53, "text": "hill climbing"}, {"st": 78, "ed": 81, "text": "easy to implement"}, {"st": 142, "ed": 144, "text": "search space"}, {"st": 170, "ed": 174, "text": "synthetic and real data"}]
[{"st": 9, "ed": 11, "text": "intelligent systems"}, {"st": 36, "ed": 38, "text": "ensemble based"}, {"st": 47, "ed": 49, "text": "benchmark datasets"}, {"st": 62, "ed": 64, "text": "class probabilities"}]
[{"st": 1, "ed": 3, "text": "exploration exploitation"}, {"st": 13, "ed": 17, "text": "multi armed bandit problems"}, {"st": 56, "ed": 58, "text": "prior information"}, {"st": 67, "ed": 69, "text": "prior knowledge"}, {"st": 109, "ed": 111, "text": "prior knowledge"}, {"st": 141, "ed": 143, "text": "optimization problem"}, {"st": 167, "ed": 169, "text": "meta learning"}, {"st": 198, "ed": 200, "text": "optimization algorithms"}, {"st": 209, "ed": 211, "text": "bandit problems"}, {"st": 237, "ed": 239, "text": "epsilon greedy"}]
[{"st": 4, "ed": 6, "text": "chain graphs"}, {"st": 18, "ed": 20, "text": "constraint based"}, {"st": 46, "ed": 48, "text": "chain graphs"}, {"st": 61, "ed": 63, "text": "learning algorithms"}, {"st": 149, "ed": 151, "text": "equivalence class"}, {"st": 160, "ed": 162, "text": "constraint based"}]
[{"st": 1, "ed": 3, "text": "reinforcement learning"}, {"st": 10, "ed": 12, "text": "domain knowledge"}, {"st": 16, "ed": 18, "text": "exploration exploitation"}, {"st": 30, "ed": 32, "text": "special cases"}, {"st": 47, "ed": 49, "text": "parameter values"}, {"st": 89, "ed": 91, "text": "proposed algorithm"}, {"st": 94, "ed": 96, "text": "sample complexity"}, {"st": 109, "ed": 111, "text": "proposed algorithm"}, {"st": 114, "ed": 116, "text": "existing algorithms"}, {"st": 118, "ed": 120, "text": "prior distributions"}, {"st": 129, "ed": 131, "text": "proposed algorithm"}, {"st": 153, "ed": 155, "text": "proposed algorithm"}]
[{"st": 5, "ed": 7, "text": "scoring function"}, {"st": 8, "ed": 10, "text": "structure learning"}, {"st": 16, "ed": 18, "text": "traditional approaches"}, {"st": 20, "ed": 22, "text": "structure learning"}, {"st": 44, "ed": 46, "text": "conditional independence"}, {"st": 59, "ed": 61, "text": "scoring function"}, {"st": 86, "ed": 88, "text": "sample complexity"}, {"st": 111, "ed": 113, "text": "generating distribution"}, {"st": 138, "ed": 140, "text": "search algorithm"}, {"st": 142, "ed": 144, "text": "empirical results"}, {"st": 155, "ed": 157, "text": "linear programming"}]
[{"st": 6, "ed": 9, "text": "a posteriori map"}, {"st": 18, "ed": 20, "text": "map inference"}, {"st": 35, "ed": 41, "text": "alternating direction method of multipliers admm"}, {"st": 73, "ed": 75, "text": "sum product"}, {"st": 79, "ed": 81, "text": "global convergence"}, {"st": 85, "ed": 87, "text": "proposed algorithm"}, {"st": 92, "ed": 95, "text": "synthetic and real"}]
[{"st": 7, "ed": 9, "text": "causal models"}, {"st": 22, "ed": 24, "text": "proposed method"}, {"st": 30, "ed": 32, "text": "continuous data"}, {"st": 96, "ed": 98, "text": "flow cytometry"}]
[{"st": 3, "ed": 5, "text": "kernel method"}]
[{"st": 0, "ed": 2, "text": "metric learning"}, {"st": 7, "ed": 9, "text": "feature space"}, {"st": 26, "ed": 28, "text": "sample complexity"}, {"st": 43, "ed": 45, "text": "sample complexity"}, {"st": 78, "ed": 80, "text": "fine tuned"}, {"st": 92, "ed": 94, "text": "analysis reveals"}, {"st": 97, "ed": 99, "text": "metric learning"}, {"st": 105, "ed": 107, "text": "based regularization"}, {"st": 121, "ed": 123, "text": "benchmark datasets"}]
[{"st": 13, "ed": 15, "text": "active learning"}, {"st": 17, "ed": 19, "text": "convex optimization"}, {"st": 67, "ed": 69, "text": "optimization algorithm"}, {"st": 81, "ed": 83, "text": "active learning"}, {"st": 106, "ed": 108, "text": "d dimensional"}, {"st": 113, "ed": 115, "text": "convex functions"}, {"st": 129, "ed": 131, "text": "real valued"}, {"st": 137, "ed": 139, "text": "coordinate descent"}, {"st": 150, "ed": 152, "text": "active learning"}, {"st": 157, "ed": 159, "text": "convergence rate"}, {"st": 163, "ed": 165, "text": "real valued"}, {"st": 176, "ed": 178, "text": "convex optimization"}, {"st": 192, "ed": 194, "text": "active learning"}]
[{"st": 7, "ed": 11, "text": "bayesian network structure learning"}, {"st": 32, "ed": 34, "text": "hill climbing"}, {"st": 53, "ed": 56, "text": "divide and conquer"}, {"st": 56, "ed": 58, "text": "constraint based"}, {"st": 79, "ed": 81, "text": "hill climbing"}, {"st": 94, "ed": 98, "text": "bayesian network structure learning"}, {"st": 106, "ed": 108, "text": "extensive experiments"}, {"st": 117, "ed": 120, "text": "goodness of fit"}, {"st": 131, "ed": 133, "text": "network structure"}, {"st": 146, "ed": 148, "text": "source code"}]
[{"st": 22, "ed": 24, "text": "statistical inference"}, {"st": 27, "ed": 29, "text": "linked data"}, {"st": 29, "ed": 31, "text": "recommender systems"}, {"st": 31, "ed": 33, "text": "computational biology"}, {"st": 34, "ed": 36, "text": "natural language"}, {"st": 53, "ed": 55, "text": "current approaches"}, {"st": 56, "ed": 58, "text": "multi relational"}, {"st": 69, "ed": 71, "text": "computational complexity"}, {"st": 88, "ed": 90, "text": "multi relational"}, {"st": 103, "ed": 109, "text": "alternating direction method of multipliers admm"}, {"st": 150, "ed": 152, "text": "multi relational"}, {"st": 168, "ed": 170, "text": "performance improvement"}, {"st": 182, "ed": 184, "text": "great potential"}]
[{"st": 0, "ed": 4, "text": "non negative matrix factorization"}, {"st": 29, "ed": 31, "text": "collaborative filtering"}, {"st": 43, "ed": 45, "text": "recommendation systems"}, {"st": 149, "ed": 151, "text": "continuous data"}]
[{"st": 0, "ed": 3, "text": "inverse reinforcement learning"}, {"st": 11, "ed": 13, "text": "reward function"}, {"st": 15, "ed": 18, "text": "markov decision process"}, {"st": 20, "ed": 22, "text": "observed behavior"}, {"st": 48, "ed": 50, "text": "reward function"}, {"st": 54, "ed": 56, "text": "inverse problem"}, {"st": 68, "ed": 70, "text": "transition model"}, {"st": 83, "ed": 85, "text": "observed behavior"}, {"st": 96, "ed": 98, "text": "inverse problem"}, {"st": 126, "ed": 128, "text": "optimization problem"}, {"st": 130, "ed": 133, "text": "takes into account"}, {"st": 152, "ed": 154, "text": "transfer learning"}, {"st": 159, "ed": 161, "text": "sample efficiency"}, {"st": 169, "ed": 171, "text": "reward functions"}]
[{"st": 7, "ed": 9, "text": "labeled data"}, {"st": 10, "ed": 12, "text": "cluster ensemble"}, {"st": 22, "ed": 24, "text": "clustering results"}, {"st": 37, "ed": 39, "text": "final results"}, {"st": 43, "ed": 45, "text": "clustering algorithms"}, {"st": 46, "ed": 48, "text": "cluster ensemble"}, {"st": 93, "ed": 95, "text": "main goal"}, {"st": 117, "ed": 119, "text": "clustering problems"}, {"st": 124, "ed": 126, "text": "cluster ensemble"}, {"st": 136, "ed": 138, "text": "graph based"}, {"st": 146, "ed": 148, "text": "spectral clustering"}, {"st": 153, "ed": 155, "text": "spectral clustering"}, {"st": 184, "ed": 186, "text": "clustering results"}, {"st": 209, "ed": 211, "text": "clustering results"}, {"st": 226, "ed": 228, "text": "approach achieves"}]
[{"st": 10, "ed": 12, "text": "bandit problems"}, {"st": 13, "ed": 16, "text": "peer to peer"}, {"st": 35, "ed": 37, "text": "bandit problem"}, {"st": 74, "ed": 76, "text": "bandit problem"}, {"st": 102, "ed": 104, "text": "real world"}]
[{"st": 4, "ed": 6, "text": "powerful tool"}, {"st": 34, "ed": 36, "text": "computationally expensive"}, {"st": 109, "ed": 112, "text": "evidence lower bound"}, {"st": 132, "ed": 134, "text": "inference algorithm"}, {"st": 178, "ed": 180, "text": "predictive performance"}, {"st": 194, "ed": 197, "text": "click through rate"}]
[{"st": 9, "ed": 11, "text": "reinforcement learning"}, {"st": 15, "ed": 18, "text": "deep q networks"}, {"st": 28, "ed": 30, "text": "policy network"}, {"st": 54, "ed": 57, "text": "positive and negative"}, {"st": 73, "ed": 75, "text": "sample complexity"}, {"st": 80, "ed": 82, "text": "negative transfer"}]
[{"st": 4, "ed": 6, "text": "loss function"}, {"st": 8, "ed": 10, "text": "multi class"}, {"st": 10, "ed": 12, "text": "neural networks"}, {"st": 49, "ed": 51, "text": "remains unclear"}, {"st": 63, "ed": 65, "text": "error rate"}, {"st": 69, "ed": 71, "text": "evaluation metrics"}, {"st": 84, "ed": 86, "text": "classification loss"}, {"st": 121, "ed": 123, "text": "loss functions"}, {"st": 146, "ed": 148, "text": "significantly outperforms"}, {"st": 151, "ed": 153, "text": "loss functions"}, {"st": 245, "ed": 248, "text": "times faster than"}, {"st": 255, "ed": 258, "text": "times faster than"}]
[{"st": 42, "ed": 44, "text": "existing methods"}, {"st": 63, "ed": 65, "text": "similarity measure"}, {"st": 76, "ed": 78, "text": "wide variety"}, {"st": 114, "ed": 116, "text": "experiments demonstrate"}, {"st": 123, "ed": 125, "text": "clustering methods"}]
[{"st": 56, "ed": 59, "text": "loopy belief propagation"}, {"st": 84, "ed": 86, "text": "greedy algorithm"}, {"st": 116, "ed": 118, "text": "significantly reduces"}]
[{"st": 0, "ed": 2, "text": "affinity propagation"}, {"st": 5, "ed": 7, "text": "based clustering"}, {"st": 29, "ed": 31, "text": "affinity propagation"}, {"st": 38, "ed": 40, "text": "hierarchical clustering"}, {"st": 47, "ed": 49, "text": "domains including"}, {"st": 53, "ed": 55, "text": "decision making"}, {"st": 61, "ed": 63, "text": "inference algorithm"}, {"st": 84, "ed": 86, "text": "graphical model"}, {"st": 91, "ed": 93, "text": "method outperforms"}, {"st": 118, "ed": 120, "text": "method outperforms"}, {"st": 128, "ed": 130, "text": "ground truth"}, {"st": 136, "ed": 138, "text": "method achieves"}, {"st": 145, "ed": 147, "text": "objective function"}]
[{"st": 7, "ed": 9, "text": "computationally efficient"}, {"st": 28, "ed": 30, "text": "nash equilibria"}]
[{"st": 22, "ed": 24, "text": "true labels"}, {"st": 54, "ed": 56, "text": "probabilistic models"}, {"st": 67, "ed": 70, "text": "hierarchical bayesian model"}, {"st": 72, "ed": 74, "text": "consistently outperforms"}, {"st": 77, "ed": 81, "text": "synthetic and real world"}]
[{"st": 0, "ed": 2, "text": "belief propagation"}, {"st": 41, "ed": 43, "text": "approximate inference"}, {"st": 78, "ed": 80, "text": "belief propagation"}, {"st": 133, "ed": 135, "text": "empirical evidence"}]
[{"st": 6, "ed": 8, "text": "maximum entropy"}, {"st": 35, "ed": 37, "text": "conditional distribution"}, {"st": 53, "ed": 55, "text": "efficiently compute"}, {"st": 78, "ed": 80, "text": "text documents"}, {"st": 98, "ed": 101, "text": "named entity recognition"}, {"st": 112, "ed": 114, "text": "shows significant"}]
[{"st": 1, "ed": 3, "text": "belief propagation"}, {"st": 6, "ed": 8, "text": "widely applied"}, {"st": 10, "ed": 12, "text": "approximate inference"}, {"st": 69, "ed": 72, "text": "markov random fields"}, {"st": 85, "ed": 88, "text": "compares favorably with"}]
[{"st": 2, "ed": 4, "text": "most probable"}, {"st": 9, "ed": 11, "text": "graphical model"}, {"st": 15, "ed": 17, "text": "np hard"}, {"st": 26, "ed": 28, "text": "belief propagation"}, {"st": 43, "ed": 45, "text": "cycle graph"}, {"st": 93, "ed": 95, "text": "free energy"}, {"st": 123, "ed": 125, "text": "fixed points"}, {"st": 141, "ed": 143, "text": "sum product"}, {"st": 201, "ed": 203, "text": "real world"}, {"st": 208, "ed": 210, "text": "exact inference"}, {"st": 211, "ed": 213, "text": "junction tree"}]
[{"st": 3, "ed": 5, "text": "imitation learning"}, {"st": 27, "ed": 29, "text": "prior knowledge"}, {"st": 49, "ed": 51, "text": "prior knowledge"}, {"st": 60, "ed": 62, "text": "prior knowledge"}, {"st": 67, "ed": 70, "text": "markov decision process"}, {"st": 107, "ed": 109, "text": "prior probability"}, {"st": 166, "ed": 168, "text": "stationary point"}, {"st": 192, "ed": 194, "text": "empirical evidence"}]
[{"st": 5, "ed": 7, "text": "structured prediction"}, {"st": 30, "ed": 32, "text": "search procedure"}, {"st": 36, "ed": 38, "text": "cost function"}, {"st": 59, "ed": 61, "text": "search spaces"}, {"st": 68, "ed": 70, "text": "structured prediction"}, {"st": 87, "ed": 89, "text": "search space"}, {"st": 90, "ed": 92, "text": "structured outputs"}, {"st": 99, "ed": 101, "text": "learning algorithms"}, {"st": 104, "ed": 106, "text": "search space"}, {"st": 112, "ed": 114, "text": "cost function"}, {"st": 118, "ed": 120, "text": "key idea"}, {"st": 124, "ed": 126, "text": "cost function"}, {"st": 169, "ed": 171, "text": "structured prediction"}]
[{"st": 8, "ed": 10, "text": "bayesian network"}, {"st": 30, "ed": 32, "text": "posterior probability"}, {"st": 33, "ed": 36, "text": "o n 2"}, {"st": 63, "ed": 65, "text": "posterior probabilities"}, {"st": 77, "ed": 80, "text": "o n 2"}, {"st": 118, "ed": 120, "text": "statistical power"}]
[{"st": 10, "ed": 12, "text": "reinforcement learning"}, {"st": 13, "ed": 15, "text": "unlike previous"}, {"st": 15, "ed": 17, "text": "model based"}, {"st": 33, "ed": 35, "text": "supervised learning"}, {"st": 61, "ed": 63, "text": "decision tree"}, {"st": 67, "ed": 69, "text": "reward function"}, {"st": 71, "ed": 74, "text": "dynamic bayesian networks"}, {"st": 79, "ed": 81, "text": "transition function"}, {"st": 95, "ed": 97, "text": "value iteration"}, {"st": 139, "ed": 141, "text": "relative error"}, {"st": 249, "ed": 251, "text": "exponential growth"}]
[{"st": 16, "ed": 18, "text": "conditional probability"}]
[{"st": 4, "ed": 6, "text": "reinforcement learning"}, {"st": 84, "ed": 86, "text": "higher levels"}]
[{"st": 10, "ed": 12, "text": "compact representation"}, {"st": 13, "ed": 15, "text": "joint probability"}, {"st": 54, "ed": 56, "text": "sample complexity"}, {"st": 130, "ed": 132, "text": "structure learning"}, {"st": 135, "ed": 137, "text": "difficult task"}, {"st": 158, "ed": 160, "text": "computational power"}]
[{"st": 8, "ed": 10, "text": "structure learning"}, {"st": 35, "ed": 39, "text": "markov chain monte carlo"}, {"st": 70, "ed": 72, "text": "gibbs sampler"}, {"st": 91, "ed": 93, "text": "simulated data"}]
[{"st": 2, "ed": 4, "text": "learning algorithms"}, {"st": 15, "ed": 18, "text": "markov decision processes"}, {"st": 20, "ed": 22, "text": "finite state"}, {"st": 27, "ed": 30, "text": "high computational cost"}, {"st": 41, "ed": 43, "text": "large scale"}, {"st": 52, "ed": 54, "text": "dynamic programming"}, {"st": 86, "ed": 88, "text": "learning algorithms"}, {"st": 106, "ed": 108, "text": "theoretical framework"}]
[{"st": 7, "ed": 9, "text": "machine learning"}, {"st": 59, "ed": 61, "text": "machine learning"}, {"st": 106, "ed": 108, "text": "generalization error"}, {"st": 150, "ed": 153, "text": "provide sufficient conditions"}, {"st": 167, "ed": 169, "text": "unlike previous"}, {"st": 179, "ed": 181, "text": "empirical evidence"}, {"st": 184, "ed": 186, "text": "real world"}, {"st": 189, "ed": 191, "text": "theoretical results"}, {"st": 199, "ed": 201, "text": "empirical evidence"}, {"st": 210, "ed": 212, "text": "real world"}, {"st": 220, "ed": 222, "text": "convergence rate"}, {"st": 235, "ed": 237, "text": "generalization ability"}]
[{"st": 0, "ed": 2, "text": "constraint based"}, {"st": 21, "ed": 23, "text": "conditional independence"}, {"st": 78, "ed": 80, "text": "chi squared"}, {"st": 88, "ed": 90, "text": "prior distribution"}, {"st": 110, "ed": 112, "text": "demonstrate empirically"}, {"st": 118, "ed": 120, "text": "causal discovery"}]
[{"st": 14, "ed": 16, "text": "bayesian networks"}, {"st": 23, "ed": 25, "text": "learning algorithm"}, {"st": 47, "ed": 49, "text": "learning algorithm"}, {"st": 59, "ed": 61, "text": "sufficiently large"}]
[{"st": 15, "ed": 17, "text": "naive bayes"}, {"st": 62, "ed": 65, "text": "naive bayes classifier"}, {"st": 112, "ed": 114, "text": "naive bayes"}, {"st": 144, "ed": 147, "text": "naive bayes classifier"}]
[{"st": 3, "ed": 5, "text": "belief propagation"}, {"st": 9, "ed": 11, "text": "approximate inference"}, {"st": 12, "ed": 15, "text": "markov random fields"}, {"st": 18, "ed": 20, "text": "fixed points"}, {"st": 34, "ed": 36, "text": "belief propagation"}, {"st": 51, "ed": 54, "text": "bethe free energy"}, {"st": 76, "ed": 78, "text": "free energy"}, {"st": 124, "ed": 126, "text": "speed ups"}]
[{"st": 9, "ed": 11, "text": "search algorithm"}, {"st": 54, "ed": 56, "text": "search algorithm"}, {"st": 67, "ed": 70, "text": "under mild assumptions"}, {"st": 84, "ed": 88, "text": "synthetic and real data"}, {"st": 97, "ed": 99, "text": "local optima"}, {"st": 113, "ed": 115, "text": "local optima"}]
[{"st": 27, "ed": 29, "text": "prediction tasks"}, {"st": 98, "ed": 100, "text": "attention mechanism"}, {"st": 126, "ed": 128, "text": "linear prediction"}, {"st": 150, "ed": 152, "text": "o sqrt"}, {"st": 169, "ed": 171, "text": "error rate"}]
[{"st": 14, "ed": 16, "text": "compact representations"}, {"st": 18, "ed": 20, "text": "conditional probability"}, {"st": 35, "ed": 37, "text": "decision tree"}, {"st": 51, "ed": 53, "text": "scoring functions"}, {"st": 59, "ed": 62, "text": "goodness of fit"}, {"st": 97, "ed": 99, "text": "posterior probability"}, {"st": 118, "ed": 120, "text": "search spaces"}, {"st": 128, "ed": 130, "text": "scoring function"}, {"st": 132, "ed": 134, "text": "search procedure"}, {"st": 150, "ed": 152, "text": "search spaces"}, {"st": 154, "ed": 156, "text": "greedy algorithm"}]
[{"st": 3, "ed": 5, "text": "bayesian networks"}, {"st": 10, "ed": 12, "text": "scoring function"}, {"st": 14, "ed": 16, "text": "heuristic search"}, {"st": 19, "ed": 21, "text": "bayesian network"}, {"st": 25, "ed": 27, "text": "scoring functions"}, {"st": 37, "ed": 39, "text": "equivalence class"}, {"st": 48, "ed": 50, "text": "scoring function"}, {"st": 56, "ed": 58, "text": "search algorithm"}, {"st": 61, "ed": 63, "text": "equivalence classes"}, {"st": 78, "ed": 80, "text": "search space"}, {"st": 89, "ed": 91, "text": "equivalence classes"}, {"st": 102, "ed": 104, "text": "heuristic search"}, {"st": 111, "ed": 113, "text": "greedy search"}, {"st": 117, "ed": 119, "text": "search space"}, {"st": 120, "ed": 122, "text": "greedy search"}, {"st": 125, "ed": 127, "text": "search space"}, {"st": 134, "ed": 136, "text": "bayesian network"}]
[{"st": 14, "ed": 16, "text": "bayesian networks"}, {"st": 37, "ed": 39, "text": "conditional probability"}, {"st": 108, "ed": 110, "text": "empirical evaluation"}]
[{"st": 53, "ed": 55, "text": "statistical methods"}, {"st": 61, "ed": 63, "text": "naive bayesian"}, {"st": 87, "ed": 89, "text": "conditional distribution"}]
[{"st": 5, "ed": 8, "text": "curse of dimensionality"}, {"st": 22, "ed": 24, "text": "randomized algorithm"}, {"st": 30, "ed": 32, "text": "constant factor"}, {"st": 41, "ed": 43, "text": "exponentially large"}, {"st": 58, "ed": 60, "text": "combinatorial optimization"}, {"st": 63, "ed": 65, "text": "randomly generated"}]
[{"st": 33, "ed": 35, "text": "network structure"}, {"st": 41, "ed": 43, "text": "prior knowledge"}, {"st": 50, "ed": 52, "text": "posterior probability"}, {"st": 54, "ed": 56, "text": "network structure"}, {"st": 60, "ed": 62, "text": "search procedure"}, {"st": 80, "ed": 82, "text": "discrete variables"}, {"st": 105, "ed": 107, "text": "continuous variables"}, {"st": 111, "ed": 114, "text": "discrete and continuous"}, {"st": 119, "ed": 121, "text": "continuous data"}, {"st": 159, "ed": 161, "text": "prior distributions"}]
[{"st": 10, "ed": 14, "text": "expectation maximization em algorithm"}, {"st": 41, "ed": 44, "text": "expectation maximization algorithm"}, {"st": 51, "ed": 53, "text": "mixture model"}, {"st": 58, "ed": 60, "text": "expectation maximization"}, {"st": 73, "ed": 75, "text": "k means"}, {"st": 83, "ed": 85, "text": "result shows"}, {"st": 86, "ed": 89, "text": "expectation maximization algorithm"}, {"st": 113, "ed": 115, "text": "k means"}]
[{"st": 5, "ed": 7, "text": "classifier performance"}, {"st": 18, "ed": 21, "text": "amazon mechanical turk"}, {"st": 24, "ed": 26, "text": "distant supervision"}, {"st": 27, "ed": 29, "text": "automatically generate"}, {"st": 39, "ed": 41, "text": "logistic regression"}, {"st": 62, "ed": 64, "text": "logistic regression"}, {"st": 73, "ed": 76, "text": "named entity recognition"}, {"st": 85, "ed": 87, "text": "significant improvement"}, {"st": 89, "ed": 91, "text": "standard model"}]
[{"st": 8, "ed": 10, "text": "error bounds"}, {"st": 12, "ed": 14, "text": "learning algorithms"}, {"st": 27, "ed": 29, "text": "error bound"}, {"st": 35, "ed": 37, "text": "rademacher complexity"}, {"st": 74, "ed": 76, "text": "error bounds"}, {"st": 86, "ed": 88, "text": "pac bayesian"}]
[{"st": 46, "ed": 48, "text": "structure learning"}, {"st": 51, "ed": 53, "text": "maximum likelihood"}, {"st": 60, "ed": 62, "text": "np hard"}, {"st": 85, "ed": 87, "text": "independence based"}, {"st": 145, "ed": 147, "text": "conditional independence"}, {"st": 195, "ed": 199, "text": "real world data sets"}, {"st": 249, "ed": 251, "text": "conditional independence"}]
[{"st": 59, "ed": 61, "text": "partially observable"}, {"st": 64, "ed": 66, "text": "standard model"}, {"st": 69, "ed": 71, "text": "generative models"}, {"st": 122, "ed": 124, "text": "partially observable"}, {"st": 139, "ed": 141, "text": "generative models"}]
[{"st": 10, "ed": 12, "text": "probabilistic models"}, {"st": 14, "ed": 16, "text": "mean field"}, {"st": 25, "ed": 28, "text": "bethe free energy"}, {"st": 48, "ed": 51, "text": "bethe free energy"}, {"st": 83, "ed": 86, "text": "a sufficient condition"}, {"st": 91, "ed": 93, "text": "message passing"}, {"st": 98, "ed": 100, "text": "fixed points"}, {"st": 103, "ed": 105, "text": "message passing"}, {"st": 107, "ed": 109, "text": "local minima"}, {"st": 128, "ed": 130, "text": "free energy"}, {"st": 135, "ed": 137, "text": "message passing"}]
[{"st": 0, "ed": 2, "text": "local search"}, {"st": 5, "ed": 7, "text": "optimization problems"}, {"st": 40, "ed": 42, "text": "local search"}, {"st": 45, "ed": 47, "text": "computational resources"}, {"st": 94, "ed": 98, "text": "multi armed bandit problems"}, {"st": 119, "ed": 121, "text": "convergence rate"}, {"st": 123, "ed": 125, "text": "local search"}, {"st": 191, "ed": 193, "text": "local search"}, {"st": 212, "ed": 214, "text": "k means"}, {"st": 215, "ed": 217, "text": "local search"}]
[{"st": 3, "ed": 5, "text": "least squares"}, {"st": 5, "ed": 7, "text": "value iteration"}, {"st": 10, "ed": 12, "text": "reinforcement learning"}, {"st": 29, "ed": 31, "text": "least squares"}, {"st": 31, "ed": 33, "text": "value iteration"}, {"st": 37, "ed": 39, "text": "epsilon greedy"}, {"st": 75, "ed": 77, "text": "tabula rasa"}, {"st": 98, "ed": 100, "text": "reinforcement learning"}, {"st": 101, "ed": 103, "text": "efficient exploration"}]
[{"st": 1, "ed": 3, "text": "computational costs"}, {"st": 12, "ed": 14, "text": "reinforcement learning"}, {"st": 33, "ed": 35, "text": "parametric models"}, {"st": 57, "ed": 59, "text": "probabilistic models"}, {"st": 64, "ed": 66, "text": "fully bayesian"}, {"st": 114, "ed": 116, "text": "thompson sampling"}, {"st": 118, "ed": 120, "text": "contextual bandit"}]
[{"st": 11, "ed": 13, "text": "gaussian process"}, {"st": 21, "ed": 23, "text": "closed form"}, {"st": 26, "ed": 28, "text": "marginal likelihood"}, {"st": 50, "ed": 52, "text": "gaussian process"}, {"st": 60, "ed": 62, "text": "gaussian process"}, {"st": 72, "ed": 74, "text": "sampling scheme"}, {"st": 99, "ed": 101, "text": "gaussian process"}, {"st": 107, "ed": 109, "text": "predictive distributions"}, {"st": 126, "ed": 128, "text": "gaussian process"}, {"st": 177, "ed": 179, "text": "computational cost"}]
[{"st": 12, "ed": 14, "text": "ground truth"}, {"st": 124, "ed": 126, "text": "dimensional manifold"}, {"st": 194, "ed": 196, "text": "numerical experiments"}]
[{"st": 6, "ed": 8, "text": "machine learning"}, {"st": 26, "ed": 28, "text": "current approaches"}, {"st": 133, "ed": 135, "text": "base classifiers"}, {"st": 169, "ed": 171, "text": "classification accuracy"}]
[{"st": 12, "ed": 14, "text": "closely related"}, {"st": 160, "ed": 162, "text": "real world"}]
[{"st": 44, "ed": 46, "text": "generated data"}, {"st": 48, "ed": 50, "text": "large scale"}, {"st": 72, "ed": 74, "text": "gaussian kernels"}, {"st": 94, "ed": 96, "text": "generated data"}, {"st": 106, "ed": 108, "text": "statistical properties"}, {"st": 110, "ed": 112, "text": "generated data"}, {"st": 112, "ed": 114, "text": "structural similarity"}, {"st": 118, "ed": 121, "text": "supervised and unsupervised"}, {"st": 133, "ed": 135, "text": "large scale"}, {"st": 151, "ed": 153, "text": "generated data"}]
[{"st": 4, "ed": 6, "text": "large scale"}, {"st": 31, "ed": 33, "text": "high dimensional"}, {"st": 48, "ed": 50, "text": "high dimensional"}, {"st": 73, "ed": 75, "text": "robust subspace"}, {"st": 167, "ed": 169, "text": "proposed method"}, {"st": 176, "ed": 178, "text": "extremely high"}, {"st": 178, "ed": 180, "text": "dimensional space"}, {"st": 184, "ed": 186, "text": "low dimensional"}]
[{"st": 1, "ed": 3, "text": "machine learning"}, {"st": 10, "ed": 12, "text": "training examples"}, {"st": 32, "ed": 34, "text": "training examples"}, {"st": 93, "ed": 95, "text": "error bounds"}, {"st": 100, "ed": 102, "text": "significantly improved"}]
[{"st": 9, "ed": 11, "text": "metric learning"}, {"st": 40, "ed": 42, "text": "input data"}, {"st": 45, "ed": 47, "text": "fisher information"}, {"st": 57, "ed": 59, "text": "input data"}, {"st": 64, "ed": 66, "text": "distance metric"}, {"st": 71, "ed": 73, "text": "metric learning"}, {"st": 78, "ed": 80, "text": "similarity measure"}, {"st": 94, "ed": 96, "text": "metric learning"}, {"st": 115, "ed": 117, "text": "metric learning"}]
[{"st": 3, "ed": 5, "text": "infinite horizon"}, {"st": 6, "ed": 8, "text": "optimal control"}, {"st": 22, "ed": 24, "text": "policy iteration"}, {"st": 26, "ed": 28, "text": "policy iteration"}, {"st": 29, "ed": 31, "text": "policy iteration"}, {"st": 37, "ed": 39, "text": "policy search"}, {"st": 40, "ed": 42, "text": "dynamic programming"}, {"st": 45, "ed": 47, "text": "infinite horizon"}, {"st": 52, "ed": 54, "text": "recently proposed"}, {"st": 54, "ed": 56, "text": "non stationary"}, {"st": 56, "ed": 58, "text": "policy iteration"}, {"st": 66, "ed": 68, "text": "performance bounds"}, {"st": 83, "ed": 86, "text": "number of iterations"}, {"st": 98, "ed": 100, "text": "performance guarantee"}, {"st": 123, "ed": 125, "text": "frac 1"}, {"st": 142, "ed": 144, "text": "performance guarantee"}, {"st": 153, "ed": 156, "text": "number of iterations"}, {"st": 182, "ed": 185, "text": "number of iterations"}, {"st": 200, "ed": 202, "text": "approximation error"}]
[{"st": 2, "ed": 4, "text": "graphical model"}, {"st": 24, "ed": 26, "text": "exact inference"}, {"st": 35, "ed": 39, "text": "markov chain monte carlo"}, {"st": 69, "ed": 71, "text": "multivariate gaussian"}, {"st": 76, "ed": 78, "text": "conditional independence"}, {"st": 98, "ed": 100, "text": "gaussian noise"}, {"st": 123, "ed": 125, "text": "expectation propagation"}, {"st": 126, "ed": 129, "text": "efficient and accurate"}, {"st": 132, "ed": 135, "text": "accuracy and speed"}]
[{"st": 2, "ed": 4, "text": "optimization problems"}, {"st": 7, "ed": 10, "text": "minimum spanning tree"}, {"st": 79, "ed": 82, "text": "computationally efficient algorithm"}]
[{"st": 15, "ed": 17, "text": "dengue fever"}, {"st": 28, "ed": 30, "text": "public health"}, {"st": 48, "ed": 50, "text": "computational intelligence"}, {"st": 63, "ed": 65, "text": "false positives"}, {"st": 105, "ed": 107, "text": "selection method"}, {"st": 127, "ed": 129, "text": "decision tree"}, {"st": 135, "ed": 137, "text": "highly accurate"}, {"st": 140, "ed": 142, "text": "predictive models"}]
[{"st": 6, "ed": 9, "text": "deep generative models"}, {"st": 27, "ed": 29, "text": "multilayer perceptron"}, {"st": 32, "ed": 34, "text": "recently proposed"}, {"st": 34, "ed": 37, "text": "generative adversarial networks"}, {"st": 44, "ed": 47, "text": "generative adversarial network"}, {"st": 62, "ed": 65, "text": "statistical hypothesis testing"}, {"st": 67, "ed": 71, "text": "maximum mean discrepancy mmd"}, {"st": 112, "ed": 114, "text": "generative network"}, {"st": 116, "ed": 118, "text": "auto encoder"}, {"st": 144, "ed": 146, "text": "generative models"}, {"st": 148, "ed": 150, "text": "baseline approaches"}]
[{"st": 3, "ed": 5, "text": "recently developed"}, {"st": 7, "ed": 9, "text": "policy gradient"}, {"st": 16, "ed": 18, "text": "minimization problem"}, {"st": 50, "ed": 52, "text": "policy gradient"}, {"st": 68, "ed": 70, "text": "operations research"}, {"st": 94, "ed": 96, "text": "policy gradient"}, {"st": 134, "ed": 136, "text": "reinforcement learning"}]
[{"st": 9, "ed": 12, "text": "gaussian graphical models"}, {"st": 84, "ed": 86, "text": "precision matrix"}, {"st": 119, "ed": 121, "text": "sufficient conditions"}, {"st": 144, "ed": 146, "text": "simulated data"}]
[{"st": 9, "ed": 11, "text": "swarm intelligence"}, {"st": 18, "ed": 21, "text": "multi armed bandit"}, {"st": 68, "ed": 70, "text": "randomly chosen"}, {"st": 74, "ed": 76, "text": "social learning"}, {"st": 154, "ed": 156, "text": "swarm intelligence"}, {"st": 167, "ed": 169, "text": "swarm intelligence"}, {"st": 180, "ed": 182, "text": "social learning"}]
[{"st": 21, "ed": 23, "text": "binary classification"}, {"st": 29, "ed": 31, "text": "active learning"}, {"st": 76, "ed": 78, "text": "excess risk"}]
[{"st": 5, "ed": 7, "text": "off policy"}, {"st": 19, "ed": 21, "text": "search engines"}, {"st": 22, "ed": 24, "text": "recommendation systems"}, {"st": 25, "ed": 27, "text": "e commerce"}, {"st": 31, "ed": 33, "text": "recent approaches"}, {"st": 34, "ed": 36, "text": "off policy"}, {"st": 49, "ed": 52, "text": "real world data"}, {"st": 54, "ed": 56, "text": "standardized test"}, {"st": 113, "ed": 115, "text": "off policy"}, {"st": 119, "ed": 121, "text": "robust optimization"}, {"st": 125, "ed": 127, "text": "supervised learning"}, {"st": 137, "ed": 139, "text": "off policy"}, {"st": 148, "ed": 150, "text": "supervised learning"}, {"st": 153, "ed": 155, "text": "large scale"}, {"st": 155, "ed": 158, "text": "real world data"}]
[{"st": 0, "ed": 2, "text": "autonomous systems"}, {"st": 32, "ed": 34, "text": "search algorithms"}, {"st": 62, "ed": 65, "text": "multi armed bandits"}, {"st": 67, "ed": 69, "text": "strong assumptions"}, {"st": 87, "ed": 89, "text": "compressive sensing"}, {"st": 162, "ed": 164, "text": "signal strength"}, {"st": 169, "ed": 171, "text": "search space"}, {"st": 204, "ed": 206, "text": "empirical performance"}, {"st": 215, "ed": 217, "text": "image data"}]
[{"st": 10, "ed": 12, "text": "parameter values"}, {"st": 44, "ed": 46, "text": "approximate bayesian"}, {"st": 59, "ed": 61, "text": "case study"}, {"st": 101, "ed": 103, "text": "parameter values"}, {"st": 132, "ed": 134, "text": "parameter values"}]
[{"st": 4, "ed": 8, "text": "markov chain monte carlo"}, {"st": 40, "ed": 42, "text": "monte carlo"}]
[{"st": 7, "ed": 9, "text": "approximate inference"}, {"st": 26, "ed": 28, "text": "importance sampling"}, {"st": 62, "ed": 64, "text": "output distribution"}, {"st": 70, "ed": 72, "text": "monte carlo"}, {"st": 77, "ed": 79, "text": "posterior distributions"}, {"st": 92, "ed": 94, "text": "proposed method"}, {"st": 123, "ed": 125, "text": "posterior inference"}, {"st": 128, "ed": 130, "text": "linear regression"}]
[{"st": 1, "ed": 3, "text": "accurate predictions"}, {"st": 6, "ed": 8, "text": "machine learning"}, {"st": 24, "ed": 26, "text": "prior knowledge"}, {"st": 28, "ed": 30, "text": "machine learning"}, {"st": 40, "ed": 42, "text": "prior knowledge"}, {"st": 64, "ed": 66, "text": "prior knowledge"}, {"st": 134, "ed": 136, "text": "user study"}, {"st": 141, "ed": 143, "text": "significantly improves"}, {"st": 143, "ed": 145, "text": "prior knowledge"}, {"st": 147, "ed": 149, "text": "prediction accuracy"}]
[{"st": 13, "ed": 15, "text": "past observations"}, {"st": 21, "ed": 23, "text": "accurate prediction"}, {"st": 35, "ed": 37, "text": "positive results"}, {"st": 88, "ed": 90, "text": "mutual information"}, {"st": 91, "ed": 93, "text": "past observations"}, {"st": 120, "ed": 122, "text": "ell 1"}, {"st": 146, "ed": 149, "text": "hidden markov model"}, {"st": 151, "ed": 153, "text": "hidden states"}, {"st": 241, "ed": 243, "text": "hidden states"}, {"st": 257, "ed": 259, "text": "ell 1"}, {"st": 288, "ed": 290, "text": "computationally tractable"}, {"st": 290, "ed": 292, "text": "learning algorithm"}]
[{"st": 11, "ed": 14, "text": "black box optimization"}, {"st": 24, "ed": 26, "text": "acquisition function"}, {"st": 46, "ed": 48, "text": "maximum likelihood"}, {"st": 56, "ed": 58, "text": "acquisition function"}, {"st": 78, "ed": 80, "text": "kernel function"}, {"st": 95, "ed": 97, "text": "convex optimization"}, {"st": 116, "ed": 118, "text": "global optimization"}]
[{"st": 62, "ed": 64, "text": "additional information"}, {"st": 64, "ed": 66, "text": "domain experts"}, {"st": 79, "ed": 81, "text": "probabilistic inference"}, {"st": 83, "ed": 85, "text": "expert knowledge"}, {"st": 96, "ed": 98, "text": "sparse linear"}, {"st": 152, "ed": 155, "text": "simulated and real"}, {"st": 158, "ed": 160, "text": "prediction accuracy"}]
[{"st": 1, "ed": 3, "text": "constrained optimization"}, {"st": 7, "ed": 9, "text": "challenging problem"}, {"st": 11, "ed": 13, "text": "wide applicability"}, {"st": 16, "ed": 18, "text": "machine learning"}, {"st": 27, "ed": 29, "text": "constrained optimization"}, {"st": 33, "ed": 35, "text": "cost function"}, {"st": 38, "ed": 40, "text": "nonlinear function"}, {"st": 50, "ed": 52, "text": "graph structured"}, {"st": 54, "ed": 56, "text": "existing methods"}, {"st": 83, "ed": 85, "text": "approximation algorithm"}, {"st": 88, "ed": 90, "text": "matching pursuit"}, {"st": 96, "ed": 98, "text": "nonlinear function"}, {"st": 117, "ed": 119, "text": "linear models"}, {"st": 122, "ed": 124, "text": "convergence rate"}, {"st": 129, "ed": 131, "text": "case study"}, {"st": 150, "ed": 152, "text": "detection task"}, {"st": 153, "ed": 155, "text": "empirical evidence"}]
[{"st": 9, "ed": 11, "text": "decision makers"}, {"st": 50, "ed": 52, "text": "objective function"}, {"st": 62, "ed": 64, "text": "objective function"}, {"st": 82, "ed": 84, "text": "gaussian process"}, {"st": 103, "ed": 105, "text": "gaussian process"}, {"st": 112, "ed": 114, "text": "objective function"}, {"st": 143, "ed": 145, "text": "gaussian process"}, {"st": 161, "ed": 163, "text": "technique called"}, {"st": 173, "ed": 176, "text": "ability to learn"}]
[{"st": 9, "ed": 11, "text": "decision makers"}, {"st": 49, "ed": 51, "text": "objective function"}, {"st": 62, "ed": 64, "text": "gaussian process"}, {"st": 83, "ed": 85, "text": "gaussian process"}, {"st": 92, "ed": 94, "text": "objective function"}, {"st": 116, "ed": 118, "text": "objective function"}, {"st": 156, "ed": 158, "text": "gaussian process"}, {"st": 174, "ed": 176, "text": "technique called"}, {"st": 186, "ed": 189, "text": "ability to learn"}]
[{"st": 24, "ed": 26, "text": "prior knowledge"}, {"st": 49, "ed": 51, "text": "practical applications"}, {"st": 149, "ed": 151, "text": "noisy data"}, {"st": 173, "ed": 175, "text": "sampling error"}, {"st": 182, "ed": 184, "text": "sampling error"}]
[{"st": 21, "ed": 24, "text": "sequential decision making"}]
[{"st": 13, "ed": 15, "text": "policy improvement"}, {"st": 24, "ed": 26, "text": "deterministic policy"}]
[{"st": 9, "ed": 11, "text": "performance measure"}, {"st": 32, "ed": 34, "text": "max margin"}, {"st": 35, "ed": 37, "text": "surrogate loss"}, {"st": 39, "ed": 41, "text": "optimization problem"}, {"st": 71, "ed": 73, "text": "recent years"}, {"st": 86, "ed": 89, "text": "online learning algorithms"}, {"st": 119, "ed": 121, "text": "learning algorithms"}, {"st": 123, "ed": 125, "text": "real world"}, {"st": 146, "ed": 148, "text": "basis functions"}]
[{"st": 0, "ed": 3, "text": "temporal difference learning"}, {"st": 44, "ed": 46, "text": "bias variance"}, {"st": 79, "ed": 81, "text": "cross validation"}, {"st": 95, "ed": 97, "text": "computationally expensive"}, {"st": 102, "ed": 104, "text": "least squares"}, {"st": 122, "ed": 124, "text": "optimization methods"}, {"st": 137, "ed": 139, "text": "parameter free"}, {"st": 141, "ed": 143, "text": "experiments demonstrate"}]
[{"st": 0, "ed": 2, "text": "contextual bandit"}, {"st": 8, "ed": 10, "text": "recommendation systems"}, {"st": 48, "ed": 50, "text": "common practice"}, {"st": 97, "ed": 99, "text": "contextual bandit"}, {"st": 104, "ed": 106, "text": "based approaches"}, {"st": 130, "ed": 132, "text": "empirical results"}, {"st": 134, "ed": 136, "text": "large scale"}, {"st": 163, "ed": 165, "text": "contextual bandit"}]
[{"st": 17, "ed": 19, "text": "previously unseen"}, {"st": 21, "ed": 23, "text": "machine learning"}, {"st": 37, "ed": 39, "text": "problem specific"}, {"st": 44, "ed": 46, "text": "important applications"}, {"st": 61, "ed": 64, "text": "the past decade"}, {"st": 65, "ed": 67, "text": "wide variety"}, {"st": 83, "ed": 85, "text": "existing models"}, {"st": 124, "ed": 126, "text": "mixed integer"}, {"st": 136, "ed": 138, "text": "empirical analysis"}, {"st": 208, "ed": 210, "text": "previous approaches"}, {"st": 217, "ed": 219, "text": "problem instances"}]
[{"st": 67, "ed": 69, "text": "data fusion"}, {"st": 102, "ed": 104, "text": "feature based"}, {"st": 118, "ed": 120, "text": "prediction task"}, {"st": 137, "ed": 139, "text": "data fusion"}, {"st": 140, "ed": 143, "text": "compares favorably to"}, {"st": 148, "ed": 150, "text": "achieves higher"}]
[{"st": 5, "ed": 7, "text": "multiple tasks"}, {"st": 11, "ed": 13, "text": "challenging research"}, {"st": 15, "ed": 17, "text": "reinforcement learning"}, {"st": 42, "ed": 44, "text": "transfer knowledge"}, {"st": 64, "ed": 66, "text": "key idea"}, {"st": 104, "ed": 106, "text": "imitation learning"}, {"st": 107, "ed": 109, "text": "real robot"}]
[{"st": 2, "ed": 4, "text": "markov network"}, {"st": 16, "ed": 18, "text": "machine learning"}, {"st": 59, "ed": 61, "text": "important issue"}, {"st": 92, "ed": 94, "text": "context specific"}, {"st": 142, "ed": 144, "text": "context specific"}, {"st": 150, "ed": 152, "text": "log linear"}, {"st": 165, "ed": 167, "text": "theoretical guarantees"}, {"st": 170, "ed": 172, "text": "independence based"}, {"st": 185, "ed": 187, "text": "log linear"}]
[{"st": 5, "ed": 7, "text": "reinforcement learning"}, {"st": 11, "ed": 13, "text": "finite horizon"}, {"st": 21, "ed": 23, "text": "constraint propagation"}, {"st": 29, "ed": 31, "text": "efficient exploration"}, {"st": 77, "ed": 79, "text": "performance guarantees"}, {"st": 97, "ed": 99, "text": "special case"}, {"st": 118, "ed": 120, "text": "computational complexity"}]
[{"st": 9, "ed": 11, "text": "probabilistic model"}, {"st": 21, "ed": 23, "text": "prior distribution"}, {"st": 44, "ed": 46, "text": "variational inference"}, {"st": 68, "ed": 70, "text": "modeling language"}]
[{"st": 22, "ed": 24, "text": "classification accuracy"}, {"st": 52, "ed": 54, "text": "boosting algorithm"}, {"st": 89, "ed": 91, "text": "base learners"}, {"st": 97, "ed": 99, "text": "proposed algorithm"}, {"st": 133, "ed": 135, "text": "weight parameters"}, {"st": 163, "ed": 165, "text": "prior knowledge"}]
[{"st": 4, "ed": 6, "text": "provable guarantees"}, {"st": 15, "ed": 17, "text": "generative model"}, {"st": 24, "ed": 26, "text": "generative model"}, {"st": 31, "ed": 33, "text": "neural net"}, {"st": 71, "ed": 73, "text": "sample complexity"}, {"st": 125, "ed": 127, "text": "neural networks"}]
[{"st": 0, "ed": 2, "text": "thompson sampling"}, {"st": 9, "ed": 12, "text": "multi armed bandits"}, {"st": 24, "ed": 26, "text": "empirical success"}, {"st": 32, "ed": 34, "text": "theoretical understanding"}, {"st": 59, "ed": 61, "text": "thompson sampling"}, {"st": 73, "ed": 75, "text": "thompson sampling"}, {"st": 78, "ed": 80, "text": "learning framework"}, {"st": 82, "ed": 84, "text": "thompson sampling"}, {"st": 92, "ed": 94, "text": "learning algorithms"}, {"st": 95, "ed": 97, "text": "thompson sampling"}, {"st": 99, "ed": 101, "text": "loss function"}, {"st": 107, "ed": 109, "text": "regret bounds"}, {"st": 118, "ed": 120, "text": "loss functions"}, {"st": 146, "ed": 148, "text": "prior distribution"}]
[{"st": 5, "ed": 7, "text": "matrix factorization"}, {"st": 54, "ed": 56, "text": "matrix factorization"}, {"st": 104, "ed": 106, "text": "variational inference"}, {"st": 108, "ed": 110, "text": "approximate posterior"}, {"st": 115, "ed": 117, "text": "massive data"}, {"st": 132, "ed": 134, "text": "approximate posterior"}, {"st": 145, "ed": 147, "text": "real world"}, {"st": 174, "ed": 176, "text": "matrix factorization"}]
[{"st": 0, "ed": 2, "text": "belief propagation"}, {"st": 62, "ed": 64, "text": "fixed point"}, {"st": 72, "ed": 74, "text": "message passing"}, {"st": 117, "ed": 119, "text": "experiments demonstrate"}, {"st": 123, "ed": 125, "text": "fast convergence"}, {"st": 126, "ed": 128, "text": "significant speedups"}, {"st": 149, "ed": 151, "text": "real world"}, {"st": 151, "ed": 153, "text": "natural language"}]
[{"st": 1, "ed": 4, "text": "linear dimensionality reduction"}, {"st": 8, "ed": 10, "text": "learning algorithms"}, {"st": 54, "ed": 56, "text": "low dimensional"}, {"st": 113, "ed": 115, "text": "spectral clustering"}, {"st": 133, "ed": 135, "text": "nearest neighbors"}, {"st": 156, "ed": 158, "text": "low dimensional"}, {"st": 162, "ed": 164, "text": "geometric structure"}, {"st": 245, "ed": 247, "text": "training set"}]
[{"st": 28, "ed": 30, "text": "hidden state"}, {"st": 35, "ed": 37, "text": "hidden states"}, {"st": 47, "ed": 49, "text": "time series"}, {"st": 54, "ed": 56, "text": "hidden states"}, {"st": 69, "ed": 71, "text": "ell 1"}, {"st": 74, "ed": 76, "text": "transition matrix"}, {"st": 102, "ed": 105, "text": "a posteriori map"}, {"st": 108, "ed": 111, "text": "expectation maximization em"}, {"st": 117, "ed": 119, "text": "transition matrix"}, {"st": 127, "ed": 129, "text": "sparse linear"}, {"st": 134, "ed": 136, "text": "predictive performance"}, {"st": 145, "ed": 147, "text": "time series"}]
[{"st": 10, "ed": 12, "text": "prediction model"}, {"st": 92, "ed": 94, "text": "iterative algorithms"}, {"st": 100, "ed": 102, "text": "conduct experiments"}, {"st": 103, "ed": 105, "text": "synthetic data"}]
[{"st": 56, "ed": 58, "text": "significantly outperforms"}, {"st": 71, "ed": 73, "text": "standard benchmark"}, {"st": 102, "ed": 105, "text": "efficient and scalable"}, {"st": 108, "ed": 110, "text": "widely adopted"}]
[{"st": 20, "ed": 22, "text": "empirical results"}, {"st": 25, "ed": 29, "text": "markov chain monte carlo"}, {"st": 29, "ed": 31, "text": "probabilistic programming"}, {"st": 35, "ed": 37, "text": "higher order"}, {"st": 37, "ed": 39, "text": "probabilistic programming"}, {"st": 61, "ed": 63, "text": "probabilistic program"}, {"st": 78, "ed": 80, "text": "probabilistic programs"}]
[{"st": 58, "ed": 61, "text": "conditional random fields"}, {"st": 69, "ed": 71, "text": "feature selection"}, {"st": 97, "ed": 100, "text": "achieve competitive results"}]
[{"st": 1, "ed": 4, "text": "propose and evaluate"}, {"st": 10, "ed": 12, "text": "instance based"}, {"st": 52, "ed": 54, "text": "ensemble methods"}, {"st": 63, "ed": 65, "text": "ensemble based"}, {"st": 116, "ed": 118, "text": "case study"}, {"st": 131, "ed": 133, "text": "ensemble methods"}, {"st": 151, "ed": 153, "text": "bias variance"}]
[{"st": 7, "ed": 9, "text": "online learning"}, {"st": 15, "ed": 17, "text": "learning agent"}, {"st": 51, "ed": 53, "text": "sample efficient"}, {"st": 76, "ed": 78, "text": "computationally efficient"}, {"st": 88, "ed": 90, "text": "o sqrt"}, {"st": 143, "ed": 145, "text": "constant factor"}]
[{"st": 5, "ed": 7, "text": "neural networks"}, {"st": 20, "ed": 22, "text": "neural networks"}, {"st": 35, "ed": 37, "text": "activation functions"}, {"st": 57, "ed": 59, "text": "computational complexity"}, {"st": 61, "ed": 63, "text": "neural networks"}, {"st": 70, "ed": 73, "text": "positive and negative"}]
[{"st": 13, "ed": 15, "text": "ell 1"}, {"st": 36, "ed": 38, "text": "nonparametric regression"}, {"st": 49, "ed": 51, "text": "a level"}, {"st": 58, "ed": 60, "text": "ell 2"}, {"st": 70, "ed": 72, "text": "minimization problem"}]
[{"st": 5, "ed": 7, "text": "feature selection"}, {"st": 10, "ed": 13, "text": "determinantal point processes"}, {"st": 32, "ed": 34, "text": "matching pursuit"}, {"st": 56, "ed": 58, "text": "variational approximation"}, {"st": 61, "ed": 64, "text": "spike and slab"}, {"st": 77, "ed": 79, "text": "mean field"}, {"st": 144, "ed": 146, "text": "proposed method"}, {"st": 150, "ed": 152, "text": "feature sets"}, {"st": 156, "ed": 158, "text": "without compromising"}]
[{"st": 9, "ed": 11, "text": "noisy data"}, {"st": 16, "ed": 18, "text": "machine learning"}, {"st": 36, "ed": 38, "text": "renewable energy"}, {"st": 85, "ed": 87, "text": "mixed integer"}, {"st": 105, "ed": 107, "text": "larger datasets"}, {"st": 125, "ed": 127, "text": "background knowledge"}]
[{"st": 6, "ed": 8, "text": "gaussian process"}, {"st": 10, "ed": 12, "text": "large scale"}, {"st": 16, "ed": 19, "text": "mixture of experts"}, {"st": 21, "ed": 23, "text": "conceptually simple"}, {"st": 36, "ed": 38, "text": "closed form"}, {"st": 62, "ed": 65, "text": "large data sets"}, {"st": 81, "ed": 84, "text": "large data sets"}, {"st": 101, "ed": 103, "text": "large scale"}, {"st": 103, "ed": 105, "text": "gaussian process"}]
[{"st": 1, "ed": 3, "text": "unsupervised learning"}, {"st": 5, "ed": 7, "text": "uniform sampling"}, {"st": 15, "ed": 17, "text": "learned features"}, {"st": 63, "ed": 65, "text": "selection algorithms"}, {"st": 67, "ed": 69, "text": "dictionary learning"}, {"st": 77, "ed": 79, "text": "selection algorithms"}]
[{"st": 7, "ed": 10, "text": "markov decision processes"}, {"st": 55, "ed": 57, "text": "value iteration"}]
[{"st": 9, "ed": 11, "text": "bayesian network"}]
[{"st": 6, "ed": 10, "text": "multi armed bandit problem"}, {"st": 57, "ed": 59, "text": "learning agent"}, {"st": 65, "ed": 67, "text": "context dependent"}, {"st": 84, "ed": 86, "text": "near optimal"}, {"st": 86, "ed": 88, "text": "regret bounds"}, {"st": 93, "ed": 96, "text": "computationally efficient algorithm"}, {"st": 102, "ed": 104, "text": "regret bounds"}, {"st": 140, "ed": 142, "text": "open question"}]
[{"st": 3, "ed": 5, "text": "thompson sampling"}, {"st": 28, "ed": 30, "text": "domain knowledge"}, {"st": 36, "ed": 38, "text": "prior distribution"}, {"st": 40, "ed": 43, "text": "exploration and exploitation"}, {"st": 80, "ed": 82, "text": "worst case"}, {"st": 120, "ed": 122, "text": "prior probability"}, {"st": 131, "ed": 133, "text": "o sqrt"}, {"st": 136, "ed": 138, "text": "o sqrt"}, {"st": 170, "ed": 172, "text": "thompson sampling"}]
[{"st": 7, "ed": 11, "text": "bayesian network structure learning"}, {"st": 29, "ed": 31, "text": "hill climbing"}, {"st": 41, "ed": 44, "text": "divide and conquer"}, {"st": 44, "ed": 46, "text": "constraint based"}, {"st": 68, "ed": 70, "text": "hill climbing"}, {"st": 113, "ed": 115, "text": "extensive experiments"}, {"st": 123, "ed": 126, "text": "goodness of fit"}, {"st": 133, "ed": 135, "text": "network structure"}, {"st": 154, "ed": 156, "text": "multi label"}, {"st": 159, "ed": 161, "text": "provide theoretical"}, {"st": 180, "ed": 182, "text": "joint distribution"}, {"st": 187, "ed": 189, "text": "multi label"}, {"st": 198, "ed": 201, "text": "multi class classification"}, {"st": 204, "ed": 206, "text": "multi class"}, {"st": 215, "ed": 217, "text": "compare favorably"}, {"st": 223, "ed": 225, "text": "classification accuracy"}, {"st": 227, "ed": 229, "text": "multi label"}, {"st": 251, "ed": 253, "text": "local neighborhood"}, {"st": 262, "ed": 264, "text": "learning framework"}, {"st": 269, "ed": 271, "text": "multi label"}, {"st": 273, "ed": 275, "text": "source code"}]
[{"st": 44, "ed": 47, "text": "branch and bound"}, {"st": 142, "ed": 144, "text": "conduct experiments"}, {"st": 146, "ed": 148, "text": "synthetic data"}, {"st": 152, "ed": 154, "text": "real world"}, {"st": 156, "ed": 158, "text": "experiments confirm"}, {"st": 178, "ed": 180, "text": "data mining"}]
[{"st": 0, "ed": 3, "text": "probabilistic graphical models"}, {"st": 28, "ed": 30, "text": "inference tasks"}, {"st": 42, "ed": 44, "text": "exact inference"}, {"st": 107, "ed": 109, "text": "graphical model"}, {"st": 169, "ed": 171, "text": "approximate inference"}, {"st": 180, "ed": 183, "text": "loopy belief propagation"}, {"st": 195, "ed": 197, "text": "time consuming"}, {"st": 198, "ed": 200, "text": "monte carlo"}, {"st": 212, "ed": 214, "text": "inference problems"}, {"st": 219, "ed": 221, "text": "computer vision"}, {"st": 225, "ed": 227, "text": "hidden variables"}]
[{"st": 0, "ed": 2, "text": "cross validation"}, {"st": 13, "ed": 15, "text": "parameter tuning"}, {"st": 29, "ed": 31, "text": "learning algorithm"}, {"st": 37, "ed": 39, "text": "computationally expensive"}, {"st": 51, "ed": 53, "text": "computational burden"}, {"st": 62, "ed": 64, "text": "previous attempts"}, {"st": 86, "ed": 88, "text": "incremental learning"}, {"st": 94, "ed": 96, "text": "big data"}, {"st": 107, "ed": 110, "text": "supervised and unsupervised"}, {"st": 121, "ed": 123, "text": "learning algorithm"}, {"st": 162, "ed": 164, "text": "incremental learning"}]
[{"st": 1, "ed": 3, "text": "true online"}, {"st": 26, "ed": 29, "text": "temporal difference learning"}, {"st": 32, "ed": 34, "text": "true online"}, {"st": 38, "ed": 40, "text": "theoretical properties"}, {"st": 71, "ed": 73, "text": "true online"}, {"st": 89, "ed": 91, "text": "real world"}, {"st": 96, "ed": 98, "text": "linear function"}, {"st": 113, "ed": 115, "text": "computational cost"}, {"st": 115, "ed": 117, "text": "learning speed"}, {"st": 127, "ed": 129, "text": "true online"}, {"st": 134, "ed": 136, "text": "feature vectors"}, {"st": 137, "ed": 139, "text": "computational overhead"}, {"st": 167, "ed": 169, "text": "learning speed"}, {"st": 170, "ed": 172, "text": "true online"}, {"st": 187, "ed": 189, "text": "true online"}, {"st": 221, "ed": 223, "text": "true online"}]
[{"st": 83, "ed": 85, "text": "learned models"}, {"st": 104, "ed": 106, "text": "input variables"}, {"st": 117, "ed": 119, "text": "elastic net"}, {"st": 133, "ed": 135, "text": "real world"}]
[{"st": 22, "ed": 25, "text": "rate of convergence"}, {"st": 31, "ed": 33, "text": "computationally expensive"}, {"st": 49, "ed": 51, "text": "policy optimization"}, {"st": 52, "ed": 55, "text": "markov decision processes"}, {"st": 67, "ed": 69, "text": "objective function"}, {"st": 108, "ed": 110, "text": "least squares"}, {"st": 132, "ed": 134, "text": "desirable properties"}, {"st": 143, "ed": 145, "text": "performance guarantees"}, {"st": 151, "ed": 153, "text": "affine transformation"}, {"st": 155, "ed": 157, "text": "parameter space"}, {"st": 168, "ed": 170, "text": "policy search"}, {"st": 179, "ed": 181, "text": "closely related"}, {"st": 184, "ed": 186, "text": "em algorithm"}, {"st": 187, "ed": 189, "text": "natural gradient"}]
[{"st": 7, "ed": 9, "text": "based clustering"}, {"st": 15, "ed": 17, "text": "side information"}, {"st": 27, "ed": 29, "text": "side information"}, {"st": 39, "ed": 41, "text": "nonparametric bayesian"}, {"st": 46, "ed": 48, "text": "probabilistic model"}, {"st": 61, "ed": 63, "text": "gibbs sampling"}, {"st": 76, "ed": 78, "text": "probabilistic model"}, {"st": 84, "ed": 86, "text": "clustering algorithm"}, {"st": 97, "ed": 99, "text": "k means"}, {"st": 105, "ed": 107, "text": "side information"}, {"st": 114, "ed": 117, "text": "number of clusters"}, {"st": 125, "ed": 127, "text": "empirical studies"}, {"st": 138, "ed": 140, "text": "clustering algorithms"}, {"st": 160, "ed": 162, "text": "side information"}]
[{"st": 6, "ed": 8, "text": "gaussian process"}, {"st": 22, "ed": 24, "text": "partially observed"}, {"st": 52, "ed": 55, "text": "semi supervised learning"}, {"st": 57, "ed": 59, "text": "missing values"}, {"st": 74, "ed": 76, "text": "special case"}, {"st": 86, "ed": 88, "text": "variational methods"}, {"st": 105, "ed": 107, "text": "missing values"}, {"st": 116, "ed": 118, "text": "extensive experiments"}, {"st": 119, "ed": 124, "text": "simulated and real world data"}, {"st": 150, "ed": 152, "text": "significantly improve"}]
[{"st": 4, "ed": 6, "text": "based approach"}, {"st": 7, "ed": 9, "text": "fast approximate"}, {"st": 10, "ed": 12, "text": "inner product"}, {"st": 33, "ed": 35, "text": "inner product"}, {"st": 39, "ed": 41, "text": "inner product"}, {"st": 62, "ed": 64, "text": "recently proposed"}, {"st": 81, "ed": 83, "text": "higher dimensional"}, {"st": 89, "ed": 91, "text": "theoretical analysis"}, {"st": 93, "ed": 95, "text": "proposed approach"}, {"st": 106, "ed": 108, "text": "small sample"}, {"st": 140, "ed": 143, "text": "deep neural networks"}, {"st": 146, "ed": 148, "text": "proposed approach"}, {"st": 148, "ed": 150, "text": "significantly outperforms"}]
[{"st": 77, "ed": 79, "text": "reinforcement learning"}, {"st": 93, "ed": 96, "text": "sequential decision making"}, {"st": 101, "ed": 104, "text": "markov decision processes"}, {"st": 116, "ed": 119, "text": "sequential decision making"}, {"st": 144, "ed": 147, "text": "ability to capture"}, {"st": 214, "ed": 216, "text": "proposed method"}, {"st": 225, "ed": 227, "text": "learning algorithm"}, {"st": 255, "ed": 257, "text": "promising results"}]
[{"st": 1, "ed": 3, "text": "mutual information"}, {"st": 15, "ed": 17, "text": "machine learning"}, {"st": 48, "ed": 50, "text": "learning algorithms"}, {"st": 55, "ed": 57, "text": "mutual information"}, {"st": 74, "ed": 77, "text": "modern machine learning"}, {"st": 89, "ed": 91, "text": "mutual information"}, {"st": 95, "ed": 97, "text": "variational inference"}, {"st": 115, "ed": 117, "text": "mutual information"}, {"st": 131, "ed": 134, "text": "variational lower bound"}, {"st": 136, "ed": 138, "text": "mutual information"}, {"st": 140, "ed": 142, "text": "convolutional networks"}, {"st": 144, "ed": 146, "text": "visual input"}]
[{"st": 3, "ed": 5, "text": "data mining"}, {"st": 20, "ed": 22, "text": "money laundering"}, {"st": 46, "ed": 49, "text": "real world data"}, {"st": 51, "ed": 53, "text": "financial institution"}]
[{"st": 4, "ed": 6, "text": "context aware"}, {"st": 23, "ed": 25, "text": "real world"}, {"st": 28, "ed": 31, "text": "multi armed bandits"}, {"st": 49, "ed": 51, "text": "context aware"}, {"st": 54, "ed": 56, "text": "exploration exploitation"}, {"st": 72, "ed": 74, "text": "theoretical analysis"}, {"st": 78, "ed": 81, "text": "multi armed bandits"}, {"st": 92, "ed": 94, "text": "real world"}, {"st": 104, "ed": 106, "text": "prediction performance"}]
[{"st": 3, "ed": 6, "text": "entities and relations"}, {"st": 14, "ed": 16, "text": "machine learning"}, {"st": 42, "ed": 44, "text": "proposed method"}, {"st": 50, "ed": 52, "text": "associative memory"}, {"st": 90, "ed": 92, "text": "extensive experiments"}, {"st": 107, "ed": 109, "text": "link prediction"}]
[{"st": 0, "ed": 2, "text": "bayesian nonparametric"}, {"st": 5, "ed": 7, "text": "gaussian processes"}, {"st": 35, "ed": 37, "text": "gaussian processes"}, {"st": 67, "ed": 69, "text": "kernel learning"}, {"st": 71, "ed": 73, "text": "reverse engineer"}, {"st": 74, "ed": 76, "text": "inductive biases"}, {"st": 118, "ed": 120, "text": "gaussian process"}]
[{"st": 4, "ed": 6, "text": "significant progress"}, {"st": 8, "ed": 10, "text": "reinforcement learning"}, {"st": 12, "ed": 14, "text": "infinite horizon"}, {"st": 14, "ed": 17, "text": "markov decision processes"}, {"st": 21, "ed": 23, "text": "sample complexity"}, {"st": 27, "ed": 29, "text": "real world"}, {"st": 31, "ed": 33, "text": "interactive learning"}, {"st": 73, "ed": 75, "text": "sample complexity"}, {"st": 80, "ed": 82, "text": "sample complexity"}, {"st": 124, "ed": 126, "text": "frac 1"}, {"st": 144, "ed": 146, "text": "frac 1"}, {"st": 193, "ed": 195, "text": "finite horizon"}]
[{"st": 0, "ed": 2, "text": "structured prediction"}, {"st": 8, "ed": 10, "text": "computer vision"}, {"st": 11, "ed": 13, "text": "natural language"}, {"st": 16, "ed": 18, "text": "structured outputs"}, {"st": 31, "ed": 33, "text": "map inference"}, {"st": 45, "ed": 47, "text": "scoring functions"}, {"st": 50, "ed": 52, "text": "accurate predictions"}, {"st": 53, "ed": 56, "text": "learning and inference"}, {"st": 56, "ed": 58, "text": "typically require"}, {"st": 76, "ed": 78, "text": "linear programming"}, {"st": 84, "ed": 86, "text": "real world"}, {"st": 100, "ed": 102, "text": "training instances"}]
[{"st": 8, "ed": 10, "text": "structural properties"}, {"st": 11, "ed": 13, "text": "deep learning"}, {"st": 35, "ed": 37, "text": "deep architecture"}, {"st": 56, "ed": 58, "text": "closed form"}, {"st": 72, "ed": 74, "text": "expressive power"}, {"st": 77, "ed": 79, "text": "jointly learn"}, {"st": 86, "ed": 88, "text": "marginal likelihood"}, {"st": 92, "ed": 95, "text": "inference and learning"}, {"st": 100, "ed": 102, "text": "training points"}, {"st": 117, "ed": 119, "text": "applications including"}, {"st": 127, "ed": 129, "text": "improved performance"}, {"st": 131, "ed": 133, "text": "gaussian processes"}, {"st": 135, "ed": 137, "text": "kernel learning"}]
[{"st": 0, "ed": 3, "text": "sum product networks"}, {"st": 60, "ed": 63, "text": "sum product networks"}, {"st": 100, "ed": 102, "text": "local search"}, {"st": 114, "ed": 117, "text": "dynamic bayesian networks"}]
[{"st": 24, "ed": 26, "text": "computation cost"}, {"st": 33, "ed": 35, "text": "time consuming"}, {"st": 85, "ed": 87, "text": "computational burden"}, {"st": 112, "ed": 114, "text": "computational burden"}, {"st": 120, "ed": 122, "text": "key idea"}, {"st": 127, "ed": 129, "text": "fitness function"}, {"st": 182, "ed": 184, "text": "faster convergence"}]
[{"st": 6, "ed": 8, "text": "active learning"}, {"st": 10, "ed": 12, "text": "multi output"}, {"st": 12, "ed": 14, "text": "gaussian process"}, {"st": 27, "ed": 29, "text": "existing works"}, {"st": 30, "ed": 32, "text": "active learning"}, {"st": 111, "ed": 113, "text": "active learning"}, {"st": 132, "ed": 134, "text": "approximation algorithm"}, {"st": 137, "ed": 139, "text": "constant factor"}, {"st": 150, "ed": 152, "text": "empirical evaluation"}, {"st": 153, "ed": 155, "text": "real world"}, {"st": 159, "ed": 161, "text": "proposed approach"}, {"st": 161, "ed": 163, "text": "outperforms existing"}, {"st": 165, "ed": 167, "text": "active learning"}]
[{"st": 8, "ed": 10, "text": "submodular function"}, {"st": 62, "ed": 64, "text": "submodular function"}, {"st": 90, "ed": 92, "text": "submodular function"}]
[{"st": 3, "ed": 6, "text": "efficient and effective"}, {"st": 6, "ed": 8, "text": "decision trees"}, {"st": 13, "ed": 15, "text": "machine learning"}, {"st": 31, "ed": 33, "text": "near optimal"}, {"st": 41, "ed": 43, "text": "decision tree"}, {"st": 51, "ed": 53, "text": "shannon entropy"}, {"st": 83, "ed": 85, "text": "shannon entropy"}, {"st": 92, "ed": 94, "text": "decision trees"}, {"st": 114, "ed": 116, "text": "shannon entropy"}, {"st": 157, "ed": 159, "text": "statistically significant"}]
[{"st": 34, "ed": 36, "text": "existing methods"}, {"st": 72, "ed": 74, "text": "specific task"}, {"st": 77, "ed": 79, "text": "convergence guarantee"}, {"st": 103, "ed": 105, "text": "learning framework"}, {"st": 108, "ed": 110, "text": "local information"}, {"st": 127, "ed": 129, "text": "spanning tree"}, {"st": 133, "ed": 135, "text": "ell 1"}, {"st": 141, "ed": 143, "text": "learning algorithm"}, {"st": 171, "ed": 173, "text": "proposed framework"}, {"st": 176, "ed": 178, "text": "large scale"}, {"st": 186, "ed": 188, "text": "real world"}, {"st": 192, "ed": 194, "text": "proposed method"}, {"st": 194, "ed": 197, "text": "compares favorably with"}, {"st": 202, "ed": 204, "text": "underlying structure"}]
[{"st": 5, "ed": 8, "text": "armed bandit problem"}, {"st": 10, "ed": 12, "text": "learning agent"}, {"st": 100, "ed": 102, "text": "sample complexity"}, {"st": 126, "ed": 128, "text": "proposed algorithm"}]
[{"st": 1, "ed": 3, "text": "time series"}, {"st": 39, "ed": 41, "text": "distance based"}, {"st": 50, "ed": 52, "text": "distance based"}, {"st": 60, "ed": 62, "text": "theoretically sound"}, {"st": 70, "ed": 72, "text": "k means"}, {"st": 96, "ed": 98, "text": "k means"}]
[{"st": 0, "ed": 2, "text": "machine learning"}, {"st": 74, "ed": 76, "text": "cost function"}, {"st": 89, "ed": 91, "text": "real world"}, {"st": 103, "ed": 106, "text": "statistical machine learning"}, {"st": 186, "ed": 188, "text": "machine learning"}, {"st": 194, "ed": 196, "text": "predictive models"}, {"st": 199, "ed": 201, "text": "prediction accuracy"}, {"st": 210, "ed": 213, "text": "user experience design"}]
[{"st": 7, "ed": 9, "text": "deep learning"}, {"st": 14, "ed": 16, "text": "multi level"}, {"st": 52, "ed": 54, "text": "deep learning"}, {"st": 61, "ed": 63, "text": "deep learning"}, {"st": 68, "ed": 71, "text": "deep belief network"}, {"st": 88, "ed": 90, "text": "method yields"}]
[{"st": 41, "ed": 43, "text": "learning framework"}, {"st": 47, "ed": 49, "text": "building block"}, {"st": 54, "ed": 56, "text": "lifelong learning"}, {"st": 83, "ed": 85, "text": "local optimum"}, {"st": 112, "ed": 114, "text": "multiple tasks"}]
[{"st": 0, "ed": 4, "text": "sum product networks spns"}, {"st": 17, "ed": 19, "text": "structure learning"}, {"st": 67, "ed": 69, "text": "missing data"}, {"st": 83, "ed": 86, "text": "standard benchmark datasets"}, {"st": 93, "ed": 95, "text": "knowledge graph"}, {"st": 103, "ed": 106, "text": "discrete and continuous"}]
[{"st": 0, "ed": 2, "text": "efficient exploration"}, {"st": 3, "ed": 5, "text": "complex environments"}, {"st": 25, "ed": 27, "text": "statistically efficient"}, {"st": 39, "ed": 41, "text": "epsilon greedy"}, {"st": 69, "ed": 71, "text": "large scale"}, {"st": 76, "ed": 78, "text": "substantially improves"}]
[{"st": 3, "ed": 5, "text": "machine learning"}, {"st": 125, "ed": 127, "text": "submodular optimization"}, {"st": 142, "ed": 144, "text": "random forests"}, {"st": 145, "ed": 147, "text": "image classification"}, {"st": 148, "ed": 150, "text": "neural networks"}]
[{"st": 2, "ed": 4, "text": "pattern mining"}, {"st": 8, "ed": 11, "text": "minimum description length"}, {"st": 16, "ed": 18, "text": "encoding scheme"}, {"st": 41, "ed": 43, "text": "probabilistic model"}, {"st": 66, "ed": 68, "text": "proposed algorithm"}, {"st": 88, "ed": 90, "text": "efficient inference"}, {"st": 103, "ed": 105, "text": "expectation maximization"}, {"st": 116, "ed": 118, "text": "submodular optimization"}, {"st": 128, "ed": 133, "text": "synthetic and real world datasets"}, {"st": 152, "ed": 154, "text": "real world"}, {"st": 180, "ed": 182, "text": "pattern mining"}]
[{"st": 0, "ed": 3, "text": "deep generative models"}, {"st": 5, "ed": 7, "text": "neural networks"}, {"st": 8, "ed": 10, "text": "recently achieved"}, {"st": 16, "ed": 20, "text": "unsupervised and semi supervised"}, {"st": 23, "ed": 26, "text": "deep generative models"}, {"st": 39, "ed": 41, "text": "generative model"}, {"st": 45, "ed": 47, "text": "variational distribution"}, {"st": 78, "ed": 81, "text": "deep generative models"}, {"st": 94, "ed": 97, "text": "semi supervised learning"}, {"st": 98, "ed": 100, "text": "mnist svhn"}]
[{"st": 0, "ed": 2, "text": "submodular function"}, {"st": 9, "ed": 11, "text": "real world"}, {"st": 11, "ed": 13, "text": "decision making"}, {"st": 16, "ed": 18, "text": "existing methods"}, {"st": 25, "ed": 27, "text": "computationally feasible"}, {"st": 62, "ed": 64, "text": "confidence bounds"}, {"st": 89, "ed": 91, "text": "confidence bounds"}, {"st": 118, "ed": 120, "text": "real world"}, {"st": 129, "ed": 131, "text": "shopping mall"}, {"st": 135, "ed": 137, "text": "performs comparably"}, {"st": 138, "ed": 140, "text": "existing methods"}]
[{"st": 5, "ed": 7, "text": "thompson sampling"}, {"st": 9, "ed": 11, "text": "reinforcement learning"}, {"st": 33, "ed": 35, "text": "thompson sampling"}]
[{"st": 4, "ed": 6, "text": "reinforcement learning"}, {"st": 14, "ed": 17, "text": "deep neural networks"}, {"st": 39, "ed": 41, "text": "improve performance"}, {"st": 56, "ed": 58, "text": "policy iteration"}, {"st": 61, "ed": 63, "text": "policy improvement"}, {"st": 70, "ed": 72, "text": "policy improvement"}, {"st": 78, "ed": 80, "text": "existing approaches"}, {"st": 110, "ed": 112, "text": "policy improvement"}, {"st": 135, "ed": 137, "text": "policy iteration"}, {"st": 149, "ed": 152, "text": "easy to implement"}, {"st": 154, "ed": 156, "text": "sample based"}]
[{"st": 0, "ed": 2, "text": "probabilistic modeling"}, {"st": 25, "ed": 27, "text": "complex models"}, {"st": 28, "ed": 30, "text": "large data"}, {"st": 63, "ed": 65, "text": "automatic differentiation"}, {"st": 65, "ed": 67, "text": "variational inference"}, {"st": 77, "ed": 79, "text": "probabilistic model"}, {"st": 89, "ed": 91, "text": "variational inference"}, {"st": 136, "ed": 138, "text": "probabilistic programming"}]
[{"st": 15, "ed": 17, "text": "active learning"}, {"st": 53, "ed": 55, "text": "computationally efficient"}, {"st": 58, "ed": 60, "text": "sample complexity"}, {"st": 76, "ed": 78, "text": "sample complexity"}, {"st": 102, "ed": 104, "text": "theoretical results"}]
[{"st": 5, "ed": 7, "text": "real world"}, {"st": 12, "ed": 14, "text": "reinforcement learning"}, {"st": 52, "ed": 54, "text": "exploration strategy"}, {"st": 136, "ed": 138, "text": "reinforcement learning"}, {"st": 158, "ed": 160, "text": "exploration strategy"}, {"st": 172, "ed": 174, "text": "reinforcement learning"}, {"st": 178, "ed": 180, "text": "real world"}]
[{"st": 6, "ed": 8, "text": "time series"}, {"st": 20, "ed": 23, "text": "problem of recovering"}, {"st": 24, "ed": 26, "text": "causal structure"}, {"st": 100, "ed": 102, "text": "joint distribution"}, {"st": 142, "ed": 144, "text": "real world"}]
[{"st": 51, "ed": 53, "text": "machine learning"}, {"st": 61, "ed": 64, "text": "deep generative models"}, {"st": 68, "ed": 70, "text": "representational power"}, {"st": 71, "ed": 73, "text": "deep learning"}, {"st": 86, "ed": 88, "text": "generative models"}, {"st": 103, "ed": 105, "text": "generative models"}, {"st": 124, "ed": 126, "text": "generalization ability"}]
[{"st": 0, "ed": 2, "text": "unlike traditional"}, {"st": 5, "ed": 7, "text": "operating systems"}, {"st": 16, "ed": 18, "text": "machine learning"}, {"st": 29, "ed": 31, "text": "machine learning"}, {"st": 50, "ed": 52, "text": "machine learning"}, {"st": 91, "ed": 93, "text": "classification tasks"}, {"st": 96, "ed": 98, "text": "training data"}, {"st": 151, "ed": 153, "text": "probabilistic program"}, {"st": 158, "ed": 160, "text": "probabilistic programs"}, {"st": 163, "ed": 165, "text": "probabilistic programs"}, {"st": 171, "ed": 173, "text": "machine learning"}, {"st": 175, "ed": 177, "text": "efficiently compute"}]
[{"st": 3, "ed": 5, "text": "parameter estimation"}, {"st": 15, "ed": 17, "text": "probabilistic inference"}, {"st": 28, "ed": 30, "text": "probabilistic models"}, {"st": 38, "ed": 40, "text": "inference algorithm"}, {"st": 96, "ed": 99, "text": "discrete and continuous"}, {"st": 130, "ed": 132, "text": "proposed approach"}]
[{"st": 4, "ed": 6, "text": "context dependent"}, {"st": 6, "ed": 8, "text": "clustering technique"}, {"st": 9, "ed": 11, "text": "recommender systems"}, {"st": 13, "ed": 15, "text": "exploration exploitation"}, {"st": 17, "ed": 20, "text": "multi armed bandits"}, {"st": 86, "ed": 88, "text": "extensive empirical"}, {"st": 90, "ed": 92, "text": "real world"}, {"st": 97, "ed": 99, "text": "prediction performance"}, {"st": 111, "ed": 114, "text": "multi armed bandit"}]
[{"st": 15, "ed": 17, "text": "predictive accuracy"}]
[{"st": 11, "ed": 14, "text": "taking into account"}, {"st": 71, "ed": 73, "text": "classification methods"}, {"st": 85, "ed": 87, "text": "naive bayes"}, {"st": 87, "ed": 90, "text": "k nearest neighbors"}, {"st": 90, "ed": 93, "text": "support vector machine"}, {"st": 101, "ed": 104, "text": "restricted boltzmann machine"}, {"st": 110, "ed": 112, "text": "extracted features"}, {"st": 147, "ed": 149, "text": "temporal resolution"}, {"st": 157, "ed": 159, "text": "previous studies"}, {"st": 168, "ed": 170, "text": "generalization capabilities"}]
[{"st": 1, "ed": 3, "text": "context specific"}, {"st": 32, "ed": 35, "text": "undirected graphical models"}, {"st": 51, "ed": 54, "text": "gaussian graphical models"}, {"st": 59, "ed": 61, "text": "context specific"}, {"st": 124, "ed": 126, "text": "linear programming"}, {"st": 129, "ed": 131, "text": "solved efficiently"}, {"st": 153, "ed": 155, "text": "real world"}, {"st": 183, "ed": 185, "text": "synthetic datasets"}, {"st": 190, "ed": 192, "text": "shows significant"}]
[{"st": 37, "ed": 39, "text": "blood sugar"}, {"st": 50, "ed": 52, "text": "algorithmic framework"}, {"st": 60, "ed": 62, "text": "domain adaptation"}, {"st": 69, "ed": 71, "text": "theoretical justification"}, {"st": 74, "ed": 76, "text": "empirical comparison"}, {"st": 77, "ed": 79, "text": "previous approaches"}, {"st": 80, "ed": 82, "text": "causal inference"}, {"st": 86, "ed": 88, "text": "deep learning"}, {"st": 89, "ed": 91, "text": "significantly outperforms"}]
[{"st": 35, "ed": 37, "text": "sentiment analysis"}, {"st": 42, "ed": 44, "text": "sentiment analysis"}, {"st": 69, "ed": 72, "text": "positive and negative"}, {"st": 91, "ed": 93, "text": "supervised learning"}, {"st": 105, "ed": 107, "text": "performance measures"}, {"st": 127, "ed": 129, "text": "directly optimizing"}, {"st": 131, "ed": 133, "text": "specific performance"}, {"st": 141, "ed": 143, "text": "performance measures"}, {"st": 169, "ed": 171, "text": "theoretical analysis"}, {"st": 179, "ed": 181, "text": "extensive experiments"}, {"st": 184, "ed": 186, "text": "real data"}, {"st": 192, "ed": 194, "text": "significantly outperform"}, {"st": 195, "ed": 197, "text": "optimization techniques"}]
[{"st": 43, "ed": 45, "text": "individual level"}, {"st": 64, "ed": 66, "text": "generalized linear"}, {"st": 107, "ed": 109, "text": "individual level"}, {"st": 115, "ed": 118, "text": "generalized linear model"}, {"st": 126, "ed": 128, "text": "proposed approach"}, {"st": 169, "ed": 171, "text": "simulated data"}, {"st": 177, "ed": 179, "text": "diminishing returns"}]
[{"st": 16, "ed": 18, "text": "ranking based"}, {"st": 25, "ed": 27, "text": "web search"}, {"st": 55, "ed": 57, "text": "empirical evaluation"}, {"st": 58, "ed": 61, "text": "real world data"}, {"st": 78, "ed": 81, "text": "learning to rank"}, {"st": 107, "ed": 109, "text": "experimentally demonstrate"}]
[{"st": 17, "ed": 20, "text": "convolutional neural networks"}, {"st": 32, "ed": 35, "text": "discrete and continuous"}, {"st": 43, "ed": 45, "text": "convolutional networks"}, {"st": 68, "ed": 71, "text": "benchmark data sets"}, {"st": 76, "ed": 78, "text": "feature representations"}]
[{"st": 0, "ed": 2, "text": "variational inference"}, {"st": 8, "ed": 10, "text": "posterior distribution"}, {"st": 30, "ed": 32, "text": "previous studies"}, {"st": 75, "ed": 77, "text": "variational parameters"}, {"st": 129, "ed": 131, "text": "comparative study"}, {"st": 134, "ed": 136, "text": "real life"}]
[{"st": 5, "ed": 7, "text": "strongly convex"}, {"st": 7, "ed": 9, "text": "online optimization"}, {"st": 18, "ed": 20, "text": "online game"}, {"st": 64, "ed": 66, "text": "simple regret"}, {"st": 87, "ed": 89, "text": "without resorting"}, {"st": 135, "ed": 137, "text": "regret bound"}]
[{"st": 8, "ed": 10, "text": "hyperparameter optimization"}, {"st": 11, "ed": 13, "text": "machine learning"}, {"st": 16, "ed": 19, "text": "support vector machines"}, {"st": 27, "ed": 29, "text": "large datasets"}, {"st": 49, "ed": 51, "text": "hyperparameter optimization"}, {"st": 54, "ed": 56, "text": "generative model"}, {"st": 58, "ed": 60, "text": "validation error"}, {"st": 64, "ed": 66, "text": "training set"}, {"st": 72, "ed": 74, "text": "optimization process"}, {"st": 92, "ed": 94, "text": "bayesian optimization"}, {"st": 107, "ed": 109, "text": "dataset size"}, {"st": 118, "ed": 120, "text": "global optimum"}, {"st": 125, "ed": 128, "text": "support vector machines"}, {"st": 129, "ed": 132, "text": "deep neural networks"}, {"st": 143, "ed": 146, "text": "times faster than"}, {"st": 151, "ed": 153, "text": "bayesian optimization"}, {"st": 156, "ed": 158, "text": "recently proposed"}]
[{"st": 4, "ed": 6, "text": "successfully applied"}, {"st": 10, "ed": 12, "text": "reinforcement learning"}, {"st": 19, "ed": 21, "text": "optimal policies"}, {"st": 68, "ed": 71, "text": "taking into account"}, {"st": 110, "ed": 112, "text": "random sampling"}]
[{"st": 1, "ed": 3, "text": "labeled training"}, {"st": 7, "ed": 9, "text": "building blocks"}, {"st": 10, "ed": 12, "text": "supervised learning"}, {"st": 18, "ed": 20, "text": "deep learning"}, {"st": 25, "ed": 27, "text": "labeled training"}, {"st": 31, "ed": 33, "text": "time consuming"}, {"st": 50, "ed": 52, "text": "training sets"}, {"st": 59, "ed": 61, "text": "weak supervision"}, {"st": 91, "ed": 93, "text": "training set"}, {"st": 97, "ed": 99, "text": "generative model"}, {"st": 104, "ed": 106, "text": "training set"}, {"st": 117, "ed": 119, "text": "generative models"}, {"st": 132, "ed": 134, "text": "loss function"}, {"st": 147, "ed": 149, "text": "discriminative models"}, {"st": 150, "ed": 152, "text": "logistic regression"}, {"st": 233, "ed": 235, "text": "machine learning"}, {"st": 237, "ed": 239, "text": "training data"}]
[{"st": 139, "ed": 141, "text": "sampling based"}, {"st": 147, "ed": 149, "text": "ground truth"}, {"st": 164, "ed": 166, "text": "random sampling"}, {"st": 185, "ed": 187, "text": "random sampling"}, {"st": 201, "ed": 203, "text": "real world"}]
[{"st": 0, "ed": 3, "text": "determinantal point processes"}, {"st": 5, "ed": 7, "text": "probabilistic models"}, {"st": 18, "ed": 20, "text": "recently gained"}, {"st": 41, "ed": 43, "text": "mathcal o"}, {"st": 69, "ed": 71, "text": "kernel matrix"}, {"st": 119, "ed": 121, "text": "stochastic optimization"}]
[{"st": 9, "ed": 11, "text": "approximation error"}, {"st": 16, "ed": 18, "text": "probabilistic inference"}, {"st": 26, "ed": 28, "text": "monte carlo"}, {"st": 30, "ed": 32, "text": "key idea"}, {"st": 49, "ed": 51, "text": "approximate inference"}, {"st": 70, "ed": 72, "text": "probabilistic programs"}, {"st": 101, "ed": 103, "text": "approximate inference"}, {"st": 116, "ed": 118, "text": "empirical results"}, {"st": 119, "ed": 121, "text": "inference problems"}, {"st": 123, "ed": 125, "text": "linear regression"}]
[{"st": 30, "ed": 32, "text": "optimization algorithm"}, {"st": 53, "ed": 55, "text": "reinforcement learning"}, {"st": 60, "ed": 62, "text": "optimization algorithm"}, {"st": 68, "ed": 70, "text": "optimization algorithm"}, {"st": 72, "ed": 74, "text": "policy search"}, {"st": 79, "ed": 81, "text": "algorithm outperforms"}, {"st": 82, "ed": 84, "text": "hand engineered"}, {"st": 88, "ed": 90, "text": "convergence speed"}]
[{"st": 0, "ed": 2, "text": "continual learning"}, {"st": 3, "ed": 6, "text": "artificial neural networks"}, {"st": 22, "ed": 25, "text": "long term memory"}, {"st": 32, "ed": 34, "text": "multi task"}, {"st": 34, "ed": 36, "text": "deep learning"}, {"st": 41, "ed": 43, "text": "previously learned"}, {"st": 45, "ed": 47, "text": "sensory input"}, {"st": 62, "ed": 65, "text": "deep neural networks"}, {"st": 70, "ed": 72, "text": "previously learned"}, {"st": 82, "ed": 84, "text": "input output"}, {"st": 88, "ed": 90, "text": "hidden layers"}, {"st": 96, "ed": 98, "text": "local optima"}, {"st": 104, "ed": 106, "text": "multi task"}, {"st": 126, "ed": 128, "text": "deep architectures"}, {"st": 130, "ed": 132, "text": "back propagation"}, {"st": 136, "ed": 138, "text": "latent factors"}, {"st": 142, "ed": 144, "text": "empirical results"}, {"st": 145, "ed": 147, "text": "non trivial"}, {"st": 149, "ed": 151, "text": "continual learning"}, {"st": 152, "ed": 154, "text": "deep linear"}, {"st": 161, "ed": 164, "text": "convolutional neural networks"}, {"st": 176, "ed": 178, "text": "domain adaptation"}, {"st": 207, "ed": 209, "text": "object categories"}]
[{"st": 15, "ed": 17, "text": "off policy"}, {"st": 36, "ed": 38, "text": "desired properties"}, {"st": 41, "ed": 43, "text": "low variance"}, {"st": 90, "ed": 92, "text": "off policy"}, {"st": 92, "ed": 94, "text": "policy evaluation"}, {"st": 100, "ed": 102, "text": "sample based"}, {"st": 111, "ed": 113, "text": "off policy"}, {"st": 145, "ed": 147, "text": "open problem"}]
[{"st": 0, "ed": 2, "text": "gibbs sampling"}, {"st": 4, "ed": 8, "text": "markov chain monte carlo"}, {"st": 76, "ed": 79, "text": "a logarithmic factor"}]
[{"st": 0, "ed": 2, "text": "probabilistic models"}, {"st": 73, "ed": 75, "text": "latent variables"}]
[{"st": 6, "ed": 8, "text": "machine learning"}, {"st": 11, "ed": 13, "text": "causal inference"}, {"st": 23, "ed": 25, "text": "individual level"}, {"st": 25, "ed": 27, "text": "causal inference"}, {"st": 28, "ed": 30, "text": "important applications"}, {"st": 38, "ed": 40, "text": "theoretical analysis"}, {"st": 51, "ed": 53, "text": "observational data"}, {"st": 83, "ed": 85, "text": "generalization error"}, {"st": 91, "ed": 93, "text": "estimation error"}, {"st": 104, "ed": 106, "text": "generalization error"}, {"st": 139, "ed": 143, "text": "maximum mean discrepancy mmd"}, {"st": 148, "ed": 150, "text": "simulated data"}]
[{"st": 21, "ed": 23, "text": "neural network"}, {"st": 26, "ed": 28, "text": "parallel computing"}, {"st": 37, "ed": 40, "text": "bayesian optimization algorithm"}, {"st": 53, "ed": 55, "text": "bayes optimal"}, {"st": 68, "ed": 70, "text": "bayes optimal"}, {"st": 85, "ed": 87, "text": "significantly faster"}, {"st": 90, "ed": 92, "text": "bayesian optimization"}, {"st": 104, "ed": 106, "text": "machine learning"}]
[{"st": 11, "ed": 13, "text": "unlabeled data"}, {"st": 19, "ed": 21, "text": "training distribution"}, {"st": 54, "ed": 56, "text": "true distribution"}, {"st": 79, "ed": 82, "text": "method of moments"}, {"st": 109, "ed": 111, "text": "exponential loss"}, {"st": 114, "ed": 116, "text": "structured output"}]
[{"st": 9, "ed": 11, "text": "neural network"}, {"st": 17, "ed": 19, "text": "structural properties"}, {"st": 21, "ed": 23, "text": "neural network"}, {"st": 84, "ed": 86, "text": "grows exponentially"}, {"st": 117, "ed": 119, "text": "batch normalization"}]
[{"st": 0, "ed": 3, "text": "multi label classification"}, {"st": 10, "ed": 12, "text": "multi label"}, {"st": 17, "ed": 19, "text": "problems including"}, {"st": 20, "ed": 22, "text": "large scale"}, {"st": 38, "ed": 40, "text": "training set"}, {"st": 48, "ed": 50, "text": "unlabeled data"}, {"st": 64, "ed": 66, "text": "based methods"}, {"st": 78, "ed": 80, "text": "low dimensional"}, {"st": 87, "ed": 89, "text": "based methods"}, {"st": 91, "ed": 94, "text": "linear dimensionality reduction"}, {"st": 101, "ed": 103, "text": "low dimensional"}, {"st": 126, "ed": 128, "text": "based method"}, {"st": 148, "ed": 150, "text": "proposed method"}, {"st": 159, "ed": 161, "text": "large scale"}, {"st": 175, "ed": 177, "text": "proposed method"}, {"st": 180, "ed": 182, "text": "multi label"}, {"st": 193, "ed": 195, "text": "real world"}, {"st": 199, "ed": 201, "text": "method outperforms"}, {"st": 204, "ed": 206, "text": "multi label"}, {"st": 209, "ed": 211, "text": "large margin"}, {"st": 214, "ed": 216, "text": "prediction performance"}]
[{"st": 1, "ed": 4, "text": "statistical relational learning"}, {"st": 5, "ed": 7, "text": "link prediction"}, {"st": 21, "ed": 23, "text": "previous studies"}, {"st": 38, "ed": 40, "text": "complex valued"}, {"st": 90, "ed": 92, "text": "dot product"}, {"st": 98, "ed": 100, "text": "dot product"}, {"st": 108, "ed": 110, "text": "large datasets"}, {"st": 126, "ed": 128, "text": "link prediction"}]
[{"st": 1, "ed": 3, "text": "logistic regression"}, {"st": 8, "ed": 10, "text": "conditional probability"}, {"st": 17, "ed": 19, "text": "multi relational"}, {"st": 26, "ed": 28, "text": "learning algorithm"}, {"st": 54, "ed": 56, "text": "structure learning"}, {"st": 64, "ed": 66, "text": "parameter learning"}, {"st": 68, "ed": 70, "text": "structure learning"}, {"st": 107, "ed": 109, "text": "parameter learning"}, {"st": 125, "ed": 127, "text": "logistic regression"}, {"st": 127, "ed": 129, "text": "learning algorithm"}, {"st": 132, "ed": 134, "text": "open source"}, {"st": 134, "ed": 136, "text": "machine learning"}, {"st": 163, "ed": 165, "text": "learning algorithm"}, {"st": 169, "ed": 171, "text": "parameter learning"}, {"st": 184, "ed": 186, "text": "logistic regression"}, {"st": 191, "ed": 193, "text": "modified version"}]
[{"st": 15, "ed": 18, "text": "kernel density estimation"}, {"st": 21, "ed": 23, "text": "relative density"}, {"st": 57, "ed": 59, "text": "nearest neighbors"}, {"st": 66, "ed": 69, "text": "k nearest neighbors"}, {"st": 73, "ed": 75, "text": "nearest neighbors"}, {"st": 77, "ed": 79, "text": "nearest neighbors"}, {"st": 87, "ed": 89, "text": "theoretical properties"}, {"st": 98, "ed": 100, "text": "false alarm"}, {"st": 109, "ed": 112, "text": "synthetic and real"}, {"st": 127, "ed": 129, "text": "outlier detection"}]
[{"st": 33, "ed": 35, "text": "user experience"}, {"st": 72, "ed": 74, "text": "highly accurate"}, {"st": 76, "ed": 78, "text": "large scale"}, {"st": 82, "ed": 84, "text": "real world"}, {"st": 96, "ed": 98, "text": "proposed algorithm"}, {"st": 100, "ed": 102, "text": "existing solutions"}, {"st": 111, "ed": 113, "text": "retrieval performance"}, {"st": 123, "ed": 125, "text": "large scale"}]
[{"st": 8, "ed": 10, "text": "missing data"}, {"st": 42, "ed": 44, "text": "deep learning"}, {"st": 45, "ed": 47, "text": "swarm intelligence"}, {"st": 58, "ed": 60, "text": "deep learning"}, {"st": 68, "ed": 70, "text": "input data"}, {"st": 72, "ed": 74, "text": "unsupervised learning"}, {"st": 85, "ed": 87, "text": "deep learning"}, {"st": 95, "ed": 97, "text": "objective function"}, {"st": 99, "ed": 101, "text": "swarm intelligence"}, {"st": 107, "ed": 109, "text": "missing data"}, {"st": 112, "ed": 114, "text": "fine tuning"}, {"st": 118, "ed": 120, "text": "error function"}]
[{"st": 4, "ed": 6, "text": "posterior sampling"}, {"st": 7, "ed": 9, "text": "reinforcement learning"}, {"st": 49, "ed": 51, "text": "regret bound"}, {"st": 54, "ed": 56, "text": "finite horizon"}, {"st": 57, "ed": 60, "text": "markov decision processes"}, {"st": 99, "ed": 101, "text": "reinforcement learning"}]
[{"st": 9, "ed": 11, "text": "reinforcement learning"}, {"st": 23, "ed": 25, "text": "large scale"}, {"st": 37, "ed": 39, "text": "reinforcement learning"}, {"st": 54, "ed": 56, "text": "learning rate"}, {"st": 61, "ed": 63, "text": "meta learning"}, {"st": 79, "ed": 82, "text": "temporal difference learning"}, {"st": 96, "ed": 98, "text": "learning speed"}, {"st": 105, "ed": 107, "text": "learning rate"}, {"st": 111, "ed": 113, "text": "objective function"}, {"st": 114, "ed": 116, "text": "temporal difference"}, {"st": 124, "ed": 126, "text": "fixed point"}, {"st": 143, "ed": 145, "text": "learning rate"}, {"st": 149, "ed": 151, "text": "meta learning"}, {"st": 175, "ed": 177, "text": "off policy"}, {"st": 185, "ed": 187, "text": "objective function"}, {"st": 203, "ed": 205, "text": "linear complexity"}, {"st": 253, "ed": 256, "text": "temporal difference learning"}, {"st": 258, "ed": 260, "text": "real world"}]
[{"st": 2, "ed": 4, "text": "classification problems"}, {"st": 7, "ed": 10, "text": "corrupted by noise"}, {"st": 55, "ed": 57, "text": "resource allocation"}, {"st": 64, "ed": 66, "text": "theoretical bounds"}]
[{"st": 0, "ed": 2, "text": "probabilistic models"}, {"st": 10, "ed": 12, "text": "representation learning"}, {"st": 65, "ed": 67, "text": "probabilistic models"}, {"st": 90, "ed": 92, "text": "feature extractors"}, {"st": 94, "ed": 97, "text": "unsupervised representation learning"}, {"st": 105, "ed": 107, "text": "image datasets"}, {"st": 113, "ed": 116, "text": "sum product networks"}]
[{"st": 6, "ed": 8, "text": "bandit algorithm"}, {"st": 28, "ed": 30, "text": "context dependent"}, {"st": 66, "ed": 68, "text": "regret bounds"}, {"st": 82, "ed": 85, "text": "number of clusters"}, {"st": 103, "ed": 105, "text": "real world"}, {"st": 112, "ed": 114, "text": "prediction performance"}]
[{"st": 42, "ed": 44, "text": "embedding models"}, {"st": 46, "ed": 48, "text": "deep architectures"}, {"st": 54, "ed": 56, "text": "feature mapping"}, {"st": 78, "ed": 80, "text": "large scale"}, {"st": 90, "ed": 92, "text": "feature mapping"}, {"st": 110, "ed": 112, "text": "computational efficiency"}, {"st": 128, "ed": 130, "text": "embedding space"}, {"st": 143, "ed": 145, "text": "significantly outperforms"}, {"st": 151, "ed": 153, "text": "embedding models"}, {"st": 155, "ed": 158, "text": "deep neural networks"}, {"st": 163, "ed": 165, "text": "test error"}, {"st": 170, "ed": 172, "text": "dimensional space"}]
[{"st": 50, "ed": 52, "text": "examples include"}, {"st": 52, "ed": 54, "text": "hierarchical bayesian"}, {"st": 56, "ed": 58, "text": "kernel methods"}, {"st": 59, "ed": 61, "text": "machine learning"}, {"st": 61, "ed": 63, "text": "clustering algorithms"}, {"st": 63, "ed": 65, "text": "dimensionality reduction"}, {"st": 79, "ed": 81, "text": "probabilistic programming"}, {"st": 90, "ed": 92, "text": "modeling language"}, {"st": 127, "ed": 129, "text": "probabilistic programs"}]
[{"st": 0, "ed": 2, "text": "knowledge graph"}, {"st": 7, "ed": 9, "text": "extracting information"}, {"st": 16, "ed": 18, "text": "missing information"}, {"st": 20, "ed": 22, "text": "statistical analysis"}, {"st": 63, "ed": 65, "text": "knowledge graph"}, {"st": 116, "ed": 118, "text": "benchmark datasets"}, {"st": 136, "ed": 138, "text": "missing information"}]
[{"st": 0, "ed": 2, "text": "multi view"}, {"st": 72, "ed": 74, "text": "clustering performance"}, {"st": 90, "ed": 92, "text": "multi view"}, {"st": 92, "ed": 94, "text": "fuzzy clustering"}, {"st": 94, "ed": 96, "text": "approach called"}, {"st": 111, "ed": 113, "text": "consensus clustering"}, {"st": 181, "ed": 184, "text": "multi view data"}, {"st": 186, "ed": 188, "text": "real world"}, {"st": 202, "ed": 204, "text": "multi view"}, {"st": 204, "ed": 206, "text": "clustering approaches"}, {"st": 209, "ed": 211, "text": "clustering accuracy"}, {"st": 213, "ed": 215, "text": "great potential"}, {"st": 218, "ed": 221, "text": "multi view data"}]
[{"st": 3, "ed": 5, "text": "loss function"}, {"st": 6, "ed": 8, "text": "neural networks"}, {"st": 15, "ed": 17, "text": "non trivial"}, {"st": 72, "ed": 74, "text": "raw data"}, {"st": 76, "ed": 78, "text": "higher level"}, {"st": 88, "ed": 90, "text": "time series"}, {"st": 116, "ed": 118, "text": "prediction error"}, {"st": 160, "ed": 162, "text": "coarse grained"}, {"st": 165, "ed": 167, "text": "time series"}, {"st": 182, "ed": 184, "text": "time series"}, {"st": 195, "ed": 197, "text": "semi supervised"}, {"st": 208, "ed": 210, "text": "time series"}, {"st": 216, "ed": 218, "text": "higher level"}]
[{"st": 6, "ed": 8, "text": "powerful tool"}, {"st": 21, "ed": 24, "text": "exploration and exploitation"}, {"st": 27, "ed": 29, "text": "optimal solution"}]
[{"st": 3, "ed": 5, "text": "machine learning"}, {"st": 14, "ed": 16, "text": "prior information"}, {"st": 36, "ed": 38, "text": "reinforcement learning"}, {"st": 59, "ed": 61, "text": "exploration exploitation"}, {"st": 77, "ed": 79, "text": "prior knowledge"}, {"st": 103, "ed": 105, "text": "recent literature"}, {"st": 113, "ed": 115, "text": "prior information"}, {"st": 162, "ed": 165, "text": "theoretical and empirical"}]
[{"st": 1, "ed": 4, "text": "gaussian mixture model"}, {"st": 23, "ed": 25, "text": "big data"}, {"st": 30, "ed": 32, "text": "parameter estimation"}, {"st": 36, "ed": 38, "text": "streaming data"}, {"st": 50, "ed": 54, "text": "expectation maximization em algorithm"}, {"st": 124, "ed": 126, "text": "moment matching"}, {"st": 127, "ed": 130, "text": "compares favorably to"}]
[{"st": 0, "ed": 2, "text": "mixture models"}, {"st": 3, "ed": 5, "text": "topic models"}, {"st": 37, "ed": 39, "text": "total number"}, {"st": 112, "ed": 114, "text": "optimization algorithms"}, {"st": 122, "ed": 124, "text": "mixture models"}, {"st": 125, "ed": 127, "text": "image patches"}, {"st": 128, "ed": 130, "text": "topic models"}, {"st": 136, "ed": 138, "text": "approach produces"}]
[{"st": 10, "ed": 12, "text": "generative models"}, {"st": 24, "ed": 26, "text": "functional form"}, {"st": 36, "ed": 38, "text": "efficiently learn"}, {"st": 48, "ed": 50, "text": "models including"}, {"st": 64, "ed": 66, "text": "learning algorithm"}, {"st": 66, "ed": 68, "text": "simultaneously learns"}, {"st": 70, "ed": 72, "text": "inference network"}, {"st": 74, "ed": 76, "text": "generative model"}, {"st": 79, "ed": 81, "text": "variational approximation"}, {"st": 83, "ed": 86, "text": "recurrent neural networks"}, {"st": 94, "ed": 96, "text": "learning algorithm"}, {"st": 98, "ed": 103, "text": "synthetic and real world datasets"}, {"st": 122, "ed": 124, "text": "significantly higher"}]
[{"st": 6, "ed": 8, "text": "global optimization"}, {"st": 13, "ed": 15, "text": "sample efficiency"}, {"st": 35, "ed": 37, "text": "machine learning"}, {"st": 54, "ed": 56, "text": "autonomous systems"}, {"st": 58, "ed": 60, "text": "automatic control"}, {"st": 70, "ed": 72, "text": "global optimization"}, {"st": 85, "ed": 87, "text": "surrogate model"}, {"st": 88, "ed": 90, "text": "gaussian process"}, {"st": 130, "ed": 132, "text": "gaussian process"}, {"st": 138, "ed": 140, "text": "typically requires"}, {"st": 140, "ed": 142, "text": "prior knowledge"}, {"st": 164, "ed": 166, "text": "originally designed"}, {"st": 178, "ed": 180, "text": "surrogate model"}, {"st": 188, "ed": 190, "text": "surrogate model"}, {"st": 200, "ed": 202, "text": "kernel function"}, {"st": 213, "ed": 215, "text": "surrogate model"}, {"st": 233, "ed": 235, "text": "local search"}, {"st": 255, "ed": 257, "text": "method outperforms"}]
[{"st": 7, "ed": 9, "text": "cluster ensemble"}, {"st": 17, "ed": 19, "text": "recent years"}, {"st": 35, "ed": 37, "text": "clustering results"}, {"st": 39, "ed": 41, "text": "cluster ensemble"}, {"st": 47, "ed": 49, "text": "final results"}, {"st": 50, "ed": 52, "text": "cluster ensemble"}, {"st": 126, "ed": 128, "text": "modeling language"}, {"st": 132, "ed": 134, "text": "clustering algorithms"}, {"st": 155, "ed": 157, "text": "similarity metric"}, {"st": 182, "ed": 184, "text": "proposed framework"}, {"st": 188, "ed": 190, "text": "final results"}, {"st": 195, "ed": 197, "text": "cluster ensemble"}]
[{"st": 11, "ed": 13, "text": "prediction error"}, {"st": 35, "ed": 37, "text": "causal structure"}, {"st": 40, "ed": 42, "text": "generation process"}, {"st": 46, "ed": 48, "text": "machine learning"}, {"st": 51, "ed": 53, "text": "additive noise"}, {"st": 87, "ed": 89, "text": "expected error"}, {"st": 93, "ed": 95, "text": "generating function"}, {"st": 96, "ed": 98, "text": "prediction model"}, {"st": 140, "ed": 142, "text": "empirical evaluations"}]
[{"st": 0, "ed": 2, "text": "autonomous driving"}, {"st": 4, "ed": 6, "text": "multi agent"}, {"st": 80, "ed": 82, "text": "traffic flow"}, {"st": 89, "ed": 92, "text": "deep reinforcement learning"}, {"st": 111, "ed": 113, "text": "autonomous driving"}, {"st": 128, "ed": 130, "text": "machine learning"}, {"st": 149, "ed": 152, "text": "markov decision process"}, {"st": 171, "ed": 173, "text": "multi agent"}, {"st": 185, "ed": 187, "text": "policy gradient"}, {"st": 258, "ed": 260, "text": "significantly reduces"}]
[{"st": 4, "ed": 6, "text": "variational bayesian"}, {"st": 20, "ed": 22, "text": "approach achieves"}, {"st": 22, "ed": 24, "text": "faster convergence"}, {"st": 24, "ed": 26, "text": "per iteration"}, {"st": 31, "ed": 33, "text": "gibbs sampling"}, {"st": 61, "ed": 63, "text": "variational bayesian"}]
[{"st": 12, "ed": 14, "text": "jointly learn"}, {"st": 15, "ed": 17, "text": "generative model"}, {"st": 35, "ed": 37, "text": "latent distribution"}, {"st": 56, "ed": 58, "text": "latent distribution"}, {"st": 69, "ed": 73, "text": "markov chain monte carlo"}, {"st": 94, "ed": 96, "text": "generative model"}, {"st": 102, "ed": 104, "text": "latent distribution"}, {"st": 121, "ed": 123, "text": "generative model"}, {"st": 127, "ed": 129, "text": "latent distribution"}, {"st": 142, "ed": 144, "text": "previously unseen"}]
[{"st": 6, "ed": 9, "text": "deep neural networks"}, {"st": 23, "ed": 25, "text": "probabilistic programming"}, {"st": 34, "ed": 36, "text": "probabilistic programming"}, {"st": 37, "ed": 39, "text": "deep learning"}, {"st": 57, "ed": 59, "text": "inference problem"}, {"st": 64, "ed": 66, "text": "probabilistic program"}, {"st": 70, "ed": 72, "text": "programming language"}, {"st": 74, "ed": 77, "text": "trained neural network"}, {"st": 80, "ed": 82, "text": "neural network"}, {"st": 89, "ed": 91, "text": "neural network"}, {"st": 93, "ed": 95, "text": "observational data"}, {"st": 99, "ed": 101, "text": "approximate inference"}, {"st": 111, "ed": 113, "text": "training objective"}, {"st": 121, "ed": 124, "text": "trained neural network"}, {"st": 129, "ed": 131, "text": "proposal distribution"}, {"st": 134, "ed": 136, "text": "importance sampling"}, {"st": 143, "ed": 145, "text": "mixture models"}, {"st": 150, "ed": 152, "text": "significant speedups"}]
[{"st": 3, "ed": 5, "text": "generative models"}, {"st": 43, "ed": 47, "text": "generative adversarial networks gans"}, {"st": 89, "ed": 91, "text": "cross product"}, {"st": 102, "ed": 104, "text": "log likelihood"}, {"st": 109, "ed": 111, "text": "conditional distributions"}]
[{"st": 7, "ed": 9, "text": "missing values"}, {"st": 41, "ed": 43, "text": "probabilistic programming"}, {"st": 64, "ed": 66, "text": "joint probability"}, {"st": 81, "ed": 83, "text": "mutual information"}, {"st": 103, "ed": 105, "text": "probabilistic programming"}, {"st": 129, "ed": 131, "text": "context specific"}, {"st": 149, "ed": 151, "text": "machine learning"}, {"st": 153, "ed": 155, "text": "real world"}]
[{"st": 4, "ed": 8, "text": "problems in machine learning"}, {"st": 9, "ed": 11, "text": "data mining"}, {"st": 15, "ed": 17, "text": "feature selection"}, {"st": 62, "ed": 64, "text": "significantly improves"}, {"st": 68, "ed": 70, "text": "feature selection"}]
[{"st": 9, "ed": 11, "text": "generative model"}, {"st": 19, "ed": 21, "text": "building blocks"}, {"st": 28, "ed": 30, "text": "building blocks"}, {"st": 67, "ed": 70, "text": "inference and learning"}, {"st": 77, "ed": 79, "text": "variational approximations"}, {"st": 84, "ed": 86, "text": "main contribution"}, {"st": 100, "ed": 102, "text": "message passing"}, {"st": 116, "ed": 118, "text": "inference engine"}, {"st": 155, "ed": 157, "text": "fast inference"}, {"st": 163, "ed": 165, "text": "functional form"}, {"st": 167, "ed": 171, "text": "convolutional neural network cnn"}]
[{"st": 10, "ed": 12, "text": "optimization problem"}, {"st": 40, "ed": 42, "text": "worst case"}, {"st": 59, "ed": 61, "text": "loss minimization"}]
[{"st": 0, "ed": 2, "text": "continuous optimization"}, {"st": 14, "ed": 16, "text": "probabilistic inference"}, {"st": 21, "ed": 23, "text": "real world"}, {"st": 23, "ed": 25, "text": "optimization problems"}, {"st": 34, "ed": 36, "text": "local optima"}, {"st": 56, "ed": 58, "text": "objective function"}, {"st": 94, "ed": 96, "text": "probabilistic inference"}, {"st": 109, "ed": 111, "text": "objective function"}, {"st": 156, "ed": 158, "text": "nonconvex optimization"}]
[{"st": 0, "ed": 2, "text": "importance sampling"}, {"st": 6, "ed": 8, "text": "machine learning"}, {"st": 9, "ed": 12, "text": "training and testing"}, {"st": 26, "ed": 28, "text": "importance sampling"}, {"st": 34, "ed": 36, "text": "importance sampling"}, {"st": 39, "ed": 42, "text": "orders of magnitude"}, {"st": 47, "ed": 50, "text": "training and testing"}, {"st": 58, "ed": 60, "text": "importance sampling"}, {"st": 65, "ed": 67, "text": "theoretical analysis"}, {"st": 71, "ed": 74, "text": "bias and variance"}, {"st": 78, "ed": 80, "text": "importance sampling"}, {"st": 89, "ed": 91, "text": "importance sampling"}, {"st": 99, "ed": 102, "text": "and vice versa"}, {"st": 112, "ed": 114, "text": "importance sampling"}]
[{"st": 1, "ed": 3, "text": "recent years"}, {"st": 3, "ed": 7, "text": "deep reinforcement learning rl"}, {"st": 16, "ed": 18, "text": "challenging task"}, {"st": 30, "ed": 33, "text": "massive amounts of"}, {"st": 43, "ed": 45, "text": "deep rl"}, {"st": 79, "ed": 81, "text": "recurrent networks"}, {"st": 83, "ed": 85, "text": "meta learning"}, {"st": 87, "ed": 89, "text": "fully supervised"}, {"st": 159, "ed": 162, "text": "proof of concept"}]
[{"st": 0, "ed": 2, "text": "gaussian processes"}, {"st": 15, "ed": 17, "text": "gaussian processes"}, {"st": 21, "ed": 23, "text": "classification regression"}, {"st": 33, "ed": 35, "text": "classification problem"}, {"st": 59, "ed": 61, "text": "big data"}, {"st": 91, "ed": 93, "text": "variational inference"}, {"st": 100, "ed": 102, "text": "marginal likelihood"}, {"st": 112, "ed": 114, "text": "kernel function"}, {"st": 116, "ed": 118, "text": "gaussian process"}, {"st": 125, "ed": 127, "text": "computational complexity"}, {"st": 164, "ed": 167, "text": "evidence lower bound"}, {"st": 169, "ed": 171, "text": "classification problem"}, {"st": 183, "ed": 185, "text": "big data"}, {"st": 219, "ed": 221, "text": "gp models"}, {"st": 235, "ed": 238, "text": "evidence lower bound"}, {"st": 278, "ed": 280, "text": "doesn t"}, {"st": 287, "ed": 289, "text": "learning rate"}]
[{"st": 15, "ed": 17, "text": "based approach"}]
[{"st": 1, "ed": 3, "text": "prediction problems"}, {"st": 40, "ed": 42, "text": "machine learning"}, {"st": 60, "ed": 62, "text": "local neighborhood"}, {"st": 108, "ed": 110, "text": "multi level"}, {"st": 114, "ed": 117, "text": "short term memory"}, {"st": 117, "ed": 119, "text": "neural nets"}, {"st": 144, "ed": 147, "text": "real world data"}]
[{"st": 0, "ed": 2, "text": "survival analysis"}, {"st": 29, "ed": 31, "text": "patient data"}, {"st": 46, "ed": 49, "text": "support vector machine"}, {"st": 56, "ed": 58, "text": "objective function"}, {"st": 68, "ed": 70, "text": "significantly lower"}, {"st": 70, "ed": 72, "text": "computational costs"}, {"st": 111, "ed": 113, "text": "larger scale"}, {"st": 124, "ed": 126, "text": "real world"}, {"st": 131, "ed": 133, "text": "outperforms existing"}, {"st": 146, "ed": 148, "text": "performs comparably"}]
[{"st": 9, "ed": 11, "text": "machine learning"}, {"st": 16, "ed": 18, "text": "complex models"}, {"st": 42, "ed": 44, "text": "linear models"}, {"st": 44, "ed": 46, "text": "decision trees"}, {"st": 117, "ed": 119, "text": "simulated annealing"}]
[{"st": 2, "ed": 4, "text": "multi task"}, {"st": 4, "ed": 6, "text": "reinforcement learning"}, {"st": 28, "ed": 30, "text": "thompson sampling"}, {"st": 44, "ed": 46, "text": "multi task"}, {"st": 56, "ed": 58, "text": "multi task"}, {"st": 58, "ed": 60, "text": "reinforcement learning"}, {"st": 70, "ed": 72, "text": "low dimensional"}]
[{"st": 7, "ed": 9, "text": "combinatorial optimization"}, {"st": 11, "ed": 13, "text": "neural networks"}, {"st": 27, "ed": 29, "text": "recurrent network"}, {"st": 49, "ed": 51, "text": "reward signal"}, {"st": 57, "ed": 59, "text": "recurrent network"}, {"st": 61, "ed": 63, "text": "policy gradient"}, {"st": 68, "ed": 70, "text": "network parameters"}, {"st": 115, "ed": 117, "text": "np hard"}, {"st": 122, "ed": 124, "text": "optimal solutions"}]
[{"st": 4, "ed": 6, "text": "method called"}, {"st": 11, "ed": 13, "text": "low dimensional"}, {"st": 52, "ed": 54, "text": "recent advances"}, {"st": 91, "ed": 93, "text": "method produces"}, {"st": 97, "ed": 99, "text": "t sne"}]
[{"st": 3, "ed": 5, "text": "causal inference"}, {"st": 10, "ed": 12, "text": "causal discovery"}, {"st": 13, "ed": 15, "text": "multiple datasets"}, {"st": 18, "ed": 20, "text": "jointly learn"}, {"st": 22, "ed": 24, "text": "causal structure"}, {"st": 37, "ed": 39, "text": "constraint based"}, {"st": 41, "ed": 43, "text": "causal discovery"}, {"st": 78, "ed": 80, "text": "statistical power"}, {"st": 141, "ed": 143, "text": "causal inference"}, {"st": 149, "ed": 151, "text": "recently proposed"}, {"st": 153, "ed": 155, "text": "causal discovery"}]
[{"st": 14, "ed": 16, "text": "missing data"}, {"st": 37, "ed": 40, "text": "stochastic variational inference"}, {"st": 58, "ed": 60, "text": "mixture model"}, {"st": 60, "ed": 62, "text": "linear regression"}, {"st": 63, "ed": 65, "text": "factor analysis"}, {"st": 70, "ed": 72, "text": "missing data"}, {"st": 94, "ed": 96, "text": "missing data"}, {"st": 98, "ed": 100, "text": "large scale"}, {"st": 104, "ed": 106, "text": "missing data"}, {"st": 110, "ed": 112, "text": "explicitly modeling"}, {"st": 113, "ed": 115, "text": "missing data"}, {"st": 116, "ed": 118, "text": "substantially improves"}, {"st": 127, "ed": 129, "text": "log likelihood"}]
[{"st": 7, "ed": 9, "text": "rank aggregation"}, {"st": 12, "ed": 14, "text": "distance measures"}, {"st": 25, "ed": 27, "text": "np hard"}, {"st": 35, "ed": 37, "text": "approximation algorithms"}]
[{"st": 4, "ed": 7, "text": "canonical correlation analysis"}, {"st": 37, "ed": 39, "text": "representation learning"}, {"st": 49, "ed": 51, "text": "representation learning"}, {"st": 63, "ed": 65, "text": "representation learning"}, {"st": 73, "ed": 75, "text": "representation learning"}, {"st": 77, "ed": 79, "text": "statistical power"}, {"st": 98, "ed": 100, "text": "stochastic optimization"}, {"st": 114, "ed": 116, "text": "downstream tasks"}, {"st": 116, "ed": 118, "text": "phonetic transcription"}, {"st": 141, "ed": 143, "text": "existing methods"}, {"st": 144, "ed": 146, "text": "phonetic transcription"}]
[{"st": 40, "ed": 42, "text": "clustering methods"}, {"st": 59, "ed": 61, "text": "adjacency matrix"}, {"st": 174, "ed": 176, "text": "cluster analysis"}, {"st": 203, "ed": 206, "text": "real world data"}]
[{"st": 51, "ed": 53, "text": "worst case"}, {"st": 92, "ed": 94, "text": "open problem"}]
[{"st": 1, "ed": 5, "text": "multi layer neural networks"}, {"st": 6, "ed": 8, "text": "dot product"}, {"st": 18, "ed": 20, "text": "weight vector"}, {"st": 29, "ed": 31, "text": "dot product"}, {"st": 63, "ed": 65, "text": "covariate shift"}, {"st": 72, "ed": 74, "text": "dot product"}, {"st": 82, "ed": 84, "text": "cosine similarity"}, {"st": 86, "ed": 88, "text": "cosine similarity"}, {"st": 89, "ed": 91, "text": "correlation coefficient"}, {"st": 93, "ed": 95, "text": "dot product"}, {"st": 96, "ed": 98, "text": "neural networks"}, {"st": 114, "ed": 116, "text": "fully connected"}, {"st": 116, "ed": 118, "text": "neural networks"}, {"st": 121, "ed": 123, "text": "convolutional networks"}, {"st": 131, "ed": 133, "text": "cifar 10"}]
[{"st": 14, "ed": 16, "text": "generative models"}, {"st": 28, "ed": 30, "text": "algorithmic framework"}, {"st": 34, "ed": 36, "text": "base learner"}, {"st": 53, "ed": 55, "text": "discriminative models"}, {"st": 58, "ed": 60, "text": "real data"}, {"st": 82, "ed": 84, "text": "empirically demonstrate"}, {"st": 90, "ed": 92, "text": "boosting algorithms"}, {"st": 100, "ed": 102, "text": "benchmark datasets"}]
[{"st": 0, "ed": 2, "text": "machine learning"}, {"st": 42, "ed": 45, "text": "deep reinforcement learning"}, {"st": 75, "ed": 78, "text": "takes advantage of"}, {"st": 79, "ed": 82, "text": "deep neural network"}, {"st": 95, "ed": 97, "text": "training data"}, {"st": 105, "ed": 107, "text": "convergence speed"}, {"st": 112, "ed": 114, "text": "previous studies"}, {"st": 136, "ed": 138, "text": "machine learning"}, {"st": 140, "ed": 142, "text": "neural network"}, {"st": 144, "ed": 147, "text": "stochastic gradient descent"}, {"st": 157, "ed": 159, "text": "neural network"}, {"st": 161, "ed": 164, "text": "multi layer perceptron"}, {"st": 165, "ed": 168, "text": "convolutional neural networks"}, {"st": 169, "ed": 172, "text": "recurrent neural networks"}, {"st": 176, "ed": 178, "text": "image classification"}, {"st": 187, "ed": 189, "text": "achieve comparable"}]
[{"st": 10, "ed": 12, "text": "reinforcement learning"}, {"st": 72, "ed": 74, "text": "multi step"}, {"st": 81, "ed": 83, "text": "off policy"}, {"st": 142, "ed": 144, "text": "experimental evaluation"}, {"st": 147, "ed": 149, "text": "significantly outperforms"}]
[{"st": 0, "ed": 2, "text": "contextual bandits"}, {"st": 17, "ed": 20, "text": "generalized linear models"}, {"st": 29, "ed": 31, "text": "linear models"}, {"st": 40, "ed": 42, "text": "theoretical analyses"}, {"st": 43, "ed": 45, "text": "contextual bandits"}, {"st": 57, "ed": 60, "text": "upper confidence bound"}, {"st": 63, "ed": 65, "text": "generalized linear"}, {"st": 65, "ed": 67, "text": "contextual bandits"}, {"st": 70, "ed": 72, "text": "tilde o"}, {"st": 79, "ed": 81, "text": "d dimensional"}, {"st": 108, "ed": 111, "text": "number of arms"}, {"st": 125, "ed": 127, "text": "finite sample"}, {"st": 127, "ed": 129, "text": "confidence bound"}, {"st": 130, "ed": 132, "text": "maximum likelihood"}, {"st": 134, "ed": 137, "text": "generalized linear models"}, {"st": 148, "ed": 151, "text": "upper confidence bound"}, {"st": 162, "ed": 164, "text": "optimal regret"}]
[{"st": 1, "ed": 3, "text": "gated recurrent"}, {"st": 3, "ed": 5, "text": "neural network"}, {"st": 15, "ed": 17, "text": "highly effective"}, {"st": 30, "ed": 32, "text": "recurrent unit"}, {"st": 114, "ed": 118, "text": "synthetic and real world"}]
[{"st": 5, "ed": 7, "text": "recently proposed"}, {"st": 10, "ed": 12, "text": "optimization algorithms"}, {"st": 22, "ed": 24, "text": "optimization algorithm"}, {"st": 32, "ed": 34, "text": "stochastic optimization"}, {"st": 40, "ed": 42, "text": "reinforcement learning"}, {"st": 52, "ed": 54, "text": "optimization algorithms"}, {"st": 62, "ed": 64, "text": "optimization algorithm"}, {"st": 64, "ed": 66, "text": "consistently outperforms"}, {"st": 68, "ed": 70, "text": "optimization algorithms"}, {"st": 85, "ed": 87, "text": "neural net"}, {"st": 94, "ed": 96, "text": "optimization algorithm"}, {"st": 99, "ed": 101, "text": "proposed method"}, {"st": 107, "ed": 109, "text": "neural net"}, {"st": 117, "ed": 119, "text": "neural nets"}]
[{"st": 2, "ed": 4, "text": "reinforcement learning"}, {"st": 60, "ed": 62, "text": "highly complex"}, {"st": 221, "ed": 223, "text": "transition function"}, {"st": 233, "ed": 235, "text": "transition function"}, {"st": 275, "ed": 277, "text": "space complexity"}]
[{"st": 3, "ed": 7, "text": "multi armed bandit problems"}, {"st": 50, "ed": 52, "text": "regret bound"}, {"st": 58, "ed": 60, "text": "d dimensional"}, {"st": 92, "ed": 94, "text": "feature vectors"}, {"st": 98, "ed": 100, "text": "greedy algorithm"}, {"st": 109, "ed": 111, "text": "regret bound"}, {"st": 120, "ed": 122, "text": "theoretical understanding"}]
[{"st": 0, "ed": 2, "text": "neural networks"}, {"st": 7, "ed": 9, "text": "supervised learning"}, {"st": 22, "ed": 24, "text": "critical applications"}, {"st": 123, "ed": 125, "text": "unsupervised fashion"}, {"st": 132, "ed": 134, "text": "decision boundaries"}, {"st": 140, "ed": 142, "text": "multiple datasets"}]
[{"st": 6, "ed": 8, "text": "global optimization"}, {"st": 18, "ed": 20, "text": "optimization methods"}, {"st": 45, "ed": 47, "text": "objective function"}, {"st": 58, "ed": 61, "text": "bayesian optimization algorithm"}, {"st": 112, "ed": 114, "text": "computational cost"}, {"st": 131, "ed": 133, "text": "acquisition function"}, {"st": 168, "ed": 170, "text": "logistic regression"}, {"st": 170, "ed": 172, "text": "deep learning"}, {"st": 172, "ed": 174, "text": "kernel learning"}]
[{"st": 22, "ed": 24, "text": "robust statistics"}, {"st": 32, "ed": 34, "text": "learning algorithm"}, {"st": 38, "ed": 40, "text": "training data"}, {"st": 42, "ed": 44, "text": "training points"}, {"st": 56, "ed": 59, "text": "modern machine learning"}, {"st": 64, "ed": 66, "text": "efficient implementation"}, {"st": 103, "ed": 105, "text": "linear models"}, {"st": 106, "ed": 109, "text": "convolutional neural networks"}, {"st": 132, "ed": 134, "text": "training set"}]
[{"st": 9, "ed": 11, "text": "reinforcement learning"}, {"st": 12, "ed": 14, "text": "finite horizon"}, {"st": 22, "ed": 24, "text": "value iteration"}, {"st": 26, "ed": 28, "text": "regret bound"}, {"st": 29, "ed": 31, "text": "tilde o"}, {"st": 37, "ed": 39, "text": "sqrt t"}, {"st": 111, "ed": 113, "text": "tilde o"}]
[{"st": 38, "ed": 40, "text": "existing algorithms"}, {"st": 85, "ed": 87, "text": "sampling based"}, {"st": 87, "ed": 89, "text": "online learning"}, {"st": 131, "ed": 133, "text": "near optimal"}, {"st": 157, "ed": 159, "text": "posterior sampling"}, {"st": 182, "ed": 184, "text": "real world"}]
[{"st": 24, "ed": 26, "text": "computationally efficient"}, {"st": 38, "ed": 40, "text": "reinforcement learning"}, {"st": 57, "ed": 59, "text": "regret bound"}, {"st": 61, "ed": 63, "text": "statistical efficiency"}]
[{"st": 8, "ed": 10, "text": "sample efficient"}, {"st": 10, "ed": 12, "text": "learning algorithms"}, {"st": 120, "ed": 122, "text": "convex geometry"}, {"st": 170, "ed": 172, "text": "margin based"}, {"st": 172, "ed": 174, "text": "active learning"}, {"st": 176, "ed": 178, "text": "active learning"}, {"st": 188, "ed": 190, "text": "geometric properties"}]
[{"st": 2, "ed": 4, "text": "reinforcement learning"}, {"st": 60, "ed": 62, "text": "highly complex"}, {"st": 181, "ed": 183, "text": "transition function"}, {"st": 193, "ed": 195, "text": "transition function"}, {"st": 207, "ed": 209, "text": "scoring function"}, {"st": 238, "ed": 240, "text": "space complexity"}, {"st": 252, "ed": 254, "text": "empirical results"}]
[{"st": 6, "ed": 8, "text": "neural networks"}, {"st": 19, "ed": 21, "text": "generalization capability"}, {"st": 22, "ed": 25, "text": "deep neural networks"}, {"st": 88, "ed": 90, "text": "training process"}, {"st": 142, "ed": 144, "text": "improved performance"}, {"st": 146, "ed": 148, "text": "significantly smaller"}]
[{"st": 0, "ed": 3, "text": "inverse reinforcement learning"}, {"st": 12, "ed": 14, "text": "reinforcement learning"}, {"st": 43, "ed": 45, "text": "real world"}, {"st": 147, "ed": 150, "text": "the paper presents"}, {"st": 152, "ed": 154, "text": "approximate inference"}, {"st": 158, "ed": 160, "text": "posterior inference"}, {"st": 192, "ed": 194, "text": "cognitive science"}]
[{"st": 3, "ed": 5, "text": "machine learning"}, {"st": 51, "ed": 53, "text": "machine learning"}, {"st": 59, "ed": 61, "text": "machine learning"}, {"st": 112, "ed": 114, "text": "machine learning"}]
[{"st": 0, "ed": 2, "text": "decision makers"}, {"st": 37, "ed": 39, "text": "supervised learning"}, {"st": 42, "ed": 44, "text": "predictive models"}, {"st": 46, "ed": 48, "text": "decision makers"}, {"st": 70, "ed": 72, "text": "supervised learning"}, {"st": 85, "ed": 87, "text": "training data"}, {"st": 103, "ed": 105, "text": "learning objective"}, {"st": 123, "ed": 125, "text": "decision making"}, {"st": 132, "ed": 134, "text": "gaussian process"}, {"st": 160, "ed": 162, "text": "decision support"}]
[{"st": 4, "ed": 6, "text": "real world"}, {"st": 24, "ed": 26, "text": "concept drift"}, {"st": 29, "ed": 31, "text": "predictive performance"}, {"st": 74, "ed": 76, "text": "labeled data"}, {"st": 100, "ed": 102, "text": "time consuming"}, {"st": 108, "ed": 110, "text": "change detection"}, {"st": 221, "ed": 223, "text": "significantly fewer"}, {"st": 223, "ed": 225, "text": "false alarms"}, {"st": 228, "ed": 230, "text": "feature based"}, {"st": 234, "ed": 236, "text": "false alarms"}]
[{"st": 0, "ed": 3, "text": "deep generative models"}, {"st": 8, "ed": 10, "text": "unlabelled data"}, {"st": 22, "ed": 25, "text": "real life data"}, {"st": 31, "ed": 33, "text": "labelled data"}, {"st": 47, "ed": 49, "text": "generative model"}, {"st": 56, "ed": 58, "text": "latent representation"}, {"st": 68, "ed": 70, "text": "labelled data"}, {"st": 81, "ed": 83, "text": "significantly improve"}, {"st": 90, "ed": 92, "text": "log likelihood"}, {"st": 103, "ed": 106, "text": "semi supervised classification"}, {"st": 119, "ed": 121, "text": "log likelihood"}]
[{"st": 52, "ed": 54, "text": "local planning"}, {"st": 97, "ed": 99, "text": "approach called"}, {"st": 111, "ed": 113, "text": "theoretical findings"}]
[{"st": 6, "ed": 8, "text": "observational data"}, {"st": 59, "ed": 61, "text": "dynamical systems"}, {"st": 78, "ed": 80, "text": "multivariate data"}, {"st": 81, "ed": 83, "text": "proposed method"}, {"st": 116, "ed": 118, "text": "multiple output"}, {"st": 129, "ed": 131, "text": "significant gains"}]
[{"st": 1, "ed": 4, "text": "reinforcement learning agents"}, {"st": 38, "ed": 40, "text": "learning process"}, {"st": 63, "ed": 65, "text": "multi agent"}, {"st": 119, "ed": 121, "text": "learning problems"}, {"st": 141, "ed": 143, "text": "sufficient conditions"}, {"st": 145, "ed": 147, "text": "learning algorithm"}]
[{"st": 0, "ed": 3, "text": "multi armed bandits"}, {"st": 6, "ed": 8, "text": "machine learning"}, {"st": 25, "ed": 28, "text": "strong theoretical guarantees"}, {"st": 35, "ed": 37, "text": "near optimal"}, {"st": 49, "ed": 52, "text": "multi armed bandits"}, {"st": 71, "ed": 73, "text": "linearly separable"}, {"st": 99, "ed": 101, "text": "performance guarantee"}]
[{"st": 15, "ed": 17, "text": "time series"}, {"st": 55, "ed": 57, "text": "recurrent network"}, {"st": 69, "ed": 71, "text": "extensive empirical"}, {"st": 74, "ed": 76, "text": "real world"}]
[{"st": 81, "ed": 83, "text": "latent variables"}, {"st": 87, "ed": 89, "text": "baseline methods"}, {"st": 115, "ed": 117, "text": "latent variable"}]
[{"st": 6, "ed": 8, "text": "cross modal"}]
[{"st": 46, "ed": 48, "text": "multi view"}, {"st": 48, "ed": 50, "text": "deep generative"}, {"st": 54, "ed": 56, "text": "generative process"}, {"st": 64, "ed": 67, "text": "mixture of gaussians"}, {"st": 74, "ed": 76, "text": "latent variables"}, {"st": 90, "ed": 92, "text": "labeled data"}, {"st": 98, "ed": 100, "text": "multi view"}, {"st": 102, "ed": 105, "text": "semi supervised learning"}, {"st": 109, "ed": 112, "text": "semi supervised classification"}, {"st": 116, "ed": 118, "text": "missing data"}, {"st": 121, "ed": 123, "text": "semi supervised"}, {"st": 123, "ed": 125, "text": "multi view"}, {"st": 125, "ed": 127, "text": "deep generative"}, {"st": 131, "ed": 134, "text": "labeled and unlabeled"}, {"st": 136, "ed": 138, "text": "multiple modalities"}, {"st": 163, "ed": 165, "text": "experiments conducted"}, {"st": 168, "ed": 170, "text": "multi modal"}]
[{"st": 4, "ed": 6, "text": "challenging tasks"}, {"st": 27, "ed": 29, "text": "search space"}, {"st": 40, "ed": 42, "text": "np hard"}, {"st": 56, "ed": 58, "text": "quantitative analysis"}, {"st": 99, "ed": 101, "text": "simulated data"}, {"st": 105, "ed": 108, "text": "discrete and continuous"}]
[{"st": 4, "ed": 6, "text": "reinforcement learning"}, {"st": 8, "ed": 12, "text": "partially observable markov decision"}, {"st": 20, "ed": 22, "text": "spectral methods"}, {"st": 31, "ed": 33, "text": "latent variable"}, {"st": 36, "ed": 39, "text": "hidden markov models"}, {"st": 62, "ed": 64, "text": "learning algorithm"}, {"st": 105, "ed": 107, "text": "expected reward"}, {"st": 117, "ed": 119, "text": "optimal regret"}]
[{"st": 37, "ed": 39, "text": "sample size"}, {"st": 42, "ed": 44, "text": "expert knowledge"}, {"st": 51, "ed": 53, "text": "prediction model"}, {"st": 74, "ed": 76, "text": "probabilistic model"}, {"st": 115, "ed": 118, "text": "multi armed bandit"}, {"st": 121, "ed": 123, "text": "multiple myeloma"}, {"st": 129, "ed": 131, "text": "expert knowledge"}, {"st": 133, "ed": 135, "text": "prediction error"}]
[{"st": 7, "ed": 10, "text": "multi armed bandit"}, {"st": 15, "ed": 17, "text": "contextual bandit"}, {"st": 47, "ed": 49, "text": "clinical trials"}, {"st": 49, "ed": 51, "text": "recommender systems"}, {"st": 59, "ed": 62, "text": "multi armed bandit"}, {"st": 65, "ed": 67, "text": "thompson sampling"}, {"st": 82, "ed": 84, "text": "thompson sampling"}, {"st": 91, "ed": 93, "text": "thompson sampling"}, {"st": 105, "ed": 107, "text": "empirical results"}, {"st": 115, "ed": 117, "text": "real life"}]
[{"st": 0, "ed": 2, "text": "existing methods"}, {"st": 4, "ed": 6, "text": "blood pressure"}, {"st": 19, "ed": 21, "text": "explicitly modeling"}, {"st": 23, "ed": 25, "text": "temporal dependencies"}, {"st": 76, "ed": 80, "text": "deep recurrent neural network"}, {"st": 84, "ed": 89, "text": "long short term memory lstm"}, {"st": 100, "ed": 102, "text": "larger scale"}, {"st": 102, "ed": 104, "text": "context information"}, {"st": 105, "ed": 107, "text": "input sequence"}, {"st": 109, "ed": 111, "text": "residual connections"}, {"st": 124, "ed": 126, "text": "rnn model"}, {"st": 137, "ed": 140, "text": "mean square error"}, {"st": 225, "ed": 227, "text": "temporal dependencies"}, {"st": 230, "ed": 232, "text": "significantly improves"}]
[{"st": 13, "ed": 15, "text": "cocktail party"}, {"st": 24, "ed": 28, "text": "deep recurrent neural networks"}, {"st": 76, "ed": 78, "text": "natural language"}, {"st": 91, "ed": 93, "text": "input vector"}, {"st": 125, "ed": 127, "text": "source separation"}, {"st": 137, "ed": 140, "text": "deep neural network"}, {"st": 168, "ed": 170, "text": "computational burden"}, {"st": 205, "ed": 207, "text": "computationally efficient"}, {"st": 210, "ed": 212, "text": "cocktail party"}, {"st": 218, "ed": 220, "text": "empirical performance"}]
[{"st": 8, "ed": 10, "text": "continuous control"}, {"st": 26, "ed": 28, "text": "exponentially large"}, {"st": 55, "ed": 57, "text": "structured prediction"}, {"st": 75, "ed": 77, "text": "high dimensional"}, {"st": 82, "ed": 84, "text": "neural networks"}, {"st": 107, "ed": 109, "text": "prediction model"}, {"st": 122, "ed": 124, "text": "compositional structure"}, {"st": 145, "ed": 147, "text": "demonstrate empirically"}, {"st": 172, "ed": 174, "text": "off policy"}, {"st": 190, "ed": 192, "text": "off policy"}]
[{"st": 0, "ed": 2, "text": "probabilistic modeling"}, {"st": 4, "ed": 6, "text": "domain knowledge"}, {"st": 15, "ed": 17, "text": "training instances"}, {"st": 24, "ed": 26, "text": "probabilistic models"}, {"st": 40, "ed": 43, "text": "deep neural networks"}, {"st": 65, "ed": 67, "text": "probabilistic program"}, {"st": 77, "ed": 79, "text": "probabilistic programming"}]
[{"st": 8, "ed": 10, "text": "attention models"}, {"st": 14, "ed": 16, "text": "object recognition"}, {"st": 27, "ed": 29, "text": "soft attention"}, {"st": 32, "ed": 34, "text": "computational cost"}, {"st": 37, "ed": 39, "text": "attention models"}, {"st": 46, "ed": 48, "text": "latent variables"}, {"st": 91, "ed": 93, "text": "variational inference"}, {"st": 96, "ed": 98, "text": "recently introduced"}, {"st": 119, "ed": 121, "text": "phoneme recognition"}, {"st": 131, "ed": 133, "text": "method outperforms"}]
[{"st": 0, "ed": 2, "text": "update rules"}, {"st": 5, "ed": 8, "text": "dynamic time warping"}, {"st": 41, "ed": 43, "text": "squared error"}, {"st": 85, "ed": 87, "text": "time series"}, {"st": 114, "ed": 116, "text": "distance based"}, {"st": 116, "ed": 118, "text": "cost functions"}, {"st": 121, "ed": 123, "text": "k means"}]
[{"st": 5, "ed": 8, "text": "deep generative models"}, {"st": 22, "ed": 25, "text": "variational auto encoder"}, {"st": 50, "ed": 52, "text": "mixture distribution"}, {"st": 54, "ed": 57, "text": "mixture of gaussians"}, {"st": 100, "ed": 102, "text": "local optima"}, {"st": 113, "ed": 115, "text": "empirical studies"}]
[{"st": 0, "ed": 2, "text": "thompson sampling"}, {"st": 28, "ed": 30, "text": "posterior distribution"}, {"st": 49, "ed": 51, "text": "thompson sampling"}, {"st": 59, "ed": 61, "text": "complex models"}, {"st": 76, "ed": 78, "text": "thompson sampling"}]
[{"st": 0, "ed": 3, "text": "deep reinforcement learning"}, {"st": 8, "ed": 12, "text": "deep q network dqn"}, {"st": 36, "ed": 39, "text": "deep neural networks"}, {"st": 52, "ed": 54, "text": "reinforcement learning"}, {"st": 73, "ed": 75, "text": "feature engineering"}, {"st": 90, "ed": 92, "text": "least squares"}, {"st": 92, "ed": 95, "text": "deep q network"}, {"st": 100, "ed": 102, "text": "feature representations"}, {"st": 113, "ed": 115, "text": "least squares"}, {"st": 125, "ed": 127, "text": "hidden layer"}, {"st": 134, "ed": 136, "text": "least squares"}, {"st": 144, "ed": 146, "text": "regularization term"}, {"st": 148, "ed": 150, "text": "least squares"}, {"st": 166, "ed": 168, "text": "atari games"}, {"st": 170, "ed": 172, "text": "significant improvement"}, {"st": 195, "ed": 197, "text": "performance improvement"}, {"st": 203, "ed": 205, "text": "batch size"}]
[{"st": 0, "ed": 2, "text": "representation learning"}, {"st": 26, "ed": 28, "text": "hierarchical structure"}, {"st": 37, "ed": 39, "text": "euclidean vector"}, {"st": 57, "ed": 59, "text": "hierarchical representations"}, {"st": 66, "ed": 68, "text": "hyperbolic space"}, {"st": 82, "ed": 84, "text": "hyperbolic geometry"}]
[{"st": 2, "ed": 5, "text": "deep generative models"}, {"st": 20, "ed": 22, "text": "typically assume"}, {"st": 23, "ed": 25, "text": "training data"}, {"st": 58, "ed": 61, "text": "deep generative models"}, {"st": 79, "ed": 81, "text": "catastrophic forgetting"}, {"st": 96, "ed": 98, "text": "catastrophic forgetting"}, {"st": 103, "ed": 106, "text": "generative adversarial networks"}]
[{"st": 0, "ed": 2, "text": "reinforcement learning"}, {"st": 8, "ed": 10, "text": "optimal policies"}, {"st": 16, "ed": 18, "text": "optimal policies"}, {"st": 19, "ed": 21, "text": "reinforcement learning"}, {"st": 31, "ed": 33, "text": "real world"}, {"st": 37, "ed": 39, "text": "learning algorithms"}, {"st": 56, "ed": 58, "text": "learning algorithm"}, {"st": 75, "ed": 77, "text": "lyapunov stability"}, {"st": 83, "ed": 85, "text": "statistical models"}, {"st": 92, "ed": 94, "text": "control policies"}, {"st": 107, "ed": 109, "text": "gaussian process"}, {"st": 155, "ed": 157, "text": "neural network"}, {"st": 161, "ed": 163, "text": "inverted pendulum"}]
[{"st": 0, "ed": 3, "text": "semi supervised learning"}, {"st": 5, "ed": 9, "text": "generative adversarial networks gans"}, {"st": 12, "ed": 14, "text": "empirical success"}, {"st": 79, "ed": 81, "text": "existing methods"}, {"st": 111, "ed": 114, "text": "semi supervised learning"}, {"st": 124, "ed": 126, "text": "labeled examples"}, {"st": 138, "ed": 141, "text": "semi supervised learning"}]
[{"st": 1, "ed": 3, "text": "reinforcement learning"}, {"st": 4, "ed": 6, "text": "agents learn"}, {"st": 11, "ed": 13, "text": "reward functions"}, {"st": 41, "ed": 43, "text": "deep rl"}, {"st": 43, "ed": 45, "text": "agent learns"}]
[{"st": 3, "ed": 5, "text": "machine learning"}, {"st": 17, "ed": 19, "text": "deep learning"}, {"st": 34, "ed": 36, "text": "deep learning"}, {"st": 57, "ed": 59, "text": "existing algorithms"}, {"st": 70, "ed": 72, "text": "control flow"}, {"st": 99, "ed": 101, "text": "multi core"}, {"st": 154, "ed": 156, "text": "deep learning"}]
[{"st": 22, "ed": 24, "text": "deep networks"}, {"st": 28, "ed": 30, "text": "context specific"}, {"st": 30, "ed": 33, "text": "probabilistic graphical models"}, {"st": 73, "ed": 75, "text": "instance specific"}, {"st": 80, "ed": 82, "text": "computational overhead"}, {"st": 105, "ed": 107, "text": "decision boundary"}, {"st": 119, "ed": 122, "text": "image and text"}, {"st": 124, "ed": 126, "text": "survival analysis"}]
[{"st": 0, "ed": 2, "text": "variational autoencoders"}, {"st": 12, "ed": 15, "text": "encoder and decoder"}, {"st": 35, "ed": 37, "text": "disentangled representations"}, {"st": 64, "ed": 66, "text": "graphical model"}, {"st": 83, "ed": 85, "text": "strong assumptions"}, {"st": 97, "ed": 99, "text": "neural networks"}, {"st": 113, "ed": 116, "text": "semi supervised learning"}, {"st": 126, "ed": 128, "text": "importance sampling"}, {"st": 134, "ed": 137, "text": "ability to learn"}, {"st": 137, "ed": 139, "text": "disentangled representations"}, {"st": 148, "ed": 150, "text": "quantitative evaluation"}]
[{"st": 7, "ed": 9, "text": "hyperparameter optimization"}, {"st": 32, "ed": 34, "text": "neural network"}, {"st": 46, "ed": 48, "text": "compressed sensing"}, {"st": 50, "ed": 52, "text": "orthogonal polynomials"}, {"st": 54, "ed": 56, "text": "uniform sampling"}, {"st": 67, "ed": 70, "text": "deep neural networks"}, {"st": 71, "ed": 73, "text": "cifar 10"}, {"st": 89, "ed": 91, "text": "significantly improved"}, {"st": 139, "ed": 141, "text": "random search"}, {"st": 147, "ed": 149, "text": "provable guarantees"}, {"st": 156, "ed": 158, "text": "sample complexity"}, {"st": 160, "ed": 162, "text": "decision trees"}, {"st": 179, "ed": 181, "text": "decision trees"}]
[{"st": 6, "ed": 8, "text": "variational autoencoders"}, {"st": 12, "ed": 14, "text": "latent code"}, {"st": 71, "ed": 73, "text": "regularization term"}, {"st": 109, "ed": 111, "text": "mutual information"}, {"st": 114, "ed": 116, "text": "latent features"}, {"st": 121, "ed": 123, "text": "latent features"}, {"st": 140, "ed": 143, "text": "qualitative and quantitative"}]
[{"st": 11, "ed": 13, "text": "sample efficiency"}, {"st": 14, "ed": 16, "text": "reinforcement learning"}, {"st": 21, "ed": 23, "text": "increasing attention"}, {"st": 25, "ed": 27, "text": "recent advances"}, {"st": 31, "ed": 33, "text": "deep networks"}]
[{"st": 2, "ed": 4, "text": "artificial intelligence"}, {"st": 17, "ed": 19, "text": "learning rule"}, {"st": 37, "ed": 39, "text": "artificial intelligence"}, {"st": 59, "ed": 61, "text": "learning rule"}, {"st": 75, "ed": 77, "text": "learning rules"}, {"st": 94, "ed": 96, "text": "learning rule"}, {"st": 124, "ed": 127, "text": "artificial neural networks"}, {"st": 142, "ed": 144, "text": "learning rules"}, {"st": 146, "ed": 148, "text": "unsupervised learning"}, {"st": 148, "ed": 150, "text": "reinforcement learning"}]
[{"st": 5, "ed": 9, "text": "generative adversarial networks gans"}, {"st": 15, "ed": 17, "text": "poorly understood"}, {"st": 73, "ed": 75, "text": "optimization procedure"}, {"st": 94, "ed": 96, "text": "recently proposed"}, {"st": 96, "ed": 98, "text": "wasserstein gan"}, {"st": 115, "ed": 117, "text": "regularization term"}, {"st": 148, "ed": 150, "text": "addressing mode"}]
[{"st": 0, "ed": 3, "text": "multi task learning"}, {"st": 12, "ed": 14, "text": "machine learning"}, {"st": 15, "ed": 17, "text": "natural language"}, {"st": 19, "ed": 21, "text": "speech recognition"}, {"st": 22, "ed": 24, "text": "computer vision"}, {"st": 52, "ed": 54, "text": "deep learning"}]
[{"st": 9, "ed": 11, "text": "neural network"}, {"st": 14, "ed": 16, "text": "random noise"}, {"st": 21, "ed": 23, "text": "mathbf x"}, {"st": 36, "ed": 40, "text": "conditional generative adversarial networks"}, {"st": 57, "ed": 59, "text": "mathbf x"}, {"st": 73, "ed": 75, "text": "unsupervised learning"}, {"st": 75, "ed": 77, "text": "supervised learning"}, {"st": 78, "ed": 81, "text": "semi supervised learning"}]
[{"st": 12, "ed": 15, "text": "support vector machine"}, {"st": 82, "ed": 84, "text": "frank wolfe"}, {"st": 84, "ed": 86, "text": "optimization algorithm"}, {"st": 90, "ed": 92, "text": "computational complexity"}, {"st": 124, "ed": 126, "text": "support vectors"}]
[{"st": 38, "ed": 40, "text": "ensemble methods"}, {"st": 50, "ed": 52, "text": "input features"}, {"st": 107, "ed": 109, "text": "recently shown"}, {"st": 142, "ed": 144, "text": "current methods"}, {"st": 152, "ed": 154, "text": "significantly improved"}, {"st": 157, "ed": 159, "text": "feature importance"}, {"st": 175, "ed": 177, "text": "random forests"}]
[{"st": 9, "ed": 12, "text": "markov decision process"}, {"st": 20, "ed": 22, "text": "related tasks"}, {"st": 23, "ed": 25, "text": "low dimensional"}, {"st": 48, "ed": 50, "text": "gaussian process"}, {"st": 55, "ed": 57, "text": "neural network"}]
[{"st": 0, "ed": 2, "text": "observational learning"}, {"st": 35, "ed": 37, "text": "social learning"}, {"st": 68, "ed": 70, "text": "observational learning"}, {"st": 77, "ed": 79, "text": "observational learning"}, {"st": 83, "ed": 85, "text": "reinforcement learning"}, {"st": 143, "ed": 145, "text": "observational learning"}, {"st": 191, "ed": 193, "text": "learning agent"}]
[{"st": 1, "ed": 5, "text": "markov chain monte carlo"}, {"st": 27, "ed": 29, "text": "problem specific"}, {"st": 61, "ed": 63, "text": "adversarial training"}, {"st": 105, "ed": 107, "text": "posterior distribution"}, {"st": 130, "ed": 132, "text": "domain specific"}, {"st": 134, "ed": 136, "text": "empirical results"}, {"st": 151, "ed": 154, "text": "deep neural networks"}, {"st": 158, "ed": 160, "text": "significantly outperform"}, {"st": 160, "ed": 162, "text": "competing methods"}]
[{"st": 10, "ed": 13, "text": "restricted boltzmann machine"}, {"st": 88, "ed": 90, "text": "reinforcement learning"}]
[{"st": 5, "ed": 7, "text": "deep learning"}, {"st": 32, "ed": 35, "text": "deep neural networks"}, {"st": 48, "ed": 50, "text": "training error"}, {"st": 69, "ed": 71, "text": "loss function"}, {"st": 81, "ed": 83, "text": "loss function"}, {"st": 84, "ed": 86, "text": "deep networks"}, {"st": 103, "ed": 105, "text": "optimization methods"}, {"st": 106, "ed": 108, "text": "random initialization"}, {"st": 122, "ed": 124, "text": "neural networks"}, {"st": 128, "ed": 130, "text": "low complexity"}, {"st": 146, "ed": 148, "text": "extensive numerical"}]
[{"st": 1, "ed": 3, "text": "reinforcement learning"}, {"st": 6, "ed": 8, "text": "efficient exploration"}, {"st": 45, "ed": 47, "text": "non trivial"}, {"st": 50, "ed": 52, "text": "deep rl"}, {"st": 74, "ed": 76, "text": "deep rl"}, {"st": 99, "ed": 101, "text": "future frames"}, {"st": 112, "ed": 114, "text": "prediction model"}, {"st": 116, "ed": 118, "text": "future frames"}, {"st": 125, "ed": 127, "text": "convolutional autoencoder"}, {"st": 130, "ed": 132, "text": "deep features"}, {"st": 165, "ed": 167, "text": "hash codes"}, {"st": 170, "ed": 172, "text": "future frames"}, {"st": 224, "ed": 226, "text": "proposed framework"}, {"st": 228, "ed": 230, "text": "performance gain"}]
[{"st": 4, "ed": 6, "text": "reinforcement learning"}, {"st": 9, "ed": 11, "text": "recent progress"}, {"st": 14, "ed": 16, "text": "high confidence"}, {"st": 33, "ed": 35, "text": "performance bounds"}, {"st": 37, "ed": 40, "text": "inverse reinforcement learning"}, {"st": 44, "ed": 46, "text": "reward function"}, {"st": 64, "ed": 67, "text": "inverse reinforcement learning"}, {"st": 80, "ed": 82, "text": "worst case"}, {"st": 92, "ed": 94, "text": "optimal policy"}, {"st": 160, "ed": 163, "text": "orders of magnitude"}, {"st": 167, "ed": 169, "text": "high confidence"}]
[{"st": 5, "ed": 7, "text": "feature selection"}, {"st": 9, "ed": 11, "text": "kernel based"}, {"st": 41, "ed": 44, "text": "constrained optimization problem"}, {"st": 65, "ed": 69, "text": "synthetic and real data"}, {"st": 72, "ed": 75, "text": "method compares favorably"}]
[{"st": 1, "ed": 3, "text": "regret bound"}, {"st": 5, "ed": 7, "text": "optimization algorithms"}, {"st": 27, "ed": 29, "text": "regret bounds"}, {"st": 60, "ed": 62, "text": "loss function"}]
[{"st": 1, "ed": 3, "text": "variational autoencoders"}, {"st": 56, "ed": 58, "text": "latent space"}, {"st": 61, "ed": 63, "text": "variational autoencoder"}, {"st": 94, "ed": 96, "text": "latent space"}, {"st": 110, "ed": 112, "text": "latent space"}, {"st": 122, "ed": 124, "text": "generated data"}]
[{"st": 4, "ed": 6, "text": "learning algorithms"}, {"st": 56, "ed": 58, "text": "regret bound"}, {"st": 65, "ed": 67, "text": "mild assumptions"}, {"st": 76, "ed": 78, "text": "open research"}, {"st": 92, "ed": 94, "text": "proposed algorithm"}, {"st": 166, "ed": 168, "text": "constant factor"}]
[{"st": 10, "ed": 12, "text": "machine learning"}, {"st": 15, "ed": 17, "text": "excellent results"}, {"st": 25, "ed": 27, "text": "sample efficiency"}, {"st": 50, "ed": 52, "text": "surrogate models"}, {"st": 57, "ed": 59, "text": "gaussian processes"}, {"st": 68, "ed": 70, "text": "surrogate model"}, {"st": 114, "ed": 116, "text": "gp model"}, {"st": 137, "ed": 139, "text": "numerical results"}, {"st": 141, "ed": 143, "text": "proposed method"}]
[{"st": 22, "ed": 24, "text": "reinforcement learning"}, {"st": 34, "ed": 36, "text": "reinforcement learning"}, {"st": 77, "ed": 79, "text": "theoretical results"}, {"st": 82, "ed": 84, "text": "policy evaluation"}, {"st": 141, "ed": 143, "text": "anecdotal evidence"}, {"st": 157, "ed": 160, "text": "theoretical and empirical"}]
[{"st": 3, "ed": 5, "text": "representation learning"}, {"st": 10, "ed": 12, "text": "reinforcement learning"}, {"st": 28, "ed": 30, "text": "reinforcement learning"}, {"st": 30, "ed": 32, "text": "sparse coding"}, {"st": 39, "ed": 41, "text": "convex objectives"}, {"st": 53, "ed": 55, "text": "sparse coding"}, {"st": 70, "ed": 72, "text": "local minima"}, {"st": 101, "ed": 103, "text": "sparse coding"}, {"st": 107, "ed": 109, "text": "learned representations"}, {"st": 113, "ed": 115, "text": "sparse representation"}, {"st": 121, "ed": 123, "text": "sparse coding"}, {"st": 126, "ed": 128, "text": "wide variety"}]
[{"st": 0, "ed": 2, "text": "domain adaptation"}, {"st": 5, "ed": 7, "text": "open problem"}, {"st": 8, "ed": 12, "text": "deep reinforcement learning rl"}, {"st": 53, "ed": 55, "text": "multi stage"}, {"st": 59, "ed": 61, "text": "representation learning"}, {"st": 111, "ed": 113, "text": "significantly outperforms"}, {"st": 116, "ed": 118, "text": "zero shot"}, {"st": 118, "ed": 120, "text": "domain adaptation"}]
[{"st": 0, "ed": 2, "text": "low rank"}, {"st": 5, "ed": 7, "text": "important applications"}, {"st": 23, "ed": 25, "text": "nuclear norm"}, {"st": 29, "ed": 31, "text": "low rank"}, {"st": 40, "ed": 42, "text": "optimization problem"}, {"st": 70, "ed": 72, "text": "low rank"}, {"st": 82, "ed": 84, "text": "singular values"}, {"st": 105, "ed": 107, "text": "proximal gradient"}, {"st": 120, "ed": 122, "text": "convergence rate"}, {"st": 130, "ed": 133, "text": "number of iterations"}, {"st": 139, "ed": 141, "text": "proposed algorithm"}, {"st": 155, "ed": 157, "text": "extensive experiments"}, {"st": 160, "ed": 162, "text": "matrix completion"}, {"st": 163, "ed": 167, "text": "robust principal component analysis"}, {"st": 170, "ed": 172, "text": "significant speedup"}, {"st": 195, "ed": 197, "text": "nuclear norm"}]
[{"st": 9, "ed": 11, "text": "big data"}, {"st": 30, "ed": 33, "text": "partial differential equations"}, {"st": 49, "ed": 51, "text": "efficient learning"}, {"st": 66, "ed": 69, "text": "partial differential equations"}, {"st": 104, "ed": 106, "text": "gaussian processes"}, {"st": 107, "ed": 109, "text": "powerful tool"}, {"st": 110, "ed": 112, "text": "probabilistic inference"}, {"st": 131, "ed": 133, "text": "proposed approach"}, {"st": 146, "ed": 148, "text": "domains including"}, {"st": 178, "ed": 180, "text": "applied mathematics"}, {"st": 181, "ed": 183, "text": "mathematical physics"}, {"st": 185, "ed": 187, "text": "learning machines"}]
[{"st": 32, "ed": 34, "text": "decision making"}, {"st": 91, "ed": 93, "text": "theoretical understanding"}, {"st": 101, "ed": 103, "text": "machine learning"}, {"st": 110, "ed": 112, "text": "machine learning"}, {"st": 130, "ed": 132, "text": "machine learning"}, {"st": 132, "ed": 134, "text": "optimization criteria"}, {"st": 144, "ed": 146, "text": "existing algorithms"}, {"st": 182, "ed": 184, "text": "big data"}, {"st": 189, "ed": 191, "text": "computer science"}]
[{"st": 22, "ed": 24, "text": "open question"}, {"st": 142, "ed": 144, "text": "objective function"}]
[{"st": 10, "ed": 13, "text": "deep reinforcement learning"}, {"st": 48, "ed": 51, "text": "deep q networks"}, {"st": 64, "ed": 66, "text": "lagrange multiplier"}, {"st": 76, "ed": 78, "text": "lagrange multiplier"}, {"st": 87, "ed": 89, "text": "atari games"}, {"st": 98, "ed": 101, "text": "deep q networks"}]
[{"st": 1, "ed": 3, "text": "stochastic optimization"}, {"st": 11, "ed": 13, "text": "cost function"}, {"st": 39, "ed": 41, "text": "convergence rate"}, {"st": 70, "ed": 72, "text": "uniform sampling"}, {"st": 76, "ed": 79, "text": "multi armed bandit"}, {"st": 108, "ed": 110, "text": "significant reduction"}, {"st": 118, "ed": 120, "text": "stochastic optimization"}]
[{"st": 8, "ed": 11, "text": "generative adversarial nets"}, {"st": 60, "ed": 62, "text": "mode collapse"}, {"st": 106, "ed": 108, "text": "training data"}, {"st": 150, "ed": 152, "text": "randomly selected"}, {"st": 153, "ed": 155, "text": "final output"}, {"st": 174, "ed": 176, "text": "theoretical analysis"}, {"st": 216, "ed": 218, "text": "parameter sharing"}, {"st": 223, "ed": 225, "text": "computational cost"}, {"st": 234, "ed": 237, "text": "scale to large"}, {"st": 240, "ed": 243, "text": "conduct extensive experiments"}, {"st": 248, "ed": 250, "text": "natural image"}, {"st": 251, "ed": 253, "text": "cifar 10"}]
[{"st": 12, "ed": 14, "text": "fixed size"}, {"st": 102, "ed": 104, "text": "recently introduced"}, {"st": 114, "ed": 117, "text": "end to end"}, {"st": 138, "ed": 140, "text": "chemical compounds"}]
[{"st": 1, "ed": 3, "text": "based approaches"}, {"st": 4, "ed": 6, "text": "multiple classifiers"}, {"st": 13, "ed": 15, "text": "unlabeled data"}, {"st": 72, "ed": 74, "text": "based approaches"}, {"st": 88, "ed": 90, "text": "theoretical analyses"}, {"st": 97, "ed": 99, "text": "theoretical foundation"}]
[{"st": 0, "ed": 2, "text": "missing data"}, {"st": 3, "ed": 5, "text": "noisy observations"}, {"st": 15, "ed": 18, "text": "multivariate time series"}, {"st": 93, "ed": 95, "text": "proposed approach"}, {"st": 112, "ed": 114, "text": "multiple output"}, {"st": 133, "ed": 135, "text": "gaussian noise"}, {"st": 144, "ed": 146, "text": "optimal policy"}, {"st": 200, "ed": 202, "text": "proposed framework"}, {"st": 202, "ed": 204, "text": "significantly outperforms"}]
[{"st": 0, "ed": 3, "text": "stochastic gradient descent"}, {"st": 7, "ed": 9, "text": "stochastic optimization"}, {"st": 105, "ed": 107, "text": "standard deviation"}, {"st": 166, "ed": 168, "text": "significantly outperforms"}, {"st": 174, "ed": 176, "text": "distributed training"}]
[{"st": 0, "ed": 2, "text": "support vector"}, {"st": 30, "ed": 32, "text": "training data"}, {"st": 53, "ed": 55, "text": "kernel function"}, {"st": 57, "ed": 59, "text": "gaussian kernel"}, {"st": 68, "ed": 70, "text": "gaussian kernel"}, {"st": 124, "ed": 126, "text": "gaussian kernel"}]
[{"st": 0, "ed": 2, "text": "transfer learning"}, {"st": 6, "ed": 8, "text": "source domain"}, {"st": 22, "ed": 24, "text": "transfer learning"}, {"st": 37, "ed": 39, "text": "transfer learning"}, {"st": 51, "ed": 53, "text": "transfer learning"}, {"st": 71, "ed": 73, "text": "transfer learning"}, {"st": 94, "ed": 96, "text": "ad hoc"}, {"st": 103, "ed": 105, "text": "educational psychology"}, {"st": 109, "ed": 111, "text": "transfer learning"}, {"st": 123, "ed": 125, "text": "transfer learning"}, {"st": 133, "ed": 135, "text": "transfer learning"}, {"st": 156, "ed": 158, "text": "transfer learning"}, {"st": 175, "ed": 177, "text": "transfer learning"}, {"st": 201, "ed": 203, "text": "extensive experiments"}, {"st": 214, "ed": 216, "text": "transfer learning"}]
[{"st": 1, "ed": 3, "text": "monte carlo"}, {"st": 22, "ed": 24, "text": "neural networks"}, {"st": 77, "ed": 79, "text": "applications including"}, {"st": 81, "ed": 84, "text": "gaussian mixture models"}, {"st": 91, "ed": 93, "text": "hand tuned"}, {"st": 96, "ed": 98, "text": "real world"}, {"st": 98, "ed": 101, "text": "named entity recognition"}]
[{"st": 4, "ed": 6, "text": "chemical properties"}, {"st": 15, "ed": 17, "text": "neural networks"}, {"st": 30, "ed": 32, "text": "generated samples"}, {"st": 68, "ed": 70, "text": "generative models"}, {"st": 71, "ed": 73, "text": "reinforcement learning"}, {"st": 76, "ed": 78, "text": "recently introduced"}]
[{"st": 50, "ed": 52, "text": "false positives"}, {"st": 98, "ed": 100, "text": "false positives"}, {"st": 146, "ed": 148, "text": "significantly improve"}]
[{"st": 10, "ed": 12, "text": "generative adversarial"}, {"st": 36, "ed": 38, "text": "decision theory"}]
[{"st": 2, "ed": 4, "text": "labeled data"}, {"st": 8, "ed": 10, "text": "discriminative models"}, {"st": 16, "ed": 18, "text": "machine learning"}, {"st": 24, "ed": 26, "text": "multiple sources"}, {"st": 27, "ed": 29, "text": "weak supervision"}, {"st": 48, "ed": 50, "text": "ground truth"}, {"st": 56, "ed": 58, "text": "weak supervision"}, {"st": 75, "ed": 77, "text": "generative model"}, {"st": 100, "ed": 102, "text": "sample complexity"}, {"st": 118, "ed": 120, "text": "sample complexity"}, {"st": 138, "ed": 140, "text": "structure learning"}, {"st": 162, "ed": 164, "text": "fully supervised"}, {"st": 178, "ed": 180, "text": "ground truth"}]
[{"st": 2, "ed": 5, "text": "restricted boltzmann machine"}, {"st": 25, "ed": 27, "text": "hidden layer"}, {"st": 40, "ed": 42, "text": "competitive performance"}, {"st": 71, "ed": 73, "text": "hidden units"}, {"st": 82, "ed": 84, "text": "hidden unit"}, {"st": 92, "ed": 94, "text": "hidden units"}, {"st": 103, "ed": 105, "text": "training strategy"}, {"st": 108, "ed": 110, "text": "key idea"}, {"st": 113, "ed": 115, "text": "training strategy"}, {"st": 119, "ed": 121, "text": "hidden units"}, {"st": 138, "ed": 140, "text": "hidden units"}, {"st": 184, "ed": 186, "text": "convergence speed"}, {"st": 192, "ed": 194, "text": "generalization ability"}, {"st": 213, "ed": 215, "text": "training strategy"}, {"st": 221, "ed": 223, "text": "generalization ability"}]
[{"st": 3, "ed": 5, "text": "organic reaction"}, {"st": 129, "ed": 131, "text": "based approach"}, {"st": 132, "ed": 134, "text": "a 10"}, {"st": 137, "ed": 140, "text": "orders of magnitude"}]
[{"st": 0, "ed": 2, "text": "random walks"}, {"st": 9, "ed": 11, "text": "deep learning"}, {"st": 27, "ed": 29, "text": "random walks"}, {"st": 61, "ed": 63, "text": "random walks"}, {"st": 70, "ed": 72, "text": "existing methods"}, {"st": 84, "ed": 86, "text": "proposed framework"}, {"st": 129, "ed": 131, "text": "proposed framework"}, {"st": 149, "ed": 151, "text": "existing methods"}]
[{"st": 0, "ed": 2, "text": "latent factor"}, {"st": 4, "ed": 6, "text": "increasingly popular"}, {"st": 8, "ed": 10, "text": "multi relational"}, {"st": 75, "ed": 78, "text": "strengths and weaknesses"}]
[{"st": 1, "ed": 3, "text": "recent years"}, {"st": 40, "ed": 42, "text": "decision making"}, {"st": 76, "ed": 78, "text": "general classification"}, {"st": 127, "ed": 129, "text": "experiment results"}]
[{"st": 5, "ed": 8, "text": "markov decision process"}, {"st": 25, "ed": 27, "text": "multi modal"}, {"st": 27, "ed": 29, "text": "optimal policy"}, {"st": 59, "ed": 61, "text": "value iteration"}, {"st": 76, "ed": 78, "text": "value iteration"}, {"st": 81, "ed": 83, "text": "fixed point"}, {"st": 146, "ed": 148, "text": "reinforcement learning"}, {"st": 150, "ed": 152, "text": "proposed method"}, {"st": 152, "ed": 154, "text": "outperforms existing"}, {"st": 159, "ed": 161, "text": "convergence speed"}]
[{"st": 0, "ed": 4, "text": "recurrent neural networks rnns"}, {"st": 15, "ed": 18, "text": "ability to learn"}, {"st": 73, "ed": 75, "text": "rnn based"}, {"st": 75, "ed": 77, "text": "generative models"}, {"st": 81, "ed": 83, "text": "user defined"}]
[{"st": 0, "ed": 4, "text": "generative adversarial networks gans"}, {"st": 121, "ed": 123, "text": "higher dimensional"}]
[{"st": 3, "ed": 6, "text": "deep neural networks"}, {"st": 36, "ed": 38, "text": "neural networks"}, {"st": 39, "ed": 41, "text": "neural networks"}, {"st": 50, "ed": 52, "text": "main contribution"}, {"st": 62, "ed": 64, "text": "neural network"}, {"st": 98, "ed": 100, "text": "search procedure"}, {"st": 135, "ed": 138, "text": "deep neural networks"}, {"st": 140, "ed": 142, "text": "image classification"}, {"st": 158, "ed": 161, "text": "deep neural networks"}]
[{"st": 0, "ed": 2, "text": "e commerce"}, {"st": 14, "ed": 16, "text": "machine learning"}, {"st": 26, "ed": 28, "text": "customer experience"}, {"st": 58, "ed": 60, "text": "approach called"}, {"st": 70, "ed": 72, "text": "e commerce"}, {"st": 78, "ed": 80, "text": "low dimensional"}, {"st": 101, "ed": 103, "text": "multi task"}, {"st": 104, "ed": 107, "text": "recurrent neural network"}, {"st": 171, "ed": 173, "text": "extremely high"}, {"st": 174, "ed": 176, "text": "tf idf"}, {"st": 186, "ed": 188, "text": "tf idf"}]
[{"st": 7, "ed": 9, "text": "reinforcement learning"}, {"st": 10, "ed": 12, "text": "partial observability"}, {"st": 36, "ed": 38, "text": "limited data"}, {"st": 78, "ed": 80, "text": "theoretical results"}]
[{"st": 3, "ed": 5, "text": "time series"}, {"st": 34, "ed": 37, "text": "short term memory"}, {"st": 38, "ed": 40, "text": "architectures including"}, {"st": 42, "ed": 44, "text": "cross modal"}, {"st": 92, "ed": 94, "text": "significantly improve"}, {"st": 104, "ed": 106, "text": "cross modal"}, {"st": 123, "ed": 125, "text": "latent variables"}]
[{"st": 0, "ed": 2, "text": "recently proposed"}, {"st": 13, "ed": 15, "text": "input output"}, {"st": 33, "ed": 35, "text": "loss function"}, {"st": 57, "ed": 60, "text": "greatest common divisor"}, {"st": 96, "ed": 98, "text": "empirically demonstrate"}, {"st": 101, "ed": 103, "text": "search procedure"}, {"st": 105, "ed": 107, "text": "significant improvements"}, {"st": 126, "ed": 128, "text": "context free"}, {"st": 131, "ed": 133, "text": "number theory"}, {"st": 133, "ed": 135, "text": "text processing"}]
[{"st": 0, "ed": 2, "text": "machine learning"}, {"st": 55, "ed": 57, "text": "machine learning"}, {"st": 72, "ed": 74, "text": "distributive justice"}, {"st": 76, "ed": 78, "text": "social sciences"}, {"st": 80, "ed": 82, "text": "provide theoretical"}, {"st": 92, "ed": 94, "text": "social sciences"}, {"st": 117, "ed": 119, "text": "distributive justice"}]
[{"st": 1, "ed": 3, "text": "policy improvement"}, {"st": 4, "ed": 6, "text": "off policy"}, {"st": 10, "ed": 12, "text": "desirable properties"}, {"st": 13, "ed": 15, "text": "reinforcement learning"}, {"st": 33, "ed": 35, "text": "policy improvement"}, {"st": 40, "ed": 42, "text": "off policy"}, {"st": 45, "ed": 47, "text": "optimization procedure"}, {"st": 57, "ed": 59, "text": "off policy"}, {"st": 60, "ed": 62, "text": "policy gradient"}, {"st": 68, "ed": 70, "text": "theoretical result"}, {"st": 73, "ed": 75, "text": "trust region"}, {"st": 75, "ed": 77, "text": "policy optimization"}, {"st": 79, "ed": 81, "text": "experience replay"}]
[{"st": 3, "ed": 5, "text": "nearest neighbor"}, {"st": 23, "ed": 25, "text": "highly scalable"}, {"st": 66, "ed": 68, "text": "spectral clustering"}, {"st": 69, "ed": 71, "text": "large data"}, {"st": 72, "ed": 74, "text": "without sacrificing"}, {"st": 77, "ed": 79, "text": "proposed method"}, {"st": 180, "ed": 182, "text": "spectral clustering"}, {"st": 197, "ed": 199, "text": "proposed method"}, {"st": 209, "ed": 211, "text": "significant speedups"}]
[{"st": 7, "ed": 9, "text": "approximate bayesian"}, {"st": 19, "ed": 21, "text": "neural network"}, {"st": 50, "ed": 52, "text": "neural network"}, {"st": 60, "ed": 62, "text": "variational inference"}, {"st": 72, "ed": 75, "text": "variational lower bound"}, {"st": 91, "ed": 93, "text": "deep learning"}, {"st": 100, "ed": 102, "text": "approximate posterior"}, {"st": 125, "ed": 127, "text": "achieve competitive"}, {"st": 142, "ed": 144, "text": "active learning"}]
[{"st": 0, "ed": 3, "text": "deep neural networks"}, {"st": 9, "ed": 11, "text": "deep models"}, {"st": 33, "ed": 35, "text": "neural networks"}, {"st": 63, "ed": 65, "text": "network architecture"}, {"st": 66, "ed": 68, "text": "deep learning"}, {"st": 95, "ed": 97, "text": "weight vector"}, {"st": 115, "ed": 117, "text": "latent space"}, {"st": 128, "ed": 130, "text": "training objective"}]
[{"st": 7, "ed": 11, "text": "restricted boltzmann machine rbm"}, {"st": 12, "ed": 14, "text": "maximum likelihood"}, {"st": 25, "ed": 27, "text": "frank wolfe"}, {"st": 44, "ed": 46, "text": "hidden unit"}, {"st": 52, "ed": 54, "text": "optimization process"}, {"st": 81, "ed": 85, "text": "number of hidden units"}, {"st": 109, "ed": 111, "text": "contrastive divergence"}]
[{"st": 0, "ed": 2, "text": "experience replay"}, {"st": 8, "ed": 10, "text": "recent advances"}, {"st": 54, "ed": 56, "text": "learning dynamics"}, {"st": 73, "ed": 75, "text": "dynamical systems"}]
[{"st": 1, "ed": 4, "text": "stochastic gradient descent"}, {"st": 9, "ed": 11, "text": "large scale"}, {"st": 11, "ed": 13, "text": "optimization problems"}, {"st": 24, "ed": 27, "text": "linear convergence rate"}, {"st": 55, "ed": 57, "text": "convergence rate"}, {"st": 78, "ed": 80, "text": "convergence rate"}, {"st": 103, "ed": 105, "text": "stratified sampling"}, {"st": 112, "ed": 114, "text": "key idea"}, {"st": 124, "ed": 127, "text": "linear convergence rate"}, {"st": 128, "ed": 130, "text": "mathcal o"}, {"st": 153, "ed": 155, "text": "convergence rate"}, {"st": 180, "ed": 182, "text": "training data"}, {"st": 185, "ed": 187, "text": "convergence rate"}, {"st": 193, "ed": 195, "text": "convergence rate"}, {"st": 196, "ed": 198, "text": "mathcal o"}]
[{"st": 0, "ed": 3, "text": "deep neural networks"}, {"st": 16, "ed": 18, "text": "computer vision"}, {"st": 35, "ed": 37, "text": "deep network"}, {"st": 41, "ed": 43, "text": "network architecture"}, {"st": 48, "ed": 50, "text": "network architecture"}, {"st": 73, "ed": 76, "text": "deep neural networks"}, {"st": 91, "ed": 93, "text": "resource constrained"}, {"st": 100, "ed": 104, "text": "deep convolutional neural network"}, {"st": 119, "ed": 121, "text": "unified framework"}, {"st": 149, "ed": 152, "text": "fully connected layers"}, {"st": 177, "ed": 179, "text": "classification accuracy"}]
[{"st": 14, "ed": 16, "text": "reinforcement learning"}, {"st": 20, "ed": 22, "text": "learning speed"}, {"st": 32, "ed": 34, "text": "model based"}, {"st": 36, "ed": 38, "text": "discrete state"}, {"st": 41, "ed": 43, "text": "learning speed"}, {"st": 82, "ed": 84, "text": "faster learning"}, {"st": 130, "ed": 132, "text": "approach called"}, {"st": 139, "ed": 141, "text": "confidence intervals"}, {"st": 155, "ed": 157, "text": "confidence intervals"}, {"st": 161, "ed": 163, "text": "decision making"}, {"st": 190, "ed": 192, "text": "optimal policy"}, {"st": 205, "ed": 207, "text": "learning speed"}]
[{"st": 1, "ed": 4, "text": "support vector machine"}, {"st": 9, "ed": 11, "text": "machine learning"}, {"st": 16, "ed": 18, "text": "statistical learning"}, {"st": 23, "ed": 25, "text": "training data"}, {"st": 50, "ed": 52, "text": "norm regularization"}, {"st": 95, "ed": 98, "text": "synthetic and real"}]
[{"st": 0, "ed": 2, "text": "deep learning"}, {"st": 9, "ed": 11, "text": "hyperparameter optimization"}, {"st": 38, "ed": 40, "text": "grid search"}, {"st": 50, "ed": 52, "text": "neural architecture"}, {"st": 56, "ed": 58, "text": "reinforcement learning"}, {"st": 70, "ed": 72, "text": "successfully applied"}, {"st": 74, "ed": 76, "text": "neural networks"}, {"st": 185, "ed": 187, "text": "multiple tasks"}, {"st": 202, "ed": 204, "text": "pre trained"}, {"st": 207, "ed": 209, "text": "transfer learning"}, {"st": 221, "ed": 223, "text": "pre trained"}, {"st": 232, "ed": 234, "text": "search space"}]
[{"st": 7, "ed": 9, "text": "neural network"}, {"st": 13, "ed": 16, "text": "graph structured data"}, {"st": 26, "ed": 28, "text": "prior methods"}, {"st": 86, "ed": 88, "text": "key challenges"}, {"st": 92, "ed": 94, "text": "neural networks"}]
[{"st": 17, "ed": 20, "text": "gaussian graphical models"}, {"st": 22, "ed": 24, "text": "previous studies"}, {"st": 63, "ed": 65, "text": "closed form"}, {"st": 72, "ed": 74, "text": "large scale"}, {"st": 79, "ed": 81, "text": "statistical analysis"}, {"st": 89, "ed": 91, "text": "convergence rates"}, {"st": 110, "ed": 112, "text": "synthetic datasets"}, {"st": 114, "ed": 117, "text": "real world data"}, {"st": 121, "ed": 123, "text": "strong performance"}, {"st": 129, "ed": 131, "text": "significant computational"}]
[{"st": 1, "ed": 3, "text": "neural network"}, {"st": 9, "ed": 11, "text": "computer vision"}, {"st": 15, "ed": 18, "text": "ability to capture"}, {"st": 93, "ed": 95, "text": "special cases"}, {"st": 102, "ed": 104, "text": "proposed approach"}, {"st": 104, "ed": 106, "text": "consistently outperforms"}, {"st": 113, "ed": 115, "text": "benchmark datasets"}]
[{"st": 0, "ed": 4, "text": "recurrent neural networks rnns"}, {"st": 10, "ed": 12, "text": "neural networks"}, {"st": 53, "ed": 55, "text": "technique called"}, {"st": 58, "ed": 61, "text": "takes advantage of"}, {"st": 116, "ed": 118, "text": "regularization term"}, {"st": 164, "ed": 166, "text": "benchmark datasets"}, {"st": 179, "ed": 181, "text": "performance improvement"}, {"st": 186, "ed": 188, "text": "image captioning"}, {"st": 191, "ed": 193, "text": "semi supervised"}, {"st": 193, "ed": 195, "text": "cifar 10"}]
[{"st": 2, "ed": 4, "text": "multitask learning"}, {"st": 111, "ed": 113, "text": "building blocks"}]
[{"st": 4, "ed": 6, "text": "open source"}, {"st": 6, "ed": 8, "text": "machine learning"}, {"st": 10, "ed": 12, "text": "probabilistic modeling"}, {"st": 14, "ed": 16, "text": "probabilistic modeling"}, {"st": 32, "ed": 34, "text": "probabilistic models"}, {"st": 39, "ed": 41, "text": "mixture models"}, {"st": 41, "ed": 44, "text": "hidden markov models"}, {"st": 98, "ed": 100, "text": "sufficient statistics"}, {"st": 126, "ed": 129, "text": "semi supervised learning"}]
[{"st": 10, "ed": 12, "text": "bandit problems"}, {"st": 46, "ed": 48, "text": "instance specific"}, {"st": 82, "ed": 84, "text": "thompson sampling"}, {"st": 114, "ed": 116, "text": "numerical experiments"}, {"st": 122, "ed": 124, "text": "bandit problem"}, {"st": 128, "ed": 130, "text": "outperforms existing"}]
[{"st": 0, "ed": 2, "text": "machine learning"}, {"st": 46, "ed": 48, "text": "machine learning"}, {"st": 58, "ed": 60, "text": "learning process"}, {"st": 71, "ed": 73, "text": "de facto"}, {"st": 75, "ed": 77, "text": "machine learning"}, {"st": 83, "ed": 85, "text": "multi objective"}, {"st": 86, "ed": 89, "text": "taking into account"}, {"st": 92, "ed": 94, "text": "task performance"}]
[{"st": 10, "ed": 13, "text": "deep neural networks"}, {"st": 31, "ed": 33, "text": "invariance properties"}, {"st": 80, "ed": 82, "text": "generalization error"}, {"st": 87, "ed": 89, "text": "extensive numerical"}, {"st": 91, "ed": 93, "text": "cifar 10"}, {"st": 98, "ed": 100, "text": "theoretical analysis"}, {"st": 110, "ed": 112, "text": "multi layer"}]
[{"st": 85, "ed": 87, "text": "monte carlo"}, {"st": 121, "ed": 123, "text": "sampling error"}, {"st": 148, "ed": 150, "text": "monte carlo"}, {"st": 164, "ed": 166, "text": "strong performance"}, {"st": 167, "ed": 170, "text": "synthetic and real"}]
[{"st": 3, "ed": 5, "text": "structured prediction"}, {"st": 18, "ed": 20, "text": "computer vision"}, {"st": 25, "ed": 27, "text": "performance guarantees"}, {"st": 28, "ed": 30, "text": "real world"}, {"st": 57, "ed": 59, "text": "map inference"}, {"st": 66, "ed": 68, "text": "main results"}, {"st": 100, "ed": 102, "text": "theoretical result"}]
[{"st": 12, "ed": 14, "text": "hidden states"}, {"st": 27, "ed": 29, "text": "poorly understood"}, {"st": 32, "ed": 35, "text": "positive or negative"}, {"st": 48, "ed": 51, "text": "positive and negative"}, {"st": 65, "ed": 67, "text": "positive results"}]
[{"st": 16, "ed": 18, "text": "renewable energy"}, {"st": 65, "ed": 67, "text": "deep learning"}]
[{"st": 6, "ed": 8, "text": "neural networks"}, {"st": 8, "ed": 11, "text": "determinantal point processes"}, {"st": 15, "ed": 17, "text": "kernel learning"}, {"st": 18, "ed": 20, "text": "gaussian processes"}, {"st": 32, "ed": 34, "text": "positive definite"}, {"st": 41, "ed": 43, "text": "mathcal o"}, {"st": 49, "ed": 51, "text": "mathcal o"}, {"st": 74, "ed": 76, "text": "surrogate models"}, {"st": 81, "ed": 83, "text": "kernel matrices"}, {"st": 95, "ed": 97, "text": "gaussian process"}, {"st": 111, "ed": 113, "text": "kernel learning"}, {"st": 120, "ed": 122, "text": "highly efficient"}]
[{"st": 1, "ed": 5, "text": "multi armed bandit problem"}, {"st": 7, "ed": 9, "text": "extensively studied"}, {"st": 38, "ed": 40, "text": "change detection"}, {"st": 44, "ed": 48, "text": "multi armed bandit problems"}, {"st": 58, "ed": 60, "text": "change detection"}, {"st": 62, "ed": 65, "text": "upper confidence bound"}]
[{"st": 75, "ed": 78, "text": "deep generative models"}, {"st": 89, "ed": 92, "text": "end to end"}, {"st": 96, "ed": 98, "text": "deep generative"}, {"st": 99, "ed": 102, "text": "recurrent neural networks"}, {"st": 112, "ed": 114, "text": "neural network"}, {"st": 119, "ed": 121, "text": "decision making"}, {"st": 178, "ed": 180, "text": "joint training"}]
[{"st": 0, "ed": 2, "text": "neural networks"}, {"st": 13, "ed": 15, "text": "neural network"}, {"st": 29, "ed": 32, "text": "trial and error"}, {"st": 50, "ed": 52, "text": "hill climbing"}, {"st": 69, "ed": 71, "text": "method yields"}, {"st": 71, "ed": 73, "text": "competitive results"}, {"st": 90, "ed": 92, "text": "cifar 10"}, {"st": 100, "ed": 102, "text": "error rate"}]
[{"st": 3, "ed": 7, "text": "multi armed bandit problem"}, {"st": 12, "ed": 14, "text": "budget constraint"}, {"st": 128, "ed": 131, "text": "upper confidence bound"}, {"st": 131, "ed": 133, "text": "ucb algorithm"}, {"st": 169, "ed": 171, "text": "o sqrt"}]
[{"st": 11, "ed": 13, "text": "machine learning"}, {"st": 15, "ed": 17, "text": "mental health"}, {"st": 24, "ed": 26, "text": "contextual information"}, {"st": 49, "ed": 51, "text": "neural network"}, {"st": 96, "ed": 98, "text": "open source"}, {"st": 111, "ed": 113, "text": "machine learning"}, {"st": 116, "ed": 118, "text": "mental health"}]
[{"st": 12, "ed": 14, "text": "reinforcement learning"}, {"st": 39, "ed": 41, "text": "computationally expensive"}, {"st": 45, "ed": 47, "text": "continuous action"}, {"st": 57, "ed": 59, "text": "policy gradient"}, {"st": 87, "ed": 89, "text": "deterministic policy"}, {"st": 109, "ed": 111, "text": "learned models"}]
[{"st": 3, "ed": 5, "text": "classification problem"}, {"st": 37, "ed": 40, "text": "sequential decision making"}, {"st": 82, "ed": 84, "text": "reinforcement learning"}, {"st": 95, "ed": 97, "text": "approach outperforms"}]
[{"st": 27, "ed": 29, "text": "real world"}, {"st": 65, "ed": 67, "text": "cognitive function"}, {"st": 101, "ed": 103, "text": "cognitive function"}]
[{"st": 4, "ed": 6, "text": "human intelligence"}, {"st": 8, "ed": 11, "text": "ability to learn"}, {"st": 25, "ed": 27, "text": "reinforcement learning"}, {"st": 102, "ed": 104, "text": "low order"}, {"st": 108, "ed": 110, "text": "significantly outperforms"}, {"st": 112, "ed": 114, "text": "neural network"}, {"st": 117, "ed": 119, "text": "training examples"}, {"st": 123, "ed": 125, "text": "achieve high"}, {"st": 158, "ed": 160, "text": "substantially improve"}]
[{"st": 43, "ed": 46, "text": "deep reinforcement learning"}, {"st": 53, "ed": 55, "text": "reinforcement learning"}, {"st": 100, "ed": 102, "text": "agent learns"}]
[{"st": 6, "ed": 8, "text": "reinforcement learning"}, {"st": 38, "ed": 41, "text": "generative adversarial network"}, {"st": 75, "ed": 77, "text": "reinforcement learning"}]
[{"st": 5, "ed": 7, "text": "deep learning"}, {"st": 38, "ed": 40, "text": "traditional approaches"}, {"st": 58, "ed": 60, "text": "application domains"}, {"st": 61, "ed": 63, "text": "machine learning"}, {"st": 83, "ed": 86, "text": "sea surface temperature"}, {"st": 91, "ed": 93, "text": "background knowledge"}, {"st": 105, "ed": 107, "text": "deep learning"}, {"st": 131, "ed": 133, "text": "differential equations"}]
[{"st": 6, "ed": 9, "text": "deep neural networks"}, {"st": 12, "ed": 14, "text": "de facto"}, {"st": 24, "ed": 26, "text": "adversarial examples"}, {"st": 61, "ed": 64, "text": "problem and propose"}, {"st": 77, "ed": 79, "text": "generative model"}, {"st": 148, "ed": 150, "text": "competitive performance"}]
[{"st": 8, "ed": 12, "text": "markov chain monte carlo"}, {"st": 15, "ed": 18, "text": "deep neural networks"}, {"st": 31, "ed": 33, "text": "monte carlo"}, {"st": 68, "ed": 70, "text": "sample size"}, {"st": 88, "ed": 91, "text": "quantitative and qualitative"}, {"st": 94, "ed": 96, "text": "real world"}, {"st": 97, "ed": 99, "text": "latent variable"}, {"st": 104, "ed": 106, "text": "open source"}]
[{"st": 0, "ed": 3, "text": "deep neural networks"}, {"st": 19, "ed": 21, "text": "input data"}, {"st": 38, "ed": 40, "text": "labeled training"}, {"st": 56, "ed": 58, "text": "classification decision"}, {"st": 82, "ed": 84, "text": "neural net"}, {"st": 114, "ed": 116, "text": "neural net"}, {"st": 122, "ed": 124, "text": "decision tree"}]
[{"st": 13, "ed": 15, "text": "partially observed"}, {"st": 42, "ed": 44, "text": "computer vision"}, {"st": 44, "ed": 46, "text": "signal processing"}, {"st": 58, "ed": 60, "text": "recent advances"}, {"st": 68, "ed": 70, "text": "big data"}, {"st": 127, "ed": 130, "text": "real world data"}, {"st": 135, "ed": 137, "text": "open source"}]
[{"st": 9, "ed": 11, "text": "classification algorithm"}, {"st": 13, "ed": 15, "text": "et al"}, {"st": 70, "ed": 72, "text": "classification models"}, {"st": 135, "ed": 137, "text": "total number"}]
[{"st": 3, "ed": 6, "text": "learning to rank"}, {"st": 20, "ed": 22, "text": "training data"}, {"st": 34, "ed": 36, "text": "feature vectors"}, {"st": 65, "ed": 67, "text": "ranking based"}, {"st": 134, "ed": 136, "text": "building block"}, {"st": 144, "ed": 146, "text": "instance based"}]
[{"st": 14, "ed": 16, "text": "deep learning"}, {"st": 18, "ed": 21, "text": "end to end"}, {"st": 31, "ed": 33, "text": "building blocks"}, {"st": 82, "ed": 84, "text": "residual networks"}, {"st": 91, "ed": 93, "text": "probabilistic programming"}, {"st": 103, "ed": 105, "text": "probabilistic models"}, {"st": 107, "ed": 109, "text": "deep network"}, {"st": 127, "ed": 129, "text": "deep learning"}]
[{"st": 10, "ed": 12, "text": "reinforcement learning"}, {"st": 26, "ed": 29, "text": "deep neural networks"}, {"st": 48, "ed": 50, "text": "generative models"}, {"st": 58, "ed": 60, "text": "predictive models"}, {"st": 77, "ed": 79, "text": "predictive models"}, {"st": 83, "ed": 85, "text": "supervised learning"}, {"st": 93, "ed": 95, "text": "trained jointly"}, {"st": 96, "ed": 98, "text": "reinforcement learning"}, {"st": 115, "ed": 118, "text": "proof of concept"}, {"st": 144, "ed": 146, "text": "melting point"}]
[{"st": 20, "ed": 22, "text": "loss function"}, {"st": 32, "ed": 34, "text": "loss function"}, {"st": 38, "ed": 40, "text": "neural network"}, {"st": 49, "ed": 51, "text": "experimental evaluation"}, {"st": 55, "ed": 57, "text": "loss function"}, {"st": 70, "ed": 72, "text": "semi supervised"}, {"st": 72, "ed": 74, "text": "multi class"}, {"st": 83, "ed": 85, "text": "neural network"}, {"st": 109, "ed": 111, "text": "deep learning"}]
[{"st": 16, "ed": 20, "text": "deep q network dqn"}, {"st": 22, "ed": 24, "text": "variational inference"}, {"st": 43, "ed": 45, "text": "variational inference"}, {"st": 50, "ed": 52, "text": "efficient exploration"}, {"st": 56, "ed": 58, "text": "large scale"}, {"st": 59, "ed": 62, "text": "markov decision process"}]
[{"st": 10, "ed": 12, "text": "matrix multiplication"}, {"st": 19, "ed": 21, "text": "fully connected"}, {"st": 31, "ed": 33, "text": "neural network"}, {"st": 36, "ed": 38, "text": "fully connected"}, {"st": 41, "ed": 43, "text": "convolutional layer"}]
[{"st": 0, "ed": 2, "text": "coordinate descent"}, {"st": 5, "ed": 7, "text": "cost function"}, {"st": 73, "ed": 75, "text": "cost function"}, {"st": 93, "ed": 96, "text": "multi armed bandit"}]
[{"st": 3, "ed": 7, "text": "deep recurrent neural networks"}, {"st": 24, "ed": 26, "text": "neural networks"}, {"st": 28, "ed": 30, "text": "recently introduced"}, {"st": 31, "ed": 33, "text": "echo state"}, {"st": 46, "ed": 49, "text": "deep neural networks"}, {"st": 76, "ed": 78, "text": "recurrent layers"}]
[{"st": 120, "ed": 122, "text": "empirically validate"}, {"st": 142, "ed": 144, "text": "reinforcement learning"}]
[{"st": 5, "ed": 7, "text": "latent representations"}, {"st": 9, "ed": 11, "text": "low dimensional"}, {"st": 22, "ed": 24, "text": "gaussian process"}, {"st": 24, "ed": 26, "text": "latent variable"}, {"st": 39, "ed": 41, "text": "variational autoencoder"}, {"st": 45, "ed": 48, "text": "low dimensional latent"}, {"st": 93, "ed": 95, "text": "approximate inference"}, {"st": 146, "ed": 148, "text": "low dimensional"}]
[{"st": 4, "ed": 6, "text": "reinforcement learning"}, {"st": 25, "ed": 27, "text": "policy improvement"}, {"st": 75, "ed": 77, "text": "computationally efficient"}, {"st": 121, "ed": 123, "text": "worst case"}]
[{"st": 4, "ed": 6, "text": "neural network"}, {"st": 23, "ed": 25, "text": "mini batch"}, {"st": 38, "ed": 40, "text": "deep learning"}, {"st": 44, "ed": 47, "text": "high computational cost"}, {"st": 96, "ed": 98, "text": "neural network"}, {"st": 100, "ed": 102, "text": "conjugate gradient"}, {"st": 111, "ed": 114, "text": "deep convolutional networks"}]
[{"st": 10, "ed": 12, "text": "policy iteration"}, {"st": 15, "ed": 17, "text": "policy improvement"}, {"st": 20, "ed": 22, "text": "trust region"}, {"st": 31, "ed": 33, "text": "trust region"}, {"st": 38, "ed": 41, "text": "kullback leibler kl"}, {"st": 54, "ed": 56, "text": "closed form"}, {"st": 155, "ed": 157, "text": "policy evaluation"}, {"st": 171, "ed": 173, "text": "closely related"}, {"st": 174, "ed": 176, "text": "policy evaluation"}, {"st": 203, "ed": 205, "text": "asymptotic analysis"}, {"st": 224, "ed": 228, "text": "multi armed bandit problem"}, {"st": 232, "ed": 234, "text": "reinforcement learning"}]
[]
[{"st": 3, "ed": 5, "text": "deep learning"}, {"st": 6, "ed": 8, "text": "recent years"}, {"st": 14, "ed": 16, "text": "optimization methods"}, {"st": 30, "ed": 32, "text": "convex optimization"}, {"st": 47, "ed": 50, "text": "non convex optimization"}, {"st": 66, "ed": 68, "text": "deep learning"}, {"st": 95, "ed": 98, "text": "vanishing and exploding"}, {"st": 126, "ed": 128, "text": "et al"}, {"st": 128, "ed": 130, "text": "layer wise"}, {"st": 180, "ed": 182, "text": "optimization methods"}, {"st": 187, "ed": 189, "text": "regret bounds"}, {"st": 213, "ed": 215, "text": "theoretical analysis"}, {"st": 217, "ed": 219, "text": "convergence properties"}, {"st": 255, "ed": 257, "text": "learning rate"}, {"st": 286, "ed": 288, "text": "theoretical analysis"}]
[{"st": 36, "ed": 38, "text": "anomaly detection"}]
[{"st": 1, "ed": 3, "text": "machine learning"}, {"st": 47, "ed": 49, "text": "learning machines"}, {"st": 63, "ed": 65, "text": "causal inference"}, {"st": 88, "ed": 90, "text": "machine learning"}]
[{"st": 4, "ed": 6, "text": "neural networks"}, {"st": 11, "ed": 13, "text": "previous works"}, {"st": 18, "ed": 20, "text": "generalization performance"}, {"st": 64, "ed": 66, "text": "training data"}]
[{"st": 7, "ed": 10, "text": "false positive rate"}, {"st": 16, "ed": 18, "text": "machine learning"}, {"st": 19, "ed": 21, "text": "existing approaches"}, {"st": 23, "ed": 25, "text": "prior knowledge"}, {"st": 48, "ed": 51, "text": "false positive rate"}, {"st": 63, "ed": 65, "text": "false positive"}, {"st": 80, "ed": 83, "text": "false positive rate"}, {"st": 88, "ed": 90, "text": "efficiently solved"}, {"st": 109, "ed": 111, "text": "false positive"}, {"st": 113, "ed": 115, "text": "theoretical analysis"}]
[{"st": 7, "ed": 9, "text": "increasingly popular"}, {"st": 10, "ed": 12, "text": "recent years"}, {"st": 109, "ed": 111, "text": "detection methods"}, {"st": 143, "ed": 145, "text": "non trivial"}, {"st": 162, "ed": 164, "text": "deep learning"}, {"st": 165, "ed": 167, "text": "outperforms existing"}, {"st": 180, "ed": 182, "text": "extensive experiments"}, {"st": 184, "ed": 187, "text": "real world data"}, {"st": 193, "ed": 195, "text": "speech signals"}, {"st": 215, "ed": 217, "text": "approach achieves"}]
[{"st": 16, "ed": 18, "text": "natural gradient"}, {"st": 29, "ed": 31, "text": "empirical analysis"}, {"st": 39, "ed": 41, "text": "sample complexity"}, {"st": 41, "ed": 43, "text": "training speed"}, {"st": 46, "ed": 48, "text": "batch size"}, {"st": 63, "ed": 65, "text": "sample complexity"}, {"st": 96, "ed": 98, "text": "sample efficiency"}]
[{"st": 7, "ed": 9, "text": "machine learning"}, {"st": 11, "ed": 13, "text": "big data"}, {"st": 15, "ed": 17, "text": "big data"}, {"st": 31, "ed": 34, "text": "each data point"}, {"st": 76, "ed": 78, "text": "big data"}, {"st": 121, "ed": 124, "text": "empirical risk minimization"}, {"st": 128, "ed": 130, "text": "machine learning"}, {"st": 132, "ed": 134, "text": "strongly convex"}, {"st": 161, "ed": 163, "text": "step size"}, {"st": 168, "ed": 170, "text": "theoretical results"}, {"st": 183, "ed": 185, "text": "random sampling"}]
[{"st": 26, "ed": 28, "text": "large scale"}, {"st": 38, "ed": 40, "text": "active learning"}, {"st": 94, "ed": 96, "text": "proposed framework"}, {"st": 97, "ed": 99, "text": "substantially improve"}, {"st": 100, "ed": 102, "text": "classification performance"}]
[{"st": 9, "ed": 11, "text": "transfer learning"}, {"st": 13, "ed": 15, "text": "reinforcement learning"}, {"st": 17, "ed": 19, "text": "cross domain"}, {"st": 49, "ed": 51, "text": "optimal policy"}, {"st": 58, "ed": 60, "text": "optimal policy"}, {"st": 67, "ed": 69, "text": "near optimal"}, {"st": 116, "ed": 118, "text": "sample efficiency"}, {"st": 122, "ed": 124, "text": "sample complexity"}]
[{"st": 2, "ed": 4, "text": "generalization properties"}, {"st": 11, "ed": 13, "text": "nonparametric regression"}, {"st": 14, "ed": 17, "text": "a reproducing kernel"}, {"st": 24, "ed": 26, "text": "distributed stochastic"}, {"st": 42, "ed": 44, "text": "generalization error"}, {"st": 71, "ed": 74, "text": "kernel ridge regression"}, {"st": 76, "ed": 79, "text": "principal component analysis"}, {"st": 104, "ed": 106, "text": "computational complexity"}, {"st": 126, "ed": 128, "text": "convergence rates"}]
[{"st": 0, "ed": 2, "text": "time series"}, {"st": 16, "ed": 18, "text": "time series"}, {"st": 21, "ed": 23, "text": "deep models"}, {"st": 24, "ed": 26, "text": "simultaneously learn"}, {"st": 42, "ed": 44, "text": "feature learning"}, {"st": 45, "ed": 50, "text": "long short term memory lstm"}, {"st": 65, "ed": 68, "text": "electronic health record"}, {"st": 90, "ed": 93, "text": "short term memory"}, {"st": 139, "ed": 141, "text": "representation learning"}]
[{"st": 6, "ed": 8, "text": "neural network"}, {"st": 20, "ed": 23, "text": "classification and regression"}, {"st": 29, "ed": 31, "text": "probabilistic model"}, {"st": 47, "ed": 49, "text": "neural networks"}, {"st": 52, "ed": 54, "text": "neural networks"}, {"st": 55, "ed": 57, "text": "continuous function"}, {"st": 84, "ed": 86, "text": "posterior distribution"}, {"st": 100, "ed": 102, "text": "neural network"}, {"st": 143, "ed": 145, "text": "parameter space"}, {"st": 154, "ed": 156, "text": "neural network"}, {"st": 188, "ed": 190, "text": "probabilistic programming"}, {"st": 207, "ed": 209, "text": "machine learning"}]
[{"st": 1, "ed": 3, "text": "text generation"}, {"st": 36, "ed": 38, "text": "machine translation"}, {"st": 117, "ed": 121, "text": "generative adversarial networks gans"}, {"st": 143, "ed": 145, "text": "originally designed"}, {"st": 180, "ed": 182, "text": "conditional gan"}, {"st": 194, "ed": 197, "text": "qualitatively and quantitatively"}, {"st": 211, "ed": 213, "text": "maximum likelihood"}]
[{"st": 3, "ed": 5, "text": "ill posed"}, {"st": 18, "ed": 20, "text": "additional information"}, {"st": 54, "ed": 56, "text": "constraint based"}, {"st": 56, "ed": 58, "text": "clustering algorithms"}, {"st": 91, "ed": 93, "text": "k means"}, {"st": 153, "ed": 155, "text": "clustering quality"}, {"st": 160, "ed": 163, "text": "number of clusters"}]
[{"st": 2, "ed": 4, "text": "expert demonstrations"}, {"st": 12, "ed": 14, "text": "training process"}, {"st": 15, "ed": 18, "text": "deep reinforcement learning"}, {"st": 29, "ed": 31, "text": "supervised learning"}, {"st": 37, "ed": 39, "text": "feature learning"}, {"st": 58, "ed": 60, "text": "reinforcement learning"}, {"st": 63, "ed": 65, "text": "existing methods"}, {"st": 68, "ed": 70, "text": "global optimum"}, {"st": 83, "ed": 85, "text": "expert demonstrations"}, {"st": 89, "ed": 91, "text": "reinforcement learning"}, {"st": 105, "ed": 107, "text": "expert demonstrations"}, {"st": 118, "ed": 120, "text": "policy gradients"}, {"st": 135, "ed": 137, "text": "reinforcement learning"}, {"st": 156, "ed": 158, "text": "reinforcement learning"}]
[{"st": 0, "ed": 2, "text": "gaussian processes"}, {"st": 8, "ed": 10, "text": "generalization properties"}, {"st": 82, "ed": 84, "text": "inductive biases"}, {"st": 88, "ed": 90, "text": "data efficient"}, {"st": 118, "ed": 120, "text": "sparsity inducing"}, {"st": 122, "ed": 124, "text": "mixture components"}, {"st": 145, "ed": 147, "text": "mathcal o"}, {"st": 150, "ed": 152, "text": "mathcal o"}, {"st": 159, "ed": 161, "text": "uncertainty estimates"}, {"st": 172, "ed": 174, "text": "ground truth"}]
[{"st": 0, "ed": 3, "text": "deep neural networks"}, {"st": 34, "ed": 37, "text": "deep neural networks"}, {"st": 39, "ed": 41, "text": "causal models"}, {"st": 76, "ed": 78, "text": "causal model"}, {"st": 94, "ed": 96, "text": "causal model"}]
[{"st": 3, "ed": 5, "text": "deep learning"}, {"st": 6, "ed": 8, "text": "temporal data"}, {"st": 30, "ed": 34, "text": "deep recurrent neural networks"}, {"st": 38, "ed": 41, "text": "short term memory"}, {"st": 62, "ed": 64, "text": "higher layers"}, {"st": 68, "ed": 70, "text": "rnn architecture"}]
[{"st": 6, "ed": 8, "text": "contextual bandit"}, {"st": 12, "ed": 14, "text": "practical applications"}, {"st": 24, "ed": 26, "text": "pre training"}, {"st": 29, "ed": 31, "text": "decision making"}, {"st": 41, "ed": 43, "text": "contextual bandit"}, {"st": 49, "ed": 51, "text": "representation learning"}, {"st": 54, "ed": 56, "text": "pre training"}]
[{"st": 0, "ed": 2, "text": "deep learning"}, {"st": 91, "ed": 93, "text": "nonlinear function"}, {"st": 96, "ed": 98, "text": "input data"}, {"st": 120, "ed": 122, "text": "pre processing"}, {"st": 145, "ed": 147, "text": "pre processing"}, {"st": 166, "ed": 168, "text": "adversarial training"}, {"st": 177, "ed": 179, "text": "adversarial training"}, {"st": 183, "ed": 185, "text": "pre processing"}, {"st": 199, "ed": 201, "text": "cifar 10"}]
[{"st": 5, "ed": 7, "text": "semi supervised"}, {"st": 7, "ed": 9, "text": "multi task"}, {"st": 39, "ed": 42, "text": "high blood pressure"}, {"st": 44, "ed": 46, "text": "sleep apnea"}, {"st": 51, "ed": 53, "text": "semi supervised"}, {"st": 56, "ed": 58, "text": "semi supervised"}, {"st": 67, "ed": 69, "text": "hand engineered"}, {"st": 98, "ed": 100, "text": "apple watch"}]
[{"st": 10, "ed": 12, "text": "signal processing"}, {"st": 27, "ed": 29, "text": "manual annotation"}, {"st": 36, "ed": 38, "text": "multi modal"}, {"st": 51, "ed": 53, "text": "time consuming"}, {"st": 86, "ed": 88, "text": "machine learning"}, {"st": 157, "ed": 159, "text": "proposed approach"}, {"st": 165, "ed": 167, "text": "open source"}, {"st": 197, "ed": 199, "text": "proposed method"}, {"st": 203, "ed": 205, "text": "significantly reduce"}]
[{"st": 1, "ed": 3, "text": "reinforcement learning"}, {"st": 28, "ed": 30, "text": "based methods"}, {"st": 58, "ed": 60, "text": "policy iteration"}, {"st": 63, "ed": 65, "text": "convergence guarantees"}, {"st": 85, "ed": 88, "text": "deep q network"}, {"st": 92, "ed": 94, "text": "multiple tasks"}, {"st": 109, "ed": 111, "text": "policy iteration"}, {"st": 137, "ed": 139, "text": "policy iteration"}, {"st": 143, "ed": 146, "text": "deep neural networks"}]
[{"st": 130, "ed": 132, "text": "extensive empirical"}, {"st": 161, "ed": 163, "text": "improved performance"}, {"st": 179, "ed": 181, "text": "predictive performance"}]
[{"st": 5, "ed": 7, "text": "large scale"}, {"st": 7, "ed": 9, "text": "time series"}, {"st": 28, "ed": 31, "text": "training and testing"}, {"st": 41, "ed": 43, "text": "time series"}, {"st": 63, "ed": 65, "text": "wide variety"}, {"st": 73, "ed": 75, "text": "highly accurate"}]
[{"st": 0, "ed": 2, "text": "adversarial examples"}, {"st": 3, "ed": 5, "text": "machine learning"}, {"st": 76, "ed": 78, "text": "arms race"}, {"st": 82, "ed": 84, "text": "differential privacy"}, {"st": 88, "ed": 90, "text": "building block"}, {"st": 122, "ed": 125, "text": "deep neural networks"}, {"st": 133, "ed": 135, "text": "theoretical guarantees"}, {"st": 145, "ed": 147, "text": "adversarial perturbations"}, {"st": 153, "ed": 156, "text": "mnist cifar 10"}, {"st": 157, "ed": 159, "text": "cifar 100"}]
[{"st": 6, "ed": 8, "text": "reinforcement learning"}, {"st": 24, "ed": 26, "text": "optimal policy"}, {"st": 55, "ed": 57, "text": "shannon entropy"}, {"st": 65, "ed": 67, "text": "optimal policy"}, {"st": 138, "ed": 140, "text": "off policy"}]
[{"st": 51, "ed": 53, "text": "training data"}, {"st": 54, "ed": 58, "text": "reproducing kernel hilbert space"}, {"st": 68, "ed": 70, "text": "learning algorithm"}, {"st": 79, "ed": 81, "text": "layer wise"}, {"st": 86, "ed": 88, "text": "geometric properties"}, {"st": 108, "ed": 111, "text": "compares favorably with"}, {"st": 115, "ed": 118, "text": "multiple kernel learning"}, {"st": 125, "ed": 127, "text": "deep architectures"}]
[{"st": 2, "ed": 4, "text": "neural networks"}, {"st": 24, "ed": 26, "text": "neural network"}, {"st": 76, "ed": 78, "text": "successfully applied"}, {"st": 80, "ed": 82, "text": "complex tasks"}, {"st": 100, "ed": 103, "text": "generative adversarial network"}, {"st": 109, "ed": 111, "text": "deep network"}, {"st": 117, "ed": 119, "text": "cifar 10"}, {"st": 136, "ed": 138, "text": "cifar 10"}, {"st": 151, "ed": 153, "text": "substantial improvement"}]
[{"st": 14, "ed": 16, "text": "machine learning"}, {"st": 24, "ed": 26, "text": "binary tree"}, {"st": 35, "ed": 37, "text": "decision rules"}, {"st": 45, "ed": 47, "text": "machine learning"}, {"st": 62, "ed": 64, "text": "input variables"}, {"st": 110, "ed": 112, "text": "machine learning"}, {"st": 152, "ed": 154, "text": "machine learning"}]
[{"st": 1, "ed": 3, "text": "reinforcement learning"}, {"st": 12, "ed": 14, "text": "video games"}, {"st": 21, "ed": 23, "text": "typically require"}, {"st": 37, "ed": 39, "text": "sample efficiency"}, {"st": 92, "ed": 94, "text": "highly efficient"}]
[{"st": 3, "ed": 6, "text": "deep q network"}, {"st": 9, "ed": 11, "text": "thompson sampling"}, {"st": 12, "ed": 14, "text": "reinforcement learning"}, {"st": 16, "ed": 18, "text": "thompson sampling"}, {"st": 26, "ed": 28, "text": "posterior sampling"}, {"st": 43, "ed": 45, "text": "output layer"}, {"st": 51, "ed": 53, "text": "linear regression"}, {"st": 62, "ed": 64, "text": "closed form"}, {"st": 85, "ed": 87, "text": "atari games"}, {"st": 96, "ed": 98, "text": "efficient exploration"}, {"st": 113, "ed": 116, "text": "deep q network"}]
[{"st": 21, "ed": 23, "text": "search algorithms"}, {"st": 47, "ed": 49, "text": "monte carlo"}, {"st": 127, "ed": 129, "text": "neural network"}, {"st": 144, "ed": 148, "text": "trained end to end"}, {"st": 166, "ed": 168, "text": "search algorithm"}, {"st": 168, "ed": 170, "text": "significantly outperformed"}]
[{"st": 0, "ed": 3, "text": "deep reinforcement learning"}, {"st": 8, "ed": 10, "text": "continuous control"}, {"st": 10, "ed": 12, "text": "problems including"}, {"st": 24, "ed": 26, "text": "open problem"}, {"st": 66, "ed": 68, "text": "continuous action"}, {"st": 102, "ed": 104, "text": "policy network"}, {"st": 113, "ed": 115, "text": "transfer learning"}]
[{"st": 3, "ed": 6, "text": "intensive care unit"}, {"st": 62, "ed": 64, "text": "false alarms"}, {"st": 104, "ed": 106, "text": "false alarm"}, {"st": 108, "ed": 111, "text": "multivariate time series"}, {"st": 113, "ed": 115, "text": "machine learning"}, {"st": 123, "ed": 125, "text": "network architecture"}, {"st": 127, "ed": 129, "text": "distant supervision"}, {"st": 153, "ed": 155, "text": "significant improvements"}, {"st": 163, "ed": 165, "text": "real world"}]
[{"st": 1, "ed": 3, "text": "real world"}, {"st": 14, "ed": 16, "text": "current approaches"}, {"st": 17, "ed": 20, "text": "learning from demonstration"}, {"st": 23, "ed": 25, "text": "supervised learning"}, {"st": 27, "ed": 29, "text": "demonstration data"}, {"st": 31, "ed": 33, "text": "reinforcement learning"}, {"st": 35, "ed": 37, "text": "improve performance"}, {"st": 70, "ed": 72, "text": "reinforcement learning"}, {"st": 98, "ed": 100, "text": "policy network"}, {"st": 116, "ed": 119, "text": "learning from demonstration"}, {"st": 142, "ed": 144, "text": "demonstration data"}, {"st": 164, "ed": 166, "text": "reinforcement learning"}, {"st": 171, "ed": 173, "text": "outperform existing"}]
[{"st": 3, "ed": 5, "text": "time series"}, {"st": 16, "ed": 18, "text": "time series"}, {"st": 27, "ed": 29, "text": "data mining"}, {"st": 58, "ed": 60, "text": "time series"}, {"st": 65, "ed": 67, "text": "missing data"}, {"st": 73, "ed": 75, "text": "missing data"}, {"st": 128, "ed": 130, "text": "constant factor"}, {"st": 145, "ed": 147, "text": "missing data"}]
[{"st": 9, "ed": 11, "text": "feature selection"}, {"st": 29, "ed": 31, "text": "feature selection"}, {"st": 48, "ed": 50, "text": "feature set"}, {"st": 59, "ed": 61, "text": "feature selection"}, {"st": 96, "ed": 98, "text": "error bounds"}, {"st": 106, "ed": 108, "text": "extensive experiments"}]
[{"st": 1, "ed": 3, "text": "causal models"}, {"st": 4, "ed": 6, "text": "observational data"}, {"st": 32, "ed": 34, "text": "generating process"}, {"st": 54, "ed": 57, "text": "continuous and discrete"}, {"st": 69, "ed": 71, "text": "causal discovery"}, {"st": 75, "ed": 77, "text": "discrete data"}, {"st": 87, "ed": 89, "text": "continuous data"}, {"st": 101, "ed": 103, "text": "important information"}, {"st": 106, "ed": 108, "text": "discrete data"}, {"st": 111, "ed": 113, "text": "approximation error"}, {"st": 124, "ed": 126, "text": "causal model"}, {"st": 130, "ed": 133, "text": "continuous and discrete"}, {"st": 146, "ed": 148, "text": "linear function"}, {"st": 155, "ed": 157, "text": "gaussian noise"}, {"st": 183, "ed": 185, "text": "scoring function"}, {"st": 198, "ed": 201, "text": "continuous and discrete"}, {"st": 205, "ed": 207, "text": "empirically demonstrate"}]
[{"st": 4, "ed": 6, "text": "data analysis"}, {"st": 60, "ed": 62, "text": "large scale"}, {"st": 89, "ed": 91, "text": "feature maps"}, {"st": 93, "ed": 96, "text": "random fourier features"}, {"st": 98, "ed": 100, "text": "large scale"}, {"st": 126, "ed": 128, "text": "significantly improve"}, {"st": 145, "ed": 148, "text": "ability to learn"}, {"st": 150, "ed": 152, "text": "large scale"}, {"st": 171, "ed": 173, "text": "prediction accuracy"}]
[{"st": 14, "ed": 16, "text": "complex systems"}, {"st": 45, "ed": 47, "text": "real world"}, {"st": 70, "ed": 73, "text": "deep reinforcement learning"}, {"st": 78, "ed": 80, "text": "real world"}, {"st": 93, "ed": 95, "text": "multi agent"}, {"st": 100, "ed": 102, "text": "real world"}, {"st": 116, "ed": 118, "text": "optimal policy"}, {"st": 123, "ed": 125, "text": "source domain"}, {"st": 159, "ed": 161, "text": "partially observed"}, {"st": 239, "ed": 241, "text": "real world"}, {"st": 284, "ed": 286, "text": "real world"}, {"st": 297, "ed": 299, "text": "multi agent"}]
[{"st": 0, "ed": 3, "text": "type 2 diabetes"}, {"st": 63, "ed": 66, "text": "multi task learning"}, {"st": 85, "ed": 87, "text": "proposed method"}, {"st": 133, "ed": 135, "text": "hierarchical bayesian"}, {"st": 138, "ed": 140, "text": "domain knowledge"}, {"st": 146, "ed": 148, "text": "proposed method"}, {"st": 158, "ed": 160, "text": "prediction performance"}, {"st": 171, "ed": 174, "text": "extensive experimental results"}, {"st": 184, "ed": 186, "text": "proposed method"}]
[{"st": 1, "ed": 4, "text": "markov decision process"}, {"st": 10, "ed": 12, "text": "reinforcement learning"}, {"st": 17, "ed": 19, "text": "agents learn"}, {"st": 44, "ed": 46, "text": "primal dual"}, {"st": 57, "ed": 59, "text": "existing methods"}, {"st": 83, "ed": 85, "text": "policy search"}, {"st": 90, "ed": 92, "text": "primal dual"}, {"st": 97, "ed": 99, "text": "off policy"}, {"st": 117, "ed": 119, "text": "likelihood ratio"}, {"st": 133, "ed": 135, "text": "sample efficiency"}, {"st": 136, "ed": 138, "text": "faster convergence"}]
[{"st": 1, "ed": 4, "text": "the past decade"}, {"st": 8, "ed": 10, "text": "machine learning"}, {"st": 36, "ed": 39, "text": "multi task learning"}, {"st": 64, "ed": 66, "text": "linear models"}, {"st": 113, "ed": 115, "text": "multi task"}, {"st": 125, "ed": 127, "text": "multi task"}, {"st": 141, "ed": 143, "text": "low dimensional"}, {"st": 151, "ed": 154, "text": "under mild assumptions"}, {"st": 170, "ed": 172, "text": "empirical results"}]
[{"st": 39, "ed": 41, "text": "worst case"}, {"st": 46, "ed": 48, "text": "constant factor"}, {"st": 76, "ed": 78, "text": "constant factor"}, {"st": 90, "ed": 92, "text": "theoretical bounds"}, {"st": 95, "ed": 97, "text": "constant factor"}]
[{"st": 20, "ed": 22, "text": "neural network"}, {"st": 47, "ed": 49, "text": "neural network"}, {"st": 71, "ed": 73, "text": "constraint satisfaction"}, {"st": 80, "ed": 82, "text": "neural network"}]
[{"st": 1, "ed": 4, "text": "continuous control tasks"}, {"st": 16, "ed": 18, "text": "policy gradient"}, {"st": 32, "ed": 34, "text": "policy gradient"}, {"st": 38, "ed": 40, "text": "policy gradient"}, {"st": 89, "ed": 91, "text": "policy gradient"}]
[{"st": 3, "ed": 5, "text": "feature selection"}, {"st": 41, "ed": 43, "text": "mutual information"}, {"st": 48, "ed": 50, "text": "response variable"}, {"st": 52, "ed": 54, "text": "conditional distribution"}, {"st": 56, "ed": 58, "text": "response variable"}, {"st": 71, "ed": 73, "text": "variational approximation"}, {"st": 75, "ed": 77, "text": "mutual information"}, {"st": 82, "ed": 85, "text": "method compares favorably"}, {"st": 94, "ed": 98, "text": "synthetic and real data"}]
[{"st": 7, "ed": 9, "text": "variational inference"}, {"st": 12, "ed": 14, "text": "policy gradient"}, {"st": 52, "ed": 54, "text": "policy gradient"}, {"st": 59, "ed": 61, "text": "neural network"}, {"st": 63, "ed": 65, "text": "reinforcement learning"}]
[{"st": 20, "ed": 22, "text": "lifelong learning"}, {"st": 26, "ed": 28, "text": "sample efficient"}, {"st": 57, "ed": 59, "text": "motion planning"}, {"st": 78, "ed": 80, "text": "intrinsic motivation"}, {"st": 95, "ed": 97, "text": "recurrent network"}]
[{"st": 0, "ed": 2, "text": "online optimization"}, {"st": 9, "ed": 11, "text": "large scale"}, {"st": 18, "ed": 20, "text": "current methods"}, {"st": 21, "ed": 24, "text": "online convex optimization"}, {"st": 40, "ed": 42, "text": "prohibitively expensive"}, {"st": 43, "ed": 45, "text": "large scale"}, {"st": 56, "ed": 59, "text": "non convex optimization"}, {"st": 60, "ed": 62, "text": "machine learning"}, {"st": 70, "ed": 72, "text": "submodular functions"}, {"st": 76, "ed": 78, "text": "diminishing returns"}, {"st": 89, "ed": 91, "text": "convex functions"}, {"st": 113, "ed": 115, "text": "frank wolfe"}, {"st": 141, "ed": 143, "text": "o sqrt"}, {"st": 145, "ed": 147, "text": "regret bounds"}, {"st": 158, "ed": 160, "text": "frank wolfe"}, {"st": 182, "ed": 184, "text": "regret bound"}]
[{"st": 2, "ed": 4, "text": "neural network"}, {"st": 18, "ed": 20, "text": "hidden layers"}, {"st": 85, "ed": 87, "text": "optimization problem"}]
[{"st": 14, "ed": 16, "text": "training set"}, {"st": 33, "ed": 36, "text": "maximum likelihood estimator"}, {"st": 44, "ed": 46, "text": "large margin"}, {"st": 55, "ed": 57, "text": "mixed integer"}, {"st": 57, "ed": 59, "text": "nonlinear programming"}]
[{"st": 7, "ed": 9, "text": "computer science"}, {"st": 51, "ed": 53, "text": "sampling distribution"}, {"st": 103, "ed": 105, "text": "problem specific"}, {"st": 132, "ed": 134, "text": "online optimization"}, {"st": 167, "ed": 169, "text": "consistently outperforms"}, {"st": 197, "ed": 199, "text": "objective function"}]
[{"st": 3, "ed": 5, "text": "reinforcement learning"}]
[{"st": 67, "ed": 69, "text": "multi agent"}, {"st": 82, "ed": 84, "text": "clustering method"}, {"st": 97, "ed": 99, "text": "multi agent"}, {"st": 118, "ed": 120, "text": "empirical study"}, {"st": 124, "ed": 127, "text": "real world data"}]
[{"st": 1, "ed": 3, "text": "loss functions"}, {"st": 4, "ed": 7, "text": "deep neural networks"}, {"st": 11, "ed": 13, "text": "geometric properties"}, {"st": 25, "ed": 27, "text": "loss functions"}, {"st": 45, "ed": 48, "text": "training and test"}, {"st": 55, "ed": 57, "text": "training procedure"}, {"st": 103, "ed": 105, "text": "improved performance"}, {"st": 116, "ed": 121, "text": "cifar 10 and cifar 100"}, {"st": 136, "ed": 138, "text": "error rate"}, {"st": 140, "ed": 142, "text": "pre trained"}]
[{"st": 0, "ed": 2, "text": "traditional methods"}, {"st": 17, "ed": 19, "text": "time consuming"}, {"st": 20, "ed": 22, "text": "error prone"}, {"st": 85, "ed": 88, "text": "electronic health record"}, {"st": 105, "ed": 107, "text": "deep learning"}, {"st": 130, "ed": 132, "text": "baseline models"}, {"st": 149, "ed": 151, "text": "accurate predictions"}]
[{"st": 1, "ed": 3, "text": "reinforcement learning"}, {"st": 8, "ed": 10, "text": "multiple tasks"}, {"st": 33, "ed": 35, "text": "neural network"}, {"st": 38, "ed": 40, "text": "multiple tasks"}, {"st": 81, "ed": 83, "text": "multiple tasks"}, {"st": 90, "ed": 92, "text": "continuous action"}, {"st": 99, "ed": 101, "text": "policy gradient"}, {"st": 103, "ed": 105, "text": "proposed framework"}, {"st": 109, "ed": 111, "text": "policy gradients"}, {"st": 114, "ed": 117, "text": "multi task learning"}, {"st": 132, "ed": 134, "text": "policy gradient"}, {"st": 152, "ed": 155, "text": "degrees of freedom"}, {"st": 177, "ed": 180, "text": "multi task learning"}, {"st": 185, "ed": 187, "text": "related methods"}, {"st": 188, "ed": 190, "text": "continuous action"}]
[{"st": 3, "ed": 5, "text": "reinforcement learning"}, {"st": 93, "ed": 95, "text": "reinforcement learning"}, {"st": 105, "ed": 107, "text": "sample complexity"}]
[{"st": 1, "ed": 3, "text": "recent advances"}, {"st": 5, "ed": 9, "text": "recurrent neural networks rnns"}, {"st": 44, "ed": 47, "text": "ability to capture"}, {"st": 104, "ed": 106, "text": "image classification"}, {"st": 116, "ed": 118, "text": "document classification"}, {"st": 135, "ed": 137, "text": "recurrent models"}]
[{"st": 15, "ed": 17, "text": "expert knowledge"}, {"st": 63, "ed": 65, "text": "efficiently learn"}, {"st": 81, "ed": 83, "text": "policy search"}, {"st": 94, "ed": 96, "text": "multi objective"}, {"st": 106, "ed": 109, "text": "proof of concept"}, {"st": 117, "ed": 119, "text": "recently developed"}, {"st": 141, "ed": 143, "text": "learning scheme"}, {"st": 146, "ed": 148, "text": "multi objective"}, {"st": 158, "ed": 160, "text": "prior knowledge"}]
[{"st": 1, "ed": 3, "text": "structure learning"}, {"st": 9, "ed": 12, "text": "statistical relational learning"}, {"st": 19, "ed": 21, "text": "noisy data"}, {"st": 31, "ed": 33, "text": "training data"}, {"st": 38, "ed": 40, "text": "real world"}, {"st": 52, "ed": 54, "text": "semi supervised"}, {"st": 54, "ed": 56, "text": "structure learning"}, {"st": 68, "ed": 70, "text": "unlabelled data"}, {"st": 86, "ed": 89, "text": "first order logic"}, {"st": 127, "ed": 129, "text": "globally optimal"}, {"st": 143, "ed": 145, "text": "event recognition"}, {"st": 148, "ed": 150, "text": "benchmark dataset"}, {"st": 151, "ed": 154, "text": "human activity recognition"}, {"st": 182, "ed": 184, "text": "underlying structure"}]
[{"st": 1, "ed": 3, "text": "neural networks"}, {"st": 36, "ed": 38, "text": "neural network"}, {"st": 52, "ed": 55, "text": "training and test"}, {"st": 59, "ed": 61, "text": "neural networks"}]
[{"st": 15, "ed": 17, "text": "challenging problem"}, {"st": 19, "ed": 21, "text": "search space"}, {"st": 33, "ed": 35, "text": "existing approaches"}, {"st": 70, "ed": 72, "text": "structure learning"}, {"st": 76, "ed": 79, "text": "constrained optimization problem"}, {"st": 153, "ed": 155, "text": "outperforms existing"}]
[{"st": 3, "ed": 5, "text": "predictive state"}, {"st": 9, "ed": 11, "text": "recurrent architecture"}, {"st": 15, "ed": 17, "text": "predictive state"}, {"st": 19, "ed": 21, "text": "reinforcement learning"}, {"st": 22, "ed": 24, "text": "partially observable"}, {"st": 25, "ed": 27, "text": "predictive state"}, {"st": 65, "ed": 67, "text": "predictive state"}, {"st": 79, "ed": 81, "text": "predictive state"}, {"st": 116, "ed": 118, "text": "predictive state"}, {"st": 158, "ed": 160, "text": "prediction error"}, {"st": 180, "ed": 182, "text": "gradient based"}, {"st": 191, "ed": 193, "text": "policy gradient"}, {"st": 213, "ed": 215, "text": "partial observability"}]
[{"st": 4, "ed": 6, "text": "machine learning"}, {"st": 24, "ed": 26, "text": "meta algorithm"}, {"st": 31, "ed": 33, "text": "base learner"}, {"st": 35, "ed": 37, "text": "least squares"}, {"st": 38, "ed": 41, "text": "stochastic gradient descent"}, {"st": 54, "ed": 57, "text": "strong theoretical guarantees"}, {"st": 60, "ed": 62, "text": "highly scalable"}, {"st": 65, "ed": 67, "text": "base learner"}, {"st": 88, "ed": 90, "text": "drug design"}, {"st": 139, "ed": 141, "text": "drug design"}, {"st": 148, "ed": 151, "text": "mean squared error"}]
[{"st": 24, "ed": 26, "text": "sufficiently large"}, {"st": 28, "ed": 30, "text": "training set"}, {"st": 47, "ed": 49, "text": "linear function"}, {"st": 50, "ed": 52, "text": "squared error"}, {"st": 56, "ed": 58, "text": "squared error"}, {"st": 62, "ed": 64, "text": "linear function"}, {"st": 71, "ed": 73, "text": "marginal distribution"}, {"st": 77, "ed": 79, "text": "input space"}, {"st": 187, "ed": 189, "text": "convex relaxation"}, {"st": 192, "ed": 194, "text": "conceptually simple"}, {"st": 194, "ed": 197, "text": "non convex optimization"}, {"st": 200, "ed": 202, "text": "linear function"}, {"st": 214, "ed": 216, "text": "least squares"}]
[{"st": 53, "ed": 56, "text": "conduct extensive experiments"}]
[{"st": 3, "ed": 5, "text": "neural networks"}, {"st": 16, "ed": 19, "text": "standard benchmark datasets"}, {"st": 20, "ed": 25, "text": "graph based semi supervised learning"}, {"st": 40, "ed": 42, "text": "hidden states"}, {"st": 44, "ed": 46, "text": "local neighborhood"}, {"st": 48, "ed": 50, "text": "fully connected"}, {"st": 57, "ed": 59, "text": "linear model"}, {"st": 64, "ed": 67, "text": "fully connected layers"}, {"st": 83, "ed": 85, "text": "significantly reduces"}, {"st": 93, "ed": 96, "text": "semi supervised learning"}, {"st": 99, "ed": 101, "text": "labeled examples"}, {"st": 124, "ed": 126, "text": "neural network"}, {"st": 131, "ed": 134, "text": "fully connected layers"}, {"st": 140, "ed": 142, "text": "attention mechanisms"}, {"st": 150, "ed": 152, "text": "attention mechanism"}, {"st": 184, "ed": 186, "text": "approach outperforms"}, {"st": 191, "ed": 193, "text": "attention weights"}]
[{"st": 12, "ed": 14, "text": "supply chain"}, {"st": 29, "ed": 31, "text": "non stationary"}, {"st": 36, "ed": 38, "text": "time series"}, {"st": 50, "ed": 52, "text": "neural network"}, {"st": 61, "ed": 63, "text": "time series"}, {"st": 107, "ed": 111, "text": "trained end to end"}, {"st": 130, "ed": 133, "text": "tens of thousands"}, {"st": 142, "ed": 144, "text": "significant improvement"}]
[{"st": 7, "ed": 10, "text": "electronic health records"}, {"st": 57, "ed": 59, "text": "deep learning"}, {"st": 107, "ed": 109, "text": "complex nonlinear"}, {"st": 124, "ed": 126, "text": "joint representation"}, {"st": 157, "ed": 159, "text": "experiment results"}, {"st": 160, "ed": 162, "text": "real world"}, {"st": 179, "ed": 181, "text": "proposed approach"}]
[{"st": 0, "ed": 2, "text": "variational inference"}, {"st": 5, "ed": 7, "text": "approximate posterior"}, {"st": 8, "ed": 10, "text": "normalizing flows"}, {"st": 22, "ed": 24, "text": "normalizing flows"}, {"st": 35, "ed": 37, "text": "normalizing flows"}, {"st": 60, "ed": 62, "text": "normalizing flows"}, {"st": 73, "ed": 75, "text": "compare favorably"}]
[{"st": 50, "ed": 53, "text": "deep neural network"}]
[{"st": 2, "ed": 5, "text": "deep reinforcement learning"}, {"st": 16, "ed": 18, "text": "video games"}, {"st": 31, "ed": 33, "text": "perform poorly"}, {"st": 48, "ed": 50, "text": "real world"}, {"st": 58, "ed": 60, "text": "maximum entropy"}, {"st": 70, "ed": 72, "text": "real world"}, {"st": 80, "ed": 82, "text": "real world"}, {"st": 87, "ed": 89, "text": "important features"}, {"st": 100, "ed": 102, "text": "exploration strategies"}, {"st": 156, "ed": 158, "text": "real world"}, {"st": 179, "ed": 181, "text": "experimental evaluation"}, {"st": 189, "ed": 191, "text": "sample efficient"}, {"st": 195, "ed": 198, "text": "deep reinforcement learning"}, {"st": 207, "ed": 211, "text": "simulated and real world"}]
[{"st": 6, "ed": 8, "text": "reinforcement learning"}, {"st": 13, "ed": 15, "text": "random search"}, {"st": 17, "ed": 19, "text": "parameter space"}, {"st": 24, "ed": 26, "text": "sample complexity"}, {"st": 41, "ed": 43, "text": "random search"}, {"st": 50, "ed": 52, "text": "continuous control"}, {"st": 58, "ed": 60, "text": "sample efficiency"}, {"st": 87, "ed": 89, "text": "control theory"}, {"st": 97, "ed": 99, "text": "random search"}, {"st": 122, "ed": 124, "text": "computational efficiency"}, {"st": 155, "ed": 157, "text": "benchmark tasks"}, {"st": 163, "ed": 165, "text": "sample efficiency"}]
[{"st": 0, "ed": 2, "text": "policy gradient"}, {"st": 5, "ed": 7, "text": "great success"}, {"st": 8, "ed": 11, "text": "deep reinforcement learning"}, {"st": 67, "ed": 69, "text": "additional assumptions"}, {"st": 85, "ed": 87, "text": "theoretical analysis"}, {"st": 90, "ed": 92, "text": "numerical results"}, {"st": 108, "ed": 110, "text": "computationally efficient"}, {"st": 110, "ed": 112, "text": "policy gradient"}, {"st": 118, "ed": 120, "text": "control problems"}, {"st": 140, "ed": 142, "text": "faster learning"}, {"st": 144, "ed": 146, "text": "reinforcement learning"}, {"st": 164, "ed": 166, "text": "additional information"}, {"st": 176, "ed": 178, "text": "partially observed"}, {"st": 179, "ed": 181, "text": "multi agent"}]
[{"st": 9, "ed": 11, "text": "reinforcement learning"}, {"st": 13, "ed": 15, "text": "natural gradient"}, {"st": 20, "ed": 24, "text": "deep q network dqn"}, {"st": 27, "ed": 29, "text": "natural gradient"}, {"st": 65, "ed": 67, "text": "natural gradient"}, {"st": 75, "ed": 77, "text": "reinforcement learning"}]
[{"st": 69, "ed": 72, "text": "deep neural networks"}]
[{"st": 13, "ed": 15, "text": "marginal probabilities"}, {"st": 16, "ed": 18, "text": "most probable"}, {"st": 23, "ed": 26, "text": "probabilistic graphical models"}, {"st": 33, "ed": 35, "text": "complex data"}, {"st": 42, "ed": 44, "text": "message passing"}, {"st": 47, "ed": 49, "text": "belief propagation"}, {"st": 80, "ed": 82, "text": "neural networks"}, {"st": 86, "ed": 88, "text": "message passing"}, {"st": 132, "ed": 134, "text": "belief propagation"}, {"st": 138, "ed": 140, "text": "message passing"}, {"st": 145, "ed": 147, "text": "training set"}]
[{"st": 30, "ed": 32, "text": "conditional distribution"}, {"st": 41, "ed": 43, "text": "random variables"}, {"st": 48, "ed": 50, "text": "causal direction"}]
[{"st": 12, "ed": 14, "text": "structured output"}, {"st": 21, "ed": 23, "text": "prediction model"}, {"st": 33, "ed": 35, "text": "structured output"}, {"st": 70, "ed": 72, "text": "structured output"}, {"st": 76, "ed": 79, "text": "labeled training data"}, {"st": 103, "ed": 105, "text": "feature space"}, {"st": 124, "ed": 126, "text": "structured output"}, {"st": 156, "ed": 158, "text": "fine grained"}, {"st": 187, "ed": 189, "text": "sentence level"}]
[{"st": 0, "ed": 3, "text": "deep reinforcement learning"}, {"st": 5, "ed": 7, "text": "successfully applied"}, {"st": 9, "ed": 11, "text": "visual input"}, {"st": 22, "ed": 24, "text": "model based"}, {"st": 29, "ed": 31, "text": "dnn based"}, {"st": 31, "ed": 33, "text": "transition model"}, {"st": 34, "ed": 36, "text": "monte carlo"}, {"st": 48, "ed": 50, "text": "transition model"}, {"st": 58, "ed": 60, "text": "step ahead"}, {"st": 79, "ed": 81, "text": "monte carlo"}, {"st": 82, "ed": 84, "text": "search algorithm"}, {"st": 106, "ed": 108, "text": "model based"}, {"st": 115, "ed": 118, "text": "deep q network"}, {"st": 126, "ed": 128, "text": "training sample"}]
[{"st": 7, "ed": 9, "text": "scene recognition"}, {"st": 38, "ed": 40, "text": "feature extraction"}, {"st": 56, "ed": 59, "text": "convolutional neural networks"}, {"st": 74, "ed": 76, "text": "image features"}, {"st": 84, "ed": 86, "text": "recognition accuracy"}]
[{"st": 14, "ed": 16, "text": "gravitational wave"}, {"st": 45, "ed": 47, "text": "gaussian noise"}, {"st": 68, "ed": 70, "text": "gravitational wave"}, {"st": 93, "ed": 95, "text": "transfer learning"}, {"st": 102, "ed": 104, "text": "deep learning"}, {"st": 107, "ed": 109, "text": "real world"}, {"st": 109, "ed": 111, "text": "object recognition"}, {"st": 118, "ed": 120, "text": "time series"}, {"st": 151, "ed": 155, "text": "deep convolutional neural networks"}, {"st": 161, "ed": 163, "text": "significantly reduces"}, {"st": 180, "ed": 182, "text": "precision recall"}, {"st": 199, "ed": 201, "text": "labeled examples"}, {"st": 207, "ed": 209, "text": "transfer learning"}, {"st": 213, "ed": 216, "text": "convolutional neural networks"}, {"st": 223, "ed": 225, "text": "feature extractors"}, {"st": 226, "ed": 228, "text": "unsupervised clustering"}, {"st": 252, "ed": 254, "text": "gravitational wave"}]
[{"st": 4, "ed": 6, "text": "e commerce"}, {"st": 15, "ed": 17, "text": "data mining"}, {"st": 52, "ed": 54, "text": "pre processing"}, {"st": 80, "ed": 82, "text": "application server"}, {"st": 85, "ed": 87, "text": "web server"}, {"st": 112, "ed": 114, "text": "transaction processing"}, {"st": 134, "ed": 136, "text": "multiple views"}]
[{"st": 12, "ed": 14, "text": "probability density"}, {"st": 18, "ed": 20, "text": "feature spaces"}, {"st": 41, "ed": 44, "text": "probability density function"}, {"st": 54, "ed": 56, "text": "class classification"}]
[{"st": 29, "ed": 31, "text": "evolutionary algorithm"}]
[{"st": 44, "ed": 46, "text": "efficiently learn"}, {"st": 87, "ed": 89, "text": "additive models"}, {"st": 128, "ed": 130, "text": "b spline"}, {"st": 152, "ed": 154, "text": "learning algorithm"}, {"st": 166, "ed": 168, "text": "additive models"}, {"st": 192, "ed": 194, "text": "large scale"}, {"st": 194, "ed": 196, "text": "image classification"}, {"st": 216, "ed": 219, "text": "orders of magnitude"}]
[{"st": 8, "ed": 10, "text": "machine learning"}, {"st": 37, "ed": 39, "text": "learning problems"}]
[{"st": 2, "ed": 4, "text": "wide variety"}, {"st": 5, "ed": 7, "text": "regularization methods"}, {"st": 32, "ed": 34, "text": "regularization parameter"}, {"st": 89, "ed": 91, "text": "machine learning"}, {"st": 115, "ed": 117, "text": "low rank"}, {"st": 143, "ed": 145, "text": "market research"}]
[{"st": 24, "ed": 26, "text": "domain knowledge"}, {"st": 43, "ed": 45, "text": "domain knowledge"}, {"st": 48, "ed": 50, "text": "learning algorithms"}, {"st": 68, "ed": 70, "text": "domain knowledge"}]
[{"st": 7, "ed": 9, "text": "multi class"}, {"st": 9, "ed": 11, "text": "image segmentation"}, {"st": 14, "ed": 17, "text": "conditional random fields"}, {"st": 50, "ed": 52, "text": "fully connected"}, {"st": 73, "ed": 75, "text": "inference algorithms"}, {"st": 77, "ed": 79, "text": "main contribution"}, {"st": 81, "ed": 83, "text": "highly efficient"}, {"st": 83, "ed": 85, "text": "approximate inference"}, {"st": 87, "ed": 89, "text": "fully connected"}, {"st": 101, "ed": 103, "text": "linear combination"}, {"st": 107, "ed": 109, "text": "experiments demonstrate"}, {"st": 116, "ed": 118, "text": "substantially improves"}]
[{"st": 10, "ed": 12, "text": "learning algorithm"}, {"st": 16, "ed": 18, "text": "handwritten digit"}, {"st": 93, "ed": 95, "text": "learning algorithm"}]
[{"st": 56, "ed": 58, "text": "spatial relations"}, {"st": 103, "ed": 105, "text": "generative model"}, {"st": 107, "ed": 109, "text": "weakly supervised"}, {"st": 148, "ed": 152, "text": "markov chain monte carlo"}, {"st": 154, "ed": 156, "text": "learning algorithm"}, {"st": 158, "ed": 160, "text": "semantically meaningful"}, {"st": 163, "ed": 165, "text": "rgb d"}]
[{"st": 5, "ed": 7, "text": "feature learning"}, {"st": 17, "ed": 19, "text": "auto encoders"}, {"st": 24, "ed": 27, "text": "convolutional neural network"}, {"st": 141, "ed": 143, "text": "learned features"}, {"st": 145, "ed": 147, "text": "pre training"}, {"st": 149, "ed": 152, "text": "detection and segmentation"}]
[{"st": 9, "ed": 11, "text": "structured prediction"}, {"st": 15, "ed": 17, "text": "convex combination"}, {"st": 19, "ed": 21, "text": "log loss"}, {"st": 22, "ed": 25, "text": "conditional random fields"}, {"st": 29, "ed": 31, "text": "hinge loss"}, {"st": 32, "ed": 35, "text": "support vector machines"}, {"st": 39, "ed": 42, "text": "a sufficient condition"}, {"st": 94, "ed": 96, "text": "demonstrate empirically"}, {"st": 132, "ed": 134, "text": "empirical comparison"}, {"st": 140, "ed": 142, "text": "margin based"}]
[{"st": 0, "ed": 2, "text": "sparse representation"}, {"st": 42, "ed": 45, "text": "l 1 norm"}, {"st": 45, "ed": 47, "text": "based regularization"}, {"st": 53, "ed": 55, "text": "l 2"}, {"st": 79, "ed": 81, "text": "classification task"}, {"st": 132, "ed": 134, "text": "scoring function"}, {"st": 143, "ed": 145, "text": "dataset size"}, {"st": 166, "ed": 169, "text": "taking into account"}, {"st": 170, "ed": 172, "text": "sparse representation"}]
[{"st": 19, "ed": 21, "text": "neural networks"}, {"st": 22, "ed": 24, "text": "image classification"}, {"st": 32, "ed": 35, "text": "rectified linear unit"}, {"st": 50, "ed": 52, "text": "computational cost"}, {"st": 100, "ed": 102, "text": "top 5"}, {"st": 114, "ed": 116, "text": "relative improvement"}, {"st": 143, "ed": 145, "text": "visual recognition"}]
[{"st": 29, "ed": 31, "text": "unified framework"}, {"st": 54, "ed": 56, "text": "contextual information"}, {"st": 81, "ed": 83, "text": "exact inference"}, {"st": 100, "ed": 102, "text": "graphical model"}, {"st": 107, "ed": 110, "text": "support vector machine"}, {"st": 122, "ed": 124, "text": "latent variables"}, {"st": 140, "ed": 142, "text": "benchmark datasets"}]
[{"st": 0, "ed": 5, "text": "deep convolutional neural networks cnns"}, {"st": 7, "ed": 9, "text": "excellent performance"}, {"st": 10, "ed": 12, "text": "image classification"}, {"st": 19, "ed": 21, "text": "object detection"}, {"st": 33, "ed": 35, "text": "detection algorithms"}, {"st": 127, "ed": 129, "text": "object detection"}, {"st": 177, "ed": 180, "text": "takes advantage of"}]
[{"st": 1, "ed": 3, "text": "statistical models"}, {"st": 4, "ed": 6, "text": "structured prediction"}, {"st": 19, "ed": 21, "text": "low order"}, {"st": 24, "ed": 26, "text": "exact inference"}, {"st": 37, "ed": 39, "text": "approximate inference"}, {"st": 49, "ed": 51, "text": "representational power"}, {"st": 63, "ed": 66, "text": "probabilistic graphical models"}, {"st": 66, "ed": 68, "text": "large margin"}, {"st": 72, "ed": 74, "text": "large margin"}, {"st": 75, "ed": 77, "text": "belief networks"}, {"st": 87, "ed": 89, "text": "fast inference"}, {"st": 129, "ed": 131, "text": "structured prediction"}, {"st": 141, "ed": 143, "text": "fully connected"}, {"st": 145, "ed": 147, "text": "multi label"}, {"st": 147, "ed": 149, "text": "scene classification"}, {"st": 153, "ed": 155, "text": "proposed approach"}, {"st": 157, "ed": 160, "text": "significant performance gains"}]
[{"st": 29, "ed": 31, "text": "face detection"}, {"st": 103, "ed": 105, "text": "benchmark datasets"}]
[{"st": 4, "ed": 6, "text": "weakly labeled"}, {"st": 12, "ed": 14, "text": "computer vision"}, {"st": 23, "ed": 25, "text": "labeled data"}, {"st": 45, "ed": 47, "text": "visual concepts"}, {"st": 48, "ed": 50, "text": "weakly labeled"}, {"st": 81, "ed": 83, "text": "image level"}, {"st": 93, "ed": 95, "text": "domain specific"}, {"st": 103, "ed": 105, "text": "scene recognition"}, {"st": 110, "ed": 112, "text": "object detection"}, {"st": 118, "ed": 120, "text": "promising performance"}, {"st": 122, "ed": 124, "text": "fully supervised"}, {"st": 125, "ed": 127, "text": "weakly supervised"}]
[{"st": 12, "ed": 14, "text": "error bounds"}, {"st": 17, "ed": 19, "text": "highly efficient"}, {"st": 66, "ed": 68, "text": "cost sensitive"}, {"st": 87, "ed": 89, "text": "search procedure"}, {"st": 94, "ed": 96, "text": "training phase"}, {"st": 102, "ed": 105, "text": "synthetic and real"}, {"st": 120, "ed": 122, "text": "cost sensitive"}, {"st": 126, "ed": 128, "text": "cost sensitive"}, {"st": 144, "ed": 146, "text": "weak classifiers"}]
[{"st": 2, "ed": 4, "text": "vision based"}, {"st": 4, "ed": 6, "text": "reinforcement learning"}, {"st": 10, "ed": 12, "text": "atari games"}, {"st": 22, "ed": 24, "text": "spatio temporal"}, {"st": 24, "ed": 26, "text": "prediction problems"}, {"st": 50, "ed": 52, "text": "atari games"}, {"st": 94, "ed": 97, "text": "propose and evaluate"}, {"st": 98, "ed": 101, "text": "deep neural network"}, {"st": 114, "ed": 117, "text": "convolutional neural networks"}]
[{"st": 0, "ed": 2, "text": "recent works"}, {"st": 4, "ed": 6, "text": "scale invariance"}, {"st": 16, "ed": 18, "text": "deep network"}, {"st": 29, "ed": 32, "text": "stochastic gradient descent"}, {"st": 42, "ed": 44, "text": "deep network"}, {"st": 47, "ed": 49, "text": "batch normalization"}, {"st": 50, "ed": 52, "text": "max pooling"}, {"st": 101, "ed": 104, "text": "stochastic gradient descent"}, {"st": 111, "ed": 113, "text": "empirical evidence"}, {"st": 116, "ed": 118, "text": "mnist dataset"}, {"st": 132, "ed": 134, "text": "batch normalization"}, {"st": 135, "ed": 137, "text": "without sacrificing"}, {"st": 138, "ed": 140, "text": "computational efficiency"}]
[{"st": 0, "ed": 2, "text": "recent works"}, {"st": 4, "ed": 6, "text": "scale invariance"}, {"st": 18, "ed": 20, "text": "deep network"}, {"st": 32, "ed": 35, "text": "stochastic gradient descent"}, {"st": 47, "ed": 49, "text": "deep networks"}, {"st": 55, "ed": 57, "text": "max pooling"}, {"st": 84, "ed": 86, "text": "weight updates"}, {"st": 87, "ed": 90, "text": "stochastic gradient descent"}, {"st": 93, "ed": 95, "text": "empirical evidence"}, {"st": 98, "ed": 100, "text": "mnist dataset"}, {"st": 108, "ed": 110, "text": "without sacrificing"}, {"st": 111, "ed": 113, "text": "computational efficiency"}, {"st": 129, "ed": 131, "text": "weight updates"}, {"st": 133, "ed": 135, "text": "image segmentation"}]
[{"st": 8, "ed": 10, "text": "generate realistic"}, {"st": 28, "ed": 32, "text": "convolutional neural network cnn"}, {"st": 69, "ed": 71, "text": "raw image"}, {"st": 86, "ed": 88, "text": "high level"}, {"st": 110, "ed": 114, "text": "recurrent neural network language"}, {"st": 116, "ed": 118, "text": "pre trained"}, {"st": 144, "ed": 146, "text": "image features"}, {"st": 147, "ed": 149, "text": "existing approaches"}, {"st": 164, "ed": 166, "text": "prediction performance"}]
[{"st": 6, "ed": 10, "text": "functional magnetic resonance imaging"}, {"st": 54, "ed": 56, "text": "time series"}, {"st": 139, "ed": 141, "text": "time series"}, {"st": 149, "ed": 151, "text": "linear combination"}, {"st": 153, "ed": 155, "text": "time series"}, {"st": 158, "ed": 160, "text": "nearest neighbors"}, {"st": 176, "ed": 178, "text": "edge weights"}, {"st": 187, "ed": 189, "text": "linear regression"}, {"st": 193, "ed": 195, "text": "edge weights"}, {"st": 219, "ed": 222, "text": "visual object recognition"}, {"st": 228, "ed": 231, "text": "support vector machines"}, {"st": 237, "ed": 239, "text": "edge weights"}, {"st": 249, "ed": 251, "text": "edge weights"}]
[{"st": 4, "ed": 6, "text": "based approach"}, {"st": 27, "ed": 30, "text": "convolutional neural network"}, {"st": 135, "ed": 137, "text": "experimental evaluation"}, {"st": 140, "ed": 142, "text": "method achieves"}]
[{"st": 0, "ed": 2, "text": "domain adaptation"}, {"st": 44, "ed": 47, "text": "discrete and continuous"}, {"st": 57, "ed": 59, "text": "domain adaptation"}, {"st": 61, "ed": 63, "text": "semi supervised"}, {"st": 147, "ed": 149, "text": "synthetic datasets"}, {"st": 151, "ed": 153, "text": "real world"}, {"st": 178, "ed": 180, "text": "domain adaptation"}]
[{"st": 9, "ed": 11, "text": "reinforcement learning"}, {"st": 31, "ed": 34, "text": "deep reinforcement learning"}, {"st": 61, "ed": 63, "text": "partial observability"}, {"st": 98, "ed": 100, "text": "conceptually simple"}, {"st": 122, "ed": 124, "text": "generalization performance"}]
[{"st": 0, "ed": 2, "text": "image generation"}, {"st": 7, "ed": 9, "text": "artificial intelligence"}, {"st": 12, "ed": 14, "text": "deep learning"}, {"st": 17, "ed": 21, "text": "generative adversarial network gan"}, {"st": 37, "ed": 40, "text": "generative adversarial network"}, {"st": 117, "ed": 119, "text": "generative model"}]
[{"st": 29, "ed": 31, "text": "challenging problem"}, {"st": 100, "ed": 102, "text": "visual field"}]
[{"st": 4, "ed": 6, "text": "spatio temporal"}, {"st": 6, "ed": 8, "text": "learning framework"}, {"st": 29, "ed": 31, "text": "spatio temporal"}, {"st": 63, "ed": 65, "text": "visual arts"}]
[{"st": 0, "ed": 4, "text": "markov random fields mrfs"}, {"st": 59, "ed": 61, "text": "fully connected"}, {"st": 70, "ed": 72, "text": "theoretical analysis"}, {"st": 81, "ed": 84, "text": "recurrent neural networks"}, {"st": 89, "ed": 91, "text": "feed forward"}, {"st": 103, "ed": 105, "text": "expressive power"}, {"st": 106, "ed": 109, "text": "deep neural networks"}, {"st": 112, "ed": 114, "text": "dependency structure"}, {"st": 129, "ed": 131, "text": "feed forward"}, {"st": 147, "ed": 149, "text": "low level"}, {"st": 149, "ed": 151, "text": "vision tasks"}]
[{"st": 76, "ed": 79, "text": "generative adversarial network"}, {"st": 81, "ed": 84, "text": "recurrent neural networks"}, {"st": 112, "ed": 114, "text": "realistic images"}, {"st": 175, "ed": 177, "text": "frame prediction"}, {"st": 179, "ed": 181, "text": "improved performance"}]
[{"st": 1, "ed": 3, "text": "key challenge"}, {"st": 45, "ed": 47, "text": "reinforcement learning"}, {"st": 67, "ed": 69, "text": "predictive models"}, {"st": 90, "ed": 92, "text": "video prediction"}, {"st": 113, "ed": 115, "text": "training set"}, {"st": 129, "ed": 131, "text": "real robot"}]
[{"st": 0, "ed": 2, "text": "face recognition"}, {"st": 59, "ed": 61, "text": "deep learning"}, {"st": 73, "ed": 75, "text": "soft margin"}, {"st": 88, "ed": 90, "text": "domain adaptation"}, {"st": 103, "ed": 105, "text": "real world"}]
[{"st": 66, "ed": 68, "text": "practical scenarios"}, {"st": 78, "ed": 80, "text": "image processing"}, {"st": 104, "ed": 106, "text": "fuzzy inference"}, {"st": 130, "ed": 132, "text": "time consuming"}, {"st": 141, "ed": 143, "text": "fuzzy inference"}, {"st": 149, "ed": 151, "text": "fuzzy inference"}, {"st": 161, "ed": 163, "text": "neural network"}, {"st": 178, "ed": 180, "text": "fuzzy inference"}, {"st": 185, "ed": 187, "text": "fuzzy clustering"}, {"st": 189, "ed": 191, "text": "proposed framework"}, {"st": 214, "ed": 216, "text": "fuzzy inference"}, {"st": 221, "ed": 223, "text": "proposed method"}, {"st": 227, "ed": 229, "text": "fuzzy inference"}]
[{"st": 1, "ed": 3, "text": "deep learning"}, {"st": 11, "ed": 14, "text": "human activity recognition"}, {"st": 38, "ed": 41, "text": "deep neural networks"}, {"st": 44, "ed": 46, "text": "real life"}, {"st": 51, "ed": 53, "text": "based approaches"}, {"st": 77, "ed": 79, "text": "real life"}, {"st": 82, "ed": 84, "text": "imbalanced datasets"}, {"st": 93, "ed": 95, "text": "activity recognition"}, {"st": 108, "ed": 113, "text": "long short term memory lstm"}, {"st": 121, "ed": 123, "text": "lstm networks"}, {"st": 142, "ed": 144, "text": "deep lstm"}, {"st": 152, "ed": 154, "text": "extensive experimental"}, {"st": 157, "ed": 159, "text": "standard benchmarks"}, {"st": 175, "ed": 177, "text": "real life"}]
[{"st": 35, "ed": 37, "text": "key challenges"}, {"st": 50, "ed": 52, "text": "moving objects"}, {"st": 59, "ed": 61, "text": "existing approaches"}, {"st": 72, "ed": 74, "text": "object localization"}, {"st": 74, "ed": 76, "text": "instance segmentation"}, {"st": 88, "ed": 90, "text": "semi supervised"}, {"st": 90, "ed": 92, "text": "object tracking"}, {"st": 124, "ed": 126, "text": "precision agriculture"}, {"st": 131, "ed": 133, "text": "early detection"}]
[{"st": 20, "ed": 23, "text": "deep neural networks"}, {"st": 43, "ed": 45, "text": "decision making"}, {"st": 74, "ed": 76, "text": "based methods"}, {"st": 87, "ed": 89, "text": "decision making"}, {"st": 92, "ed": 95, "text": "quantitative and qualitative"}, {"st": 118, "ed": 120, "text": "decision making"}]
[{"st": 4, "ed": 6, "text": "big data"}, {"st": 6, "ed": 9, "text": "k means clustering"}, {"st": 11, "ed": 13, "text": "widely adopted"}, {"st": 23, "ed": 25, "text": "computational cost"}, {"st": 48, "ed": 50, "text": "k means"}, {"st": 72, "ed": 74, "text": "k means"}, {"st": 79, "ed": 81, "text": "k means"}, {"st": 86, "ed": 89, "text": "k nearest neighbors"}, {"st": 92, "ed": 94, "text": "k means"}, {"st": 105, "ed": 107, "text": "nearest neighbors"}, {"st": 112, "ed": 114, "text": "nearest neighbors"}, {"st": 145, "ed": 148, "text": "k nearest neighbor"}, {"st": 156, "ed": 158, "text": "k means"}, {"st": 163, "ed": 165, "text": "k means"}, {"st": 167, "ed": 169, "text": "proposed algorithm"}]
[{"st": 46, "ed": 48, "text": "facial expression"}]
[{"st": 10, "ed": 12, "text": "action recognition"}, {"st": 15, "ed": 17, "text": "visible spectrum"}, {"st": 23, "ed": 25, "text": "lighting conditions"}, {"st": 30, "ed": 32, "text": "action recognition"}, {"st": 37, "ed": 39, "text": "visible spectrum"}, {"st": 41, "ed": 44, "text": "received much attention"}, {"st": 58, "ed": 60, "text": "imaging data"}, {"st": 65, "ed": 67, "text": "action recognition"}, {"st": 78, "ed": 83, "text": "convolutional neural network cnn architecture"}, {"st": 106, "ed": 108, "text": "optical flow"}, {"st": 118, "ed": 120, "text": "visible spectrum"}, {"st": 163, "ed": 165, "text": "weighted average"}, {"st": 169, "ed": 171, "text": "neural nets"}, {"st": 219, "ed": 221, "text": "optical flow"}]
[{"st": 5, "ed": 7, "text": "artificial intelligence"}, {"st": 10, "ed": 12, "text": "multiple tasks"}, {"st": 41, "ed": 43, "text": "real world"}, {"st": 61, "ed": 64, "text": "short term memory"}, {"st": 71, "ed": 73, "text": "deep generative"}, {"st": 86, "ed": 88, "text": "deep generative"}, {"st": 102, "ed": 104, "text": "training data"}, {"st": 105, "ed": 107, "text": "previous tasks"}, {"st": 129, "ed": 131, "text": "image classification"}]
[{"st": 3, "ed": 5, "text": "conditional generative"}, {"st": 8, "ed": 10, "text": "low dimensional"}, {"st": 12, "ed": 14, "text": "multiple modalities"}, {"st": 19, "ed": 21, "text": "latent space"}, {"st": 39, "ed": 41, "text": "constrained optimization"}, {"st": 82, "ed": 84, "text": "latent space"}, {"st": 95, "ed": 97, "text": "latent space"}, {"st": 103, "ed": 105, "text": "latent spaces"}, {"st": 111, "ed": 113, "text": "objective function"}]
[{"st": 8, "ed": 13, "text": "deep convolutional neural network cnn"}, {"st": 17, "ed": 20, "text": "degrees of freedom"}, {"st": 37, "ed": 39, "text": "existing approaches"}, {"st": 46, "ed": 48, "text": "feature extraction"}, {"st": 121, "ed": 123, "text": "rgb image"}, {"st": 147, "ed": 150, "text": "compares favorably to"}]
[{"st": 12, "ed": 14, "text": "recently achieved"}, {"st": 35, "ed": 37, "text": "hyperparameter tuning"}, {"st": 46, "ed": 48, "text": "min max"}, {"st": 51, "ed": 53, "text": "optimization problem"}, {"st": 77, "ed": 79, "text": "empirical results"}, {"st": 110, "ed": 112, "text": "min max"}, {"st": 134, "ed": 136, "text": "point clouds"}, {"st": 144, "ed": 146, "text": "domain adaptation"}, {"st": 157, "ed": 159, "text": "iterative procedure"}]
[{"st": 0, "ed": 2, "text": "object detection"}, {"st": 35, "ed": 37, "text": "lighting conditions"}, {"st": 47, "ed": 49, "text": "rgb d"}, {"st": 62, "ed": 64, "text": "object detection"}, {"st": 85, "ed": 89, "text": "convolutional neural network cnn"}, {"st": 92, "ed": 94, "text": "multiple modalities"}, {"st": 120, "ed": 122, "text": "rgb d"}, {"st": 145, "ed": 147, "text": "rgb d"}]
[{"st": 0, "ed": 3, "text": "deep neural networks"}, {"st": 23, "ed": 25, "text": "specific task"}, {"st": 61, "ed": 64, "text": "deep neural networks"}, {"st": 119, "ed": 121, "text": "catastrophic forgetting"}, {"st": 145, "ed": 147, "text": "small scale"}, {"st": 169, "ed": 171, "text": "catastrophic forgetting"}, {"st": 172, "ed": 174, "text": "neural networks"}, {"st": 185, "ed": 187, "text": "real world"}, {"st": 220, "ed": 222, "text": "catastrophic forgetting"}]
[{"st": 0, "ed": 2, "text": "previous research"}, {"st": 7, "ed": 9, "text": "facial expressions"}, {"st": 59, "ed": 61, "text": "context dependent"}, {"st": 95, "ed": 97, "text": "neural network"}, {"st": 99, "ed": 101, "text": "gaussian process"}, {"st": 118, "ed": 120, "text": "hand crafted"}, {"st": 130, "ed": 132, "text": "benchmark dataset"}, {"st": 154, "ed": 156, "text": "intra class"}, {"st": 183, "ed": 185, "text": "existing models"}]
[{"st": 45, "ed": 47, "text": "key idea"}, {"st": 113, "ed": 115, "text": "prediction model"}, {"st": 165, "ed": 167, "text": "virtual reality"}, {"st": 208, "ed": 210, "text": "performance improvement"}]
[{"st": 8, "ed": 10, "text": "generative models"}, {"st": 62, "ed": 64, "text": "time consuming"}, {"st": 68, "ed": 70, "text": "transfer learning"}, {"st": 79, "ed": 81, "text": "transfer knowledge"}, {"st": 96, "ed": 98, "text": "transfer learning"}, {"st": 193, "ed": 196, "text": "publicly available datasets"}, {"st": 200, "ed": 202, "text": "approach outperforms"}]
[{"st": 7, "ed": 9, "text": "neural networks"}, {"st": 44, "ed": 46, "text": "adversarial networks"}, {"st": 49, "ed": 51, "text": "loss function"}, {"st": 52, "ed": 54, "text": "transfer knowledge"}, {"st": 59, "ed": 61, "text": "proposed method"}, {"st": 76, "ed": 78, "text": "network size"}, {"st": 95, "ed": 97, "text": "classification accuracy"}]
[{"st": 13, "ed": 16, "text": "end to end"}, {"st": 22, "ed": 25, "text": "end to end"}, {"st": 38, "ed": 41, "text": "end to end"}, {"st": 78, "ed": 81, "text": "end to end"}, {"st": 99, "ed": 101, "text": "object detection"}, {"st": 105, "ed": 108, "text": "end to end"}]
[{"st": 19, "ed": 21, "text": "challenging tasks"}, {"st": 64, "ed": 66, "text": "multi label"}, {"st": 104, "ed": 107, "text": "zero shot learning"}, {"st": 136, "ed": 138, "text": "real world"}, {"st": 138, "ed": 141, "text": "human action recognition"}, {"st": 144, "ed": 146, "text": "multi label"}, {"st": 146, "ed": 149, "text": "zero shot learning"}, {"st": 149, "ed": 152, "text": "problem and propose"}, {"st": 172, "ed": 174, "text": "multi label"}, {"st": 178, "ed": 180, "text": "side information"}, {"st": 201, "ed": 203, "text": "latent embedding"}, {"st": 205, "ed": 207, "text": "multi label"}, {"st": 207, "ed": 209, "text": "zero shot"}, {"st": 214, "ed": 216, "text": "latent embedding"}, {"st": 224, "ed": 226, "text": "temporal coherence"}, {"st": 255, "ed": 257, "text": "multi label"}, {"st": 257, "ed": 260, "text": "zero shot learning"}, {"st": 264, "ed": 266, "text": "multi label"}, {"st": 282, "ed": 284, "text": "multi label"}, {"st": 284, "ed": 286, "text": "zero shot"}]
[{"st": 29, "ed": 31, "text": "pattern recognition"}, {"st": 46, "ed": 48, "text": "pattern recognition"}, {"st": 70, "ed": 74, "text": "convolutional neural network cnn"}, {"st": 78, "ed": 80, "text": "pattern recognition"}, {"st": 88, "ed": 90, "text": "input images"}, {"st": 105, "ed": 107, "text": "problems involving"}, {"st": 118, "ed": 120, "text": "top 5"}]
[{"st": 20, "ed": 22, "text": "collected data"}, {"st": 34, "ed": 36, "text": "collected data"}, {"st": 51, "ed": 53, "text": "predictive model"}, {"st": 72, "ed": 74, "text": "open world"}, {"st": 125, "ed": 127, "text": "key challenge"}, {"st": 148, "ed": 150, "text": "prediction model"}, {"st": 198, "ed": 200, "text": "multiple objects"}]
[{"st": 19, "ed": 21, "text": "autonomous driving"}, {"st": 68, "ed": 70, "text": "domain shift"}, {"st": 79, "ed": 81, "text": "source domain"}, {"st": 97, "ed": 99, "text": "baseline methods"}]
[{"st": 12, "ed": 15, "text": "deep neural networks"}, {"st": 54, "ed": 56, "text": "field theory"}, {"st": 111, "ed": 113, "text": "residual networks"}, {"st": 149, "ed": 151, "text": "quantum field"}, {"st": 158, "ed": 160, "text": "quantum field"}, {"st": 191, "ed": 193, "text": "phase transition"}]
[{"st": 5, "ed": 7, "text": "magnetic resonance"}, {"st": 14, "ed": 17, "text": "convolutional neural network"}, {"st": 19, "ed": 21, "text": "deep learning"}, {"st": 66, "ed": 68, "text": "myocardial infarction"}, {"st": 73, "ed": 75, "text": "model based"}, {"st": 90, "ed": 92, "text": "u net"}, {"st": 92, "ed": 95, "text": "convolutional neural network"}, {"st": 120, "ed": 122, "text": "input data"}, {"st": 134, "ed": 136, "text": "u net"}, {"st": 156, "ed": 158, "text": "myocardial infarction"}, {"st": 162, "ed": 165, "text": "acute myocardial infarction"}, {"st": 204, "ed": 208, "text": "signal to noise ratio"}, {"st": 217, "ed": 219, "text": "deep learning"}, {"st": 240, "ed": 242, "text": "u net"}, {"st": 256, "ed": 258, "text": "model based"}]
[{"st": 6, "ed": 8, "text": "object tracking"}, {"st": 37, "ed": 39, "text": "generative modeling"}, {"st": 48, "ed": 50, "text": "multiple objects"}, {"st": 56, "ed": 58, "text": "external memory"}, {"st": 63, "ed": 65, "text": "external memory"}, {"st": 96, "ed": 98, "text": "conduct experiments"}, {"st": 120, "ed": 122, "text": "method achieves"}]
[{"st": 9, "ed": 11, "text": "challenging task"}, {"st": 26, "ed": 28, "text": "fully supervised"}, {"st": 62, "ed": 64, "text": "lighting conditions"}, {"st": 80, "ed": 82, "text": "original input"}, {"st": 107, "ed": 109, "text": "large scale"}, {"st": 109, "ed": 111, "text": "unlabeled data"}, {"st": 126, "ed": 128, "text": "object categories"}, {"st": 128, "ed": 130, "text": "lighting conditions"}, {"st": 132, "ed": 134, "text": "extensive experiments"}]
[{"st": 32, "ed": 34, "text": "automatically generate"}, {"st": 44, "ed": 46, "text": "input image"}, {"st": 84, "ed": 86, "text": "approach yields"}, {"st": 136, "ed": 138, "text": "significantly improved"}]
[{"st": 12, "ed": 14, "text": "proposed algorithm"}, {"st": 26, "ed": 28, "text": "external memory"}, {"st": 64, "ed": 67, "text": "long term memory"}, {"st": 87, "ed": 89, "text": "large scale"}, {"st": 119, "ed": 121, "text": "object tracking"}, {"st": 143, "ed": 145, "text": "visual tracking"}, {"st": 158, "ed": 160, "text": "top 5"}, {"st": 162, "ed": 165, "text": "accuracy and robustness"}]
[{"st": 8, "ed": 10, "text": "transfer learning"}, {"st": 24, "ed": 26, "text": "key insight"}, {"st": 44, "ed": 46, "text": "similarity function"}, {"st": 52, "ed": 54, "text": "domain adaptation"}, {"st": 66, "ed": 68, "text": "pairwise constraints"}, {"st": 94, "ed": 96, "text": "source domain"}, {"st": 108, "ed": 110, "text": "transfer learning"}, {"st": 116, "ed": 119, "text": "unsupervised domain adaptation"}, {"st": 123, "ed": 125, "text": "loss function"}, {"st": 141, "ed": 143, "text": "similarity metric"}, {"st": 153, "ed": 155, "text": "unsupervised clustering"}, {"st": 189, "ed": 191, "text": "clustering algorithm"}, {"st": 248, "ed": 250, "text": "cross domain"}, {"st": 270, "ed": 272, "text": "doesn t"}, {"st": 282, "ed": 284, "text": "domain adaptation"}]
[{"st": 0, "ed": 2, "text": "adversarial perturbations"}, {"st": 9, "ed": 11, "text": "machine learning"}, {"st": 12, "ed": 14, "text": "recent works"}, {"st": 29, "ed": 31, "text": "existing methods"}, {"st": 129, "ed": 131, "text": "generator network"}, {"st": 137, "ed": 139, "text": "adversarial perturbations"}, {"st": 147, "ed": 149, "text": "wide variety"}, {"st": 153, "ed": 155, "text": "experimental evaluation"}, {"st": 172, "ed": 174, "text": "wide variety"}]
[{"st": 22, "ed": 24, "text": "photo realistic"}, {"st": 49, "ed": 51, "text": "domains including"}, {"st": 55, "ed": 58, "text": "deep reinforcement learning"}, {"st": 58, "ed": 60, "text": "imitation learning"}, {"st": 64, "ed": 67, "text": "visual question answering"}, {"st": 67, "ed": 70, "text": "unsupervised representation learning"}, {"st": 70, "ed": 72, "text": "object detection"}]
[{"st": 0, "ed": 2, "text": "machine learning"}, {"st": 6, "ed": 8, "text": "adversarial perturbations"}, {"st": 31, "ed": 33, "text": "adversarial perturbations"}, {"st": 48, "ed": 50, "text": "existing methods"}, {"st": 56, "ed": 58, "text": "task specific"}, {"st": 63, "ed": 65, "text": "training data"}, {"st": 119, "ed": 121, "text": "extracted features"}, {"st": 137, "ed": 139, "text": "vision tasks"}, {"st": 141, "ed": 143, "text": "object recognition"}, {"st": 204, "ed": 206, "text": "deep learning"}, {"st": 218, "ed": 220, "text": "multiple tasks"}, {"st": 224, "ed": 226, "text": "training data"}]
[{"st": 6, "ed": 9, "text": "convolutional neural network"}, {"st": 30, "ed": 33, "text": "high computational cost"}, {"st": 66, "ed": 68, "text": "computational efficiency"}, {"st": 91, "ed": 93, "text": "low cost"}, {"st": 100, "ed": 102, "text": "standard datasets"}, {"st": 125, "ed": 130, "text": "cifar 10 and cifar 100"}]
[{"st": 7, "ed": 10, "text": "optical character recognition"}, {"st": 13, "ed": 15, "text": "deep learning"}, {"st": 16, "ed": 19, "text": "vulnerable to adversarial"}]
[{"st": 4, "ed": 6, "text": "artificial intelligence"}, {"st": 31, "ed": 33, "text": "visual reasoning"}, {"st": 36, "ed": 38, "text": "cognitive psychology"}, {"st": 45, "ed": 47, "text": "visual question"}, {"st": 90, "ed": 92, "text": "deep learning"}, {"st": 97, "ed": 99, "text": "deep learning"}, {"st": 139, "ed": 141, "text": "zero shot"}, {"st": 150, "ed": 152, "text": "network architectures"}]
[{"st": 14, "ed": 16, "text": "previous methods"}, {"st": 30, "ed": 32, "text": "large scale"}, {"st": 60, "ed": 63, "text": "multiple instance learning"}, {"st": 67, "ed": 69, "text": "building blocks"}, {"st": 71, "ed": 73, "text": "large scale"}, {"st": 87, "ed": 89, "text": "shared space"}, {"st": 128, "ed": 130, "text": "extensive experiments"}, {"st": 131, "ed": 133, "text": "significant improvements"}]
[{"st": 13, "ed": 15, "text": "pattern recognition"}, {"st": 21, "ed": 23, "text": "learning algorithms"}, {"st": 30, "ed": 32, "text": "pattern recognition"}, {"st": 84, "ed": 86, "text": "pattern recognition"}, {"st": 97, "ed": 99, "text": "conditional independence"}, {"st": 134, "ed": 136, "text": "learning algorithms"}, {"st": 144, "ed": 146, "text": "classification error"}]
[{"st": 86, "ed": 88, "text": "mathematical model"}, {"st": 110, "ed": 112, "text": "invariant features"}]
[{"st": 5, "ed": 7, "text": "building block"}, {"st": 30, "ed": 32, "text": "sparse representation"}, {"st": 42, "ed": 44, "text": "existing models"}, {"st": 54, "ed": 56, "text": "preliminary experiments"}, {"st": 65, "ed": 67, "text": "object recognition"}]
[{"st": 8, "ed": 10, "text": "important role"}, {"st": 25, "ed": 27, "text": "simplicial complex"}, {"st": 46, "ed": 48, "text": "simplicial complex"}, {"st": 50, "ed": 52, "text": "unlabeled samples"}, {"st": 97, "ed": 99, "text": "promising performance"}]
[{"st": 10, "ed": 12, "text": "complex network"}, {"st": 16, "ed": 18, "text": "clustering algorithms"}, {"st": 29, "ed": 32, "text": "k nearest neighbor"}, {"st": 38, "ed": 40, "text": "directed graph"}, {"st": 67, "ed": 69, "text": "complex network"}, {"st": 81, "ed": 84, "text": "each data point"}, {"st": 91, "ed": 94, "text": "k nearest neighbors"}, {"st": 139, "ed": 142, "text": "each data point"}, {"st": 212, "ed": 215, "text": "rates of convergence"}, {"st": 216, "ed": 218, "text": "clustering algorithms"}]
[{"st": 8, "ed": 10, "text": "hidden state"}, {"st": 29, "ed": 31, "text": "generative framework"}, {"st": 68, "ed": 70, "text": "online learning"}]
[{"st": 1, "ed": 3, "text": "empirical study"}, {"st": 11, "ed": 13, "text": "boosting algorithms"}, {"st": 22, "ed": 24, "text": "multi class"}, {"st": 48, "ed": 50, "text": "classification algorithms"}, {"st": 52, "ed": 54, "text": "neural nets"}, {"st": 65, "ed": 67, "text": "experiment results"}, {"st": 103, "ed": 105, "text": "boosting algorithms"}, {"st": 118, "ed": 120, "text": "deep learning"}, {"st": 123, "ed": 125, "text": "boosting algorithms"}]
[{"st": 24, "ed": 26, "text": "relevant features"}, {"st": 27, "ed": 29, "text": "feature selection"}, {"st": 37, "ed": 39, "text": "feature importance"}, {"st": 56, "ed": 58, "text": "feature importance"}, {"st": 62, "ed": 64, "text": "neural networks"}, {"st": 74, "ed": 76, "text": "feature importance"}, {"st": 78, "ed": 80, "text": "correlation coefficient"}, {"st": 90, "ed": 94, "text": "feed forward neural network"}, {"st": 102, "ed": 104, "text": "empirical evaluation"}, {"st": 119, "ed": 121, "text": "feature selection"}]
[{"st": 4, "ed": 6, "text": "topic modeling"}, {"st": 12, "ed": 14, "text": "higher order"}, {"st": 42, "ed": 44, "text": "topic models"}, {"st": 48, "ed": 50, "text": "higher order"}, {"st": 55, "ed": 58, "text": "markov random field"}, {"st": 69, "ed": 73, "text": "latent dirichlet allocation lda"}, {"st": 74, "ed": 76, "text": "topic models"}, {"st": 84, "ed": 87, "text": "loopy belief propagation"}, {"st": 90, "ed": 92, "text": "approximate inference"}, {"st": 110, "ed": 112, "text": "higher order"}, {"st": 139, "ed": 142, "text": "extensive experimental results"}, {"st": 146, "ed": 148, "text": "higher order"}, {"st": 156, "ed": 158, "text": "topic modeling"}, {"st": 167, "ed": 169, "text": "topic models"}, {"st": 171, "ed": 174, "text": "text and image"}, {"st": 183, "ed": 185, "text": "link prediction"}, {"st": 185, "ed": 187, "text": "document classification"}]
[{"st": 20, "ed": 22, "text": "mid level"}, {"st": 90, "ed": 92, "text": "clustering problem"}, {"st": 102, "ed": 104, "text": "iterative procedure"}, {"st": 115, "ed": 117, "text": "cross validation"}, {"st": 135, "ed": 137, "text": "mid level"}, {"st": 137, "ed": 139, "text": "visual representation"}, {"st": 148, "ed": 150, "text": "visual words"}, {"st": 166, "ed": 168, "text": "scene classification"}]
[{"st": 6, "ed": 8, "text": "message passing"}, {"st": 10, "ed": 12, "text": "map estimation"}, {"st": 21, "ed": 23, "text": "message passing"}, {"st": 25, "ed": 27, "text": "special cases"}, {"st": 45, "ed": 47, "text": "message passing"}, {"st": 83, "ed": 85, "text": "higher order"}, {"st": 92, "ed": 94, "text": "real world"}]
[{"st": 12, "ed": 14, "text": "deep learning"}, {"st": 31, "ed": 34, "text": "effectiveness and efficiency"}, {"st": 36, "ed": 38, "text": "convolutional architectures"}, {"st": 70, "ed": 72, "text": "classification performance"}, {"st": 93, "ed": 95, "text": "recently published"}, {"st": 115, "ed": 117, "text": "computational power"}, {"st": 124, "ed": 126, "text": "convolutional architecture"}, {"st": 149, "ed": 151, "text": "computational power"}]
[{"st": 0, "ed": 2, "text": "machine learning"}, {"st": 7, "ed": 9, "text": "computer vision"}, {"st": 16, "ed": 20, "text": "amounts of training data"}, {"st": 30, "ed": 33, "text": "trained from scratch"}, {"st": 59, "ed": 61, "text": "neural networks"}, {"st": 75, "ed": 77, "text": "significantly faster"}, {"st": 78, "ed": 80, "text": "previous methods"}, {"st": 82, "ed": 84, "text": "neural networks"}, {"st": 105, "ed": 107, "text": "feed forward"}, {"st": 184, "ed": 186, "text": "neural network"}]
[{"st": 52, "ed": 54, "text": "vision community"}, {"st": 114, "ed": 116, "text": "based clustering"}, {"st": 158, "ed": 160, "text": "initial results"}, {"st": 168, "ed": 170, "text": "scene classification"}]
[{"st": 5, "ed": 7, "text": "parameter estimation"}, {"st": 8, "ed": 10, "text": "weakly supervised"}, {"st": 13, "ed": 15, "text": "training sample"}, {"st": 32, "ed": 34, "text": "missing information"}, {"st": 42, "ed": 44, "text": "previous methods"}, {"st": 58, "ed": 60, "text": "latent variables"}, {"st": 65, "ed": 67, "text": "accurate predictions"}, {"st": 72, "ed": 74, "text": "latent variables"}, {"st": 94, "ed": 96, "text": "conditional distribution"}, {"st": 102, "ed": 104, "text": "latent variables"}, {"st": 107, "ed": 109, "text": "input output"}, {"st": 121, "ed": 123, "text": "latent variables"}, {"st": 158, "ed": 160, "text": "latent variables"}, {"st": 174, "ed": 176, "text": "loss functions"}, {"st": 179, "ed": 181, "text": "latent variables"}, {"st": 195, "ed": 197, "text": "challenging problems"}, {"st": 197, "ed": 199, "text": "object detection"}]
[{"st": 3, "ed": 5, "text": "statistical learning"}, {"st": 22, "ed": 26, "text": "functional magnetic resonance imaging"}, {"st": 66, "ed": 68, "text": "time series"}, {"st": 74, "ed": 76, "text": "functional connectivity"}, {"st": 111, "ed": 113, "text": "linear regression"}, {"st": 118, "ed": 120, "text": "functional connectivity"}, {"st": 132, "ed": 134, "text": "statistical learning"}, {"st": 136, "ed": 138, "text": "proposed method"}, {"st": 164, "ed": 167, "text": "k nearest neighbour"}, {"st": 167, "ed": 169, "text": "k nn"}, {"st": 170, "ed": 173, "text": "support vector machine"}, {"st": 195, "ed": 197, "text": "classification performance"}]
[{"st": 8, "ed": 10, "text": "ranking based"}, {"st": 28, "ed": 30, "text": "ranking based"}, {"st": 32, "ed": 35, "text": "zero shot learning"}, {"st": 41, "ed": 43, "text": "zero shot"}, {"st": 49, "ed": 51, "text": "pre trained"}, {"st": 62, "ed": 64, "text": "real world"}]
[{"st": 1, "ed": 3, "text": "computational costs"}, {"st": 23, "ed": 25, "text": "dimensionality reduction"}, {"st": 33, "ed": 35, "text": "entire dataset"}, {"st": 37, "ed": 39, "text": "nystr om"}, {"st": 42, "ed": 44, "text": "main challenges"}, {"st": 115, "ed": 117, "text": "local geometry"}, {"st": 155, "ed": 157, "text": "performance improvement"}]
[{"st": 6, "ed": 8, "text": "policy gradient"}, {"st": 10, "ed": 12, "text": "reinforcement learning"}, {"st": 18, "ed": 21, "text": "end to end"}, {"st": 41, "ed": 43, "text": "image captioning"}, {"st": 45, "ed": 47, "text": "reinforcement learning"}, {"st": 63, "ed": 65, "text": "significant gains"}, {"st": 118, "ed": 120, "text": "inference algorithm"}, {"st": 131, "ed": 133, "text": "reward signal"}, {"st": 169, "ed": 171, "text": "directly optimizing"}]
[{"st": 4, "ed": 6, "text": "deep learning"}, {"st": 7, "ed": 9, "text": "object detection"}, {"st": 49, "ed": 51, "text": "visual attention"}, {"st": 62, "ed": 64, "text": "attention model"}, {"st": 66, "ed": 68, "text": "attention model"}, {"st": 101, "ed": 103, "text": "bounding box"}, {"st": 111, "ed": 113, "text": "attention model"}, {"st": 145, "ed": 147, "text": "bounding box"}, {"st": 157, "ed": 159, "text": "pascal voc"}, {"st": 203, "ed": 206, "text": "pascal voc 2007"}]
[{"st": 7, "ed": 9, "text": "unsupervised learning"}, {"st": 10, "ed": 12, "text": "unsupervised learning"}, {"st": 22, "ed": 24, "text": "decision making"}, {"st": 32, "ed": 34, "text": "supervised learning"}]
[{"st": 0, "ed": 3, "text": "support vector machines"}, {"st": 8, "ed": 10, "text": "supervised classification"}, {"st": 13, "ed": 15, "text": "land cover"}, {"st": 22, "ed": 24, "text": "statistical learning"}, {"st": 61, "ed": 63, "text": "classification tasks"}, {"st": 65, "ed": 67, "text": "remote sensing"}, {"st": 101, "ed": 103, "text": "land cover"}, {"st": 127, "ed": 129, "text": "classification accuracy"}]
[{"st": 10, "ed": 12, "text": "computer vision"}, {"st": 26, "ed": 28, "text": "visual features"}, {"st": 31, "ed": 33, "text": "object recognition"}, {"st": 41, "ed": 43, "text": "hierarchical structure"}, {"st": 73, "ed": 75, "text": "feature extractors"}, {"st": 93, "ed": 95, "text": "hierarchical structure"}, {"st": 124, "ed": 126, "text": "hand tuned"}]
[{"st": 0, "ed": 2, "text": "generative models"}, {"st": 30, "ed": 32, "text": "computer vision"}, {"st": 65, "ed": 67, "text": "message passing"}, {"st": 67, "ed": 69, "text": "inference algorithms"}, {"st": 71, "ed": 73, "text": "expectation propagation"}, {"st": 76, "ed": 78, "text": "message passing"}, {"st": 96, "ed": 98, "text": "message passing"}]
[{"st": 2, "ed": 4, "text": "neural network"}, {"st": 8, "ed": 11, "text": "end to end"}, {"st": 28, "ed": 30, "text": "a 7"}, {"st": 105, "ed": 108, "text": "end to end"}, {"st": 127, "ed": 129, "text": "character recognition"}, {"st": 145, "ed": 148, "text": "end to end"}]
[{"st": 12, "ed": 14, "text": "analysis reveals"}, {"st": 26, "ed": 28, "text": "learning algorithm"}, {"st": 35, "ed": 37, "text": "class conditional"}]
[{"st": 0, "ed": 2, "text": "boosting algorithms"}, {"st": 29, "ed": 31, "text": "cost sensitive"}]
[{"st": 19, "ed": 21, "text": "cost sensitive"}, {"st": 52, "ed": 54, "text": "theoretical analysis"}, {"st": 75, "ed": 77, "text": "empirical study"}, {"st": 110, "ed": 112, "text": "cost sensitive"}, {"st": 112, "ed": 114, "text": "weight initialization"}]
[{"st": 5, "ed": 7, "text": "real world"}, {"st": 20, "ed": 22, "text": "vision language"}, {"st": 49, "ed": 51, "text": "point cloud"}, {"st": 51, "ed": 53, "text": "natural language"}, {"st": 60, "ed": 62, "text": "embedding space"}, {"st": 69, "ed": 71, "text": "semantically meaningful"}, {"st": 104, "ed": 106, "text": "pre train"}, {"st": 110, "ed": 112, "text": "fine tune"}, {"st": 114, "ed": 116, "text": "embedding space"}, {"st": 148, "ed": 150, "text": "significant improvements"}, {"st": 166, "ed": 169, "text": "end to end"}]
[{"st": 6, "ed": 8, "text": "hot topic"}, {"st": 10, "ed": 12, "text": "remote sensing"}, {"st": 22, "ed": 24, "text": "feature extraction"}, {"st": 36, "ed": 38, "text": "deep learning"}, {"st": 83, "ed": 85, "text": "proposed framework"}, {"st": 102, "ed": 104, "text": "spatial information"}, {"st": 116, "ed": 118, "text": "classification accuracy"}]
[{"st": 4, "ed": 6, "text": "object recognition"}, {"st": 25, "ed": 27, "text": "invariant representation"}, {"st": 47, "ed": 49, "text": "deep architectures"}, {"st": 55, "ed": 57, "text": "object category"}, {"st": 58, "ed": 60, "text": "deep learning"}, {"st": 88, "ed": 90, "text": "deep architectures"}, {"st": 91, "ed": 95, "text": "convolutional neural networks cnn"}, {"st": 105, "ed": 107, "text": "pose estimation"}, {"st": 133, "ed": 135, "text": "distributed representations"}, {"st": 146, "ed": 148, "text": "object category"}, {"st": 158, "ed": 160, "text": "multi view"}]
[{"st": 48, "ed": 50, "text": "logistic function"}, {"st": 74, "ed": 76, "text": "logistic regression"}, {"st": 95, "ed": 97, "text": "optimization method"}, {"st": 105, "ed": 107, "text": "coordinate descent"}, {"st": 109, "ed": 111, "text": "grid search"}, {"st": 140, "ed": 142, "text": "improved performance"}, {"st": 148, "ed": 150, "text": "hinge loss"}, {"st": 153, "ed": 155, "text": "wide variety"}, {"st": 160, "ed": 162, "text": "uc irvine"}, {"st": 172, "ed": 174, "text": "visual information"}, {"st": 199, "ed": 201, "text": "max margin"}, {"st": 203, "ed": 205, "text": "larger scale"}, {"st": 205, "ed": 207, "text": "benchmark problems"}, {"st": 227, "ed": 230, "text": "support vector machine"}, {"st": 238, "ed": 240, "text": "structured prediction"}]
[{"st": 3, "ed": 5, "text": "predict future"}, {"st": 46, "ed": 49, "text": "deep neural networks"}, {"st": 56, "ed": 58, "text": "future frames"}, {"st": 76, "ed": 78, "text": "excellent performance"}, {"st": 81, "ed": 83, "text": "prediction tasks"}, {"st": 103, "ed": 106, "text": "mean squared error"}, {"st": 107, "ed": 109, "text": "adversarial loss"}, {"st": 130, "ed": 134, "text": "trained end to end"}, {"st": 149, "ed": 151, "text": "latent structure"}, {"st": 209, "ed": 211, "text": "internal representations"}]
[{"st": 6, "ed": 8, "text": "pose estimation"}, {"st": 69, "ed": 71, "text": "weakly supervised"}, {"st": 88, "ed": 90, "text": "ground truth"}, {"st": 97, "ed": 99, "text": "real images"}, {"st": 101, "ed": 103, "text": "ground truth"}, {"st": 115, "ed": 117, "text": "real images"}, {"st": 143, "ed": 145, "text": "domain shift"}, {"st": 146, "ed": 150, "text": "synthetic and real data"}]
[{"st": 27, "ed": 29, "text": "generative model"}, {"st": 31, "ed": 33, "text": "latent variables"}, {"st": 37, "ed": 40, "text": "end to end"}, {"st": 48, "ed": 50, "text": "natural images"}, {"st": 80, "ed": 82, "text": "posterior inference"}, {"st": 83, "ed": 85, "text": "latent variables"}, {"st": 91, "ed": 93, "text": "generative models"}, {"st": 105, "ed": 107, "text": "image reconstruction"}]
[{"st": 28, "ed": 30, "text": "partial observability"}, {"st": 38, "ed": 40, "text": "image space"}, {"st": 55, "ed": 57, "text": "neural network"}, {"st": 68, "ed": 70, "text": "object categories"}, {"st": 93, "ed": 95, "text": "encoder decoder"}, {"st": 98, "ed": 102, "text": "trained end to end"}, {"st": 160, "ed": 162, "text": "latent factors"}]
[{"st": 30, "ed": 32, "text": "deep networks"}, {"st": 36, "ed": 38, "text": "problem specific"}, {"st": 59, "ed": 61, "text": "prior knowledge"}, {"st": 64, "ed": 66, "text": "compression scheme"}, {"st": 94, "ed": 96, "text": "feed forward"}, {"st": 100, "ed": 102, "text": "extensive experiments"}, {"st": 128, "ed": 130, "text": "deep model"}]
[{"st": 0, "ed": 2, "text": "visual recognition"}, {"st": 27, "ed": 29, "text": "low resolution"}, {"st": 65, "ed": 67, "text": "deep learning"}, {"st": 68, "ed": 71, "text": "taking advantage of"}, {"st": 74, "ed": 76, "text": "super resolution"}, {"st": 76, "ed": 78, "text": "domain adaptation"}, {"st": 85, "ed": 87, "text": "deep learning"}, {"st": 156, "ed": 158, "text": "tasks including"}, {"st": 160, "ed": 162, "text": "digit recognition"}]
[{"st": 6, "ed": 8, "text": "neural network"}, {"st": 18, "ed": 20, "text": "spatio temporal"}, {"st": 21, "ed": 24, "text": "recurrent neural network"}, {"st": 37, "ed": 40, "text": "convolutional neural network"}, {"st": 55, "ed": 58, "text": "spatial and temporal"}, {"st": 62, "ed": 64, "text": "neural activity"}, {"st": 67, "ed": 69, "text": "multiple scales"}, {"st": 98, "ed": 101, "text": "taking advantage of"}, {"st": 158, "ed": 160, "text": "deep learning"}, {"st": 169, "ed": 171, "text": "internal representation"}]
[{"st": 1, "ed": 6, "text": "deep convolutional neural networks cnn"}, {"st": 10, "ed": 12, "text": "adversarial samples"}, {"st": 43, "ed": 45, "text": "activation functions"}, {"st": 66, "ed": 68, "text": "classification tasks"}, {"st": 69, "ed": 71, "text": "arbitrary precision"}, {"st": 109, "ed": 113, "text": "mnist and cifar 10"}]
[{"st": 7, "ed": 9, "text": "facial expressions"}, {"st": 53, "ed": 55, "text": "efficient algorithms"}, {"st": 77, "ed": 79, "text": "traditional approaches"}, {"st": 80, "ed": 82, "text": "time series"}, {"st": 87, "ed": 90, "text": "multi task learning"}, {"st": 94, "ed": 96, "text": "shared representation"}, {"st": 114, "ed": 117, "text": "restricted boltzmann machines"}, {"st": 135, "ed": 137, "text": "multi task"}, {"st": 140, "ed": 142, "text": "multi task"}, {"st": 143, "ed": 146, "text": "restricted boltzmann machines"}, {"st": 154, "ed": 157, "text": "publicly available datasets"}, {"st": 169, "ed": 171, "text": "classification performance"}]
[{"st": 16, "ed": 18, "text": "feature vector"}, {"st": 94, "ed": 96, "text": "jointly learn"}, {"st": 136, "ed": 138, "text": "image representations"}]
[{"st": 4, "ed": 6, "text": "information science"}, {"st": 22, "ed": 24, "text": "motor control"}, {"st": 46, "ed": 48, "text": "object recognition"}, {"st": 87, "ed": 89, "text": "main contributions"}, {"st": 98, "ed": 100, "text": "feature extraction"}, {"st": 116, "ed": 119, "text": "deep neural network"}, {"st": 131, "ed": 133, "text": "structural information"}, {"st": 164, "ed": 166, "text": "hand written"}, {"st": 187, "ed": 189, "text": "visual recognition"}, {"st": 194, "ed": 196, "text": "input samples"}]
[{"st": 10, "ed": 12, "text": "visual representation"}, {"st": 67, "ed": 69, "text": "visual representation"}, {"st": 71, "ed": 75, "text": "convolutional neural network cnn"}, {"st": 79, "ed": 81, "text": "complementary information"}, {"st": 86, "ed": 88, "text": "image datasets"}, {"st": 90, "ed": 92, "text": "qualitative results"}, {"st": 109, "ed": 111, "text": "pre training"}, {"st": 117, "ed": 119, "text": "significant gains"}, {"st": 125, "ed": 127, "text": "benchmark datasets"}, {"st": 142, "ed": 144, "text": "pose estimation"}]
[{"st": 0, "ed": 2, "text": "visual recognition"}, {"st": 40, "ed": 43, "text": "recurrent neural network"}, {"st": 50, "ed": 53, "text": "end to end"}, {"st": 119, "ed": 121, "text": "internal representation"}, {"st": 138, "ed": 141, "text": "end to end"}, {"st": 154, "ed": 156, "text": "look ahead"}]
[{"st": 1, "ed": 3, "text": "recent advances"}, {"st": 4, "ed": 7, "text": "deep neural networks"}, {"st": 11, "ed": 13, "text": "vision based"}, {"st": 13, "ed": 15, "text": "reinforcement learning"}, {"st": 39, "ed": 41, "text": "real world"}, {"st": 63, "ed": 65, "text": "reinforcement learning"}, {"st": 68, "ed": 70, "text": "visual information"}, {"st": 91, "ed": 94, "text": "first person shooter"}, {"st": 94, "ed": 96, "text": "video game"}, {"st": 154, "ed": 157, "text": "deep neural networks"}, {"st": 161, "ed": 163, "text": "experience replay"}, {"st": 194, "ed": 196, "text": "reinforcement learning"}]
[{"st": 66, "ed": 68, "text": "false alarm"}, {"st": 78, "ed": 80, "text": "method called"}, {"st": 156, "ed": 159, "text": "support vector machine"}, {"st": 161, "ed": 163, "text": "radial basis"}, {"st": 167, "ed": 169, "text": "extracted features"}, {"st": 208, "ed": 210, "text": "feature extraction"}, {"st": 226, "ed": 228, "text": "genetic algorithm"}, {"st": 249, "ed": 251, "text": "extracted features"}]
[{"st": 24, "ed": 26, "text": "existing methods"}, {"st": 40, "ed": 42, "text": "real world"}, {"st": 52, "ed": 54, "text": "labeled data"}, {"st": 71, "ed": 73, "text": "prediction model"}, {"st": 99, "ed": 101, "text": "object appearance"}, {"st": 106, "ed": 108, "text": "previously unseen"}, {"st": 114, "ed": 116, "text": "real world"}, {"st": 141, "ed": 143, "text": "accurate prediction"}, {"st": 172, "ed": 174, "text": "proposed method"}, {"st": 180, "ed": 183, "text": "quantitatively and qualitatively"}]
[{"st": 6, "ed": 9, "text": "deep neural networks"}, {"st": 14, "ed": 16, "text": "rigid body"}, {"st": 19, "ed": 21, "text": "point cloud"}, {"st": 34, "ed": 36, "text": "point wise"}, {"st": 58, "ed": 60, "text": "point wise"}]
[{"st": 1, "ed": 4, "text": "artificial neural networks"}, {"st": 6, "ed": 8, "text": "great promise"}, {"st": 9, "ed": 11, "text": "applications including"}, {"st": 11, "ed": 13, "text": "computer vision"}, {"st": 14, "ed": 16, "text": "speech recognition"}, {"st": 39, "ed": 41, "text": "convex functions"}, {"st": 50, "ed": 52, "text": "theoretical guarantees"}, {"st": 57, "ed": 59, "text": "activation functions"}, {"st": 62, "ed": 64, "text": "recent years"}, {"st": 133, "ed": 135, "text": "local minima"}, {"st": 136, "ed": 138, "text": "stationary points"}, {"st": 140, "ed": 142, "text": "training objective"}, {"st": 159, "ed": 161, "text": "optimization algorithms"}, {"st": 163, "ed": 165, "text": "convex problems"}, {"st": 192, "ed": 194, "text": "sufficient conditions"}, {"st": 212, "ed": 214, "text": "global optimization"}, {"st": 217, "ed": 219, "text": "squared error"}, {"st": 226, "ed": 228, "text": "training data"}, {"st": 233, "ed": 235, "text": "local minima"}]
[{"st": 6, "ed": 8, "text": "deep features"}, {"st": 24, "ed": 26, "text": "imagenet dataset"}, {"st": 49, "ed": 51, "text": "pre training"}, {"st": 63, "ed": 65, "text": "training examples"}, {"st": 70, "ed": 72, "text": "object classes"}, {"st": 72, "ed": 74, "text": "improve performance"}, {"st": 88, "ed": 90, "text": "fine grained"}, {"st": 111, "ed": 113, "text": "fine grained"}, {"st": 131, "ed": 134, "text": "pre trained cnn"}, {"st": 140, "ed": 142, "text": "imagenet dataset"}, {"st": 154, "ed": 156, "text": "scene classification"}, {"st": 168, "ed": 170, "text": "pre training"}, {"st": 197, "ed": 199, "text": "fine grained"}]
[{"st": 6, "ed": 10, "text": "convolutional neural network cnn"}, {"st": 18, "ed": 20, "text": "vision tasks"}, {"st": 41, "ed": 43, "text": "vision tasks"}, {"st": 77, "ed": 79, "text": "deep architecture"}, {"st": 83, "ed": 85, "text": "training sets"}, {"st": 94, "ed": 96, "text": "limited memory"}, {"st": 113, "ed": 115, "text": "without compromising"}, {"st": 123, "ed": 126, "text": "end to end"}, {"st": 161, "ed": 163, "text": "competitive performance"}]
[{"st": 4, "ed": 7, "text": "end to end"}, {"st": 10, "ed": 13, "text": "static and dynamic"}, {"st": 23, "ed": 25, "text": "unlike traditional"}, {"st": 32, "ed": 35, "text": "end to end"}, {"st": 64, "ed": 68, "text": "recurrent neural network rnn"}, {"st": 83, "ed": 85, "text": "spatial transformer"}]
[{"st": 3, "ed": 5, "text": "weakly supervised"}, {"st": 17, "ed": 19, "text": "autonomous driving"}, {"st": 32, "ed": 34, "text": "proposed method"}, {"st": 47, "ed": 49, "text": "manual annotation"}, {"st": 99, "ed": 101, "text": "large scale"}, {"st": 116, "ed": 118, "text": "wide variety"}]
[{"st": 42, "ed": 44, "text": "convolutional layer"}, {"st": 61, "ed": 63, "text": "unlike previous"}, {"st": 69, "ed": 71, "text": "wide variety"}, {"st": 78, "ed": 81, "text": "fully connected layers"}, {"st": 87, "ed": 89, "text": "structured outputs"}, {"st": 102, "ed": 104, "text": "reinforcement learning"}, {"st": 115, "ed": 117, "text": "fine grained"}, {"st": 133, "ed": 135, "text": "image classification"}, {"st": 137, "ed": 140, "text": "visual question answering"}, {"st": 140, "ed": 142, "text": "vqa models"}, {"st": 150, "ed": 152, "text": "image classification"}, {"st": 174, "ed": 176, "text": "adversarial images"}, {"st": 178, "ed": 180, "text": "previous methods"}, {"st": 181, "ed": 183, "text": "weakly supervised"}, {"st": 211, "ed": 213, "text": "attention based"}, {"st": 234, "ed": 236, "text": "deep networks"}, {"st": 247, "ed": 249, "text": "deep network"}, {"st": 256, "ed": 260, "text": "available at https github.com"}]
[{"st": 13, "ed": 15, "text": "higher level"}, {"st": 35, "ed": 37, "text": "object detection"}, {"st": 71, "ed": 73, "text": "training data"}, {"st": 88, "ed": 91, "text": "significant performance gains"}, {"st": 115, "ed": 117, "text": "performance gain"}, {"st": 120, "ed": 124, "text": "deep convolutional neural network"}, {"st": 160, "ed": 162, "text": "detection performance"}, {"st": 167, "ed": 169, "text": "previous works"}]
[{"st": 13, "ed": 15, "text": "convolutional layers"}, {"st": 16, "ed": 19, "text": "deep neural networks"}, {"st": 196, "ed": 198, "text": "convolutional layers"}, {"st": 203, "ed": 205, "text": "improves performance"}, {"st": 243, "ed": 245, "text": "performance improvements"}]
[{"st": 52, "ed": 54, "text": "supervised learning"}, {"st": 66, "ed": 68, "text": "sensory input"}, {"st": 95, "ed": 98, "text": "conduct extensive experiments"}, {"st": 116, "ed": 118, "text": "approach outperforms"}, {"st": 130, "ed": 132, "text": "trained models"}, {"st": 160, "ed": 162, "text": "previously unseen"}]
[{"st": 4, "ed": 6, "text": "complex environments"}, {"st": 27, "ed": 29, "text": "reinforcement learning"}, {"st": 36, "ed": 38, "text": "task performance"}, {"st": 56, "ed": 58, "text": "jointly learning"}, {"st": 61, "ed": 63, "text": "reinforcement learning"}, {"st": 81, "ed": 83, "text": "sensory input"}, {"st": 102, "ed": 104, "text": "detailed analysis"}]
[{"st": 19, "ed": 21, "text": "computer vision"}, {"st": 21, "ed": 23, "text": "tasks including"}, {"st": 26, "ed": 28, "text": "object detection"}, {"st": 103, "ed": 105, "text": "multi class"}, {"st": 130, "ed": 132, "text": "approach outperforms"}, {"st": 132, "ed": 134, "text": "existing methods"}]
[{"st": 28, "ed": 30, "text": "visual attention"}, {"st": 89, "ed": 91, "text": "visual input"}, {"st": 92, "ed": 94, "text": "real life"}, {"st": 213, "ed": 215, "text": "task specific"}]
[{"st": 4, "ed": 6, "text": "receptive fields"}, {"st": 13, "ed": 15, "text": "receptive field"}, {"st": 22, "ed": 24, "text": "visual tasks"}, {"st": 49, "ed": 51, "text": "receptive field"}, {"st": 58, "ed": 60, "text": "gaussian distribution"}, {"st": 75, "ed": 77, "text": "receptive field"}, {"st": 91, "ed": 93, "text": "skip connections"}]
[{"st": 8, "ed": 11, "text": "deep neural network"}, {"st": 29, "ed": 31, "text": "non linearity"}, {"st": 40, "ed": 42, "text": "sign function"}, {"st": 70, "ed": 72, "text": "non linearity"}, {"st": 77, "ed": 79, "text": "deep learning"}, {"st": 98, "ed": 100, "text": "efficient implementation"}, {"st": 109, "ed": 111, "text": "batch normalization"}, {"st": 175, "ed": 177, "text": "low precision"}]
[{"st": 3, "ed": 5, "text": "neural architecture"}, {"st": 67, "ed": 69, "text": "spatial memory"}, {"st": 97, "ed": 99, "text": "neural net"}, {"st": 126, "ed": 128, "text": "experiments demonstrate"}]
[{"st": 1, "ed": 3, "text": "labeled data"}, {"st": 10, "ed": 12, "text": "facial expression"}, {"st": 19, "ed": 21, "text": "deep networks"}, {"st": 38, "ed": 40, "text": "fine tuning"}, {"st": 44, "ed": 46, "text": "pre trained"}, {"st": 49, "ed": 51, "text": "face verification"}, {"st": 70, "ed": 72, "text": "face verification"}, {"st": 97, "ed": 99, "text": "feature representations"}]
[{"st": 5, "ed": 9, "text": "deep convolutional neural network"}, {"st": 11, "ed": 13, "text": "automatic segmentation"}, {"st": 18, "ed": 20, "text": "magnetic resonance"}, {"st": 24, "ed": 27, "text": "end to end"}, {"st": 28, "ed": 30, "text": "based approach"}, {"st": 34, "ed": 36, "text": "jointly learns"}, {"st": 38, "ed": 40, "text": "feature representation"}, {"st": 42, "ed": 44, "text": "multi class"}, {"st": 50, "ed": 52, "text": "based approach"}, {"st": 77, "ed": 79, "text": "class imbalance"}, {"st": 136, "ed": 138, "text": "network architecture"}, {"st": 141, "ed": 143, "text": "convolutional layers"}, {"st": 145, "ed": 147, "text": "batch normalization"}, {"st": 148, "ed": 150, "text": "non linearities"}, {"st": 152, "ed": 155, "text": "fully connected layers"}, {"st": 172, "ed": 174, "text": "fully connected"}, {"st": 174, "ed": 177, "text": "conditional random field"}, {"st": 203, "ed": 206, "text": "compares favorably to"}, {"st": 215, "ed": 217, "text": "based method"}, {"st": 232, "ed": 234, "text": "fine tuning"}, {"st": 235, "ed": 237, "text": "pre trained"}, {"st": 241, "ed": 243, "text": "training sample"}, {"st": 251, "ed": 253, "text": "larger datasets"}]
[{"st": 68, "ed": 70, "text": "deep learning"}, {"st": 79, "ed": 81, "text": "object detection"}, {"st": 100, "ed": 103, "text": "fully convolutional network"}, {"st": 142, "ed": 144, "text": "motor control"}]
[{"st": 3, "ed": 5, "text": "visual perception"}, {"st": 8, "ed": 10, "text": "image classification"}, {"st": 34, "ed": 36, "text": "existing methods"}, {"st": 46, "ed": 48, "text": "object instances"}, {"st": 76, "ed": 78, "text": "reinforcement learning"}, {"st": 107, "ed": 109, "text": "compact representation"}, {"st": 113, "ed": 115, "text": "object categories"}, {"st": 162, "ed": 164, "text": "object categories"}, {"st": 178, "ed": 180, "text": "deep rl"}, {"st": 207, "ed": 209, "text": "large scale"}]
[{"st": 13, "ed": 16, "text": "short term memory"}, {"st": 17, "ed": 20, "text": "recurrent neural networks"}, {"st": 27, "ed": 29, "text": "lstm models"}, {"st": 42, "ed": 44, "text": "multi level"}, {"st": 56, "ed": 58, "text": "lstm network"}, {"st": 93, "ed": 95, "text": "multi level"}, {"st": 191, "ed": 193, "text": "higher level"}, {"st": 214, "ed": 216, "text": "higher levels"}, {"st": 255, "ed": 257, "text": "lstm models"}]
[{"st": 2, "ed": 4, "text": "structured output"}, {"st": 58, "ed": 60, "text": "image segmentation"}, {"st": 85, "ed": 87, "text": "ground truth"}, {"st": 89, "ed": 92, "text": "multi label classification"}, {"st": 119, "ed": 121, "text": "multi label"}, {"st": 123, "ed": 125, "text": "image segmentation"}]
[{"st": 167, "ed": 169, "text": "without resorting"}, {"st": 186, "ed": 188, "text": "supervised learning"}, {"st": 195, "ed": 197, "text": "neural networks"}]
[{"st": 1, "ed": 3, "text": "natural image"}, {"st": 16, "ed": 18, "text": "image description"}, {"st": 43, "ed": 45, "text": "semi supervised"}, {"st": 46, "ed": 48, "text": "generative framework"}, {"st": 74, "ed": 77, "text": "generative adversarial network"}, {"st": 89, "ed": 91, "text": "multi level"}, {"st": 106, "ed": 108, "text": "attention mechanisms"}, {"st": 120, "ed": 122, "text": "multi level"}, {"st": 130, "ed": 132, "text": "sentence level"}, {"st": 141, "ed": 143, "text": "adversarial training"}, {"st": 150, "ed": 152, "text": "generate realistic"}, {"st": 164, "ed": 167, "text": "image and video"}, {"st": 178, "ed": 182, "text": "supervised and semi supervised"}, {"st": 183, "ed": 185, "text": "qualitative results"}]
[{"st": 3, "ed": 5, "text": "imitation learning"}, {"st": 17, "ed": 19, "text": "expert demonstrations"}, {"st": 29, "ed": 31, "text": "latent factors"}, {"st": 49, "ed": 51, "text": "latent structure"}, {"st": 52, "ed": 54, "text": "expert demonstrations"}, {"st": 64, "ed": 66, "text": "generative adversarial"}, {"st": 66, "ed": 68, "text": "imitation learning"}, {"st": 130, "ed": 132, "text": "latent structure"}, {"st": 133, "ed": 135, "text": "expert demonstrations"}, {"st": 137, "ed": 139, "text": "semantically meaningful"}, {"st": 139, "ed": 142, "text": "factors of variation"}]
[{"st": 66, "ed": 68, "text": "deep network"}, {"st": 71, "ed": 73, "text": "adversarial training"}, {"st": 82, "ed": 84, "text": "random noise"}, {"st": 85, "ed": 87, "text": "user defined"}, {"st": 99, "ed": 102, "text": "convolutional neural network"}, {"st": 166, "ed": 168, "text": "proposed method"}]
[{"st": 7, "ed": 9, "text": "image pixels"}, {"st": 16, "ed": 18, "text": "image patches"}, {"st": 42, "ed": 44, "text": "image content"}, {"st": 61, "ed": 63, "text": "valuable information"}, {"st": 70, "ed": 72, "text": "generic framework"}, {"st": 112, "ed": 114, "text": "visual cues"}, {"st": 127, "ed": 129, "text": "standard datasets"}]
[{"st": 2, "ed": 7, "text": "computer vision and machine learning"}, {"st": 12, "ed": 14, "text": "optimization problems"}, {"st": 18, "ed": 20, "text": "objective functions"}, {"st": 23, "ed": 29, "text": "alternating direction method of multipliers admm"}, {"st": 109, "ed": 111, "text": "convergence analysis"}, {"st": 116, "ed": 118, "text": "numerical results"}]
[{"st": 5, "ed": 8, "text": "zero shot learning"}, {"st": 13, "ed": 15, "text": "previous approaches"}, {"st": 77, "ed": 81, "text": "convolutional neural network cnn"}, {"st": 167, "ed": 169, "text": "significant improvements"}]
[{"st": 17, "ed": 19, "text": "real world"}, {"st": 21, "ed": 23, "text": "domain specific"}, {"st": 27, "ed": 29, "text": "object categories"}, {"st": 54, "ed": 56, "text": "domain specific"}, {"st": 76, "ed": 78, "text": "computer vision"}, {"st": 134, "ed": 136, "text": "weak supervision"}, {"st": 149, "ed": 151, "text": "deep network"}, {"st": 158, "ed": 160, "text": "face alignment"}, {"st": 173, "ed": 175, "text": "deep network"}, {"st": 177, "ed": 179, "text": "training samples"}, {"st": 207, "ed": 209, "text": "conducted experiments"}, {"st": 210, "ed": 212, "text": "real world"}]
[{"st": 1, "ed": 3, "text": "image captioning"}, {"st": 12, "ed": 15, "text": "image and sentence"}, {"st": 32, "ed": 34, "text": "training data"}, {"st": 37, "ed": 39, "text": "cross domain"}, {"st": 39, "ed": 41, "text": "image captioning"}, {"st": 48, "ed": 50, "text": "adversarial training"}, {"st": 72, "ed": 74, "text": "multi modal"}, {"st": 92, "ed": 94, "text": "multi modal"}, {"st": 134, "ed": 136, "text": "policy gradient"}, {"st": 154, "ed": 156, "text": "additional supervision"}, {"st": 166, "ed": 168, "text": "source domain"}]
[{"st": 26, "ed": 28, "text": "neural network"}, {"st": 30, "ed": 32, "text": "point based"}, {"st": 36, "ed": 38, "text": "real world"}, {"st": 52, "ed": 54, "text": "latent factors"}, {"st": 56, "ed": 58, "text": "low dimensional"}, {"st": 60, "ed": 62, "text": "feature space"}, {"st": 75, "ed": 77, "text": "feature space"}, {"st": 94, "ed": 96, "text": "feature space"}, {"st": 107, "ed": 109, "text": "predictive power"}, {"st": 110, "ed": 114, "text": "recurrent neural networks rnns"}, {"st": 119, "ed": 121, "text": "classification accuracy"}, {"st": 129, "ed": 131, "text": "existing models"}, {"st": 133, "ed": 135, "text": "sensory data"}]
[{"st": 1, "ed": 3, "text": "lifelong learning"}, {"st": 10, "ed": 12, "text": "challenging research"}, {"st": 49, "ed": 51, "text": "real world"}, {"st": 51, "ed": 53, "text": "object recognition"}, {"st": 86, "ed": 88, "text": "specifically designed"}, {"st": 90, "ed": 92, "text": "object recognition"}, {"st": 94, "ed": 96, "text": "baseline approaches"}]
[{"st": 31, "ed": 33, "text": "face perception"}, {"st": 35, "ed": 37, "text": "extensively studied"}, {"st": 39, "ed": 41, "text": "computational models"}, {"st": 69, "ed": 71, "text": "deep representations"}]
[{"st": 3, "ed": 5, "text": "partial information"}, {"st": 12, "ed": 14, "text": "visual perception"}, {"st": 29, "ed": 31, "text": "computational models"}, {"st": 46, "ed": 49, "text": "pieces of evidence"}, {"st": 123, "ed": 125, "text": "feed forward"}, {"st": 134, "ed": 136, "text": "recognition performance"}]
[{"st": 4, "ed": 6, "text": "fully convolutional"}, {"st": 6, "ed": 8, "text": "neural network"}, {"st": 44, "ed": 46, "text": "fully convolutional"}, {"st": 53, "ed": 55, "text": "network outputs"}, {"st": 144, "ed": 146, "text": "pre defined"}, {"st": 148, "ed": 150, "text": "precision recall"}, {"st": 191, "ed": 195, "text": "available at https github.com"}]
[{"st": 0, "ed": 2, "text": "real world"}, {"st": 8, "ed": 10, "text": "multiple modalities"}, {"st": 28, "ed": 30, "text": "multimodal data"}, {"st": 72, "ed": 74, "text": "computer vision"}, {"st": 75, "ed": 77, "text": "natural language"}, {"st": 116, "ed": 119, "text": "deep learning based"}, {"st": 151, "ed": 153, "text": "representation learning"}, {"st": 163, "ed": 165, "text": "image caption"}, {"st": 226, "ed": 228, "text": "ms coco"}, {"st": 264, "ed": 266, "text": "ms coco"}]
[{"st": 10, "ed": 12, "text": "deep cnn"}, {"st": 24, "ed": 26, "text": "feature extractors"}, {"st": 34, "ed": 36, "text": "object classification"}, {"st": 39, "ed": 41, "text": "fine grained"}, {"st": 50, "ed": 52, "text": "feature extraction"}, {"st": 61, "ed": 63, "text": "clustering algorithm"}, {"st": 91, "ed": 93, "text": "supervised training"}, {"st": 94, "ed": 96, "text": "deep cnn"}, {"st": 97, "ed": 99, "text": "large datasets"}, {"st": 110, "ed": 112, "text": "carefully designed"}]
[{"st": 3, "ed": 5, "text": "object detectors"}, {"st": 31, "ed": 33, "text": "deep model"}, {"st": 34, "ed": 36, "text": "object detection"}, {"st": 82, "ed": 85, "text": "fully convolutional network"}, {"st": 88, "ed": 90, "text": "spatial resolution"}, {"st": 95, "ed": 97, "text": "pooling layer"}, {"st": 120, "ed": 122, "text": "bounding box"}, {"st": 145, "ed": 148, "text": "pascal voc 2007"}]
[{"st": 23, "ed": 25, "text": "recent years"}, {"st": 94, "ed": 96, "text": "neural networks"}, {"st": 149, "ed": 151, "text": "facial expressions"}, {"st": 187, "ed": 189, "text": "recognition performance"}]
[{"st": 18, "ed": 20, "text": "approach produces"}, {"st": 87, "ed": 91, "text": "trained end to end"}]
[{"st": 3, "ed": 8, "text": "deep convolutional neural networks cnns"}, {"st": 10, "ed": 12, "text": "real world"}, {"st": 28, "ed": 30, "text": "learning scheme"}, {"st": 54, "ed": 56, "text": "without compromising"}, {"st": 77, "ed": 79, "text": "existing approaches"}, {"st": 80, "ed": 82, "text": "proposed method"}, {"st": 93, "ed": 95, "text": "training process"}, {"st": 116, "ed": 118, "text": "large networks"}, {"st": 141, "ed": 143, "text": "empirically demonstrate"}, {"st": 154, "ed": 156, "text": "cnn models"}, {"st": 163, "ed": 165, "text": "image classification"}]
[{"st": 41, "ed": 43, "text": "zero shot"}, {"st": 43, "ed": 45, "text": "classification task"}, {"st": 50, "ed": 52, "text": "previous methods"}, {"st": 57, "ed": 59, "text": "embedding space"}, {"st": 62, "ed": 64, "text": "visual features"}, {"st": 89, "ed": 91, "text": "classification tasks"}, {"st": 97, "ed": 99, "text": "zero shot"}, {"st": 129, "ed": 131, "text": "training examples"}, {"st": 143, "ed": 145, "text": "supervised learning"}, {"st": 149, "ed": 151, "text": "generative models"}]
[{"st": 35, "ed": 37, "text": "typically requires"}, {"st": 37, "ed": 39, "text": "multiple views"}, {"st": 44, "ed": 46, "text": "class labels"}, {"st": 89, "ed": 91, "text": "key idea"}, {"st": 101, "ed": 105, "text": "conditional generative adversarial networks"}, {"st": 105, "ed": 107, "text": "gan framework"}, {"st": 111, "ed": 113, "text": "fine grained"}, {"st": 122, "ed": 124, "text": "extensive experiments"}, {"st": 126, "ed": 128, "text": "synthetic datasets"}, {"st": 134, "ed": 136, "text": "significantly outperforms"}, {"st": 161, "ed": 165, "text": "available at https github.com"}]
[{"st": 0, "ed": 2, "text": "generative models"}, {"st": 6, "ed": 8, "text": "unsupervised learning"}, {"st": 10, "ed": 12, "text": "applications including"}, {"st": 59, "ed": 61, "text": "noise reduction"}, {"st": 83, "ed": 85, "text": "proposed method"}, {"st": 111, "ed": 113, "text": "anomaly detection"}, {"st": 133, "ed": 135, "text": "mnist handwritten"}, {"st": 144, "ed": 146, "text": "real world"}, {"st": 159, "ed": 161, "text": "significantly improve"}]
[{"st": 0, "ed": 2, "text": "fine tuning"}, {"st": 4, "ed": 9, "text": "deep convolutional neural network cnn"}, {"st": 32, "ed": 34, "text": "fine tune"}, {"st": 50, "ed": 52, "text": "ms coco"}]
[{"st": 3, "ed": 5, "text": "facial expressions"}, {"st": 26, "ed": 28, "text": "spatial resolution"}, {"st": 69, "ed": 71, "text": "deep network"}, {"st": 74, "ed": 76, "text": "super resolution"}, {"st": 86, "ed": 88, "text": "training strategy"}, {"st": 127, "ed": 129, "text": "proposed framework"}, {"st": 138, "ed": 140, "text": "significantly improved"}, {"st": 142, "ed": 144, "text": "recognition performance"}]
[{"st": 2, "ed": 5, "text": "deep neural networks"}, {"st": 7, "ed": 9, "text": "adversarial attacks"}, {"st": 32, "ed": 34, "text": "image classification"}, {"st": 37, "ed": 39, "text": "adversarial perturbations"}, {"st": 47, "ed": 49, "text": "natural images"}, {"st": 87, "ed": 89, "text": "hidden layers"}]
[{"st": 2, "ed": 4, "text": "recent developments"}, {"st": 6, "ed": 8, "text": "neural networks"}, {"st": 10, "ed": 12, "text": "impressive performance"}, {"st": 34, "ed": 36, "text": "input space"}, {"st": 97, "ed": 99, "text": "critical applications"}, {"st": 109, "ed": 111, "text": "open set"}, {"st": 128, "ed": 130, "text": "denoising autoencoders"}, {"st": 143, "ed": 145, "text": "input space"}, {"st": 148, "ed": 150, "text": "training distribution"}]
[{"st": 24, "ed": 26, "text": "wide variety"}, {"st": 40, "ed": 43, "text": "deep neural networks"}, {"st": 67, "ed": 69, "text": "imitation learning"}, {"st": 93, "ed": 95, "text": "prior methods"}, {"st": 111, "ed": 113, "text": "significantly fewer"}, {"st": 125, "ed": 128, "text": "simulated and real"}, {"st": 132, "ed": 135, "text": "ability to learn"}, {"st": 137, "ed": 140, "text": "end to end"}]
[{"st": 0, "ed": 2, "text": "deep learning"}, {"st": 25, "ed": 27, "text": "visible light"}, {"st": 44, "ed": 48, "text": "convolutional neural network cnn"}, {"st": 68, "ed": 70, "text": "classification accuracy"}, {"st": 125, "ed": 127, "text": "optimization technique"}, {"st": 141, "ed": 143, "text": "higher accuracy"}]
[{"st": 9, "ed": 12, "text": "modern machine learning"}, {"st": 16, "ed": 18, "text": "time consuming"}, {"st": 32, "ed": 34, "text": "synthetic data"}, {"st": 36, "ed": 38, "text": "ground truth"}, {"st": 47, "ed": 49, "text": "simulated data"}, {"st": 64, "ed": 66, "text": "domain adaptation"}, {"st": 85, "ed": 87, "text": "extensively evaluate"}, {"st": 107, "ed": 109, "text": "domain adaptation"}, {"st": 117, "ed": 119, "text": "domain adaptation"}, {"st": 129, "ed": 131, "text": "synthetic data"}, {"st": 132, "ed": 134, "text": "domain adaptation"}, {"st": 142, "ed": 144, "text": "real world"}, {"st": 160, "ed": 162, "text": "randomly generated"}, {"st": 172, "ed": 175, "text": "real world data"}, {"st": 181, "ed": 183, "text": "real world"}, {"st": 187, "ed": 189, "text": "real world"}, {"st": 200, "ed": 202, "text": "real world"}]
[{"st": 26, "ed": 28, "text": "accurate prediction"}, {"st": 56, "ed": 58, "text": "fully convolutional"}, {"st": 58, "ed": 60, "text": "deep network"}, {"st": 61, "ed": 63, "text": "u net"}, {"st": 79, "ed": 81, "text": "accurately predict"}, {"st": 84, "ed": 86, "text": "prostate cancer"}]
[{"st": 13, "ed": 15, "text": "deep learning"}, {"st": 50, "ed": 53, "text": "visual object recognition"}, {"st": 68, "ed": 70, "text": "off line"}, {"st": 70, "ed": 72, "text": "large scale"}, {"st": 72, "ed": 74, "text": "image retrieval"}, {"st": 106, "ed": 108, "text": "object categorization"}, {"st": 117, "ed": 119, "text": "object recognition"}, {"st": 123, "ed": 125, "text": "image retrieval"}, {"st": 130, "ed": 132, "text": "deep learning"}, {"st": 153, "ed": 155, "text": "deep learning"}]
[{"st": 1, "ed": 4, "text": "deep neural networks"}, {"st": 51, "ed": 53, "text": "optimization problem"}, {"st": 92, "ed": 94, "text": "proposed method"}, {"st": 106, "ed": 108, "text": "batch normalization"}, {"st": 117, "ed": 119, "text": "image datasets"}, {"st": 120, "ed": 124, "text": "cifar 10 cifar 100"}, {"st": 128, "ed": 130, "text": "supervised learning"}, {"st": 136, "ed": 139, "text": "convolutional neural networks"}, {"st": 170, "ed": 172, "text": "ladder network"}, {"st": 173, "ed": 176, "text": "semi supervised learning"}, {"st": 179, "ed": 181, "text": "mnist dataset"}, {"st": 183, "ed": 185, "text": "method outperforms"}, {"st": 206, "ed": 208, "text": "labeled samples"}]
[{"st": 1, "ed": 3, "text": "based approaches"}, {"st": 24, "ed": 26, "text": "multi task"}, {"st": 26, "ed": 28, "text": "domain adaptation"}, {"st": 41, "ed": 43, "text": "neural network"}, {"st": 49, "ed": 51, "text": "instance segmentation"}, {"st": 76, "ed": 78, "text": "transfer learning"}, {"st": 90, "ed": 92, "text": "domain adversarial"}, {"st": 96, "ed": 98, "text": "trained model"}, {"st": 120, "ed": 122, "text": "real world"}]
[{"st": 47, "ed": 49, "text": "theoretical guarantees"}, {"st": 77, "ed": 80, "text": "semi supervised learning"}, {"st": 101, "ed": 103, "text": "max margin"}, {"st": 103, "ed": 105, "text": "invariant features"}, {"st": 118, "ed": 120, "text": "face recognition"}, {"st": 129, "ed": 131, "text": "large scale"}]
[{"st": 8, "ed": 10, "text": "human intelligence"}, {"st": 15, "ed": 17, "text": "previously unseen"}, {"st": 55, "ed": 58, "text": "end to end"}, {"st": 66, "ed": 68, "text": "explicitly model"}, {"st": 103, "ed": 106, "text": "deep reinforcement learning"}]
[{"st": 0, "ed": 2, "text": "deep learning"}, {"st": 5, "ed": 7, "text": "convolutional neural"}, {"st": 11, "ed": 13, "text": "previous methods"}, {"st": 14, "ed": 16, "text": "challenging tasks"}, {"st": 92, "ed": 94, "text": "deep learning"}, {"st": 100, "ed": 102, "text": "excellent performance"}]
[{"st": 4, "ed": 6, "text": "open source"}, {"st": 8, "ed": 10, "text": "autonomous driving"}, {"st": 33, "ed": 35, "text": "open source"}, {"st": 81, "ed": 83, "text": "autonomous driving"}, {"st": 88, "ed": 91, "text": "end to end"}, {"st": 94, "ed": 96, "text": "imitation learning"}, {"st": 98, "ed": 101, "text": "end to end"}, {"st": 132, "ed": 134, "text": "autonomous driving"}]
[{"st": 36, "ed": 39, "text": "visual and textual"}, {"st": 52, "ed": 54, "text": "attention mechanism"}, {"st": 99, "ed": 101, "text": "attention mechanism"}, {"st": 105, "ed": 109, "text": "visual question answering vqa"}]
[{"st": 27, "ed": 29, "text": "big data"}, {"st": 29, "ed": 31, "text": "machine learning"}, {"st": 79, "ed": 81, "text": "machine learning"}, {"st": 98, "ed": 100, "text": "machine learning"}, {"st": 121, "ed": 123, "text": "machine learning"}, {"st": 136, "ed": 138, "text": "machine learning"}, {"st": 169, "ed": 171, "text": "machine learning"}, {"st": 188, "ed": 190, "text": "machine learning"}]
[{"st": 3, "ed": 6, "text": "spatio temporal data"}, {"st": 13, "ed": 15, "text": "domains including"}, {"st": 17, "ed": 19, "text": "social sciences"}, {"st": 27, "ed": 30, "text": "spatio temporal data"}, {"st": 42, "ed": 44, "text": "data mining"}, {"st": 51, "ed": 54, "text": "spatial and temporal"}, {"st": 81, "ed": 84, "text": "spatio temporal data"}, {"st": 93, "ed": 95, "text": "data mining"}, {"st": 110, "ed": 112, "text": "spatio temporal"}, {"st": 119, "ed": 122, "text": "spatio temporal data"}, {"st": 125, "ed": 127, "text": "data mining"}, {"st": 145, "ed": 147, "text": "data mining"}, {"st": 153, "ed": 155, "text": "spatio temporal"}, {"st": 155, "ed": 157, "text": "data mining"}, {"st": 164, "ed": 166, "text": "change detection"}, {"st": 167, "ed": 169, "text": "pattern mining"}, {"st": 169, "ed": 171, "text": "anomaly detection"}, {"st": 180, "ed": 183, "text": "spatio temporal data"}]
[{"st": 28, "ed": 30, "text": "unlike existing"}, {"st": 32, "ed": 34, "text": "supervised learning"}, {"st": 76, "ed": 78, "text": "loss function"}, {"st": 92, "ed": 94, "text": "loss function"}, {"st": 95, "ed": 97, "text": "empirically evaluated"}, {"st": 116, "ed": 118, "text": "loss functions"}, {"st": 119, "ed": 121, "text": "reinforcement learning"}, {"st": 124, "ed": 126, "text": "distribution matching"}, {"st": 136, "ed": 138, "text": "loss function"}]
[{"st": 9, "ed": 11, "text": "image classification"}, {"st": 15, "ed": 17, "text": "existing approaches"}, {"st": 96, "ed": 99, "text": "convolutional neural network"}, {"st": 104, "ed": 106, "text": "image classification"}, {"st": 113, "ed": 115, "text": "proposed algorithm"}, {"st": 121, "ed": 123, "text": "detection task"}, {"st": 125, "ed": 127, "text": "image classification"}, {"st": 128, "ed": 130, "text": "noisy data"}, {"st": 133, "ed": 135, "text": "large scale"}, {"st": 146, "ed": 148, "text": "error rate"}, {"st": 162, "ed": 164, "text": "weakly supervised"}, {"st": 171, "ed": 173, "text": "performance gain"}, {"st": 184, "ed": 186, "text": "image classification"}]
[{"st": 1, "ed": 3, "text": "current methods"}, {"st": 5, "ed": 9, "text": "convolutional neural networks cnns"}, {"st": 10, "ed": 12, "text": "visualization techniques"}, {"st": 91, "ed": 93, "text": "scene recognition"}, {"st": 110, "ed": 112, "text": "distributed representations"}]
[{"st": 25, "ed": 28, "text": "deep neural networks"}, {"st": 51, "ed": 53, "text": "training examples"}, {"st": 72, "ed": 74, "text": "generative model"}, {"st": 164, "ed": 166, "text": "cifar 100"}]
[{"st": 55, "ed": 58, "text": "end to end"}, {"st": 66, "ed": 68, "text": "latent vector"}, {"st": 185, "ed": 187, "text": "conditional generative"}, {"st": 202, "ed": 204, "text": "generated image"}, {"st": 219, "ed": 221, "text": "generated images"}, {"st": 232, "ed": 234, "text": "generated images"}]
[{"st": 38, "ed": 40, "text": "adversarial examples"}, {"st": 110, "ed": 112, "text": "bounding box"}, {"st": 136, "ed": 138, "text": "adversarial examples"}]
[{"st": 9, "ed": 11, "text": "deep architectures"}, {"st": 23, "ed": 25, "text": "recent works"}, {"st": 33, "ed": 35, "text": "weight parameters"}, {"st": 43, "ed": 45, "text": "deep architecture"}, {"st": 61, "ed": 63, "text": "deep architecture"}, {"st": 82, "ed": 85, "text": "deep learning framework"}, {"st": 109, "ed": 111, "text": "multiple levels"}, {"st": 123, "ed": 125, "text": "higher level"}, {"st": 138, "ed": 140, "text": "proposed framework"}, {"st": 174, "ed": 176, "text": "multiple tasks"}, {"st": 182, "ed": 185, "text": "coarse to fine"}, {"st": 213, "ed": 215, "text": "multiple tasks"}]
[{"st": 19, "ed": 21, "text": "large scale"}, {"st": 25, "ed": 29, "text": "synthetic and real world"}, {"st": 92, "ed": 95, "text": "sheds light on"}]
[{"st": 19, "ed": 21, "text": "previous works"}, {"st": 52, "ed": 54, "text": "deep networks"}, {"st": 63, "ed": 66, "text": "visual question answering"}, {"st": 90, "ed": 92, "text": "deep networks"}, {"st": 121, "ed": 124, "text": "visual and textual"}, {"st": 139, "ed": 141, "text": "proposed method"}, {"st": 149, "ed": 152, "text": "visual question answering"}]
[{"st": 0, "ed": 2, "text": "deep learning"}, {"st": 3, "ed": 5, "text": "achieved impressive"}, {"st": 50, "ed": 52, "text": "deep learning"}, {"st": 55, "ed": 57, "text": "machine learning"}, {"st": 65, "ed": 67, "text": "deep learning"}, {"st": 71, "ed": 74, "text": "deep neural network"}, {"st": 90, "ed": 92, "text": "hyperparameter optimization"}, {"st": 149, "ed": 151, "text": "hyperparameter optimization"}]
[{"st": 6, "ed": 9, "text": "artificial intelligence ai"}, {"st": 68, "ed": 70, "text": "artificial intelligence"}]
[{"st": 18, "ed": 20, "text": "deep convolutional"}, {"st": 56, "ed": 58, "text": "previous works"}, {"st": 59, "ed": 61, "text": "deep architectures"}, {"st": 91, "ed": 93, "text": "unified framework"}, {"st": 118, "ed": 121, "text": "mnist and cifar10"}, {"st": 147, "ed": 149, "text": "explicit knowledge"}]
[{"st": 54, "ed": 56, "text": "object classes"}, {"st": 106, "ed": 108, "text": "significantly improve"}]
[{"st": 30, "ed": 32, "text": "unlike existing"}, {"st": 34, "ed": 36, "text": "typically requires"}, {"st": 36, "ed": 38, "text": "multiple views"}, {"st": 43, "ed": 45, "text": "class labels"}, {"st": 94, "ed": 96, "text": "key idea"}, {"st": 106, "ed": 110, "text": "conditional generative adversarial networks"}, {"st": 110, "ed": 112, "text": "gan framework"}, {"st": 116, "ed": 118, "text": "fine grained"}, {"st": 127, "ed": 129, "text": "extensive experiments"}, {"st": 131, "ed": 133, "text": "synthetic datasets"}, {"st": 134, "ed": 136, "text": "real world"}, {"st": 144, "ed": 146, "text": "significantly outperforms"}]
[{"st": 1, "ed": 4, "text": "deep neural networks"}, {"st": 7, "ed": 9, "text": "learned representations"}, {"st": 31, "ed": 34, "text": "extreme learning machines"}, {"st": 73, "ed": 77, "text": "deep convolutional neural network"}, {"st": 125, "ed": 128, "text": "deep neural networks"}]
[{"st": 44, "ed": 46, "text": "domain shift"}, {"st": 104, "ed": 106, "text": "demonstration data"}, {"st": 110, "ed": 112, "text": "previous tasks"}, {"st": 115, "ed": 117, "text": "prior knowledge"}, {"st": 123, "ed": 125, "text": "prior knowledge"}, {"st": 159, "ed": 161, "text": "meta learning"}]
[{"st": 21, "ed": 23, "text": "recent progress"}, {"st": 24, "ed": 26, "text": "visual recognition"}, {"st": 27, "ed": 29, "text": "machine vision"}, {"st": 35, "ed": 38, "text": "ability to learn"}, {"st": 50, "ed": 54, "text": "convolutional neural networks cnns"}, {"st": 69, "ed": 71, "text": "intra class"}, {"st": 97, "ed": 101, "text": "visual question answering vqa"}, {"st": 123, "ed": 125, "text": "working memory"}]
[{"st": 2, "ed": 4, "text": "deep model"}, {"st": 21, "ed": 23, "text": "deep learning"}, {"st": 36, "ed": 38, "text": "deep learning"}, {"st": 47, "ed": 49, "text": "key insight"}, {"st": 81, "ed": 83, "text": "deep model"}, {"st": 102, "ed": 104, "text": "classification task"}, {"st": 107, "ed": 109, "text": "deep model"}]
[{"st": 11, "ed": 13, "text": "machine learning"}, {"st": 28, "ed": 30, "text": "pattern recognition"}, {"st": 32, "ed": 34, "text": "machine vision"}, {"st": 48, "ed": 50, "text": "training data"}, {"st": 103, "ed": 105, "text": "training data"}, {"st": 138, "ed": 140, "text": "machine learning"}]
[{"st": 6, "ed": 8, "text": "pattern recognition"}, {"st": 9, "ed": 11, "text": "machine learning"}, {"st": 27, "ed": 29, "text": "training data"}, {"st": 58, "ed": 61, "text": "modern machine learning"}, {"st": 67, "ed": 69, "text": "computer vision"}, {"st": 71, "ed": 73, "text": "reinforcement learning"}]
[{"st": 132, "ed": 134, "text": "real datasets"}, {"st": 140, "ed": 142, "text": "handwritten digits"}]
[{"st": 9, "ed": 11, "text": "previously unseen"}, {"st": 43, "ed": 45, "text": "deep network"}, {"st": 88, "ed": 90, "text": "previously unseen"}, {"st": 114, "ed": 116, "text": "success rate"}]
[{"st": 2, "ed": 4, "text": "clustering algorithm"}, {"st": 20, "ed": 22, "text": "additional information"}, {"st": 33, "ed": 35, "text": "internal structure"}, {"st": 53, "ed": 55, "text": "message passing"}, {"st": 59, "ed": 61, "text": "pairwise similarity"}, {"st": 62, "ed": 64, "text": "cost function"}, {"st": 84, "ed": 86, "text": "clustering algorithm"}, {"st": 124, "ed": 126, "text": "local information"}, {"st": 183, "ed": 187, "text": "synthetic and real world"}]
[{"st": 5, "ed": 7, "text": "learning objective"}, {"st": 10, "ed": 13, "text": "deep neural network"}, {"st": 15, "ed": 18, "text": "end to end"}, {"st": 26, "ed": 28, "text": "instance segmentation"}, {"st": 60, "ed": 62, "text": "learning objective"}, {"st": 68, "ed": 71, "text": "fully convolutional network"}, {"st": 119, "ed": 121, "text": "strong performance"}, {"st": 142, "ed": 144, "text": "autonomous driving"}]
[{"st": 8, "ed": 10, "text": "image text"}, {"st": 13, "ed": 15, "text": "latent semantic"}, {"st": 35, "ed": 37, "text": "fine grained"}, {"st": 39, "ed": 42, "text": "vision and language"}, {"st": 44, "ed": 46, "text": "image text"}, {"st": 49, "ed": 51, "text": "prior works"}, {"st": 78, "ed": 80, "text": "multi step"}, {"st": 109, "ed": 111, "text": "image regions"}, {"st": 120, "ed": 122, "text": "image text"}, {"st": 124, "ed": 126, "text": "approach achieves"}, {"st": 134, "ed": 136, "text": "ms coco"}, {"st": 142, "ed": 144, "text": "approach outperforms"}, {"st": 159, "ed": 161, "text": "image retrieval"}, {"st": 170, "ed": 172, "text": "ms coco"}, {"st": 180, "ed": 182, "text": "image retrieval"}]
[{"st": 7, "ed": 9, "text": "3d shapes"}, {"st": 22, "ed": 24, "text": "text descriptions"}, {"st": 37, "ed": 39, "text": "metric learning"}, {"st": 43, "ed": 45, "text": "cross modal"}, {"st": 49, "ed": 51, "text": "joint representation"}, {"st": 64, "ed": 66, "text": "3d shapes"}, {"st": 81, "ed": 83, "text": "natural language"}, {"st": 114, "ed": 116, "text": "wasserstein gan"}, {"st": 120, "ed": 122, "text": "3d shapes"}, {"st": 131, "ed": 133, "text": "natural language"}]
[{"st": 3, "ed": 5, "text": "data mining"}, {"st": 75, "ed": 78, "text": "p adic number"}, {"st": 104, "ed": 106, "text": "hierarchical clustering"}, {"st": 107, "ed": 109, "text": "case studies"}]
[{"st": 0, "ed": 2, "text": "observational data"}, {"st": 18, "ed": 20, "text": "multi layer"}, {"st": 49, "ed": 51, "text": "multi layer"}, {"st": 73, "ed": 75, "text": "matrix factorization"}, {"st": 127, "ed": 129, "text": "real world"}, {"st": 129, "ed": 131, "text": "social network"}, {"st": 137, "ed": 139, "text": "competitive performance"}, {"st": 151, "ed": 153, "text": "baseline methods"}]
[{"st": 12, "ed": 14, "text": "machine learning"}, {"st": 110, "ed": 112, "text": "training set"}, {"st": 140, "ed": 142, "text": "convex functions"}, {"st": 144, "ed": 146, "text": "convex set"}, {"st": 161, "ed": 163, "text": "binary classification"}]
[{"st": 0, "ed": 2, "text": "sparse coding"}, {"st": 19, "ed": 21, "text": "machine learning"}, {"st": 47, "ed": 49, "text": "image processing"}, {"st": 137, "ed": 139, "text": "image denoising"}]
[{"st": 23, "ed": 25, "text": "anomaly detection"}, {"st": 51, "ed": 53, "text": "dissimilarity measure"}, {"st": 81, "ed": 83, "text": "linear combination"}, {"st": 104, "ed": 106, "text": "multiple times"}, {"st": 126, "ed": 128, "text": "anomaly detection"}, {"st": 154, "ed": 156, "text": "multiple times"}, {"st": 165, "ed": 167, "text": "scales linearly"}, {"st": 177, "ed": 179, "text": "linear combinations"}]
[{"st": 0, "ed": 2, "text": "recent results"}, {"st": 3, "ed": 5, "text": "compressive sensing"}, {"st": 22, "ed": 24, "text": "based regularization"}, {"st": 30, "ed": 32, "text": "convex relaxations"}, {"st": 43, "ed": 45, "text": "primal dual"}, {"st": 70, "ed": 72, "text": "minimization problems"}, {"st": 78, "ed": 80, "text": "convex relaxations"}, {"st": 84, "ed": 86, "text": "ell 0"}, {"st": 86, "ed": 88, "text": "minimization problem"}, {"st": 90, "ed": 92, "text": "ell 1"}, {"st": 92, "ed": 94, "text": "minimization problem"}, {"st": 99, "ed": 101, "text": "ell 0"}, {"st": 102, "ed": 104, "text": "minimization problem"}, {"st": 106, "ed": 108, "text": "group sparsity"}, {"st": 109, "ed": 111, "text": "structured data"}, {"st": 113, "ed": 115, "text": "ell 1"}, {"st": 127, "ed": 129, "text": "non trivial"}, {"st": 133, "ed": 135, "text": "group sparsity"}, {"st": 141, "ed": 143, "text": "real world"}]
[{"st": 1, "ed": 3, "text": "image annotation"}, {"st": 8, "ed": 10, "text": "machine learning"}, {"st": 21, "ed": 24, "text": "input and output"}, {"st": 27, "ed": 29, "text": "multiple objects"}, {"st": 103, "ed": 106, "text": "input and output"}, {"st": 128, "ed": 130, "text": "discriminative learning"}, {"st": 132, "ed": 134, "text": "variational inference"}, {"st": 148, "ed": 150, "text": "approach achieves"}]
[{"st": 8, "ed": 10, "text": "computer vision"}, {"st": 52, "ed": 54, "text": "expert knowledge"}, {"st": 121, "ed": 123, "text": "learning task"}, {"st": 139, "ed": 141, "text": "parameter space"}]
[{"st": 0, "ed": 4, "text": "nonnegative matrix factorization nmf"}, {"st": 14, "ed": 16, "text": "face recognition"}, {"st": 16, "ed": 18, "text": "motion segmentation"}, {"st": 27, "ed": 29, "text": "high dimensional"}, {"st": 32, "ed": 34, "text": "linear representation"}, {"st": 36, "ed": 38, "text": "low dimensional"}, {"st": 90, "ed": 92, "text": "real world"}, {"st": 111, "ed": 114, "text": "nonnegative matrix factorization"}, {"st": 124, "ed": 126, "text": "additive noise"}, {"st": 137, "ed": 139, "text": "additive noise"}, {"st": 148, "ed": 150, "text": "proposed method"}, {"st": 174, "ed": 176, "text": "iterative optimization"}, {"st": 180, "ed": 182, "text": "theoretical justification"}]
[{"st": 17, "ed": 19, "text": "semi supervised"}, {"st": 30, "ed": 32, "text": "clustering results"}, {"st": 44, "ed": 46, "text": "special case"}, {"st": 73, "ed": 75, "text": "proposed approach"}, {"st": 78, "ed": 80, "text": "classification accuracies"}, {"st": 84, "ed": 86, "text": "data model"}]
[{"st": 54, "ed": 56, "text": "image segmentation"}, {"st": 58, "ed": 60, "text": "approach outperforms"}, {"st": 61, "ed": 63, "text": "global optimization"}]
[{"st": 2, "ed": 4, "text": "vision tasks"}, {"st": 9, "ed": 11, "text": "object recognition"}, {"st": 21, "ed": 23, "text": "low dimensional"}, {"st": 23, "ed": 25, "text": "linear subspaces"}, {"st": 30, "ed": 32, "text": "image space"}, {"st": 46, "ed": 48, "text": "ell 1"}, {"st": 54, "ed": 56, "text": "exhaustive search"}, {"st": 58, "ed": 60, "text": "large scale"}, {"st": 66, "ed": 68, "text": "computational burden"}, {"st": 88, "ed": 90, "text": "lower dimensional"}, {"st": 97, "ed": 99, "text": "small scale"}, {"st": 124, "ed": 126, "text": "high dimensional"}, {"st": 147, "ed": 149, "text": "low order"}, {"st": 173, "ed": 175, "text": "proposed algorithm"}]
[{"st": 24, "ed": 26, "text": "image classification"}, {"st": 28, "ed": 31, "text": "deep belief networks"}, {"st": 76, "ed": 78, "text": "main result"}, {"st": 91, "ed": 93, "text": "approximate solution"}, {"st": 98, "ed": 100, "text": "sparse coding"}, {"st": 119, "ed": 121, "text": "image classification"}]
[{"st": 0, "ed": 2, "text": "sparse coding"}, {"st": 17, "ed": 19, "text": "computer vision"}, {"st": 27, "ed": 29, "text": "sparse coding"}, {"st": 36, "ed": 38, "text": "sparse coding"}, {"st": 40, "ed": 42, "text": "sparse coding"}, {"st": 49, "ed": 51, "text": "unsupervised manner"}, {"st": 73, "ed": 75, "text": "sparse coding"}, {"st": 83, "ed": 85, "text": "class conditional"}, {"st": 87, "ed": 89, "text": "sparse codes"}, {"st": 92, "ed": 94, "text": "feature space"}, {"st": 100, "ed": 102, "text": "training set"}, {"st": 116, "ed": 118, "text": "sparse coding"}, {"st": 122, "ed": 124, "text": "matching problem"}, {"st": 126, "ed": 128, "text": "class conditional"}, {"st": 153, "ed": 155, "text": "unlabeled data"}]
[{"st": 0, "ed": 4, "text": "nonnegative matrix factorization nmf"}, {"st": 12, "ed": 14, "text": "pattern recognition"}, {"st": 27, "ed": 29, "text": "low rank"}, {"st": 38, "ed": 40, "text": "linear representation"}, {"st": 53, "ed": 55, "text": "compact representation"}, {"st": 95, "ed": 98, "text": "multiple kernel learning"}, {"st": 127, "ed": 129, "text": "kernel learning"}, {"st": 133, "ed": 135, "text": "kernel learning"}, {"st": 145, "ed": 147, "text": "encouraging results"}, {"st": 149, "ed": 151, "text": "proposed algorithm"}, {"st": 159, "ed": 161, "text": "clustering algorithms"}]
[{"st": 21, "ed": 23, "text": "classification error"}, {"st": 37, "ed": 39, "text": "extremely high"}, {"st": 60, "ed": 62, "text": "object detection"}, {"st": 66, "ed": 68, "text": "feature selection"}, {"st": 71, "ed": 74, "text": "takes into account"}, {"st": 89, "ed": 91, "text": "special case"}, {"st": 109, "ed": 111, "text": "et al"}, {"st": 118, "ed": 120, "text": "boosting algorithm"}, {"st": 124, "ed": 126, "text": "cost function"}, {"st": 132, "ed": 134, "text": "boosting algorithm"}, {"st": 147, "ed": 149, "text": "object detection"}, {"st": 155, "ed": 157, "text": "boosting algorithm"}, {"st": 163, "ed": 165, "text": "object detection"}]
[{"st": 0, "ed": 3, "text": "unsupervised feature learning"}, {"st": 5, "ed": 7, "text": "impressive results"}, {"st": 17, "ed": 19, "text": "object classification"}, {"st": 28, "ed": 30, "text": "unlabeled data"}, {"st": 30, "ed": 33, "text": "unsupervised feature learning"}, {"st": 48, "ed": 50, "text": "supervised classification"}, {"st": 62, "ed": 64, "text": "unsupervised learning"}, {"st": 68, "ed": 70, "text": "low level"}, {"st": 72, "ed": 74, "text": "image patches"}, {"st": 87, "ed": 89, "text": "unsupervised methods"}, {"st": 91, "ed": 93, "text": "low level"}, {"st": 102, "ed": 104, "text": "computer vision"}, {"st": 112, "ed": 116, "text": "restricted boltzmann machines rbms"}, {"st": 116, "ed": 118, "text": "performs comparably"}, {"st": 119, "ed": 121, "text": "hand crafted"}, {"st": 128, "ed": 130, "text": "compact representations"}]
[{"st": 0, "ed": 3, "text": "the paper presents"}, {"st": 10, "ed": 12, "text": "t sne"}, {"st": 34, "ed": 37, "text": "o n 2"}, {"st": 48, "ed": 50, "text": "pairwise similarities"}, {"st": 52, "ed": 54, "text": "input data"}, {"st": 104, "ed": 106, "text": "t sne"}]
[{"st": 0, "ed": 2, "text": "image denoising"}, {"st": 5, "ed": 7, "text": "probabilistic model"}, {"st": 9, "ed": 11, "text": "image patches"}, {"st": 21, "ed": 23, "text": "denoising autoencoder"}, {"st": 57, "ed": 59, "text": "deep learning"}, {"st": 64, "ed": 66, "text": "image denoising"}, {"st": 83, "ed": 85, "text": "empirically evaluate"}, {"st": 131, "ed": 133, "text": "hidden layers"}]
[{"st": 13, "ed": 16, "text": "multi layer perceptron"}, {"st": 84, "ed": 86, "text": "optimization methods"}, {"st": 126, "ed": 128, "text": "local optimum"}, {"st": 136, "ed": 138, "text": "hidden neurons"}]
[{"st": 6, "ed": 8, "text": "deep learning"}, {"st": 48, "ed": 50, "text": "generative model"}, {"st": 56, "ed": 58, "text": "latent representations"}, {"st": 62, "ed": 64, "text": "context sensitive"}, {"st": 69, "ed": 71, "text": "temporal dependencies"}, {"st": 114, "ed": 116, "text": "feature extraction"}]
[{"st": 0, "ed": 2, "text": "large scale"}, {"st": 24, "ed": 26, "text": "hamming distance"}, {"st": 28, "ed": 31, "text": "locality sensitive hashing"}, {"st": 50, "ed": 52, "text": "labeled data"}, {"st": 54, "ed": 57, "text": "distance metric learning"}, {"st": 61, "ed": 64, "text": "precision and recall"}, {"st": 66, "ed": 68, "text": "k means"}]
[{"st": 8, "ed": 11, "text": "hidden markov models"}, {"st": 14, "ed": 17, "text": "static and dynamic"}, {"st": 20, "ed": 23, "text": "image and video"}, {"st": 59, "ed": 61, "text": "dimensionality reduction"}, {"st": 62, "ed": 64, "text": "time series"}, {"st": 78, "ed": 80, "text": "em algorithm"}, {"st": 105, "ed": 107, "text": "handwritten digit"}, {"st": 112, "ed": 114, "text": "object tracking"}]
[{"st": 6, "ed": 8, "text": "product rule"}, {"st": 9, "ed": 11, "text": "classification problems"}, {"st": 12, "ed": 15, "text": "supervised machine learning"}, {"st": 26, "ed": 28, "text": "product rule"}, {"st": 37, "ed": 39, "text": "conditional independence"}, {"st": 47, "ed": 49, "text": "product rule"}, {"st": 85, "ed": 87, "text": "product rule"}]
[{"st": 21, "ed": 23, "text": "random fields"}, {"st": 68, "ed": 70, "text": "proposed approach"}, {"st": 98, "ed": 100, "text": "intra class"}]
[{"st": 0, "ed": 3, "text": "k nearest neighbor"}, {"st": 4, "ed": 6, "text": "k nnc"}, {"st": 20, "ed": 23, "text": "k nearest neighbor"}, {"st": 43, "ed": 45, "text": "k nnc"}, {"st": 47, "ed": 50, "text": "k nearest neighbors"}, {"st": 54, "ed": 56, "text": "nearest neighbors"}, {"st": 60, "ed": 62, "text": "linear interpolation"}, {"st": 65, "ed": 67, "text": "training set"}, {"st": 69, "ed": 71, "text": "training set"}, {"st": 82, "ed": 84, "text": "reduction techniques"}, {"st": 100, "ed": 102, "text": "k nnc"}, {"st": 106, "ed": 108, "text": "nearest neighbors"}, {"st": 112, "ed": 114, "text": "gaussian distribution"}, {"st": 116, "ed": 118, "text": "linear interpolation"}, {"st": 122, "ed": 124, "text": "k nnc"}, {"st": 142, "ed": 144, "text": "proposed method"}, {"st": 145, "ed": 147, "text": "closely related"}, {"st": 157, "ed": 159, "text": "experimentally demonstrate"}, {"st": 166, "ed": 168, "text": "proposed method"}]
[{"st": 35, "ed": 37, "text": "k means"}, {"st": 49, "ed": 51, "text": "distance function"}, {"st": 56, "ed": 58, "text": "k means"}, {"st": 62, "ed": 64, "text": "k means"}, {"st": 66, "ed": 68, "text": "k means"}, {"st": 73, "ed": 75, "text": "k means"}, {"st": 95, "ed": 97, "text": "k means"}]
[{"st": 0, "ed": 2, "text": "recent advances"}, {"st": 8, "ed": 10, "text": "computer vision"}, {"st": 27, "ed": 29, "text": "sparse coding"}, {"st": 37, "ed": 39, "text": "positive definite"}, {"st": 50, "ed": 52, "text": "recently introduced"}, {"st": 67, "ed": 69, "text": "sparse coding"}, {"st": 107, "ed": 109, "text": "sparse coding"}, {"st": 118, "ed": 120, "text": "classification tasks"}, {"st": 120, "ed": 122, "text": "face recognition"}, {"st": 122, "ed": 124, "text": "texture classification"}, {"st": 124, "ed": 127, "text": "person re identification"}, {"st": 131, "ed": 133, "text": "sparse coding"}, {"st": 133, "ed": 135, "text": "approach achieves"}, {"st": 151, "ed": 153, "text": "sparse coding"}]
[{"st": 0, "ed": 2, "text": "vision problems"}, {"st": 7, "ed": 9, "text": "motion segmentation"}, {"st": 10, "ed": 13, "text": "semi supervised learning"}, {"st": 18, "ed": 20, "text": "subspace segmentation"}, {"st": 28, "ed": 30, "text": "low dimensional"}, {"st": 37, "ed": 39, "text": "low rank"}, {"st": 46, "ed": 48, "text": "subspace segmentation"}, {"st": 76, "ed": 79, "text": "low rank matrix"}, {"st": 97, "ed": 100, "text": "divide and conquer"}, {"st": 102, "ed": 104, "text": "large scale"}, {"st": 104, "ed": 106, "text": "subspace segmentation"}, {"st": 130, "ed": 132, "text": "subspace segmentation"}, {"st": 138, "ed": 140, "text": "face recognition"}, {"st": 152, "ed": 154, "text": "subspace segmentation"}, {"st": 155, "ed": 157, "text": "large scale"}, {"st": 157, "ed": 160, "text": "semi supervised learning"}, {"st": 162, "ed": 164, "text": "event detection"}]
[{"st": 2, "ed": 4, "text": "facial expressions"}, {"st": 5, "ed": 7, "text": "spatio temporal"}, {"st": 12, "ed": 14, "text": "kernel methods"}, {"st": 17, "ed": 19, "text": "facial landmark"}, {"st": 28, "ed": 30, "text": "support vector"}, {"st": 35, "ed": 38, "text": "dynamic time warping"}]
[{"st": 8, "ed": 10, "text": "clustering algorithms"}, {"st": 42, "ed": 45, "text": "k means clustering"}, {"st": 66, "ed": 68, "text": "large scale"}, {"st": 80, "ed": 82, "text": "power law"}, {"st": 107, "ed": 109, "text": "k means"}, {"st": 118, "ed": 121, "text": "taking advantage of"}, {"st": 143, "ed": 145, "text": "power law"}, {"st": 188, "ed": 190, "text": "spectral clustering"}, {"st": 196, "ed": 198, "text": "clustering algorithm"}, {"st": 199, "ed": 201, "text": "variational inference"}, {"st": 215, "ed": 217, "text": "synthetic datasets"}, {"st": 218, "ed": 220, "text": "real world"}]
[{"st": 57, "ed": 59, "text": "local learning"}, {"st": 66, "ed": 68, "text": "hidden units"}, {"st": 75, "ed": 77, "text": "achieve competitive"}, {"st": 80, "ed": 82, "text": "wide variety"}, {"st": 100, "ed": 102, "text": "hand crafted"}, {"st": 102, "ed": 104, "text": "spatio temporal"}, {"st": 122, "ed": 124, "text": "parameter estimation"}]
[{"st": 52, "ed": 54, "text": "hyperparameter optimization"}, {"st": 61, "ed": 63, "text": "facial expression"}, {"st": 71, "ed": 73, "text": "hyperparameter optimization"}, {"st": 95, "ed": 97, "text": "hyperparameter optimization"}, {"st": 155, "ed": 157, "text": "hyperparameter optimization"}, {"st": 159, "ed": 161, "text": "domain expertise"}, {"st": 170, "ed": 172, "text": "image classification"}]
[{"st": 17, "ed": 19, "text": "log partition"}, {"st": 73, "ed": 75, "text": "unified framework"}, {"st": 106, "ed": 108, "text": "maximum likelihood"}, {"st": 111, "ed": 113, "text": "maximum margin"}, {"st": 113, "ed": 115, "text": "parameter estimation"}, {"st": 118, "ed": 121, "text": "conditional random fields"}, {"st": 125, "ed": 127, "text": "posterior probabilities"}, {"st": 131, "ed": 134, "text": "a posteriori map"}, {"st": 137, "ed": 139, "text": "maximum margin"}, {"st": 140, "ed": 142, "text": "parameter estimation"}, {"st": 143, "ed": 145, "text": "image denoising"}]
[{"st": 39, "ed": 41, "text": "exact inference"}, {"st": 50, "ed": 52, "text": "approximate inference"}, {"st": 56, "ed": 58, "text": "linear programming"}, {"st": 113, "ed": 115, "text": "l 2"}]
[{"st": 10, "ed": 12, "text": "graph clustering"}, {"st": 47, "ed": 49, "text": "graph clustering"}, {"st": 62, "ed": 64, "text": "distance based"}, {"st": 132, "ed": 134, "text": "graph clustering"}, {"st": 146, "ed": 148, "text": "special cases"}]
[{"st": 21, "ed": 23, "text": "domain shift"}, {"st": 40, "ed": 42, "text": "scene understanding"}, {"st": 48, "ed": 50, "text": "performance degradation"}, {"st": 63, "ed": 65, "text": "real world"}, {"st": 76, "ed": 78, "text": "domain adaptation"}, {"st": 84, "ed": 86, "text": "key idea"}, {"st": 96, "ed": 98, "text": "max margin"}, {"st": 98, "ed": 100, "text": "domain adaptation"}, {"st": 132, "ed": 134, "text": "large scale"}]
[{"st": 0, "ed": 2, "text": "convex optimization"}, {"st": 50, "ed": 52, "text": "regularization term"}, {"st": 57, "ed": 59, "text": "cost function"}, {"st": 64, "ed": 66, "text": "regularization terms"}, {"st": 81, "ed": 83, "text": "without sacrificing"}, {"st": 87, "ed": 89, "text": "convex optimization"}, {"st": 102, "ed": 104, "text": "recently developed"}, {"st": 113, "ed": 115, "text": "group sparse"}, {"st": 124, "ed": 126, "text": "speech enhancement"}]
[{"st": 1, "ed": 3, "text": "recent years"}, {"st": 4, "ed": 6, "text": "spectral analysis"}, {"st": 9, "ed": 11, "text": "kernel matrices"}, {"st": 20, "ed": 22, "text": "low dimensional"}, {"st": 35, "ed": 37, "text": "spectral methods"}, {"st": 64, "ed": 66, "text": "selection process"}, {"st": 72, "ed": 74, "text": "kernel based"}, {"st": 75, "ed": 77, "text": "partial information"}, {"st": 81, "ed": 83, "text": "spectral analysis"}, {"st": 102, "ed": 104, "text": "performance bounds"}, {"st": 128, "ed": 130, "text": "real world"}, {"st": 136, "ed": 138, "text": "computer vision"}, {"st": 139, "ed": 142, "text": "low dimensional manifold"}]
[]
[{"st": 90, "ed": 92, "text": "generative models"}, {"st": 97, "ed": 99, "text": "real valued"}, {"st": 110, "ed": 112, "text": "transfer learning"}, {"st": 117, "ed": 119, "text": "real world"}]
[{"st": 31, "ed": 33, "text": "feature vectors"}, {"st": 44, "ed": 46, "text": "feature extraction"}]
[{"st": 1, "ed": 4, "text": "hidden markov model"}, {"st": 9, "ed": 11, "text": "generative model"}, {"st": 14, "ed": 16, "text": "sequential data"}, {"st": 27, "ed": 29, "text": "hidden markov"}, {"st": 49, "ed": 51, "text": "proposed algorithm"}, {"st": 100, "ed": 102, "text": "generative model"}, {"st": 122, "ed": 124, "text": "optimization problem"}, {"st": 125, "ed": 127, "text": "efficiently solved"}, {"st": 141, "ed": 143, "text": "proposed algorithm"}, {"st": 153, "ed": 155, "text": "tasks involving"}, {"st": 155, "ed": 157, "text": "time series"}, {"st": 160, "ed": 162, "text": "hierarchical clustering"}, {"st": 163, "ed": 165, "text": "motion capture"}]
[{"st": 5, "ed": 7, "text": "discrete optimization"}, {"st": 46, "ed": 48, "text": "pair wise"}, {"st": 77, "ed": 80, "text": "coarse to fine"}, {"st": 89, "ed": 91, "text": "significant improvement"}]
[{"st": 7, "ed": 9, "text": "energy minimization"}, {"st": 17, "ed": 19, "text": "computer vision"}, {"st": 84, "ed": 86, "text": "expressive power"}, {"st": 88, "ed": 90, "text": "objective function"}, {"st": 107, "ed": 109, "text": "computer vision"}, {"st": 124, "ed": 126, "text": "approximation algorithms"}]
[{"st": 13, "ed": 15, "text": "multi class"}, {"st": 15, "ed": 17, "text": "image classification"}, {"st": 43, "ed": 46, "text": "support vector machines"}, {"st": 49, "ed": 52, "text": "directed acyclic graph"}, {"st": 53, "ed": 56, "text": "artificial neural networks"}, {"st": 62, "ed": 64, "text": "sign language"}, {"st": 74, "ed": 76, "text": "multi class"}, {"st": 95, "ed": 98, "text": "accuracy and efficiency"}, {"st": 118, "ed": 120, "text": "statistically significant"}]
[{"st": 22, "ed": 25, "text": "block coordinate descent"}, {"st": 129, "ed": 131, "text": "rank factorization"}, {"st": 136, "ed": 138, "text": "low rank"}, {"st": 142, "ed": 145, "text": "extensive experimental results"}, {"st": 148, "ed": 150, "text": "proposed method"}]
[{"st": 1, "ed": 3, "text": "classification problems"}, {"st": 14, "ed": 16, "text": "supervised setting"}, {"st": 18, "ed": 21, "text": "training and test"}, {"st": 38, "ed": 40, "text": "feature vectors"}, {"st": 71, "ed": 73, "text": "supervised learning"}]
[{"st": 9, "ed": 11, "text": "challenging task"}, {"st": 14, "ed": 16, "text": "real applications"}, {"st": 20, "ed": 22, "text": "compressive sensing"}, {"st": 34, "ed": 37, "text": "low rank matrix"}, {"st": 52, "ed": 54, "text": "recent theoretical"}, {"st": 57, "ed": 59, "text": "nuclear norm"}, {"st": 61, "ed": 63, "text": "convex relaxation"}, {"st": 65, "ed": 67, "text": "low rank"}, {"st": 68, "ed": 70, "text": "recovery problem"}, {"st": 76, "ed": 78, "text": "nuclear norm"}, {"st": 85, "ed": 87, "text": "nuclear norm"}, {"st": 92, "ed": 94, "text": "singular values"}, {"st": 116, "ed": 118, "text": "multi stage"}, {"st": 127, "ed": 129, "text": "nuclear norm"}, {"st": 150, "ed": 152, "text": "matrix completion"}, {"st": 158, "ed": 160, "text": "proposed method"}, {"st": 167, "ed": 169, "text": "low rank"}, {"st": 172, "ed": 174, "text": "extensive experiments"}]
[{"st": 4, "ed": 6, "text": "visual recognition"}, {"st": 9, "ed": 11, "text": "image representations"}, {"st": 29, "ed": 33, "text": "convolutional neural network cnn"}, {"st": 41, "ed": 43, "text": "unlike traditional"}, {"st": 45, "ed": 47, "text": "neural networks"}, {"st": 57, "ed": 59, "text": "classification task"}, {"st": 66, "ed": 68, "text": "feature map"}, {"st": 90, "ed": 92, "text": "network architectures"}, {"st": 95, "ed": 97, "text": "similar accuracy"}, {"st": 117, "ed": 119, "text": "neural network"}, {"st": 134, "ed": 136, "text": "visual recognition"}, {"st": 145, "ed": 147, "text": "digit recognition"}, {"st": 149, "ed": 151, "text": "mnist dataset"}, {"st": 155, "ed": 157, "text": "cifar 10"}]
[{"st": 0, "ed": 3, "text": "person re identification"}, {"st": 94, "ed": 96, "text": "graph matching"}, {"st": 99, "ed": 101, "text": "edge weights"}, {"st": 109, "ed": 111, "text": "co occurrences"}, {"st": 119, "ed": 121, "text": "co occurrence"}, {"st": 135, "ed": 137, "text": "co occurrences"}]
[{"st": 0, "ed": 2, "text": "multi task"}, {"st": 2, "ed": 4, "text": "feature learning"}, {"st": 45, "ed": 47, "text": "ell 1"}, {"st": 47, "ed": 49, "text": "ell 1"}, {"st": 59, "ed": 61, "text": "multi stage"}, {"st": 61, "ed": 63, "text": "multi task"}, {"st": 63, "ed": 65, "text": "feature learning"}, {"st": 83, "ed": 85, "text": "ell 1"}, {"st": 85, "ed": 87, "text": "ell 1"}, {"st": 111, "ed": 113, "text": "ell 1"}, {"st": 113, "ed": 115, "text": "ell 1"}, {"st": 142, "ed": 144, "text": "feature selection"}, {"st": 160, "ed": 162, "text": "previously proposed"}, {"st": 170, "ed": 172, "text": "empirical studies"}, {"st": 174, "ed": 178, "text": "synthetic and real world"}]
[{"st": 4, "ed": 7, "text": "nearest neighbor search"}, {"st": 9, "ed": 11, "text": "inner product"}, {"st": 55, "ed": 57, "text": "inner product"}, {"st": 90, "ed": 92, "text": "selection algorithm"}, {"st": 107, "ed": 110, "text": "accuracy and efficiency"}, {"st": 118, "ed": 121, "text": "theoretical and empirical"}, {"st": 125, "ed": 127, "text": "large scale"}, {"st": 131, "ed": 133, "text": "sift features"}, {"st": 134, "ed": 136, "text": "linear models"}]
[{"st": 0, "ed": 2, "text": "pattern recognition"}, {"st": 3, "ed": 5, "text": "machine learning"}, {"st": 22, "ed": 24, "text": "machine learning"}, {"st": 54, "ed": 56, "text": "unified framework"}, {"st": 58, "ed": 60, "text": "pattern recognition"}, {"st": 91, "ed": 93, "text": "pattern recognition"}, {"st": 113, "ed": 115, "text": "pattern recognition"}, {"st": 116, "ed": 118, "text": "machine learning"}, {"st": 122, "ed": 124, "text": "pattern recognition"}, {"st": 140, "ed": 142, "text": "supervised learning"}, {"st": 145, "ed": 147, "text": "feature selection"}, {"st": 168, "ed": 170, "text": "cross validation"}]
[{"st": 10, "ed": 12, "text": "robust subspace"}, {"st": 22, "ed": 24, "text": "low dimensional"}, {"st": 27, "ed": 29, "text": "higher dimensional"}, {"st": 46, "ed": 48, "text": "proposed algorithm"}, {"st": 72, "ed": 74, "text": "computational complexity"}, {"st": 105, "ed": 107, "text": "global minimum"}, {"st": 146, "ed": 148, "text": "limit point"}, {"st": 152, "ed": 154, "text": "numerical experiments"}, {"st": 155, "ed": 159, "text": "synthetic and real data"}]
[{"st": 1, "ed": 4, "text": "convolutional neural networks"}, {"st": 8, "ed": 10, "text": "computationally expensive"}, {"st": 15, "ed": 17, "text": "scales linearly"}, {"st": 27, "ed": 30, "text": "recurrent neural network"}, {"st": 35, "ed": 37, "text": "extracting information"}, {"st": 61, "ed": 64, "text": "convolutional neural networks"}, {"st": 88, "ed": 90, "text": "input image"}, {"st": 102, "ed": 104, "text": "reinforcement learning"}, {"st": 107, "ed": 109, "text": "task specific"}, {"st": 116, "ed": 118, "text": "image classification"}, {"st": 121, "ed": 123, "text": "significantly outperforms"}, {"st": 124, "ed": 127, "text": "convolutional neural network"}, {"st": 136, "ed": 138, "text": "control problem"}]
[{"st": 3, "ed": 6, "text": "k means clustering"}, {"st": 7, "ed": 9, "text": "np hard"}, {"st": 21, "ed": 23, "text": "k means"}, {"st": 88, "ed": 90, "text": "cluster centers"}, {"st": 94, "ed": 96, "text": "k means"}, {"st": 107, "ed": 109, "text": "k means"}, {"st": 129, "ed": 131, "text": "k means"}, {"st": 139, "ed": 141, "text": "k means"}, {"st": 157, "ed": 159, "text": "k means"}, {"st": 169, "ed": 171, "text": "de facto"}, {"st": 185, "ed": 187, "text": "k means"}, {"st": 197, "ed": 199, "text": "cluster centers"}, {"st": 215, "ed": 217, "text": "k means"}]
[{"st": 4, "ed": 6, "text": "feature selection"}, {"st": 10, "ed": 12, "text": "related tasks"}, {"st": 34, "ed": 36, "text": "feature selection"}, {"st": 37, "ed": 39, "text": "multiple tasks"}, {"st": 45, "ed": 47, "text": "probabilistic framework"}, {"st": 63, "ed": 65, "text": "norm regularization"}, {"st": 79, "ed": 81, "text": "optimization problem"}, {"st": 112, "ed": 114, "text": "convex optimization"}, {"st": 137, "ed": 139, "text": "building block"}, {"st": 175, "ed": 177, "text": "empirical evaluations"}]
[{"st": 11, "ed": 13, "text": "graphical model"}, {"st": 14, "ed": 17, "text": "continuous and discrete"}, {"st": 29, "ed": 32, "text": "continuous and discrete"}, {"st": 45, "ed": 47, "text": "structure learning"}, {"st": 48, "ed": 51, "text": "gaussian graphical models"}, {"st": 52, "ed": 54, "text": "structure learning"}, {"st": 83, "ed": 85, "text": "group lasso"}]
[{"st": 73, "ed": 75, "text": "machine learning"}, {"st": 75, "ed": 77, "text": "pac bayes"}, {"st": 88, "ed": 90, "text": "real valued"}, {"st": 108, "ed": 110, "text": "provide evidence"}]
[{"st": 22, "ed": 24, "text": "social network"}, {"st": 81, "ed": 83, "text": "multi layer"}, {"st": 132, "ed": 134, "text": "low dimensional"}, {"st": 143, "ed": 145, "text": "important information"}, {"st": 156, "ed": 158, "text": "clustering methods"}, {"st": 164, "ed": 169, "text": "synthetic and real world datasets"}, {"st": 186, "ed": 188, "text": "generic framework"}, {"st": 194, "ed": 196, "text": "learning problems"}]
[{"st": 11, "ed": 13, "text": "component analysis"}, {"st": 32, "ed": 34, "text": "component analysis"}, {"st": 43, "ed": 45, "text": "component analysis"}, {"st": 57, "ed": 59, "text": "component analysis"}, {"st": 62, "ed": 65, "text": "principal component analysis"}, {"st": 66, "ed": 69, "text": "linear discriminant analysis"}, {"st": 94, "ed": 98, "text": "markov random fields mrfs"}, {"st": 106, "ed": 108, "text": "component analysis"}, {"st": 129, "ed": 131, "text": "maximum likelihood"}, {"st": 136, "ed": 138, "text": "joint probability"}, {"st": 138, "ed": 140, "text": "density function"}, {"st": 159, "ed": 162, "text": "expectation maximization em"}, {"st": 181, "ed": 183, "text": "theoretical analysis"}, {"st": 187, "ed": 192, "text": "simulated and real world data"}, {"st": 197, "ed": 199, "text": "proposed framework"}]
[{"st": 1, "ed": 3, "text": "l 2"}, {"st": 8, "ed": 10, "text": "widely applied"}, {"st": 15, "ed": 17, "text": "computer vision"}, {"st": 17, "ed": 19, "text": "pattern recognition"}, {"st": 33, "ed": 35, "text": "l 2"}, {"st": 49, "ed": 51, "text": "iterative algorithm"}, {"st": 56, "ed": 58, "text": "l 2"}, {"st": 71, "ed": 74, "text": "0 p 1"}, {"st": 97, "ed": 99, "text": "l 2"}, {"st": 121, "ed": 123, "text": "l 2"}, {"st": 128, "ed": 131, "text": "0 p 1"}, {"st": 143, "ed": 145, "text": "l 2"}, {"st": 178, "ed": 180, "text": "computational biology"}, {"st": 189, "ed": 192, "text": "0 p 1"}]
[{"st": 3, "ed": 5, "text": "computer vision"}, {"st": 5, "ed": 7, "text": "machine learning"}, {"st": 20, "ed": 22, "text": "sparse representation"}, {"st": 101, "ed": 103, "text": "image patches"}, {"st": 192, "ed": 194, "text": "special case"}]
[{"st": 4, "ed": 6, "text": "successfully applied"}, {"st": 24, "ed": 26, "text": "low dimensional"}, {"st": 45, "ed": 47, "text": "input space"}, {"st": 63, "ed": 66, "text": "kernel ridge regression"}, {"st": 107, "ed": 109, "text": "training data"}, {"st": 117, "ed": 119, "text": "low dimensional"}, {"st": 141, "ed": 143, "text": "convex optimization"}, {"st": 156, "ed": 158, "text": "approximation error"}, {"st": 166, "ed": 168, "text": "error bound"}, {"st": 189, "ed": 191, "text": "input images"}, {"st": 193, "ed": 195, "text": "low dimensional"}]
[{"st": 1, "ed": 3, "text": "visual tracking"}, {"st": 17, "ed": 19, "text": "main challenges"}, {"st": 90, "ed": 92, "text": "random walk"}, {"st": 94, "ed": 96, "text": "riemannian manifold"}, {"st": 119, "ed": 121, "text": "positive semidefinite"}, {"st": 158, "ed": 160, "text": "posterior distribution"}, {"st": 178, "ed": 180, "text": "principled approach"}, {"st": 207, "ed": 210, "text": "principal component analysis"}, {"st": 223, "ed": 225, "text": "comparable performance"}]
[{"st": 2, "ed": 5, "text": "k means clustering"}, {"st": 5, "ed": 7, "text": "method works"}, {"st": 37, "ed": 39, "text": "existing algorithms"}, {"st": 51, "ed": 53, "text": "distance matrix"}, {"st": 66, "ed": 68, "text": "k means"}, {"st": 86, "ed": 88, "text": "k means"}]
[{"st": 1, "ed": 3, "text": "low rank"}, {"st": 4, "ed": 6, "text": "learning framework"}, {"st": 7, "ed": 9, "text": "subspace clustering"}, {"st": 20, "ed": 22, "text": "face images"}, {"st": 31, "ed": 33, "text": "low dimensional"}, {"st": 36, "ed": 38, "text": "subspace clustering"}, {"st": 41, "ed": 43, "text": "extensively studied"}, {"st": 58, "ed": 60, "text": "low dimensional"}, {"st": 62, "ed": 64, "text": "low dimensional"}, {"st": 70, "ed": 72, "text": "real world"}, {"st": 93, "ed": 95, "text": "linear transformation"}, {"st": 102, "ed": 104, "text": "convex surrogate"}, {"st": 104, "ed": 106, "text": "nuclear norm"}, {"st": 112, "ed": 114, "text": "linear transformation"}, {"st": 116, "ed": 118, "text": "low rank"}, {"st": 157, "ed": 159, "text": "robust subspace"}, {"st": 163, "ed": 165, "text": "robust subspace"}, {"st": 173, "ed": 175, "text": "subspace clustering"}, {"st": 177, "ed": 179, "text": "theoretical results"}, {"st": 191, "ed": 193, "text": "low rank"}, {"st": 203, "ed": 205, "text": "subspace clustering"}, {"st": 209, "ed": 211, "text": "robust pca"}, {"st": 215, "ed": 217, "text": "class labels"}, {"st": 226, "ed": 228, "text": "low rank"}, {"st": 235, "ed": 237, "text": "extensive experiments"}, {"st": 245, "ed": 247, "text": "proposed approach"}, {"st": 247, "ed": 249, "text": "significantly outperforms"}, {"st": 255, "ed": 257, "text": "subspace clustering"}]
[{"st": 10, "ed": 12, "text": "linear programming"}, {"st": 20, "ed": 23, "text": "k means clustering"}, {"st": 98, "ed": 100, "text": "pairwise distances"}]
[{"st": 14, "ed": 16, "text": "object appearance"}, {"st": 46, "ed": 48, "text": "visual semantic"}, {"st": 88, "ed": 90, "text": "nearest neighbor"}, {"st": 90, "ed": 93, "text": "latent dirichlet allocation"}, {"st": 129, "ed": 131, "text": "visual tasks"}]
[{"st": 0, "ed": 3, "text": "linear discriminant analysis"}, {"st": 10, "ed": 12, "text": "dimensionality reduction"}, {"st": 14, "ed": 16, "text": "previous studies"}, {"st": 31, "ed": 33, "text": "object detection"}, {"st": 47, "ed": 49, "text": "instance level"}, {"st": 55, "ed": 57, "text": "directly applied"}, {"st": 58, "ed": 61, "text": "semi supervised classification"}, {"st": 72, "ed": 74, "text": "latent variable"}, {"st": 81, "ed": 83, "text": "instance level"}, {"st": 91, "ed": 93, "text": "semi supervised"}, {"st": 94, "ed": 96, "text": "level labels"}, {"st": 120, "ed": 122, "text": "latent variable"}, {"st": 143, "ed": 145, "text": "competitive results"}, {"st": 170, "ed": 172, "text": "ranking based"}, {"st": 178, "ed": 180, "text": "proposed method"}, {"st": 183, "ed": 185, "text": "semantically meaningful"}]
[{"st": 5, "ed": 7, "text": "clustering algorithm"}, {"st": 13, "ed": 16, "text": "l 1 norm"}, {"st": 18, "ed": 20, "text": "pair wise"}, {"st": 33, "ed": 35, "text": "group sparsity"}, {"st": 49, "ed": 51, "text": "non trivial"}, {"st": 65, "ed": 68, "text": "l 1 norm"}, {"st": 93, "ed": 95, "text": "inverse problems"}, {"st": 171, "ed": 173, "text": "regularization parameter"}, {"st": 175, "ed": 177, "text": "pair wise"}, {"st": 191, "ed": 193, "text": "group sparse"}]
[{"st": 4, "ed": 6, "text": "spectral clustering"}, {"st": 9, "ed": 11, "text": "subspace clustering"}, {"st": 14, "ed": 16, "text": "similarity graph"}, {"st": 25, "ed": 27, "text": "recent works"}, {"st": 32, "ed": 34, "text": "low rank"}, {"st": 35, "ed": 37, "text": "ell 2"}, {"st": 83, "ed": 85, "text": "large scale"}, {"st": 117, "ed": 119, "text": "similarity graph"}, {"st": 134, "ed": 136, "text": "unified framework"}, {"st": 140, "ed": 142, "text": "subspace clustering"}, {"st": 151, "ed": 153, "text": "large scale"}, {"st": 158, "ed": 160, "text": "large scale"}, {"st": 188, "ed": 190, "text": "error bounds"}, {"st": 200, "ed": 203, "text": "extensive experimental results"}, {"st": 205, "ed": 208, "text": "benchmark data sets"}, {"st": 211, "ed": 213, "text": "methods outperform"}, {"st": 214, "ed": 216, "text": "recently proposed"}, {"st": 220, "ed": 222, "text": "large scale"}]
[{"st": 0, "ed": 2, "text": "submodular functions"}, {"st": 11, "ed": 13, "text": "special case"}, {"st": 28, "ed": 30, "text": "computer vision"}, {"st": 80, "ed": 82, "text": "higher order"}, {"st": 86, "ed": 88, "text": "image patches"}, {"st": 96, "ed": 98, "text": "expressive power"}, {"st": 110, "ed": 112, "text": "higher order"}, {"st": 120, "ed": 122, "text": "discriminative learning"}, {"st": 132, "ed": 134, "text": "higher order"}, {"st": 163, "ed": 165, "text": "quadratic programming"}, {"st": 180, "ed": 182, "text": "cutting plane"}, {"st": 196, "ed": 198, "text": "vision problems"}, {"st": 205, "ed": 207, "text": "efficiently solve"}, {"st": 230, "ed": 232, "text": "hand tuned"}, {"st": 247, "ed": 249, "text": "higher order"}, {"st": 253, "ed": 255, "text": "parameter values"}]
[{"st": 12, "ed": 14, "text": "activation functions"}, {"st": 15, "ed": 18, "text": "convolutional neural network"}, {"st": 19, "ed": 23, "text": "rectified linear unit relu"}, {"st": 24, "ed": 27, "text": "rectified linear unit"}, {"st": 30, "ed": 33, "text": "rectified linear unit"}, {"st": 39, "ed": 42, "text": "rectified linear units"}, {"st": 47, "ed": 49, "text": "activation function"}, {"st": 51, "ed": 53, "text": "image classification"}, {"st": 96, "ed": 98, "text": "small scale"}, {"st": 129, "ed": 131, "text": "cifar 100"}]
[{"st": 4, "ed": 6, "text": "batch normalization"}, {"st": 16, "ed": 18, "text": "deep networks"}, {"st": 20, "ed": 22, "text": "covariate shift"}, {"st": 48, "ed": 50, "text": "standard deviation"}, {"st": 54, "ed": 56, "text": "hidden layers"}, {"st": 60, "ed": 62, "text": "parameter values"}, {"st": 65, "ed": 67, "text": "training epochs"}, {"st": 80, "ed": 82, "text": "batch size"}, {"st": 100, "ed": 102, "text": "covariate shift"}, {"st": 128, "ed": 130, "text": "standard deviation"}, {"st": 149, "ed": 152, "text": "rectified linear units"}, {"st": 153, "ed": 155, "text": "gaussian distribution"}, {"st": 156, "ed": 158, "text": "deep networks"}]
[{"st": 2, "ed": 4, "text": "belief propagation"}, {"st": 7, "ed": 9, "text": "bipartite graph"}, {"st": 13, "ed": 15, "text": "hidden variables"}, {"st": 26, "ed": 29, "text": "independent component analysis"}, {"st": 73, "ed": 75, "text": "generative model"}]
[{"st": 8, "ed": 10, "text": "semi supervised"}, {"st": 10, "ed": 12, "text": "image classification"}, {"st": 14, "ed": 17, "text": "unsupervised representation learning"}, {"st": 29, "ed": 32, "text": "unsupervised representation learning"}, {"st": 53, "ed": 56, "text": "labeled and unlabeled"}, {"st": 112, "ed": 114, "text": "image features"}, {"st": 126, "ed": 128, "text": "features extracted"}, {"st": 145, "ed": 148, "text": "unsupervised representation learning"}, {"st": 150, "ed": 152, "text": "logistic regression"}, {"st": 165, "ed": 169, "text": "supervised and semi supervised"}]
[{"st": 5, "ed": 7, "text": "big data"}, {"st": 17, "ed": 19, "text": "recognition systems"}, {"st": 34, "ed": 36, "text": "training data"}, {"st": 69, "ed": 71, "text": "open world"}, {"st": 138, "ed": 140, "text": "open world"}, {"st": 151, "ed": 153, "text": "incremental learning"}, {"st": 173, "ed": 175, "text": "local learning"}, {"st": 186, "ed": 188, "text": "metric learning"}, {"st": 204, "ed": 206, "text": "large scale"}, {"st": 217, "ed": 219, "text": "methods outperform"}, {"st": 228, "ed": 230, "text": "online learning"}, {"st": 238, "ed": 240, "text": "open world"}]
[{"st": 10, "ed": 12, "text": "learning algorithms"}, {"st": 12, "ed": 14, "text": "predictive models"}, {"st": 29, "ed": 31, "text": "data stream"}, {"st": 36, "ed": 38, "text": "parameter tuning"}, {"st": 55, "ed": 57, "text": "class labels"}, {"st": 74, "ed": 76, "text": "streaming data"}, {"st": 91, "ed": 93, "text": "feature space"}, {"st": 100, "ed": 102, "text": "active learning"}, {"st": 116, "ed": 118, "text": "extensive experiments"}, {"st": 119, "ed": 121, "text": "standard benchmarks"}]
[{"st": 7, "ed": 9, "text": "domain adaptation"}, {"st": 14, "ed": 16, "text": "probabilistic framework"}, {"st": 17, "ed": 20, "text": "gaussian processes gps"}, {"st": 24, "ed": 26, "text": "domain specific"}, {"st": 31, "ed": 33, "text": "facial expression"}, {"st": 53, "ed": 55, "text": "multiple source"}, {"st": 111, "ed": 113, "text": "proposed approach"}, {"st": 115, "ed": 118, "text": "publicly available datasets"}, {"st": 119, "ed": 121, "text": "multi class"}, {"st": 123, "ed": 125, "text": "multi label"}, {"st": 126, "ed": 128, "text": "facial expression"}, {"st": 152, "ed": 154, "text": "proposed approach"}, {"st": 154, "ed": 156, "text": "consistently outperforms"}, {"st": 157, "ed": 160, "text": "source and target"}]
[{"st": 8, "ed": 11, "text": "end to end"}, {"st": 43, "ed": 45, "text": "spatial transformer"}, {"st": 46, "ed": 48, "text": "recurrent network"}, {"st": 74, "ed": 76, "text": "context aware"}]
[{"st": 1, "ed": 3, "text": "significant progress"}, {"st": 11, "ed": 13, "text": "face verification"}, {"st": 25, "ed": 27, "text": "deep cnn"}, {"st": 27, "ed": 29, "text": "based approach"}, {"st": 31, "ed": 33, "text": "low dimensional"}, {"st": 44, "ed": 46, "text": "face verification"}, {"st": 50, "ed": 52, "text": "performance improvements"}, {"st": 63, "ed": 65, "text": "post processing"}, {"st": 80, "ed": 82, "text": "proposed algorithm"}, {"st": 82, "ed": 84, "text": "performs comparably"}, {"st": 102, "ed": 104, "text": "training data"}, {"st": 112, "ed": 114, "text": "proposed method"}, {"st": 125, "ed": 127, "text": "deep cnn"}, {"st": 140, "ed": 142, "text": "deep features"}]
[{"st": 10, "ed": 12, "text": "consensus clustering"}, {"st": 28, "ed": 30, "text": "least square"}, {"st": 43, "ed": 45, "text": "consensus clustering"}, {"st": 110, "ed": 112, "text": "consensus clustering"}]
[{"st": 42, "ed": 44, "text": "error correcting"}, {"st": 51, "ed": 53, "text": "feature selection"}, {"st": 74, "ed": 76, "text": "classification problem"}, {"st": 78, "ed": 80, "text": "binary classification"}, {"st": 99, "ed": 101, "text": "image features"}, {"st": 104, "ed": 107, "text": "principal components analysis"}, {"st": 123, "ed": 126, "text": "support vector machine"}, {"st": 132, "ed": 134, "text": "empirical results"}, {"st": 137, "ed": 139, "text": "ensemble method"}, {"st": 141, "ed": 144, "text": "real world data"}, {"st": 164, "ed": 166, "text": "ensemble methods"}, {"st": 189, "ed": 191, "text": "feature selection"}]
[{"st": 5, "ed": 8, "text": "variational auto encoder"}, {"st": 10, "ed": 12, "text": "significantly improves"}, {"st": 23, "ed": 25, "text": "latent variable"}, {"st": 52, "ed": 54, "text": "desired properties"}, {"st": 74, "ed": 76, "text": "achieve high"}]
[{"st": 39, "ed": 41, "text": "improved performance"}, {"st": 42, "ed": 44, "text": "image classification"}, {"st": 50, "ed": 52, "text": "random fourier"}, {"st": 64, "ed": 67, "text": "principal component analysis"}, {"st": 89, "ed": 91, "text": "constant factor"}, {"st": 109, "ed": 111, "text": "performance improvements"}, {"st": 114, "ed": 116, "text": "experiments conducted"}, {"st": 118, "ed": 120, "text": "pascal voc"}, {"st": 129, "ed": 131, "text": "statistically significant"}]
[{"st": 0, "ed": 2, "text": "sparse coding"}, {"st": 8, "ed": 10, "text": "local features"}, {"st": 24, "ed": 26, "text": "spatio temporal"}, {"st": 55, "ed": 57, "text": "multi view"}, {"st": 57, "ed": 59, "text": "feature learning"}, {"st": 62, "ed": 64, "text": "hidden variables"}, {"st": 104, "ed": 106, "text": "invariant features"}, {"st": 113, "ed": 115, "text": "learning representations"}]
[{"st": 6, "ed": 8, "text": "fully bayesian"}, {"st": 8, "ed": 10, "text": "latent variable"}, {"st": 25, "ed": 27, "text": "latent space"}, {"st": 36, "ed": 38, "text": "multiple views"}, {"st": 44, "ed": 46, "text": "previous approaches"}, {"st": 84, "ed": 86, "text": "extremely high"}, {"st": 112, "ed": 114, "text": "trained model"}, {"st": 149, "ed": 151, "text": "latent space"}]
[{"st": 1, "ed": 3, "text": "recent years"}, {"st": 3, "ed": 5, "text": "total variation"}, {"st": 13, "ed": 15, "text": "successfully applied"}, {"st": 16, "ed": 18, "text": "image processing"}, {"st": 35, "ed": 37, "text": "supervised learning"}, {"st": 43, "ed": 45, "text": "supervised learning"}, {"st": 55, "ed": 57, "text": "tikhonov regularization"}, {"st": 65, "ed": 67, "text": "squared loss"}, {"st": 69, "ed": 71, "text": "total variation"}, {"st": 105, "ed": 107, "text": "radial basis"}, {"st": 132, "ed": 134, "text": "supervised learning"}, {"st": 134, "ed": 136, "text": "tasks including"}, {"st": 138, "ed": 141, "text": "multi class classification"}, {"st": 147, "ed": 149, "text": "extensive experiments"}, {"st": 151, "ed": 153, "text": "promising results"}]
[{"st": 7, "ed": 9, "text": "sparse coding"}, {"st": 32, "ed": 34, "text": "block coordinate"}, {"st": 43, "ed": 45, "text": "sparse coding"}, {"st": 50, "ed": 52, "text": "feed forward"}, {"st": 64, "ed": 66, "text": "sparse codes"}, {"st": 84, "ed": 86, "text": "training objective"}, {"st": 130, "ed": 133, "text": "orders of magnitude"}, {"st": 143, "ed": 145, "text": "performance degradation"}, {"st": 147, "ed": 149, "text": "proposed framework"}, {"st": 154, "ed": 156, "text": "large scale"}]
[{"st": 0, "ed": 2, "text": "random projections"}, {"st": 7, "ed": 9, "text": "machine learning"}, {"st": 16, "ed": 18, "text": "random projection"}, {"st": 19, "ed": 21, "text": "non trivial"}, {"st": 33, "ed": 35, "text": "random projection"}, {"st": 43, "ed": 45, "text": "binary classification"}, {"st": 57, "ed": 59, "text": "provide theoretical"}]
[{"st": 6, "ed": 8, "text": "dimensionality reduction"}, {"st": 10, "ed": 12, "text": "dimensionality reduction"}, {"st": 17, "ed": 19, "text": "objective function"}, {"st": 27, "ed": 29, "text": "training error"}, {"st": 65, "ed": 68, "text": "linear dimensionality reduction"}, {"st": 106, "ed": 108, "text": "transfer learning"}]
[{"st": 4, "ed": 6, "text": "low rank"}, {"st": 32, "ed": 34, "text": "low rank"}, {"st": 39, "ed": 41, "text": "clustering performance"}, {"st": 55, "ed": 57, "text": "random walk"}, {"st": 73, "ed": 75, "text": "approximation error"}, {"st": 77, "ed": 80, "text": "kullback leibler divergence"}, {"st": 88, "ed": 90, "text": "discriminative model"}, {"st": 140, "ed": 142, "text": "strong performance"}, {"st": 150, "ed": 152, "text": "large scale"}]
[{"st": 2, "ed": 4, "text": "structured data"}, {"st": 11, "ed": 13, "text": "poorly understood"}, {"st": 55, "ed": 57, "text": "metric learning"}]
[{"st": 1, "ed": 3, "text": "invariant representations"}, {"st": 8, "ed": 10, "text": "machine learning"}, {"st": 23, "ed": 25, "text": "invariant feature"}, {"st": 28, "ed": 30, "text": "linear transformations"}, {"st": 32, "ed": 34, "text": "feature learning"}, {"st": 42, "ed": 45, "text": "restricted boltzmann machine"}, {"st": 60, "ed": 62, "text": "feature representation"}, {"st": 73, "ed": 75, "text": "invariant feature"}, {"st": 75, "ed": 77, "text": "learning framework"}, {"st": 83, "ed": 85, "text": "unsupervised learning"}, {"st": 98, "ed": 100, "text": "image classification"}, {"st": 100, "ed": 102, "text": "benchmark datasets"}, {"st": 106, "ed": 108, "text": "cifar 10"}, {"st": 116, "ed": 118, "text": "classification performance"}, {"st": 128, "ed": 130, "text": "method achieves"}, {"st": 137, "ed": 139, "text": "classification tasks"}, {"st": 145, "ed": 147, "text": "wide applicability"}]
[{"st": 12, "ed": 14, "text": "machine learning"}, {"st": 15, "ed": 17, "text": "computer vision"}, {"st": 26, "ed": 28, "text": "assignment problem"}, {"st": 100, "ed": 102, "text": "learning parameters"}, {"st": 105, "ed": 107, "text": "objective function"}, {"st": 112, "ed": 114, "text": "key feature"}, {"st": 120, "ed": 123, "text": "takes advantage of"}, {"st": 138, "ed": 140, "text": "symmetric group"}, {"st": 154, "ed": 156, "text": "outperform existing"}]
[{"st": 0, "ed": 2, "text": "natural image"}, {"st": 11, "ed": 13, "text": "prior knowledge"}, {"st": 16, "ed": 18, "text": "latent tree"}, {"st": 23, "ed": 25, "text": "image denoising"}, {"st": 38, "ed": 40, "text": "large scale"}, {"st": 40, "ed": 42, "text": "approximate bayesian"}, {"st": 42, "ed": 44, "text": "inference algorithm"}, {"st": 45, "ed": 47, "text": "linear models"}, {"st": 50, "ed": 52, "text": "latent tree"}, {"st": 68, "ed": 70, "text": "improved performance"}, {"st": 72, "ed": 74, "text": "map estimation"}]
[{"st": 0, "ed": 2, "text": "visual perception"}, {"st": 4, "ed": 6, "text": "challenging problem"}, {"st": 21, "ed": 23, "text": "invariant representation"}, {"st": 46, "ed": 48, "text": "generative model"}, {"st": 50, "ed": 52, "text": "latent variables"}, {"st": 62, "ed": 64, "text": "deep belief"}, {"st": 92, "ed": 94, "text": "latent variable"}, {"st": 118, "ed": 120, "text": "experiments demonstrate"}]
[{"st": 0, "ed": 2, "text": "functional neuroimaging"}, {"st": 35, "ed": 37, "text": "supervised learning"}]
[{"st": 13, "ed": 15, "text": "key idea"}, {"st": 45, "ed": 47, "text": "scales linearly"}, {"st": 60, "ed": 62, "text": "spatial features"}, {"st": 82, "ed": 84, "text": "mixture model"}, {"st": 89, "ed": 91, "text": "density function"}, {"st": 117, "ed": 119, "text": "extensive experiments"}]
[{"st": 0, "ed": 2, "text": "latent feature"}, {"st": 16, "ed": 18, "text": "latent feature"}, {"st": 46, "ed": 48, "text": "invariant features"}, {"st": 58, "ed": 60, "text": "real images"}, {"st": 63, "ed": 65, "text": "computational cost"}, {"st": 76, "ed": 78, "text": "real images"}, {"st": 81, "ed": 83, "text": "efficient inference"}, {"st": 85, "ed": 87, "text": "cross correlation"}, {"st": 93, "ed": 96, "text": "theoretically and empirically"}, {"st": 109, "ed": 111, "text": "image reconstruction"}]
[{"st": 2, "ed": 5, "text": "support vector machine"}, {"st": 11, "ed": 13, "text": "quadratic programming"}, {"st": 16, "ed": 18, "text": "computational complexity"}, {"st": 19, "ed": 21, "text": "prohibitively expensive"}, {"st": 22, "ed": 24, "text": "large scale"}, {"st": 26, "ed": 28, "text": "optimization methods"}, {"st": 30, "ed": 32, "text": "directly applied"}, {"st": 45, "ed": 47, "text": "objective function"}, {"st": 48, "ed": 51, "text": "under mild conditions"}, {"st": 58, "ed": 60, "text": "efficient algorithms"}, {"st": 98, "ed": 100, "text": "feature space"}, {"st": 129, "ed": 131, "text": "frank wolfe"}, {"st": 212, "ed": 214, "text": "machine learning"}]
[{"st": 0, "ed": 2, "text": "matching pursuit"}, {"st": 8, "ed": 10, "text": "matching pursuit"}, {"st": 18, "ed": 20, "text": "large scale"}, {"st": 20, "ed": 22, "text": "sparse recovery"}, {"st": 63, "ed": 65, "text": "ell 1"}, {"st": 80, "ed": 82, "text": "sparse recovery"}, {"st": 87, "ed": 89, "text": "numerical experiments"}, {"st": 90, "ed": 92, "text": "compressive sensing"}, {"st": 93, "ed": 95, "text": "face recognition"}, {"st": 114, "ed": 116, "text": "sparse recovery"}, {"st": 126, "ed": 129, "text": "times faster than"}, {"st": 130, "ed": 132, "text": "ell 1"}, {"st": 145, "ed": 147, "text": "sparse recovery"}]
[{"st": 7, "ed": 9, "text": "classifier performance"}]
[{"st": 21, "ed": 23, "text": "ill posed"}, {"st": 47, "ed": 49, "text": "map estimation"}, {"st": 57, "ed": 59, "text": "parameter settings"}, {"st": 84, "ed": 86, "text": "recently proposed"}, {"st": 88, "ed": 90, "text": "variational bayesian"}, {"st": 98, "ed": 100, "text": "image space"}, {"st": 111, "ed": 113, "text": "cost function"}, {"st": 119, "ed": 121, "text": "closed form"}, {"st": 140, "ed": 142, "text": "remains unclear"}, {"st": 190, "ed": 192, "text": "noise level"}, {"st": 209, "ed": 211, "text": "local minima"}, {"st": 213, "ed": 215, "text": "scale invariance"}]
[{"st": 0, "ed": 2, "text": "topic modeling"}, {"st": 4, "ed": 7, "text": "latent dirichlet allocation"}, {"st": 16, "ed": 18, "text": "scene recognition"}, {"st": 25, "ed": 27, "text": "topic model"}, {"st": 64, "ed": 66, "text": "visual scene"}, {"st": 79, "ed": 81, "text": "discriminative power"}, {"st": 92, "ed": 94, "text": "training objective"}, {"st": 110, "ed": 112, "text": "visual words"}, {"st": 124, "ed": 126, "text": "image classification"}, {"st": 144, "ed": 147, "text": "compares favorably to"}, {"st": 148, "ed": 150, "text": "topic models"}]
[{"st": 0, "ed": 2, "text": "feature selection"}, {"st": 7, "ed": 9, "text": "pre processing"}, {"st": 27, "ed": 29, "text": "feature space"}, {"st": 58, "ed": 60, "text": "important features"}, {"st": 110, "ed": 112, "text": "feature selection"}]
[{"st": 8, "ed": 10, "text": "parametric models"}, {"st": 11, "ed": 13, "text": "gaussian process"}, {"st": 24, "ed": 26, "text": "gp model"}, {"st": 41, "ed": 43, "text": "time series"}, {"st": 72, "ed": 74, "text": "variational inference"}, {"st": 83, "ed": 85, "text": "significantly faster"}, {"st": 92, "ed": 94, "text": "time series"}, {"st": 102, "ed": 104, "text": "salient features"}, {"st": 118, "ed": 120, "text": "inference algorithm"}]
[{"st": 0, "ed": 2, "text": "low rank"}, {"st": 19, "ed": 21, "text": "low rank"}, {"st": 24, "ed": 27, "text": "achieved great success"}, {"st": 31, "ed": 33, "text": "computer vision"}, {"st": 33, "ed": 35, "text": "data mining"}, {"st": 35, "ed": 37, "text": "signal processing"}, {"st": 51, "ed": 53, "text": "low rank"}, {"st": 57, "ed": 59, "text": "low rank"}, {"st": 65, "ed": 67, "text": "matrix completion"}, {"st": 91, "ed": 93, "text": "low rank"}, {"st": 115, "ed": 117, "text": "low rank"}, {"st": 119, "ed": 121, "text": "challenging problems"}, {"st": 132, "ed": 134, "text": "low rank"}, {"st": 152, "ed": 154, "text": "low rank"}]
[{"st": 31, "ed": 33, "text": "linear combination"}, {"st": 41, "ed": 43, "text": "sparse representation"}, {"st": 58, "ed": 61, "text": "support vector machine"}, {"st": 67, "ed": 69, "text": "structured sparsity"}, {"st": 121, "ed": 123, "text": "sparse representation"}, {"st": 135, "ed": 137, "text": "low rank"}, {"st": 148, "ed": 150, "text": "low rank"}]
[{"st": 0, "ed": 2, "text": "time series"}, {"st": 22, "ed": 24, "text": "similarity measure"}, {"st": 30, "ed": 32, "text": "time series"}, {"st": 44, "ed": 46, "text": "time series"}, {"st": 63, "ed": 65, "text": "large scale"}, {"st": 76, "ed": 78, "text": "similarity measures"}, {"st": 79, "ed": 81, "text": "time series"}, {"st": 100, "ed": 102, "text": "time series"}, {"st": 107, "ed": 109, "text": "wide variety"}, {"st": 118, "ed": 120, "text": "classification accuracy"}, {"st": 144, "ed": 146, "text": "statistical significance"}, {"st": 191, "ed": 193, "text": "evaluation criteria"}]
[{"st": 57, "ed": 59, "text": "random projection"}, {"st": 75, "ed": 77, "text": "random projections"}, {"st": 102, "ed": 104, "text": "random projection"}, {"st": 126, "ed": 128, "text": "random vectors"}, {"st": 161, "ed": 163, "text": "theoretical analysis"}, {"st": 169, "ed": 171, "text": "empirical results"}, {"st": 172, "ed": 174, "text": "real world"}, {"st": 174, "ed": 176, "text": "face recognition"}]
[{"st": 6, "ed": 8, "text": "incomplete data"}, {"st": 42, "ed": 44, "text": "challenging problem"}, {"st": 50, "ed": 52, "text": "existing approaches"}, {"st": 60, "ed": 62, "text": "latent factors"}, {"st": 78, "ed": 80, "text": "probabilistic model"}, {"st": 83, "ed": 85, "text": "fully bayesian"}, {"st": 89, "ed": 91, "text": "sparsity inducing"}, {"st": 94, "ed": 96, "text": "latent factors"}, {"st": 117, "ed": 119, "text": "bayesian inference"}, {"st": 121, "ed": 123, "text": "scales linearly"}, {"st": 133, "ed": 135, "text": "parameter free"}, {"st": 145, "ed": 147, "text": "low rank"}, {"st": 151, "ed": 153, "text": "predictive distributions"}, {"st": 156, "ed": 158, "text": "extensive simulations"}, {"st": 159, "ed": 161, "text": "synthetic data"}, {"st": 171, "ed": 173, "text": "ground truth"}, {"st": 194, "ed": 197, "text": "real world applications"}, {"st": 202, "ed": 204, "text": "image synthesis"}, {"st": 207, "ed": 209, "text": "method outperforms"}]
[{"st": 40, "ed": 42, "text": "hidden markov"}, {"st": 45, "ed": 47, "text": "hierarchical clustering"}, {"st": 54, "ed": 56, "text": "image patches"}, {"st": 71, "ed": 74, "text": "hierarchical bayesian model"}, {"st": 105, "ed": 108, "text": "probabilistic topic models"}]
[{"st": 9, "ed": 11, "text": "low rank"}, {"st": 13, "ed": 15, "text": "sparse matrix"}, {"st": 15, "ed": 17, "text": "minimization problems"}, {"st": 27, "ed": 29, "text": "least squares"}, {"st": 38, "ed": 40, "text": "objective function"}, {"st": 62, "ed": 64, "text": "low rank"}, {"st": 65, "ed": 67, "text": "minimization problem"}, {"st": 68, "ed": 70, "text": "squared loss"}, {"st": 82, "ed": 84, "text": "low rank"}, {"st": 86, "ed": 88, "text": "minimization problems"}, {"st": 106, "ed": 108, "text": "ell 2"}, {"st": 111, "ed": 113, "text": "low rank"}, {"st": 127, "ed": 129, "text": "stationary point"}, {"st": 129, "ed": 131, "text": "globally optimal"}, {"st": 159, "ed": 161, "text": "ell 2"}, {"st": 163, "ed": 165, "text": "extensive experiments"}, {"st": 167, "ed": 171, "text": "synthetic and real data"}]
[{"st": 11, "ed": 13, "text": "visual recognition"}, {"st": 55, "ed": 57, "text": "sparse coding"}, {"st": 64, "ed": 66, "text": "linear subspaces"}, {"st": 101, "ed": 103, "text": "sparse coding"}, {"st": 110, "ed": 112, "text": "closed form"}, {"st": 124, "ed": 126, "text": "non linearity"}, {"st": 133, "ed": 135, "text": "sparse coding"}, {"st": 136, "ed": 139, "text": "dictionary learning algorithms"}, {"st": 147, "ed": 149, "text": "classification tasks"}, {"st": 155, "ed": 157, "text": "face recognition"}, {"st": 161, "ed": 163, "text": "texture classification"}]
[{"st": 3, "ed": 5, "text": "object categorization"}, {"st": 27, "ed": 29, "text": "approach called"}, {"st": 30, "ed": 32, "text": "receptive field"}, {"st": 36, "ed": 38, "text": "receptive fields"}, {"st": 72, "ed": 74, "text": "submodular function"}, {"st": 76, "ed": 78, "text": "similarity graph"}, {"st": 95, "ed": 97, "text": "similarity graph"}, {"st": 105, "ed": 107, "text": "similarity metric"}, {"st": 115, "ed": 117, "text": "pairwise distances"}, {"st": 128, "ed": 130, "text": "low level"}]
[{"st": 0, "ed": 2, "text": "fine grained"}, {"st": 23, "ed": 25, "text": "co occurrence"}, {"st": 33, "ed": 35, "text": "highly correlated"}, {"st": 46, "ed": 48, "text": "intra class"}, {"st": 66, "ed": 69, "text": "distance metric learning"}, {"st": 135, "ed": 137, "text": "feature representation"}, {"st": 156, "ed": 158, "text": "feature vectors"}, {"st": 162, "ed": 164, "text": "mathcal o"}, {"st": 169, "ed": 171, "text": "mathcal o"}, {"st": 181, "ed": 183, "text": "multi stage"}, {"st": 183, "ed": 185, "text": "metric learning"}, {"st": 189, "ed": 191, "text": "large scale"}, {"st": 202, "ed": 204, "text": "mathcal o"}, {"st": 208, "ed": 210, "text": "empirical study"}, {"st": 212, "ed": 214, "text": "benchmark datasets"}, {"st": 220, "ed": 223, "text": "effective and efficient"}]
[{"st": 0, "ed": 2, "text": "computer vision"}, {"st": 25, "ed": 27, "text": "generative models"}, {"st": 44, "ed": 46, "text": "latent variables"}, {"st": 49, "ed": 51, "text": "bayesian posterior"}, {"st": 62, "ed": 64, "text": "generative models"}, {"st": 65, "ed": 67, "text": "computer vision"}, {"st": 99, "ed": 101, "text": "generative models"}, {"st": 102, "ed": 104, "text": "computer vision"}, {"st": 116, "ed": 118, "text": "computer vision"}, {"st": 134, "ed": 136, "text": "experiments demonstrate"}, {"st": 139, "ed": 141, "text": "generative models"}, {"st": 180, "ed": 182, "text": "computer vision"}, {"st": 184, "ed": 186, "text": "significant improvements"}]
[{"st": 0, "ed": 2, "text": "semi supervised"}, {"st": 7, "ed": 9, "text": "clustering methods"}, {"st": 11, "ed": 13, "text": "side information"}, {"st": 30, "ed": 32, "text": "current methods"}, {"st": 40, "ed": 42, "text": "side information"}, {"st": 76, "ed": 78, "text": "semi supervised"}, {"st": 90, "ed": 92, "text": "clustering method"}, {"st": 125, "ed": 127, "text": "semi supervised"}, {"st": 127, "ed": 129, "text": "spectral clustering"}, {"st": 131, "ed": 133, "text": "pairwise constraints"}, {"st": 147, "ed": 149, "text": "taylor expansion"}, {"st": 185, "ed": 187, "text": "reduction potential"}, {"st": 217, "ed": 219, "text": "image datasets"}, {"st": 228, "ed": 230, "text": "machine learning"}]
[{"st": 3, "ed": 5, "text": "sparse representations"}, {"st": 11, "ed": 13, "text": "excellent results"}, {"st": 15, "ed": 17, "text": "visual recognition"}, {"st": 26, "ed": 28, "text": "sparse representations"}, {"st": 43, "ed": 45, "text": "large scale"}, {"st": 50, "ed": 52, "text": "computational power"}, {"st": 65, "ed": 67, "text": "sparse coding"}, {"st": 93, "ed": 95, "text": "dictionary learning"}, {"st": 99, "ed": 101, "text": "low complexity"}, {"st": 110, "ed": 112, "text": "jointly learns"}, {"st": 115, "ed": 117, "text": "linear classifier"}, {"st": 127, "ed": 129, "text": "solved efficiently"}, {"st": 135, "ed": 137, "text": "conduct experiments"}, {"st": 144, "ed": 146, "text": "learning algorithm"}, {"st": 152, "ed": 154, "text": "classification problem"}, {"st": 170, "ed": 172, "text": "sparse coding"}, {"st": 221, "ed": 223, "text": "classification methods"}]
[{"st": 2, "ed": 4, "text": "feature learning"}, {"st": 58, "ed": 60, "text": "linear models"}, {"st": 62, "ed": 64, "text": "higher order"}, {"st": 153, "ed": 155, "text": "multi step"}, {"st": 165, "ed": 167, "text": "prediction tasks"}, {"st": 170, "ed": 172, "text": "higher order"}, {"st": 179, "ed": 181, "text": "significant improvement"}, {"st": 183, "ed": 185, "text": "linear models"}]
[{"st": 19, "ed": 21, "text": "directed graph"}, {"st": 29, "ed": 31, "text": "real world"}, {"st": 55, "ed": 57, "text": "random walk"}, {"st": 86, "ed": 89, "text": "easy to implement"}, {"st": 92, "ed": 94, "text": "large scale"}, {"st": 136, "ed": 138, "text": "existing methods"}, {"st": 143, "ed": 145, "text": "based methods"}, {"st": 152, "ed": 154, "text": "computational complexity"}, {"st": 156, "ed": 158, "text": "generalization error"}]
[{"st": 0, "ed": 4, "text": "content based image retrieval"}, {"st": 16, "ed": 18, "text": "large scale"}, {"st": 38, "ed": 40, "text": "content based"}, {"st": 40, "ed": 42, "text": "face image"}, {"st": 47, "ed": 49, "text": "photo sharing"}, {"st": 73, "ed": 75, "text": "large scale"}, {"st": 86, "ed": 88, "text": "challenging problems"}, {"st": 92, "ed": 94, "text": "sparse representation"}, {"st": 97, "ed": 99, "text": "significant improvement"}]
[{"st": 7, "ed": 9, "text": "clustering algorithms"}, {"st": 13, "ed": 15, "text": "group sparse"}, {"st": 34, "ed": 36, "text": "ell 1"}, {"st": 39, "ed": 41, "text": "pair wise"}, {"st": 72, "ed": 74, "text": "efficiently solved"}, {"st": 84, "ed": 86, "text": "group sparse"}]
[{"st": 5, "ed": 7, "text": "social media"}, {"st": 21, "ed": 23, "text": "large scale"}, {"st": 38, "ed": 41, "text": "semi supervised learning"}, {"st": 53, "ed": 55, "text": "generalization ability"}, {"st": 61, "ed": 63, "text": "labeled data"}, {"st": 80, "ed": 82, "text": "multi modal"}, {"st": 99, "ed": 101, "text": "logistic regression"}, {"st": 139, "ed": 141, "text": "local geometry"}, {"st": 145, "ed": 147, "text": "multi view"}, {"st": 156, "ed": 158, "text": "loss function"}, {"st": 166, "ed": 169, "text": "conduct extensive experiments"}, {"st": 190, "ed": 192, "text": "logistic regression"}]
[{"st": 1, "ed": 4, "text": "principal component analysis"}, {"st": 4, "ed": 6, "text": "sparse pca"}, {"st": 72, "ed": 74, "text": "rotation matrix"}, {"st": 112, "ed": 114, "text": "performance bounds"}, {"st": 152, "ed": 154, "text": "sparse pca"}, {"st": 171, "ed": 173, "text": "sparse pca"}]
[{"st": 0, "ed": 3, "text": "multi label classification"}, {"st": 23, "ed": 25, "text": "multi label"}, {"st": 60, "ed": 62, "text": "pairwise constraints"}, {"st": 64, "ed": 66, "text": "multi label"}, {"st": 76, "ed": 78, "text": "generalization ability"}, {"st": 90, "ed": 92, "text": "multi label"}, {"st": 108, "ed": 111, "text": "multi label classification"}, {"st": 111, "ed": 113, "text": "framework named"}, {"st": 114, "ed": 116, "text": "pairwise constraint"}, {"st": 118, "ed": 120, "text": "multi label"}, {"st": 130, "ed": 132, "text": "pairwise constraint"}, {"st": 150, "ed": 152, "text": "base classifiers"}, {"st": 169, "ed": 171, "text": "pairwise constraints"}, {"st": 180, "ed": 182, "text": "empirical studies"}, {"st": 188, "ed": 190, "text": "proposed method"}]
[{"st": 22, "ed": 24, "text": "visual words"}, {"st": 51, "ed": 53, "text": "natural language"}]
[{"st": 12, "ed": 14, "text": "interpretable models"}, {"st": 19, "ed": 21, "text": "sparse models"}, {"st": 36, "ed": 38, "text": "large scale"}, {"st": 38, "ed": 41, "text": "unsupervised and supervised"}, {"st": 54, "ed": 56, "text": "sparse representations"}, {"st": 76, "ed": 78, "text": "efficient algorithms"}, {"st": 83, "ed": 86, "text": "batch and online"}, {"st": 90, "ed": 92, "text": "application domains"}, {"st": 101, "ed": 103, "text": "loss functions"}, {"st": 105, "ed": 107, "text": "ell 1"}, {"st": 162, "ed": 164, "text": "cost functions"}, {"st": 170, "ed": 172, "text": "ell 1"}, {"st": 174, "ed": 176, "text": "hinge loss"}, {"st": 191, "ed": 193, "text": "sparse codes"}, {"st": 208, "ed": 210, "text": "convergence guarantees"}, {"st": 212, "ed": 214, "text": "proposed algorithm"}, {"st": 226, "ed": 228, "text": "case studies"}, {"st": 234, "ed": 236, "text": "cost functions"}, {"st": 244, "ed": 246, "text": "image annotation"}]
[{"st": 1, "ed": 3, "text": "recent years"}, {"st": 4, "ed": 6, "text": "nuclear norm"}, {"st": 36, "ed": 38, "text": "nuclear norm"}, {"st": 40, "ed": 42, "text": "singular values"}, {"st": 54, "ed": 56, "text": "nuclear norm"}, {"st": 60, "ed": 62, "text": "natural extension"}, {"st": 73, "ed": 75, "text": "singular values"}, {"st": 94, "ed": 96, "text": "global optimal"}, {"st": 119, "ed": 121, "text": "theoretical properties"}, {"st": 133, "ed": 135, "text": "quadratic programming"}, {"st": 151, "ed": 153, "text": "global optimum"}, {"st": 161, "ed": 163, "text": "convex optimization"}, {"st": 175, "ed": 177, "text": "globally optimal"}]
[{"st": 72, "ed": 74, "text": "parametric models"}, {"st": 107, "ed": 109, "text": "detection theory"}, {"st": 120, "ed": 122, "text": "clustering algorithm"}, {"st": 151, "ed": 154, "text": "extensive experimental results"}, {"st": 164, "ed": 166, "text": "image processing"}]
[{"st": 4, "ed": 6, "text": "unsupervised learning"}, {"st": 6, "ed": 8, "text": "technique called"}, {"st": 20, "ed": 22, "text": "sparse coding"}, {"st": 45, "ed": 47, "text": "interpretable models"}, {"st": 52, "ed": 54, "text": "efficient implementation"}, {"st": 89, "ed": 91, "text": "active set"}, {"st": 96, "ed": 98, "text": "open source"}, {"st": 114, "ed": 116, "text": "computer vision"}]
[{"st": 0, "ed": 4, "text": "nonnegative matrix factorization nmf"}, {"st": 5, "ed": 7, "text": "group sparsity"}, {"st": 12, "ed": 15, "text": "probabilistic graphical model"}, {"st": 18, "ed": 20, "text": "observed data"}, {"st": 28, "ed": 30, "text": "variational bayesian"}, {"st": 41, "ed": 43, "text": "supervised learning"}, {"st": 52, "ed": 54, "text": "feature extractor"}, {"st": 65, "ed": 67, "text": "class labels"}, {"st": 83, "ed": 85, "text": "group sparse"}, {"st": 90, "ed": 92, "text": "class label"}, {"st": 95, "ed": 97, "text": "low dimensional"}, {"st": 119, "ed": 121, "text": "face recognition"}, {"st": 122, "ed": 124, "text": "facial expression"}]
[{"st": 7, "ed": 10, "text": "support vector machines"}, {"st": 20, "ed": 22, "text": "semi definite"}, {"st": 62, "ed": 64, "text": "latent variable"}, {"st": 75, "ed": 77, "text": "similarity measures"}, {"st": 78, "ed": 80, "text": "large margin"}, {"st": 122, "ed": 125, "text": "training and test"}, {"st": 132, "ed": 134, "text": "cifar 10"}, {"st": 139, "ed": 141, "text": "similarity measures"}]
[{"st": 3, "ed": 5, "text": "cluster ensemble"}, {"st": 8, "ed": 11, "text": "number of clusters"}, {"st": 19, "ed": 21, "text": "similarity matrix"}, {"st": 35, "ed": 37, "text": "random walk"}]
[{"st": 4, "ed": 6, "text": "consensus clustering"}, {"st": 16, "ed": 19, "text": "number of clusters"}, {"st": 21, "ed": 23, "text": "final solution"}, {"st": 28, "ed": 30, "text": "similarity matrix"}, {"st": 46, "ed": 48, "text": "dimension reduction"}, {"st": 50, "ed": 52, "text": "clustering algorithms"}, {"st": 105, "ed": 107, "text": "random walk"}, {"st": 137, "ed": 140, "text": "number of clusters"}, {"st": 144, "ed": 146, "text": "previous methods"}]
[{"st": 3, "ed": 5, "text": "framework called"}, {"st": 63, "ed": 65, "text": "hidden variables"}, {"st": 70, "ed": 72, "text": "maximum likelihood"}, {"st": 80, "ed": 82, "text": "sparse models"}, {"st": 90, "ed": 92, "text": "learned models"}]
[{"st": 15, "ed": 17, "text": "low level"}, {"st": 17, "ed": 19, "text": "sensory data"}, {"st": 25, "ed": 27, "text": "application domains"}, {"st": 36, "ed": 38, "text": "traditional approaches"}, {"st": 44, "ed": 46, "text": "supervised learning"}, {"st": 47, "ed": 49, "text": "generative models"}, {"st": 52, "ed": 55, "text": "hidden markov models"}, {"st": 77, "ed": 79, "text": "supervised training"}, {"st": 93, "ed": 95, "text": "semi supervised"}, {"st": 99, "ed": 101, "text": "discriminative models"}, {"st": 104, "ed": 107, "text": "conditional random field"}, {"st": 110, "ed": 112, "text": "maximum entropy"}, {"st": 126, "ed": 129, "text": "labeled and unlabeled"}, {"st": 169, "ed": 172, "text": "hidden markov model"}]
[{"st": 0, "ed": 3, "text": "dictionary learning algorithms"}, {"st": 15, "ed": 17, "text": "input signal"}, {"st": 21, "ed": 23, "text": "sparse linear"}, {"st": 37, "ed": 39, "text": "recent studies"}, {"st": 44, "ed": 46, "text": "feature level"}, {"st": 51, "ed": 53, "text": "sparse representation"}, {"st": 66, "ed": 68, "text": "dictionary learning"}, {"st": 107, "ed": 109, "text": "latent features"}, {"st": 109, "ed": 111, "text": "sparse codes"}, {"st": 171, "ed": 173, "text": "face recognition"}, {"st": 173, "ed": 175, "text": "multi view"}, {"st": 175, "ed": 177, "text": "face recognition"}, {"st": 177, "ed": 179, "text": "multi view"}, {"st": 196, "ed": 199, "text": "dictionary learning algorithms"}, {"st": 205, "ed": 207, "text": "computationally efficient"}]
[{"st": 0, "ed": 4, "text": "canonical correlation analysis cca"}, {"st": 18, "ed": 20, "text": "theoretical foundation"}, {"st": 28, "ed": 30, "text": "multi view"}, {"st": 50, "ed": 52, "text": "real world"}, {"st": 63, "ed": 65, "text": "ad hoc"}, {"st": 78, "ed": 81, "text": "multi view data"}, {"st": 139, "ed": 141, "text": "canonical correlation"}, {"st": 152, "ed": 154, "text": "multi view"}, {"st": 154, "ed": 156, "text": "canonical correlation"}, {"st": 175, "ed": 177, "text": "solved efficiently"}, {"st": 182, "ed": 184, "text": "least squares"}, {"st": 229, "ed": 231, "text": "tasks including"}, {"st": 231, "ed": 233, "text": "large scale"}, {"st": 241, "ed": 243, "text": "image annotation"}]
[{"st": 8, "ed": 10, "text": "magnetic resonance"}, {"st": 10, "ed": 12, "text": "mr images"}, {"st": 14, "ed": 16, "text": "human brain"}, {"st": 25, "ed": 28, "text": "deep artificial neural"}, {"st": 68, "ed": 70, "text": "local spatial"}, {"st": 130, "ed": 132, "text": "mr images"}, {"st": 137, "ed": 139, "text": "competitive results"}, {"st": 143, "ed": 145, "text": "error rate"}]
[{"st": 0, "ed": 2, "text": "pattern recognition"}, {"st": 28, "ed": 30, "text": "deep learning"}, {"st": 38, "ed": 41, "text": "convolutional neural networks"}, {"st": 80, "ed": 83, "text": "convolutional neural networks"}]
[{"st": 14, "ed": 16, "text": "deep networks"}, {"st": 20, "ed": 22, "text": "adversarial perturbations"}, {"st": 30, "ed": 32, "text": "theoretical framework"}, {"st": 39, "ed": 41, "text": "adversarial perturbations"}, {"st": 64, "ed": 66, "text": "adversarial perturbations"}, {"st": 110, "ed": 112, "text": "tasks involving"}, {"st": 124, "ed": 126, "text": "adversarial perturbations"}, {"st": 134, "ed": 136, "text": "theoretical framework"}, {"st": 158, "ed": 160, "text": "classification task"}, {"st": 181, "ed": 183, "text": "random noise"}]
[{"st": 0, "ed": 3, "text": "dictionary learning algorithms"}, {"st": 15, "ed": 17, "text": "input signal"}, {"st": 21, "ed": 23, "text": "linear combination"}, {"st": 35, "ed": 37, "text": "ell 1"}, {"st": 44, "ed": 46, "text": "recent studies"}, {"st": 51, "ed": 53, "text": "sparse representation"}, {"st": 54, "ed": 56, "text": "structured sparsity"}, {"st": 68, "ed": 70, "text": "dictionary learning"}, {"st": 76, "ed": 78, "text": "hyperspectral image"}, {"st": 102, "ed": 104, "text": "learned features"}, {"st": 118, "ed": 120, "text": "proposed algorithm"}, {"st": 143, "ed": 145, "text": "dictionary learning"}]
[{"st": 2, "ed": 4, "text": "previous works"}, {"st": 24, "ed": 26, "text": "undesired edges"}, {"st": 58, "ed": 60, "text": "undesired edges"}]
[{"st": 7, "ed": 9, "text": "based clustering"}, {"st": 36, "ed": 38, "text": "delaunay triangulation"}, {"st": 76, "ed": 78, "text": "gradient based"}, {"st": 81, "ed": 83, "text": "based methods"}]
[{"st": 12, "ed": 14, "text": "machine learning"}, {"st": 15, "ed": 17, "text": "pattern recognition"}, {"st": 17, "ed": 19, "text": "pairwise constraint"}, {"st": 38, "ed": 40, "text": "pairwise constraint"}, {"st": 58, "ed": 60, "text": "pairwise constraints"}, {"st": 75, "ed": 77, "text": "pairwise constraints"}, {"st": 81, "ed": 83, "text": "constraint propagation"}, {"st": 97, "ed": 99, "text": "pairwise constraint"}, {"st": 125, "ed": 127, "text": "existing literature"}, {"st": 139, "ed": 141, "text": "pairwise constraint"}]
[{"st": 31, "ed": 33, "text": "kernel density"}, {"st": 43, "ed": 45, "text": "mean shift"}, {"st": 49, "ed": 52, "text": "theory and practice"}, {"st": 56, "ed": 58, "text": "kernel density"}, {"st": 60, "ed": 62, "text": "mean shift"}, {"st": 72, "ed": 74, "text": "mean shift"}, {"st": 74, "ed": 76, "text": "theoretical results"}, {"st": 77, "ed": 79, "text": "mean shift"}, {"st": 85, "ed": 87, "text": "scale space"}, {"st": 88, "ed": 90, "text": "spectral clustering"}, {"st": 115, "ed": 117, "text": "large datasets"}, {"st": 120, "ed": 122, "text": "image segmentation"}]
[{"st": 1, "ed": 3, "text": "machine learning"}, {"st": 19, "ed": 22, "text": "support vector machine"}, {"st": 37, "ed": 40, "text": "a posteriori map"}, {"st": 42, "ed": 44, "text": "maximum likelihood"}, {"st": 47, "ed": 49, "text": "probabilistic models"}, {"st": 70, "ed": 72, "text": "exponential family"}, {"st": 99, "ed": 101, "text": "frank wolfe"}, {"st": 136, "ed": 138, "text": "maximum likelihood"}, {"st": 141, "ed": 143, "text": "outperforms existing"}, {"st": 147, "ed": 149, "text": "image segmentation"}]
[{"st": 4, "ed": 6, "text": "ladder network"}, {"st": 17, "ed": 19, "text": "encoder decoder"}, {"st": 60, "ed": 62, "text": "recurrent connections"}, {"st": 89, "ed": 92, "text": "spatial and temporal"}, {"st": 109, "ed": 112, "text": "encoder and decoder"}, {"st": 128, "ed": 130, "text": "mnist dataset"}, {"st": 131, "ed": 133, "text": "competitive results"}]
[{"st": 0, "ed": 2, "text": "machine learning"}, {"st": 7, "ed": 9, "text": "accurately predict"}, {"st": 58, "ed": 60, "text": "deep learning"}, {"st": 62, "ed": 66, "text": "convolutional neural networks cnn"}, {"st": 159, "ed": 161, "text": "gaussian process"}, {"st": 167, "ed": 169, "text": "input data"}, {"st": 170, "ed": 172, "text": "grey matter"}, {"st": 174, "ed": 176, "text": "white matter"}]
[{"st": 7, "ed": 9, "text": "performance measure"}, {"st": 10, "ed": 12, "text": "large scale"}, {"st": 12, "ed": 14, "text": "image classification"}, {"st": 35, "ed": 37, "text": "previous research"}, {"st": 41, "ed": 43, "text": "special case"}, {"st": 73, "ed": 75, "text": "recently proposed"}, {"st": 86, "ed": 88, "text": "optimization algorithms"}, {"st": 95, "ed": 97, "text": "softmax loss"}, {"st": 172, "ed": 174, "text": "pascal voc"}, {"st": 191, "ed": 193, "text": "ms coco"}, {"st": 200, "ed": 202, "text": "efficient algorithms"}, {"st": 211, "ed": 213, "text": "loss functions"}]
[{"st": 3, "ed": 5, "text": "predictive model"}, {"st": 66, "ed": 68, "text": "unsupervised learning"}, {"st": 79, "ed": 81, "text": "supervised training"}, {"st": 101, "ed": 103, "text": "predict future"}, {"st": 127, "ed": 129, "text": "training set"}]
[{"st": 40, "ed": 42, "text": "existing models"}, {"st": 82, "ed": 84, "text": "deep generative"}, {"st": 88, "ed": 90, "text": "prior distribution"}, {"st": 149, "ed": 151, "text": "improved performance"}, {"st": 152, "ed": 154, "text": "transfer learning"}]
[{"st": 4, "ed": 6, "text": "input image"}, {"st": 13, "ed": 15, "text": "computer vision"}, {"st": 16, "ed": 20, "text": "convolutional neural networks cnns"}, {"st": 24, "ed": 26, "text": "input image"}, {"st": 29, "ed": 31, "text": "feature map"}, {"st": 92, "ed": 94, "text": "receptive field"}, {"st": 104, "ed": 106, "text": "computational complexity"}, {"st": 112, "ed": 114, "text": "feature maps"}, {"st": 159, "ed": 161, "text": "competitive results"}]
[{"st": 8, "ed": 11, "text": "deep neural networks"}, {"st": 41, "ed": 43, "text": "input data"}, {"st": 96, "ed": 98, "text": "neural network"}, {"st": 115, "ed": 117, "text": "computer vision"}]
[{"st": 17, "ed": 20, "text": "deep neural networks"}, {"st": 21, "ed": 23, "text": "machine learning"}, {"st": 35, "ed": 38, "text": "optical coherence tomography"}, {"st": 68, "ed": 70, "text": "deep learning"}, {"st": 83, "ed": 87, "text": "age related macular degeneration"}, {"st": 146, "ed": 148, "text": "cross validation"}, {"st": 167, "ed": 169, "text": "image level"}, {"st": 204, "ed": 207, "text": "deep neural network"}, {"st": 219, "ed": 221, "text": "image level"}, {"st": 278, "ed": 280, "text": "deep learning"}, {"st": 299, "ed": 302, "text": "computer aided diagnosis"}]
[{"st": 2, "ed": 5, "text": "stochastic gradient descent"}, {"st": 12, "ed": 14, "text": "large scale"}, {"st": 14, "ed": 17, "text": "empirical risk minimization"}, {"st": 31, "ed": 33, "text": "batch size"}, {"st": 39, "ed": 41, "text": "batch size"}, {"st": 47, "ed": 49, "text": "stochastic optimization"}, {"st": 66, "ed": 68, "text": "optimization process"}, {"st": 72, "ed": 74, "text": "batch size"}, {"st": 88, "ed": 90, "text": "learning rate"}, {"st": 98, "ed": 100, "text": "batch size"}, {"st": 112, "ed": 114, "text": "batch size"}, {"st": 124, "ed": 126, "text": "objective function"}, {"st": 132, "ed": 134, "text": "learning rate"}, {"st": 145, "ed": 147, "text": "batch size"}, {"st": 149, "ed": 151, "text": "learning rate"}, {"st": 161, "ed": 163, "text": "image classification"}, {"st": 165, "ed": 167, "text": "batch size"}, {"st": 175, "ed": 177, "text": "learning rate"}]
[{"st": 1, "ed": 3, "text": "comparative study"}, {"st": 7, "ed": 10, "text": "gaussian mixture model"}, {"st": 12, "ed": 15, "text": "radial basis function"}, {"st": 30, "ed": 32, "text": "machine learning"}, {"st": 53, "ed": 57, "text": "expectation maximization em algorithm"}, {"st": 108, "ed": 110, "text": "recognition accuracy"}]
[{"st": 3, "ed": 5, "text": "discriminative features"}, {"st": 15, "ed": 17, "text": "real world"}, {"st": 17, "ed": 19, "text": "image classification"}, {"st": 22, "ed": 24, "text": "face verification"}, {"st": 33, "ed": 35, "text": "input image"}, {"st": 39, "ed": 41, "text": "high level"}, {"st": 68, "ed": 70, "text": "input image"}, {"st": 77, "ed": 79, "text": "input image"}, {"st": 93, "ed": 95, "text": "face verification"}, {"st": 96, "ed": 98, "text": "real world"}, {"st": 98, "ed": 100, "text": "object recognition"}, {"st": 114, "ed": 116, "text": "deep learning"}, {"st": 122, "ed": 124, "text": "comparable results"}, {"st": 143, "ed": 145, "text": "low level"}]
[{"st": 4, "ed": 6, "text": "statistical learning"}, {"st": 23, "ed": 25, "text": "doesn t"}, {"st": 31, "ed": 33, "text": "labeled data"}, {"st": 38, "ed": 40, "text": "source domain"}, {"st": 48, "ed": 50, "text": "domain adaptation"}, {"st": 52, "ed": 54, "text": "labeled data"}, {"st": 61, "ed": 63, "text": "unseen data"}, {"st": 72, "ed": 74, "text": "domain transfer"}, {"st": 78, "ed": 80, "text": "application domains"}]
[{"st": 9, "ed": 11, "text": "sparse pca"}, {"st": 15, "ed": 18, "text": "divide and conquer"}, {"st": 20, "ed": 22, "text": "main idea"}, {"st": 27, "ed": 29, "text": "sparse pca"}, {"st": 41, "ed": 43, "text": "closed form"}, {"st": 62, "ed": 64, "text": "sparse pca"}, {"st": 78, "ed": 80, "text": "proposed method"}, {"st": 88, "ed": 90, "text": "sparse pca"}, {"st": 98, "ed": 100, "text": "sparse pca"}, {"st": 107, "ed": 109, "text": "proposed algorithm"}, {"st": 112, "ed": 114, "text": "stationary point"}, {"st": 119, "ed": 121, "text": "computational complexity"}, {"st": 134, "ed": 136, "text": "proposed method"}, {"st": 139, "ed": 141, "text": "extensive experiments"}, {"st": 146, "ed": 150, "text": "synthetic and real data"}]
[{"st": 1, "ed": 3, "text": "automatic segmentation"}, {"st": 9, "ed": 11, "text": "mr images"}, {"st": 15, "ed": 17, "text": "challenging task"}, {"st": 39, "ed": 41, "text": "multi class"}, {"st": 138, "ed": 140, "text": "previously learned"}, {"st": 147, "ed": 149, "text": "context information"}, {"st": 168, "ed": 170, "text": "proposed approach"}]
[{"st": 5, "ed": 7, "text": "statistical classification"}, {"st": 16, "ed": 18, "text": "image segmentation"}, {"st": 25, "ed": 27, "text": "hidden states"}, {"st": 106, "ed": 108, "text": "hidden markov"}, {"st": 119, "ed": 121, "text": "potts model"}, {"st": 144, "ed": 147, "text": "goodness of fit"}, {"st": 153, "ed": 155, "text": "parameter estimation"}, {"st": 167, "ed": 169, "text": "relative improvement"}, {"st": 176, "ed": 178, "text": "statistical analysis"}, {"st": 179, "ed": 182, "text": "synthetic and real"}, {"st": 237, "ed": 239, "text": "error rate"}]
[{"st": 7, "ed": 9, "text": "training process"}, {"st": 41, "ed": 43, "text": "noise conditions"}, {"st": 75, "ed": 77, "text": "computationally efficient"}, {"st": 122, "ed": 124, "text": "baseline models"}]
[{"st": 0, "ed": 3, "text": "support vector machines"}, {"st": 18, "ed": 20, "text": "computer vision"}, {"st": 132, "ed": 134, "text": "kernel learning"}]
[{"st": 1, "ed": 3, "text": "k nn"}, {"st": 7, "ed": 9, "text": "central role"}, {"st": 10, "ed": 12, "text": "increasingly popular"}, {"st": 19, "ed": 21, "text": "vision tasks"}, {"st": 24, "ed": 27, "text": "efficient and effective"}, {"st": 30, "ed": 32, "text": "k nn"}, {"st": 38, "ed": 40, "text": "large scale"}, {"st": 54, "ed": 56, "text": "k nn"}, {"st": 127, "ed": 130, "text": "theoretical and empirical"}, {"st": 130, "ed": 133, "text": "accuracy and efficiency"}, {"st": 137, "ed": 139, "text": "k nn"}, {"st": 149, "ed": 151, "text": "large scale"}]
[{"st": 12, "ed": 14, "text": "modified version"}, {"st": 20, "ed": 22, "text": "clustering algorithm"}, {"st": 37, "ed": 39, "text": "pair wise"}, {"st": 85, "ed": 87, "text": "closely related"}, {"st": 120, "ed": 122, "text": "synthetic data"}, {"st": 125, "ed": 127, "text": "breast cancer"}, {"st": 134, "ed": 136, "text": "group sparsity"}]
[{"st": 46, "ed": 48, "text": "image representation"}, {"st": 52, "ed": 54, "text": "image representation"}, {"st": 97, "ed": 100, "text": "taking into account"}, {"st": 166, "ed": 168, "text": "theoretical result"}, {"st": 176, "ed": 178, "text": "previously reported"}, {"st": 183, "ed": 185, "text": "recently proposed"}, {"st": 218, "ed": 220, "text": "image representation"}]
[{"st": 2, "ed": 4, "text": "gaussian process"}, {"st": 14, "ed": 17, "text": "gaussian process gp"}, {"st": 20, "ed": 22, "text": "gp regression"}, {"st": 34, "ed": 36, "text": "gp model"}, {"st": 41, "ed": 43, "text": "exponential family"}, {"st": 51, "ed": 53, "text": "efficient algorithms"}, {"st": 54, "ed": 56, "text": "approximate inference"}, {"st": 67, "ed": 69, "text": "gp model"}, {"st": 72, "ed": 74, "text": "inference algorithms"}, {"st": 91, "ed": 93, "text": "task specific"}, {"st": 106, "ed": 108, "text": "gp models"}, {"st": 122, "ed": 124, "text": "closed form"}, {"st": 127, "ed": 129, "text": "efficient inference"}, {"st": 141, "ed": 143, "text": "closed form"}, {"st": 154, "ed": 156, "text": "approximate inference"}, {"st": 159, "ed": 161, "text": "wide variety"}]
[{"st": 7, "ed": 9, "text": "training examples"}, {"st": 38, "ed": 40, "text": "vision community"}, {"st": 105, "ed": 107, "text": "vision tasks"}]
[{"st": 64, "ed": 66, "text": "learning algorithm"}, {"st": 68, "ed": 70, "text": "biologically inspired"}, {"st": 111, "ed": 113, "text": "outperform existing"}, {"st": 113, "ed": 115, "text": "hand engineered"}, {"st": 117, "ed": 119, "text": "motion features"}]
[{"st": 3, "ed": 5, "text": "dimensionality reduction"}, {"st": 6, "ed": 8, "text": "random projection"}, {"st": 18, "ed": 20, "text": "pairwise distances"}, {"st": 47, "ed": 49, "text": "random matrix"}, {"st": 53, "ed": 55, "text": "feature selection"}, {"st": 66, "ed": 68, "text": "theoretical result"}, {"st": 72, "ed": 74, "text": "random matrix"}, {"st": 84, "ed": 86, "text": "feature selection"}, {"st": 97, "ed": 99, "text": "sufficiently large"}, {"st": 117, "ed": 119, "text": "random projection"}, {"st": 120, "ed": 122, "text": "theoretical result"}, {"st": 140, "ed": 142, "text": "synthetic data"}]
[{"st": 36, "ed": 38, "text": "linear transformation"}, {"st": 41, "ed": 43, "text": "nuclear norm"}, {"st": 49, "ed": 51, "text": "linear transformation"}, {"st": 53, "ed": 55, "text": "low rank"}]
[{"st": 21, "ed": 23, "text": "unlike traditional"}, {"st": 40, "ed": 43, "text": "trial and error"}, {"st": 53, "ed": 55, "text": "training data"}, {"st": 61, "ed": 63, "text": "empirical loss"}, {"st": 148, "ed": 150, "text": "pose estimation"}, {"st": 164, "ed": 166, "text": "proposed method"}, {"st": 166, "ed": 168, "text": "significantly outperforms"}, {"st": 176, "ed": 178, "text": "error reduction"}]
[{"st": 1, "ed": 4, "text": "supervised machine learning"}, {"st": 23, "ed": 25, "text": "ground truth"}, {"st": 31, "ed": 33, "text": "time consuming"}, {"st": 52, "ed": 55, "text": "human activity recognition"}, {"st": 65, "ed": 67, "text": "proposed method"}, {"st": 74, "ed": 76, "text": "time series"}, {"st": 78, "ed": 81, "text": "hidden markov model"}, {"st": 97, "ed": 101, "text": "expectation maximization em algorithm"}, {"st": 108, "ed": 110, "text": "proposed method"}, {"st": 110, "ed": 113, "text": "takes into account"}, {"st": 153, "ed": 155, "text": "proposed approach"}, {"st": 159, "ed": 162, "text": "supervised and unsupervised"}]
[{"st": 49, "ed": 51, "text": "finite set"}, {"st": 72, "ed": 74, "text": "ell 1"}, {"st": 103, "ed": 105, "text": "computer vision"}, {"st": 113, "ed": 115, "text": "robust pca"}, {"st": 128, "ed": 130, "text": "faster training"}]
[{"st": 6, "ed": 8, "text": "low level"}, {"st": 18, "ed": 20, "text": "feature construction"}, {"st": 85, "ed": 88, "text": "multi class classification"}, {"st": 114, "ed": 116, "text": "cifar 10"}, {"st": 125, "ed": 127, "text": "deep learning"}, {"st": 133, "ed": 135, "text": "proposed method"}, {"st": 141, "ed": 143, "text": "raw pixels"}]
[{"st": 7, "ed": 9, "text": "pattern recognition"}, {"st": 23, "ed": 25, "text": "performance improvement"}, {"st": 43, "ed": 45, "text": "prior works"}, {"st": 140, "ed": 142, "text": "loss function"}, {"st": 161, "ed": 163, "text": "classification loss"}, {"st": 186, "ed": 188, "text": "pattern recognition"}, {"st": 198, "ed": 200, "text": "real world"}, {"st": 200, "ed": 203, "text": "classification and regression"}]
[{"st": 0, "ed": 2, "text": "face verification"}, {"st": 4, "ed": 6, "text": "challenging problem"}, {"st": 31, "ed": 33, "text": "training data"}, {"st": 50, "ed": 53, "text": "multi task learning"}, {"st": 57, "ed": 59, "text": "gaussian process"}, {"st": 59, "ed": 61, "text": "latent variable"}, {"st": 74, "ed": 76, "text": "existing methods"}, {"st": 82, "ed": 84, "text": "multiple source"}, {"st": 88, "ed": 90, "text": "generalization performance"}, {"st": 91, "ed": 93, "text": "face verification"}, {"st": 105, "ed": 107, "text": "complex data"}, {"st": 112, "ed": 114, "text": "capture complex"}, {"st": 120, "ed": 122, "text": "extensive experiments"}, {"st": 175, "ed": 177, "text": "face verification"}]
[{"st": 6, "ed": 8, "text": "powerful tool"}, {"st": 44, "ed": 46, "text": "large scale"}, {"st": 54, "ed": 56, "text": "computational complexity"}, {"st": 71, "ed": 73, "text": "practical applications"}, {"st": 84, "ed": 86, "text": "rank approximation"}, {"st": 101, "ed": 103, "text": "cost function"}, {"st": 150, "ed": 152, "text": "substantially improves"}, {"st": 159, "ed": 162, "text": "curse of dimensionality"}, {"st": 169, "ed": 173, "text": "synthetic and real world"}]
[{"st": 4, "ed": 6, "text": "l 0"}, {"st": 9, "ed": 11, "text": "penalty functions"}, {"st": 27, "ed": 29, "text": "penalty functions"}, {"st": 30, "ed": 32, "text": "singular values"}, {"st": 37, "ed": 39, "text": "low rank"}, {"st": 44, "ed": 46, "text": "convex optimization"}, {"st": 49, "ed": 51, "text": "low rank"}, {"st": 51, "ed": 53, "text": "minimization problem"}, {"st": 70, "ed": 72, "text": "penalty functions"}, {"st": 96, "ed": 98, "text": "nuclear norm"}, {"st": 105, "ed": 107, "text": "low rank"}, {"st": 122, "ed": 124, "text": "weight vector"}, {"st": 137, "ed": 139, "text": "closed form"}, {"st": 148, "ed": 150, "text": "objective function"}, {"st": 154, "ed": 156, "text": "limit point"}, {"st": 160, "ed": 162, "text": "extensive experiments"}, {"st": 164, "ed": 166, "text": "synthetic data"}, {"st": 167, "ed": 169, "text": "real images"}, {"st": 174, "ed": 176, "text": "low rank"}]
[{"st": 6, "ed": 8, "text": "linear subspaces"}, {"st": 13, "ed": 15, "text": "visual recognition"}, {"st": 26, "ed": 28, "text": "linear subspaces"}, {"st": 31, "ed": 33, "text": "euclidean geometry"}, {"st": 54, "ed": 57, "text": "support vector machines"}, {"st": 60, "ed": 62, "text": "recent studies"}, {"st": 77, "ed": 79, "text": "positive definite"}, {"st": 111, "ed": 113, "text": "positive definite"}, {"st": 133, "ed": 135, "text": "sparse coding"}]
[{"st": 1, "ed": 3, "text": "structured outputs"}, {"st": 69, "ed": 71, "text": "method works"}]
[{"st": 2, "ed": 4, "text": "class classification"}, {"st": 71, "ed": 73, "text": "class classification"}, {"st": 89, "ed": 91, "text": "based approach"}, {"st": 94, "ed": 96, "text": "input data"}, {"st": 165, "ed": 167, "text": "dissimilarity measure"}, {"st": 169, "ed": 171, "text": "input data"}, {"st": 181, "ed": 183, "text": "global optimization"}, {"st": 240, "ed": 242, "text": "feature based"}]
[{"st": 1, "ed": 3, "text": "low rank"}, {"st": 16, "ed": 18, "text": "machine learning"}, {"st": 20, "ed": 22, "text": "computer vision"}, {"st": 44, "ed": 47, "text": "l 1 norm"}, {"st": 48, "ed": 50, "text": "trace norm"}, {"st": 61, "ed": 63, "text": "per iteration"}, {"st": 70, "ed": 72, "text": "large scale"}, {"st": 82, "ed": 85, "text": "low rank matrix"}, {"st": 89, "ed": 91, "text": "low rank"}, {"st": 102, "ed": 104, "text": "matrix completion"}, {"st": 114, "ed": 116, "text": "principal component"}, {"st": 124, "ed": 126, "text": "small scale"}, {"st": 127, "ed": 129, "text": "trace norm"}, {"st": 146, "ed": 148, "text": "large scale"}, {"st": 162, "ed": 168, "text": "alternating direction method of multipliers admm"}, {"st": 169, "ed": 171, "text": "efficiently solve"}, {"st": 178, "ed": 180, "text": "convergence analysis"}]
[{"st": 0, "ed": 2, "text": "sparse coding"}, {"st": 70, "ed": 72, "text": "sparse coding"}, {"st": 102, "ed": 104, "text": "sparse coding"}, {"st": 134, "ed": 137, "text": "each data point"}, {"st": 146, "ed": 148, "text": "sparse codes"}, {"st": 157, "ed": 159, "text": "approximation error"}, {"st": 168, "ed": 170, "text": "sparse coding"}, {"st": 182, "ed": 184, "text": "objective function"}, {"st": 187, "ed": 189, "text": "sparse codes"}, {"st": 198, "ed": 200, "text": "iterative algorithm"}]
[{"st": 3, "ed": 5, "text": "massive data"}, {"st": 9, "ed": 11, "text": "increasingly popular"}, {"st": 21, "ed": 23, "text": "current methods"}, {"st": 124, "ed": 126, "text": "computational cost"}, {"st": 132, "ed": 135, "text": "o n 2"}, {"st": 137, "ed": 139, "text": "extensive experiments"}, {"st": 141, "ed": 143, "text": "benchmark datasets"}, {"st": 160, "ed": 163, "text": "times faster than"}]
[{"st": 7, "ed": 9, "text": "mutual information"}, {"st": 18, "ed": 20, "text": "machine learning"}, {"st": 47, "ed": 49, "text": "machine learning"}, {"st": 60, "ed": 62, "text": "mutual information"}, {"st": 72, "ed": 75, "text": "kullback leibler kl"}, {"st": 85, "ed": 87, "text": "cost function"}, {"st": 93, "ed": 95, "text": "gaussian processes"}, {"st": 119, "ed": 121, "text": "theoretical analysis"}]
[{"st": 12, "ed": 14, "text": "riemannian manifold"}, {"st": 19, "ed": 21, "text": "low dimensional"}, {"st": 33, "ed": 35, "text": "clustering algorithm"}, {"st": 36, "ed": 38, "text": "computationally efficient"}, {"st": 44, "ed": 46, "text": "positive definite"}, {"st": 51, "ed": 53, "text": "clustering problem"}, {"st": 63, "ed": 65, "text": "application domains"}, {"st": 88, "ed": 90, "text": "clustering algorithm"}, {"st": 93, "ed": 95, "text": "affinity matrix"}, {"st": 108, "ed": 110, "text": "local geometry"}, {"st": 114, "ed": 116, "text": "sparse coding"}, {"st": 128, "ed": 130, "text": "theoretical guarantees"}, {"st": 159, "ed": 163, "text": "synthetic and real data"}, {"st": 168, "ed": 170, "text": "proposed method"}]
[{"st": 8, "ed": 10, "text": "sparse representation"}, {"st": 16, "ed": 19, "text": "takes into account"}, {"st": 24, "ed": 26, "text": "complementary information"}, {"st": 53, "ed": 55, "text": "low rank"}, {"st": 69, "ed": 71, "text": "low rank"}, {"st": 81, "ed": 83, "text": "classification problem"}, {"st": 116, "ed": 118, "text": "training samples"}, {"st": 120, "ed": 122, "text": "feature space"}, {"st": 144, "ed": 146, "text": "optimal solution"}, {"st": 148, "ed": 150, "text": "extensive experiments"}]
[{"st": 4, "ed": 6, "text": "ensemble based"}, {"st": 11, "ed": 13, "text": "diabetic retinopathy"}, {"st": 21, "ed": 23, "text": "features extracted"}, {"st": 29, "ed": 31, "text": "image processing"}, {"st": 34, "ed": 36, "text": "image level"}, {"st": 36, "ed": 38, "text": "quality assessment"}, {"st": 49, "ed": 51, "text": "optic disc"}, {"st": 68, "ed": 70, "text": "machine learning"}, {"st": 104, "ed": 106, "text": "highly competitive"}, {"st": 113, "ed": 115, "text": "image processing"}]
[{"st": 0, "ed": 3, "text": "determinantal point processes"}, {"st": 32, "ed": 34, "text": "kernel matrix"}, {"st": 54, "ed": 56, "text": "kernel matrix"}, {"st": 58, "ed": 60, "text": "kernel functions"}, {"st": 69, "ed": 71, "text": "parameter estimation"}, {"st": 77, "ed": 79, "text": "large margin"}, {"st": 90, "ed": 92, "text": "maximum likelihood"}, {"st": 94, "ed": 96, "text": "large margin"}, {"st": 96, "ed": 98, "text": "loss function"}, {"st": 122, "ed": 124, "text": "extensive empirical"}, {"st": 141, "ed": 143, "text": "kernel matrix"}]
[{"st": 3, "ed": 5, "text": "spectral graph"}, {"st": 20, "ed": 22, "text": "recent years"}, {"st": 36, "ed": 38, "text": "strong performance"}, {"st": 44, "ed": 46, "text": "spectral graph"}, {"st": 57, "ed": 60, "text": "number of clusters"}, {"st": 94, "ed": 96, "text": "image segmentation"}, {"st": 109, "ed": 111, "text": "power law"}, {"st": 121, "ed": 123, "text": "power law"}, {"st": 132, "ed": 134, "text": "power law"}, {"st": 141, "ed": 144, "text": "number of clusters"}, {"st": 181, "ed": 183, "text": "iterative algorithm"}, {"st": 193, "ed": 195, "text": "proposed algorithm"}, {"st": 200, "ed": 202, "text": "map inference"}]
[{"st": 33, "ed": 35, "text": "hand engineered"}, {"st": 42, "ed": 46, "text": "convolutional neural networks cnns"}, {"st": 47, "ed": 49, "text": "automatically learn"}, {"st": 65, "ed": 67, "text": "post processing"}, {"st": 84, "ed": 86, "text": "network architecture"}, {"st": 90, "ed": 92, "text": "neural networks"}, {"st": 106, "ed": 108, "text": "higher order"}, {"st": 119, "ed": 121, "text": "network architecture"}, {"st": 148, "ed": 150, "text": "proposed approach"}, {"st": 188, "ed": 191, "text": "end to end"}]
[{"st": 9, "ed": 12, "text": "locality sensitive hashing"}, {"st": 21, "ed": 23, "text": "vision community"}, {"st": 25, "ed": 28, "text": "approximate nearest neighbor"}, {"st": 32, "ed": 36, "text": "reproducing kernel hilbert space"}, {"st": 86, "ed": 88, "text": "retrieval performance"}, {"st": 93, "ed": 95, "text": "analysis reveals"}, {"st": 100, "ed": 102, "text": "empirical performance"}, {"st": 110, "ed": 112, "text": "large scale"}, {"st": 113, "ed": 115, "text": "image retrieval"}]
[{"st": 37, "ed": 39, "text": "riemannian manifold"}, {"st": 41, "ed": 43, "text": "positive definite"}, {"st": 51, "ed": 53, "text": "distance measures"}, {"st": 54, "ed": 56, "text": "optimal transport"}, {"st": 63, "ed": 65, "text": "distance metrics"}, {"st": 110, "ed": 113, "text": "k nearest neighbor"}]
[{"st": 32, "ed": 34, "text": "binary data"}, {"st": 46, "ed": 48, "text": "low dimensional"}]
[{"st": 0, "ed": 3, "text": "statistical machine learning"}, {"st": 23, "ed": 25, "text": "multivariate analysis"}, {"st": 33, "ed": 35, "text": "supervised learning"}, {"st": 53, "ed": 55, "text": "unsupervised learning"}, {"st": 78, "ed": 80, "text": "functional neuroimaging"}, {"st": 84, "ed": 86, "text": "scikit learn"}, {"st": 88, "ed": 90, "text": "machine learning"}, {"st": 100, "ed": 102, "text": "scikit learn"}, {"st": 108, "ed": 110, "text": "statistical learning"}, {"st": 112, "ed": 115, "text": "supervised and unsupervised"}]
[{"st": 0, "ed": 2, "text": "hash codes"}, {"st": 24, "ed": 26, "text": "random forest"}, {"st": 40, "ed": 42, "text": "random forest"}, {"st": 47, "ed": 49, "text": "deep learning"}, {"st": 60, "ed": 62, "text": "large scale"}, {"st": 64, "ed": 66, "text": "random forest"}, {"st": 167, "ed": 169, "text": "near optimal"}, {"st": 176, "ed": 178, "text": "large scale"}, {"st": 185, "ed": 187, "text": "proposed approach"}, {"st": 187, "ed": 189, "text": "significantly outperforms"}]
[{"st": 0, "ed": 2, "text": "deep learning"}]
[{"st": 13, "ed": 15, "text": "low dimensional"}, {"st": 111, "ed": 113, "text": "main contributions"}, {"st": 141, "ed": 143, "text": "efficiently learn"}, {"st": 181, "ed": 183, "text": "numerical experiments"}, {"st": 185, "ed": 189, "text": "synthetic and real data"}, {"st": 199, "ed": 201, "text": "learning algorithms"}, {"st": 202, "ed": 204, "text": "existing approaches"}, {"st": 223, "ed": 226, "text": "kernel k means"}]
[{"st": 8, "ed": 10, "text": "valuable information"}, {"st": 20, "ed": 22, "text": "theoretically sound"}, {"st": 25, "ed": 27, "text": "kernel based"}, {"st": 47, "ed": 49, "text": "multi scale"}, {"st": 68, "ed": 70, "text": "positive definite"}, {"st": 84, "ed": 86, "text": "benchmark datasets"}, {"st": 96, "ed": 98, "text": "performance gains"}, {"st": 100, "ed": 102, "text": "proposed method"}, {"st": 112, "ed": 114, "text": "recently introduced"}]
[{"st": 20, "ed": 22, "text": "cluster analysis"}, {"st": 27, "ed": 29, "text": "data analysis"}, {"st": 43, "ed": 45, "text": "semi supervised"}, {"st": 46, "ed": 48, "text": "clustering algorithm"}, {"st": 82, "ed": 84, "text": "labeled data"}, {"st": 103, "ed": 105, "text": "fully automatic"}]
[{"st": 9, "ed": 11, "text": "feature vectors"}, {"st": 14, "ed": 16, "text": "vector representations"}, {"st": 33, "ed": 35, "text": "multiple domains"}, {"st": 39, "ed": 41, "text": "common space"}, {"st": 42, "ed": 44, "text": "linear transformations"}, {"st": 48, "ed": 50, "text": "closely related"}, {"st": 64, "ed": 66, "text": "closely related"}, {"st": 71, "ed": 73, "text": "cross domain"}, {"st": 81, "ed": 83, "text": "spectral graph"}, {"st": 92, "ed": 94, "text": "multivariate analysis"}, {"st": 100, "ed": 103, "text": "canonical correlation analysis"}, {"st": 103, "ed": 105, "text": "correspondence analysis"}, {"st": 116, "ed": 118, "text": "pattern recognition"}, {"st": 166, "ed": 168, "text": "cross domain"}, {"st": 178, "ed": 180, "text": "spectral graph"}, {"st": 195, "ed": 197, "text": "associative memory"}, {"st": 199, "ed": 201, "text": "neural networks"}, {"st": 211, "ed": 213, "text": "cross validation"}, {"st": 220, "ed": 222, "text": "common space"}, {"st": 224, "ed": 226, "text": "regularization parameter"}]
[{"st": 6, "ed": 8, "text": "deep structured"}, {"st": 8, "ed": 11, "text": "conditional random field"}, {"st": 73, "ed": 76, "text": "spatial and temporal"}, {"st": 84, "ed": 86, "text": "deep structured"}, {"st": 86, "ed": 89, "text": "probabilistic graphical model"}, {"st": 129, "ed": 131, "text": "experiment results"}, {"st": 160, "ed": 162, "text": "baseline methods"}, {"st": 164, "ed": 166, "text": "mean shift"}]
[{"st": 16, "ed": 18, "text": "real valued"}, {"st": 23, "ed": 25, "text": "low dimensional"}, {"st": 39, "ed": 41, "text": "hash function"}, {"st": 113, "ed": 116, "text": "encoder and decoder"}, {"st": 126, "ed": 128, "text": "image retrieval"}, {"st": 130, "ed": 132, "text": "precision recall"}, {"st": 141, "ed": 143, "text": "hash function"}]
[{"st": 1, "ed": 3, "text": "recently proposed"}, {"st": 3, "ed": 5, "text": "clustering method"}, {"st": 48, "ed": 50, "text": "undesired edges"}, {"st": 69, "ed": 71, "text": "undesired edges"}, {"st": 84, "ed": 86, "text": "affinity propagation"}, {"st": 116, "ed": 119, "text": "synthetic and real"}]
[{"st": 47, "ed": 49, "text": "density estimation"}, {"st": 55, "ed": 57, "text": "stochastic optimization"}, {"st": 63, "ed": 65, "text": "map estimation"}, {"st": 68, "ed": 70, "text": "latent variable"}, {"st": 74, "ed": 76, "text": "optimization problem"}, {"st": 82, "ed": 84, "text": "stochastic optimization"}, {"st": 93, "ed": 97, "text": "real and synthetic data"}, {"st": 113, "ed": 115, "text": "quasi newton"}, {"st": 124, "ed": 126, "text": "gradient based"}, {"st": 145, "ed": 147, "text": "existing methods"}, {"st": 150, "ed": 152, "text": "significantly faster"}]
[{"st": 15, "ed": 17, "text": "feature vector"}, {"st": 35, "ed": 37, "text": "optimization problem"}, {"st": 43, "ed": 45, "text": "discrete variables"}, {"st": 57, "ed": 59, "text": "continuous optimization"}, {"st": 92, "ed": 94, "text": "hash function"}, {"st": 109, "ed": 111, "text": "hash functions"}, {"st": 114, "ed": 116, "text": "loss functions"}, {"st": 129, "ed": 131, "text": "hash functions"}, {"st": 178, "ed": 180, "text": "hash functions"}, {"st": 200, "ed": 202, "text": "optimization algorithms"}]
[{"st": 0, "ed": 4, "text": "nonnegative matrix factorization nmf"}, {"st": 9, "ed": 11, "text": "feature extraction"}, {"st": 15, "ed": 17, "text": "successfully applied"}, {"st": 43, "ed": 45, "text": "kernel based"}, {"st": 57, "ed": 59, "text": "multi objective"}, {"st": 68, "ed": 70, "text": "objective functions"}, {"st": 75, "ed": 77, "text": "feature spaces"}, {"st": 94, "ed": 96, "text": "multi objective"}, {"st": 108, "ed": 110, "text": "optimal solutions"}]
[{"st": 11, "ed": 13, "text": "dimensionality reduction"}, {"st": 15, "ed": 17, "text": "underlying structure"}, {"st": 20, "ed": 22, "text": "multivariate data"}, {"st": 24, "ed": 26, "text": "low dimensional"}, {"st": 29, "ed": 31, "text": "dimensionality reduction"}, {"st": 72, "ed": 74, "text": "based method"}]
[{"st": 0, "ed": 3, "text": "conditional random fields"}, {"st": 12, "ed": 14, "text": "computer vision"}, {"st": 22, "ed": 24, "text": "image pixels"}, {"st": 31, "ed": 33, "text": "efficient inference"}, {"st": 46, "ed": 48, "text": "fully connected"}, {"st": 57, "ed": 59, "text": "approximate inference"}, {"st": 86, "ed": 88, "text": "fully connected"}, {"st": 100, "ed": 102, "text": "low rank"}, {"st": 112, "ed": 114, "text": "proposed algorithm"}, {"st": 117, "ed": 120, "text": "quasi newton method"}, {"st": 121, "ed": 124, "text": "takes advantage of"}, {"st": 125, "ed": 128, "text": "low rank matrix"}, {"st": 136, "ed": 138, "text": "experiments demonstrate"}, {"st": 145, "ed": 147, "text": "fully connected"}]
[{"st": 9, "ed": 12, "text": "end to end"}, {"st": 35, "ed": 38, "text": "convolutional neural networks"}, {"st": 71, "ed": 74, "text": "end to end"}]
[{"st": 16, "ed": 18, "text": "prior knowledge"}, {"st": 25, "ed": 27, "text": "prior information"}, {"st": 112, "ed": 114, "text": "machine teaching"}, {"st": 122, "ed": 124, "text": "visual concepts"}, {"st": 133, "ed": 135, "text": "labeled images"}, {"st": 186, "ed": 188, "text": "real world"}]
[{"st": 0, "ed": 3, "text": "principal component analysis"}, {"st": 10, "ed": 12, "text": "low dimensional"}, {"st": 78, "ed": 80, "text": "low dimensional"}, {"st": 104, "ed": 106, "text": "learned features"}]
[{"st": 3, "ed": 7, "text": "supervised and semi supervised"}, {"st": 29, "ed": 31, "text": "active learning"}, {"st": 77, "ed": 79, "text": "expected error"}, {"st": 84, "ed": 86, "text": "expected error"}, {"st": 122, "ed": 124, "text": "active learning"}]
[{"st": 0, "ed": 2, "text": "deep structured"}, {"st": 5, "ed": 7, "text": "great promise"}, {"st": 18, "ed": 20, "text": "deep structured"}, {"st": 21, "ed": 23, "text": "learning scheme"}, {"st": 28, "ed": 33, "text": "deep convolutional neural networks cnns"}, {"st": 41, "ed": 43, "text": "message passing"}, {"st": 45, "ed": 47, "text": "structured prediction"}, {"st": 48, "ed": 51, "text": "conditional random fields"}, {"st": 81, "ed": 83, "text": "structured learning"}, {"st": 144, "ed": 146, "text": "network parameters"}, {"st": 166, "ed": 168, "text": "image segmentation"}, {"st": 170, "ed": 173, "text": "pascal voc 2012"}, {"st": 202, "ed": 204, "text": "impressive performance"}]
[{"st": 7, "ed": 9, "text": "feature map"}, {"st": 39, "ed": 41, "text": "invariant feature"}, {"st": 43, "ed": 45, "text": "kernel methods"}, {"st": 50, "ed": 52, "text": "feature map"}, {"st": 73, "ed": 75, "text": "feature map"}, {"st": 110, "ed": 112, "text": "error rates"}, {"st": 117, "ed": 120, "text": "empirical risk minimization"}, {"st": 127, "ed": 129, "text": "sample complexity"}, {"st": 131, "ed": 133, "text": "learning algorithm"}, {"st": 136, "ed": 138, "text": "invariant representation"}, {"st": 144, "ed": 146, "text": "supervised learning"}]
[{"st": 8, "ed": 10, "text": "transfer learning"}, {"st": 11, "ed": 13, "text": "pre trained"}, {"st": 13, "ed": 18, "text": "deep convolutional neural networks cnns"}, {"st": 27, "ed": 29, "text": "least squares"}, {"st": 40, "ed": 43, "text": "training and testing"}, {"st": 45, "ed": 47, "text": "competitive performance"}, {"st": 59, "ed": 61, "text": "grid search"}, {"st": 62, "ed": 64, "text": "cross validation"}, {"st": 83, "ed": 85, "text": "transfer learning"}, {"st": 101, "ed": 103, "text": "fixed point"}, {"st": 131, "ed": 133, "text": "transfer learning"}, {"st": 137, "ed": 139, "text": "visual recognition"}, {"st": 152, "ed": 154, "text": "prediction accuracy"}]
[{"st": 4, "ed": 6, "text": "natural images"}, {"st": 21, "ed": 24, "text": "recurrent neural networks"}, {"st": 57, "ed": 59, "text": "multi dimensional"}, {"st": 60, "ed": 63, "text": "short term memory"}, {"st": 105, "ed": 107, "text": "image datasets"}, {"st": 109, "ed": 111, "text": "promising results"}, {"st": 114, "ed": 116, "text": "texture synthesis"}]
[{"st": 0, "ed": 2, "text": "representation learning"}, {"st": 6, "ed": 9, "text": "massive amounts of"}, {"st": 9, "ed": 11, "text": "labeled data"}, {"st": 22, "ed": 24, "text": "parametric models"}, {"st": 25, "ed": 27, "text": "neural networks"}, {"st": 53, "ed": 55, "text": "human perception"}, {"st": 91, "ed": 93, "text": "similarity function"}, {"st": 106, "ed": 109, "text": "unsupervised feature learning"}, {"st": 131, "ed": 133, "text": "latent factor"}, {"st": 173, "ed": 175, "text": "metric learning"}, {"st": 179, "ed": 181, "text": "generative models"}, {"st": 183, "ed": 185, "text": "side information"}, {"st": 197, "ed": 199, "text": "proposed approach"}, {"st": 201, "ed": 203, "text": "latent spaces"}]
[{"st": 3, "ed": 5, "text": "multi label"}, {"st": 33, "ed": 35, "text": "loss function"}, {"st": 36, "ed": 38, "text": "multi label"}, {"st": 45, "ed": 47, "text": "wasserstein distance"}, {"st": 63, "ed": 65, "text": "wasserstein distance"}, {"st": 81, "ed": 83, "text": "efficient learning"}, {"st": 96, "ed": 98, "text": "wasserstein distance"}, {"st": 108, "ed": 110, "text": "statistical learning"}, {"st": 139, "ed": 141, "text": "real data"}, {"st": 148, "ed": 150, "text": "creative commons"}, {"st": 155, "ed": 157, "text": "doesn t"}]
[{"st": 74, "ed": 76, "text": "k nn"}, {"st": 79, "ed": 81, "text": "spanning tree"}, {"st": 105, "ed": 107, "text": "previously proposed"}, {"st": 132, "ed": 134, "text": "k nn"}, {"st": 159, "ed": 161, "text": "special case"}]
[{"st": 0, "ed": 2, "text": "recent progress"}, {"st": 34, "ed": 36, "text": "image caption"}, {"st": 65, "ed": 67, "text": "visual perception"}, {"st": 98, "ed": 100, "text": "visual scene"}, {"st": 119, "ed": 121, "text": "higher level"}, {"st": 146, "ed": 148, "text": "published results"}]
[{"st": 16, "ed": 18, "text": "dynamical systems"}, {"st": 26, "ed": 28, "text": "deep generative"}, {"st": 34, "ed": 36, "text": "variational autoencoders"}, {"st": 44, "ed": 46, "text": "latent space"}, {"st": 63, "ed": 65, "text": "optimal control"}, {"st": 67, "ed": 69, "text": "latent space"}, {"st": 74, "ed": 76, "text": "image sequences"}, {"st": 78, "ed": 80, "text": "strong performance"}]
[{"st": 0, "ed": 2, "text": "feature selection"}, {"st": 15, "ed": 17, "text": "classification accuracy"}, {"st": 43, "ed": 45, "text": "extremely high"}, {"st": 46, "ed": 48, "text": "feature space"}, {"st": 51, "ed": 53, "text": "training samples"}, {"st": 98, "ed": 100, "text": "feature selection"}, {"st": 108, "ed": 110, "text": "stability selection"}, {"st": 113, "ed": 115, "text": "elastic net"}, {"st": 146, "ed": 149, "text": "synthetic and real"}, {"st": 163, "ed": 165, "text": "feature selection"}, {"st": 170, "ed": 172, "text": "stability selection"}, {"st": 198, "ed": 200, "text": "feature selection"}, {"st": 220, "ed": 222, "text": "feature selection"}, {"st": 234, "ed": 238, "text": "attention deficit hyperactivity disorder"}]
[{"st": 0, "ed": 2, "text": "low rank"}, {"st": 7, "ed": 9, "text": "machine learning"}, {"st": 10, "ed": 12, "text": "computer vision"}, {"st": 16, "ed": 18, "text": "recent studies"}, {"st": 20, "ed": 22, "text": "nuclear norm"}, {"st": 24, "ed": 26, "text": "convex surrogate"}, {"st": 32, "ed": 34, "text": "singular values"}, {"st": 40, "ed": 42, "text": "nuclear norm"}, {"st": 80, "ed": 82, "text": "low rank"}, {"st": 99, "ed": 101, "text": "convex objective"}, {"st": 104, "ed": 106, "text": "large scale"}, {"st": 120, "ed": 122, "text": "low rank"}, {"st": 135, "ed": 137, "text": "motion segmentation"}, {"st": 144, "ed": 146, "text": "proposed method"}, {"st": 152, "ed": 154, "text": "subspace clustering"}]
[{"st": 0, "ed": 2, "text": "subspace clustering"}, {"st": 5, "ed": 7, "text": "ell 1"}, {"st": 7, "ed": 9, "text": "ell 2"}, {"st": 10, "ed": 12, "text": "nuclear norm"}, {"st": 21, "ed": 23, "text": "theoretical guarantees"}, {"st": 41, "ed": 43, "text": "ell 1"}, {"st": 77, "ed": 79, "text": "large scale"}, {"st": 79, "ed": 81, "text": "convex optimization"}, {"st": 86, "ed": 88, "text": "ell 2"}, {"st": 89, "ed": 91, "text": "nuclear norm"}, {"st": 94, "ed": 96, "text": "closed form"}, {"st": 100, "ed": 102, "text": "strong assumptions"}, {"st": 120, "ed": 122, "text": "subspace clustering"}, {"st": 135, "ed": 137, "text": "computationally efficient"}, {"st": 150, "ed": 152, "text": "synthetic data"}, {"st": 154, "ed": 156, "text": "theoretical analysis"}, {"st": 159, "ed": 161, "text": "handwritten digit"}, {"st": 167, "ed": 169, "text": "approach achieves"}]
[{"st": 0, "ed": 2, "text": "random forests"}, {"st": 6, "ed": 8, "text": "ensemble method"}, {"st": 23, "ed": 25, "text": "random forests"}, {"st": 39, "ed": 41, "text": "random forests"}, {"st": 114, "ed": 116, "text": "proposed algorithm"}, {"st": 118, "ed": 120, "text": "random forests"}, {"st": 123, "ed": 125, "text": "theoretical analysis"}, {"st": 134, "ed": 137, "text": "theory and practice"}, {"st": 138, "ed": 140, "text": "random forests"}, {"st": 147, "ed": 150, "text": "benchmark data sets"}, {"st": 163, "ed": 165, "text": "outperforms previous"}]
[{"st": 14, "ed": 16, "text": "image recognition"}, {"st": 76, "ed": 78, "text": "key idea"}, {"st": 139, "ed": 142, "text": "convolutional neural networks"}]
[{"st": 44, "ed": 46, "text": "undesired edges"}, {"st": 75, "ed": 77, "text": "undesired edges"}, {"st": 124, "ed": 126, "text": "salient features"}, {"st": 128, "ed": 130, "text": "undesired edges"}, {"st": 134, "ed": 136, "text": "undesired edges"}, {"st": 156, "ed": 158, "text": "method called"}, {"st": 189, "ed": 191, "text": "method called"}, {"st": 210, "ed": 212, "text": "hierarchical clustering"}, {"st": 243, "ed": 245, "text": "visualization method"}, {"st": 248, "ed": 250, "text": "undesired edges"}, {"st": 259, "ed": 261, "text": "based clustering"}, {"st": 292, "ed": 294, "text": "cluster analysis"}]
[{"st": 4, "ed": 6, "text": "deep learning"}, {"st": 26, "ed": 29, "text": "restricted boltzmann machines"}, {"st": 54, "ed": 56, "text": "generative model"}, {"st": 66, "ed": 68, "text": "real life"}, {"st": 74, "ed": 77, "text": "takes advantage of"}]
[{"st": 21, "ed": 23, "text": "similarity based"}, {"st": 23, "ed": 25, "text": "anomaly detection"}, {"st": 38, "ed": 40, "text": "nearest neighbor"}, {"st": 52, "ed": 54, "text": "application domains"}, {"st": 60, "ed": 62, "text": "dissimilarity measure"}, {"st": 93, "ed": 95, "text": "linear combination"}, {"st": 114, "ed": 116, "text": "anomaly detection"}, {"st": 118, "ed": 120, "text": "anomaly detection"}, {"st": 126, "ed": 128, "text": "multiple times"}, {"st": 145, "ed": 147, "text": "similarity based"}, {"st": 147, "ed": 149, "text": "anomaly detection"}, {"st": 154, "ed": 156, "text": "dissimilarity measure"}, {"st": 165, "ed": 167, "text": "anomaly detection"}, {"st": 186, "ed": 188, "text": "multiple times"}, {"st": 202, "ed": 204, "text": "linear combinations"}, {"st": 214, "ed": 218, "text": "synthetic and real data"}]
[{"st": 20, "ed": 22, "text": "increasing attention"}, {"st": 61, "ed": 64, "text": "end to end"}, {"st": 69, "ed": 74, "text": "deep convolutional neural network cnn"}, {"st": 79, "ed": 81, "text": "key feature"}, {"st": 107, "ed": 109, "text": "feature representation"}, {"st": 114, "ed": 116, "text": "feature maps"}]
[{"st": 1, "ed": 3, "text": "sparse coding"}, {"st": 3, "ed": 5, "text": "based clustering"}, {"st": 15, "ed": 18, "text": "efficiency and scalability"}, {"st": 23, "ed": 25, "text": "recent years"}, {"st": 25, "ed": 27, "text": "deep learning"}, {"st": 33, "ed": 35, "text": "highly effective"}, {"st": 35, "ed": 38, "text": "efficient and scalable"}, {"st": 38, "ed": 40, "text": "feature learning"}, {"st": 49, "ed": 51, "text": "sparse coding"}, {"st": 51, "ed": 53, "text": "based clustering"}, {"st": 58, "ed": 60, "text": "deep learning"}, {"st": 65, "ed": 67, "text": "deep model"}, {"st": 71, "ed": 73, "text": "feed forward"}, {"st": 73, "ed": 75, "text": "network structure"}, {"st": 84, "ed": 86, "text": "sparse coding"}, {"st": 92, "ed": 94, "text": "task specific"}, {"st": 94, "ed": 96, "text": "loss functions"}, {"st": 104, "ed": 106, "text": "deep learning"}, {"st": 107, "ed": 109, "text": "sparse coding"}, {"st": 141, "ed": 143, "text": "extensive experiments"}]
[{"st": 21, "ed": 23, "text": "finite mixture"}, {"st": 26, "ed": 28, "text": "central role"}, {"st": 39, "ed": 41, "text": "maximum likelihood"}, {"st": 44, "ed": 47, "text": "expectation maximization em"}, {"st": 55, "ed": 57, "text": "mixture model"}, {"st": 70, "ed": 72, "text": "gaussian mixture"}, {"st": 96, "ed": 98, "text": "random variable"}, {"st": 105, "ed": 107, "text": "model selection"}, {"st": 111, "ed": 114, "text": "minimum message length"}, {"st": 117, "ed": 119, "text": "weight initialization"}, {"st": 150, "ed": 152, "text": "clustering technique"}, {"st": 159, "ed": 161, "text": "audio visual"}]
[{"st": 49, "ed": 51, "text": "salient features"}, {"st": 74, "ed": 76, "text": "nearest neighbor"}, {"st": 124, "ed": 126, "text": "method called"}, {"st": 128, "ed": 130, "text": "nearest neighbor"}]
[{"st": 1, "ed": 5, "text": "non negative matrix factorization"}, {"st": 11, "ed": 13, "text": "low dimensional"}, {"st": 99, "ed": 101, "text": "semi supervised"}, {"st": 114, "ed": 116, "text": "prior information"}, {"st": 148, "ed": 150, "text": "low dimensional"}, {"st": 162, "ed": 166, "text": "non negative matrix factorization"}]
[{"st": 2, "ed": 5, "text": "massive amounts of"}, {"st": 26, "ed": 28, "text": "subspace clustering"}, {"st": 33, "ed": 35, "text": "computational complexity"}, {"st": 37, "ed": 39, "text": "large scale"}, {"st": 43, "ed": 45, "text": "random sampling"}, {"st": 69, "ed": 71, "text": "large scale"}, {"st": 86, "ed": 89, "text": "probability density function"}, {"st": 91, "ed": 93, "text": "observed data"}, {"st": 107, "ed": 109, "text": "computational burden"}, {"st": 121, "ed": 123, "text": "extensive numerical"}, {"st": 125, "ed": 129, "text": "synthetic and real data"}, {"st": 137, "ed": 139, "text": "competitive performance"}, {"st": 149, "ed": 151, "text": "subspace clustering"}, {"st": 151, "ed": 153, "text": "big data"}]
[{"st": 7, "ed": 9, "text": "deep learning"}, {"st": 23, "ed": 25, "text": "unified framework"}, {"st": 59, "ed": 61, "text": "parameter sharing"}, {"st": 63, "ed": 65, "text": "statistical modeling"}, {"st": 109, "ed": 111, "text": "keyword spotting"}, {"st": 114, "ed": 116, "text": "speech recognition"}, {"st": 125, "ed": 127, "text": "low rank"}]
[{"st": 3, "ed": 5, "text": "random field"}, {"st": 50, "ed": 52, "text": "learning process"}, {"st": 56, "ed": 58, "text": "exponential family"}, {"st": 92, "ed": 94, "text": "local binary"}, {"st": 148, "ed": 150, "text": "maximum likelihood"}, {"st": 155, "ed": 157, "text": "objective function"}, {"st": 168, "ed": 170, "text": "proposed method"}, {"st": 189, "ed": 191, "text": "texture synthesis"}, {"st": 216, "ed": 218, "text": "large scale"}]
[{"st": 4, "ed": 6, "text": "natural images"}, {"st": 30, "ed": 32, "text": "natural images"}, {"st": 52, "ed": 54, "text": "natural images"}, {"st": 59, "ed": 61, "text": "learning algorithms"}, {"st": 66, "ed": 68, "text": "critical phenomena"}, {"st": 82, "ed": 84, "text": "critical phenomena"}, {"st": 88, "ed": 91, "text": "condensed matter physics"}, {"st": 108, "ed": 110, "text": "natural images"}, {"st": 167, "ed": 170, "text": "probabilistic graphical model"}, {"st": 176, "ed": 178, "text": "unsupervised learning"}, {"st": 187, "ed": 189, "text": "supervised learning"}, {"st": 191, "ed": 193, "text": "conditional distributions"}, {"st": 200, "ed": 202, "text": "conditional distributions"}, {"st": 203, "ed": 205, "text": "logistic regression"}, {"st": 233, "ed": 235, "text": "hidden units"}, {"st": 257, "ed": 259, "text": "machine learning"}, {"st": 262, "ed": 264, "text": "natural image"}]
[{"st": 34, "ed": 36, "text": "open problems"}]
[{"st": 1, "ed": 3, "text": "multiple instance"}, {"st": 21, "ed": 23, "text": "class classification"}, {"st": 52, "ed": 54, "text": "positive class"}, {"st": 73, "ed": 76, "text": "positive and negative"}, {"st": 97, "ed": 99, "text": "discriminative features"}, {"st": 102, "ed": 104, "text": "positive class"}, {"st": 105, "ed": 107, "text": "existing methods"}, {"st": 122, "ed": 125, "text": "detection and classification"}, {"st": 130, "ed": 134, "text": "multiple instance learning mil"}, {"st": 134, "ed": 137, "text": "dictionary learning algorithms"}]
[{"st": 5, "ed": 7, "text": "high level"}, {"st": 9, "ed": 11, "text": "deep architecture"}, {"st": 32, "ed": 34, "text": "computer vision"}, {"st": 76, "ed": 79, "text": "convolutional neural network"}, {"st": 92, "ed": 94, "text": "level features"}, {"st": 99, "ed": 101, "text": "level features"}, {"st": 140, "ed": 142, "text": "image data"}, {"st": 144, "ed": 146, "text": "synthetic data"}, {"st": 154, "ed": 156, "text": "deep architectures"}, {"st": 167, "ed": 169, "text": "real world"}]
[{"st": 0, "ed": 2, "text": "numerous applications"}, {"st": 3, "ed": 5, "text": "data mining"}, {"st": 6, "ed": 8, "text": "machine learning"}, {"st": 15, "ed": 19, "text": "robust principal component analysis"}, {"st": 30, "ed": 32, "text": "nuclear norm"}, {"st": 33, "ed": 35, "text": "convex surrogate"}, {"st": 53, "ed": 55, "text": "low rank"}, {"st": 66, "ed": 68, "text": "real world"}, {"st": 71, "ed": 73, "text": "nuclear norm"}, {"st": 79, "ed": 81, "text": "singular values"}, {"st": 86, "ed": 88, "text": "ell 1"}, {"st": 91, "ed": 93, "text": "singular values"}, {"st": 95, "ed": 97, "text": "approximation error"}, {"st": 124, "ed": 126, "text": "nuclear norm"}, {"st": 149, "ed": 151, "text": "minimization problem"}, {"st": 156, "ed": 158, "text": "lagrange multiplier"}, {"st": 158, "ed": 160, "text": "based optimization"}, {"st": 166, "ed": 168, "text": "method outperforms"}]
[{"st": 2, "ed": 4, "text": "predict future"}, {"st": 14, "ed": 16, "text": "internal representation"}, {"st": 52, "ed": 54, "text": "optical flow"}, {"st": 61, "ed": 63, "text": "computer vision"}, {"st": 68, "ed": 70, "text": "frame prediction"}, {"st": 75, "ed": 77, "text": "vision applications"}, {"st": 105, "ed": 107, "text": "convolutional network"}, {"st": 109, "ed": 111, "text": "future frames"}, {"st": 126, "ed": 130, "text": "mean squared error mse"}, {"st": 130, "ed": 132, "text": "loss function"}, {"st": 138, "ed": 140, "text": "feature learning"}, {"st": 142, "ed": 144, "text": "multi scale"}, {"st": 146, "ed": 148, "text": "adversarial training"}, {"st": 162, "ed": 164, "text": "published results"}, {"st": 166, "ed": 169, "text": "recurrent neural networks"}]
[{"st": 4, "ed": 6, "text": "impressive performance"}, {"st": 8, "ed": 10, "text": "computer vision"}, {"st": 55, "ed": 58, "text": "low rank tensor"}, {"st": 96, "ed": 98, "text": "low rank"}, {"st": 106, "ed": 108, "text": "significant speedup"}, {"st": 110, "ed": 112, "text": "low rank"}, {"st": 125, "ed": 127, "text": "cifar 10"}, {"st": 130, "ed": 132, "text": "low rank"}, {"st": 152, "ed": 154, "text": "proposed method"}, {"st": 155, "ed": 157, "text": "cifar 10"}, {"st": 180, "ed": 182, "text": "vgg 16"}, {"st": 192, "ed": 194, "text": "empirical success"}, {"st": 196, "ed": 199, "text": "low rank tensor"}]
[{"st": 0, "ed": 2, "text": "unsupervised methods"}, {"st": 23, "ed": 25, "text": "sparse representations"}, {"st": 39, "ed": 41, "text": "higher level"}, {"st": 59, "ed": 61, "text": "sparse coding"}, {"st": 64, "ed": 66, "text": "joint representation"}, {"st": 68, "ed": 70, "text": "cross modal"}, {"st": 81, "ed": 83, "text": "cross modal"}, {"st": 89, "ed": 91, "text": "multimodal data"}, {"st": 106, "ed": 108, "text": "becoming increasingly"}, {"st": 119, "ed": 121, "text": "substantially improve"}, {"st": 122, "ed": 124, "text": "classification performance"}, {"st": 134, "ed": 136, "text": "deep architectures"}, {"st": 139, "ed": 141, "text": "proposed framework"}, {"st": 147, "ed": 149, "text": "highly nonlinear"}, {"st": 161, "ed": 163, "text": "image denoising"}, {"st": 164, "ed": 166, "text": "event detection"}, {"st": 180, "ed": 182, "text": "image text"}, {"st": 183, "ed": 185, "text": "sentiment classification"}, {"st": 187, "ed": 189, "text": "image text"}]
[{"st": 3, "ed": 5, "text": "computer vision"}, {"st": 43, "ed": 45, "text": "task specific"}, {"st": 86, "ed": 88, "text": "natural images"}]
[{"st": 11, "ed": 13, "text": "machine learning"}, {"st": 25, "ed": 27, "text": "similarity metric"}, {"st": 73, "ed": 75, "text": "feature vectors"}, {"st": 84, "ed": 86, "text": "feature extractor"}, {"st": 107, "ed": 110, "text": "multi label classification"}, {"st": 114, "ed": 116, "text": "faster convergence"}, {"st": 119, "ed": 121, "text": "higher accuracy"}]
[{"st": 5, "ed": 7, "text": "image classification"}]
[{"st": 8, "ed": 10, "text": "computer vision"}, {"st": 17, "ed": 19, "text": "benchmark datasets"}, {"st": 39, "ed": 41, "text": "class labels"}, {"st": 89, "ed": 91, "text": "loss functions"}, {"st": 137, "ed": 139, "text": "softmax loss"}]
[{"st": 5, "ed": 7, "text": "optical flow"}, {"st": 13, "ed": 15, "text": "supervised learning"}, {"st": 45, "ed": 47, "text": "optical flow"}, {"st": 49, "ed": 51, "text": "convolutional networks"}, {"st": 83, "ed": 85, "text": "large scale"}, {"st": 100, "ed": 102, "text": "convolutional network"}, {"st": 131, "ed": 133, "text": "flow estimation"}]
[{"st": 3, "ed": 5, "text": "computer vision"}, {"st": 8, "ed": 10, "text": "robust pca"}, {"st": 38, "ed": 40, "text": "low rank"}, {"st": 51, "ed": 53, "text": "optimization problem"}, {"st": 55, "ed": 57, "text": "np hard"}, {"st": 57, "ed": 59, "text": "convex relaxations"}, {"st": 71, "ed": 73, "text": "performance guarantees"}, {"st": 160, "ed": 162, "text": "matrix completion"}]
[{"st": 4, "ed": 6, "text": "machine learning"}, {"st": 30, "ed": 32, "text": "proposed approach"}, {"st": 45, "ed": 47, "text": "surface temperature"}, {"st": 129, "ed": 131, "text": "deep learning"}]
[{"st": 1, "ed": 3, "text": "deep generative"}, {"st": 17, "ed": 19, "text": "dictionary learning"}, {"st": 39, "ed": 42, "text": "support vector machine"}, {"st": 50, "ed": 52, "text": "max margin"}, {"st": 63, "ed": 65, "text": "latent features"}, {"st": 74, "ed": 76, "text": "max margin"}, {"st": 87, "ed": 89, "text": "monte carlo"}, {"st": 89, "ed": 91, "text": "expectation maximization"}, {"st": 102, "ed": 104, "text": "large scale"}, {"st": 108, "ed": 110, "text": "excellent results"}, {"st": 114, "ed": 116, "text": "benchmark datasets"}, {"st": 127, "ed": 129, "text": "highly competitive"}]
[{"st": 0, "ed": 3, "text": "markov random field"}, {"st": 9, "ed": 11, "text": "approximation algorithms"}, {"st": 57, "ed": 59, "text": "learning algorithm"}]
[{"st": 6, "ed": 8, "text": "learned representations"}, {"st": 18, "ed": 20, "text": "variational autoencoder"}, {"st": 22, "ed": 25, "text": "generative adversarial network"}, {"st": 29, "ed": 31, "text": "feature representations"}, {"st": 81, "ed": 83, "text": "similarity measures"}, {"st": 102, "ed": 104, "text": "visual features"}]
[{"st": 6, "ed": 8, "text": "unsupervised learning"}, {"st": 16, "ed": 18, "text": "multi dimensional"}, {"st": 26, "ed": 29, "text": "classification and regression"}, {"st": 72, "ed": 74, "text": "feature space"}, {"st": 142, "ed": 144, "text": "a 10"}, {"st": 178, "ed": 180, "text": "multiple labels"}, {"st": 184, "ed": 186, "text": "feature space"}, {"st": 186, "ed": 188, "text": "feed forward"}, {"st": 190, "ed": 192, "text": "linear complexity"}, {"st": 219, "ed": 221, "text": "supervised learning"}]
[{"st": 0, "ed": 3, "text": "deep convolutional networks"}, {"st": 28, "ed": 30, "text": "linear filter"}]
[{"st": 8, "ed": 10, "text": "fast approximate"}, {"st": 10, "ed": 13, "text": "nearest neighbor search"}, {"st": 23, "ed": 25, "text": "objective functions"}, {"st": 27, "ed": 29, "text": "hash functions"}, {"st": 33, "ed": 35, "text": "objective functions"}, {"st": 53, "ed": 55, "text": "hash functions"}, {"st": 95, "ed": 97, "text": "hash function"}, {"st": 134, "ed": 136, "text": "objective function"}, {"st": 142, "ed": 145, "text": "precision and recall"}]
[{"st": 7, "ed": 9, "text": "feature maps"}, {"st": 10, "ed": 12, "text": "recently gained"}, {"st": 20, "ed": 23, "text": "training and testing"}, {"st": 25, "ed": 27, "text": "kernel based"}, {"st": 30, "ed": 32, "text": "random projection"}, {"st": 35, "ed": 38, "text": "curse of dimensionality"}, {"st": 42, "ed": 44, "text": "feature space"}, {"st": 46, "ed": 48, "text": "low dimensional"}, {"st": 58, "ed": 60, "text": "random projection"}, {"st": 84, "ed": 86, "text": "hand written"}, {"st": 92, "ed": 94, "text": "object database"}, {"st": 99, "ed": 101, "text": "object classification"}, {"st": 109, "ed": 111, "text": "random projection"}]
[{"st": 3, "ed": 5, "text": "deep learning"}, {"st": 70, "ed": 72, "text": "training examples"}, {"st": 80, "ed": 82, "text": "deep learning"}, {"st": 86, "ed": 89, "text": "stochastic gradient descent"}, {"st": 129, "ed": 131, "text": "learning algorithm"}, {"st": 140, "ed": 142, "text": "adversarial perturbations"}, {"st": 157, "ed": 159, "text": "adversarial examples"}, {"st": 173, "ed": 175, "text": "extensive simulations"}, {"st": 177, "ed": 179, "text": "deep learning"}, {"st": 182, "ed": 184, "text": "network architectures"}]
[{"st": 52, "ed": 55, "text": "bag of words"}, {"st": 60, "ed": 62, "text": "real valued"}, {"st": 84, "ed": 86, "text": "deep model"}, {"st": 131, "ed": 133, "text": "image retrieval"}, {"st": 138, "ed": 140, "text": "deep architecture"}, {"st": 142, "ed": 144, "text": "compact representation"}, {"st": 151, "ed": 153, "text": "side information"}]
[{"st": 29, "ed": 31, "text": "joint distribution"}, {"st": 32, "ed": 34, "text": "multiple modalities"}, {"st": 58, "ed": 60, "text": "conditional distributions"}, {"st": 65, "ed": 67, "text": "variational methods"}, {"st": 76, "ed": 78, "text": "deep model"}, {"st": 89, "ed": 91, "text": "latent representation"}, {"st": 102, "ed": 104, "text": "joint representation"}, {"st": 131, "ed": 134, "text": "qualitatively and quantitatively"}]
[{"st": 24, "ed": 26, "text": "large scale"}, {"st": 52, "ed": 54, "text": "ensemble learning"}, {"st": 67, "ed": 71, "text": "deep convolutional neural networks"}]
[{"st": 24, "ed": 26, "text": "deep convolutional"}, {"st": 55, "ed": 57, "text": "plant science"}, {"st": 60, "ed": 62, "text": "united states"}, {"st": 85, "ed": 87, "text": "time consuming"}, {"st": 101, "ed": 103, "text": "proposed framework"}, {"st": 197, "ed": 199, "text": "post processing"}]
[{"st": 14, "ed": 18, "text": "deep convolutional neural network"}, {"st": 19, "ed": 21, "text": "feature extractors"}, {"st": 25, "ed": 27, "text": "network structure"}, {"st": 45, "ed": 47, "text": "square integrable"}, {"st": 65, "ed": 67, "text": "natural images"}, {"st": 80, "ed": 82, "text": "main contribution"}, {"st": 93, "ed": 95, "text": "structural properties"}]
[{"st": 4, "ed": 6, "text": "subspace clustering"}, {"st": 11, "ed": 14, "text": "each data point"}, {"st": 16, "ed": 18, "text": "linear combination"}, {"st": 29, "ed": 31, "text": "ell 1"}, {"st": 31, "ed": 33, "text": "ell 2"}, {"st": 36, "ed": 38, "text": "ell 1"}, {"st": 68, "ed": 70, "text": "ell 2"}, {"st": 71, "ed": 73, "text": "nuclear norm"}, {"st": 88, "ed": 90, "text": "ell 1"}, {"st": 90, "ed": 92, "text": "ell 2"}, {"st": 93, "ed": 95, "text": "nuclear norm"}, {"st": 123, "ed": 125, "text": "elastic net"}, {"st": 130, "ed": 132, "text": "ell 1"}, {"st": 133, "ed": 135, "text": "ell 2"}, {"st": 146, "ed": 148, "text": "active set"}, {"st": 155, "ed": 157, "text": "geometric analysis"}, {"st": 160, "ed": 162, "text": "theoretical justification"}, {"st": 174, "ed": 176, "text": "ell 2"}, {"st": 182, "ed": 184, "text": "ell 1"}, {"st": 187, "ed": 189, "text": "elastic net"}, {"st": 197, "ed": 199, "text": "active set"}, {"st": 207, "ed": 209, "text": "clustering performance"}, {"st": 213, "ed": 215, "text": "large scale"}]
[{"st": 5, "ed": 8, "text": "deep neural networks"}, {"st": 12, "ed": 14, "text": "image datasets"}, {"st": 21, "ed": 23, "text": "class conditional"}, {"st": 35, "ed": 37, "text": "feature space"}, {"st": 41, "ed": 43, "text": "features extracted"}, {"st": 59, "ed": 61, "text": "hand crafted"}, {"st": 61, "ed": 63, "text": "feature extraction"}, {"st": 64, "ed": 66, "text": "low dimensional"}, {"st": 95, "ed": 98, "text": "convolutional neural network"}, {"st": 110, "ed": 112, "text": "error rate"}, {"st": 120, "ed": 122, "text": "intrinsic dimension"}, {"st": 134, "ed": 136, "text": "higher dimensional"}, {"st": 139, "ed": 141, "text": "handwritten digits"}, {"st": 143, "ed": 145, "text": "object recognition"}, {"st": 175, "ed": 177, "text": "dimensional manifold"}]
[{"st": 2, "ed": 4, "text": "object recognition"}, {"st": 8, "ed": 10, "text": "lifelong learning"}, {"st": 13, "ed": 15, "text": "agent learns"}, {"st": 22, "ed": 24, "text": "object classes"}, {"st": 39, "ed": 41, "text": "least squares"}, {"st": 70, "ed": 72, "text": "training examples"}, {"st": 95, "ed": 97, "text": "machine learning"}, {"st": 97, "ed": 99, "text": "benchmark dataset"}, {"st": 102, "ed": 104, "text": "object recognition"}, {"st": 109, "ed": 111, "text": "empirical evidence"}, {"st": 114, "ed": 116, "text": "approach achieves"}, {"st": 119, "ed": 121, "text": "classification performance"}]
[{"st": 7, "ed": 9, "text": "image representation"}, {"st": 15, "ed": 17, "text": "unlike traditional"}, {"st": 17, "ed": 19, "text": "kernel methods"}, {"st": 26, "ed": 28, "text": "prediction task"}, {"st": 45, "ed": 47, "text": "recently introduced"}, {"st": 55, "ed": 57, "text": "unsupervised learning"}, {"st": 77, "ed": 80, "text": "convolutional neural network"}, {"st": 92, "ed": 94, "text": "linear subspace"}, {"st": 95, "ed": 98, "text": "a reproducing kernel"}, {"st": 106, "ed": 108, "text": "method achieves"}, {"st": 109, "ed": 111, "text": "competitive performance"}, {"st": 112, "ed": 114, "text": "image classification"}, {"st": 117, "ed": 119, "text": "deep learning"}, {"st": 122, "ed": 124, "text": "cifar 10"}, {"st": 129, "ed": 132, "text": "image super resolution"}]
[{"st": 2, "ed": 4, "text": "pattern recognition"}, {"st": 24, "ed": 26, "text": "driving styles"}, {"st": 27, "ed": 30, "text": "k means clustering"}, {"st": 31, "ed": 34, "text": "support vector machine"}, {"st": 79, "ed": 81, "text": "support vectors"}, {"st": 82, "ed": 85, "text": "k means clustering"}, {"st": 107, "ed": 109, "text": "clustering results"}, {"st": 110, "ed": 113, "text": "support vector machine"}, {"st": 143, "ed": 145, "text": "cross validation"}, {"st": 165, "ed": 167, "text": "driving styles"}]
[{"st": 0, "ed": 3, "text": "deep neural networks"}, {"st": 19, "ed": 21, "text": "weight decay"}, {"st": 25, "ed": 27, "text": "impressive performance"}, {"st": 31, "ed": 33, "text": "training set"}, {"st": 50, "ed": 52, "text": "regularization methods"}, {"st": 73, "ed": 75, "text": "l 2"}, {"st": 119, "ed": 121, "text": "benchmark datasets"}, {"st": 121, "ed": 124, "text": "mnist and cifar10"}, {"st": 137, "ed": 139, "text": "small sample"}, {"st": 148, "ed": 150, "text": "source code"}, {"st": 158, "ed": 160, "text": "https github.com"}]
[{"st": 28, "ed": 30, "text": "recently proposed"}, {"st": 42, "ed": 46, "text": "reproducing kernel hilbert spaces"}, {"st": 50, "ed": 52, "text": "classification algorithm"}, {"st": 101, "ed": 103, "text": "standard classification"}, {"st": 112, "ed": 115, "text": "support vector machines"}, {"st": 119, "ed": 121, "text": "computational complexity"}, {"st": 124, "ed": 127, "text": "random fourier features"}, {"st": 134, "ed": 136, "text": "convergence rates"}, {"st": 138, "ed": 140, "text": "proposed method"}, {"st": 142, "ed": 144, "text": "experiments demonstrate"}, {"st": 144, "ed": 146, "text": "strong performance"}]
[{"st": 15, "ed": 17, "text": "image classification"}, {"st": 69, "ed": 72, "text": "deep neural networks"}, {"st": 75, "ed": 77, "text": "image representations"}, {"st": 119, "ed": 121, "text": "image features"}, {"st": 144, "ed": 146, "text": "shared representation"}, {"st": 161, "ed": 163, "text": "standard benchmarks"}]
[{"st": 8, "ed": 11, "text": "semi supervised learning"}, {"st": 12, "ed": 16, "text": "deep convolutional neural networks"}, {"st": 18, "ed": 21, "text": "semi supervised learning"}, {"st": 27, "ed": 29, "text": "unlabeled data"}, {"st": 48, "ed": 50, "text": "regularization term"}, {"st": 58, "ed": 60, "text": "multiple classes"}, {"st": 68, "ed": 70, "text": "decision boundary"}, {"st": 87, "ed": 89, "text": "proposed approach"}, {"st": 112, "ed": 114, "text": "object recognition"}]
[{"st": 25, "ed": 27, "text": "iterative reconstruction"}, {"st": 88, "ed": 91, "text": "synthetic and real"}]
[{"st": 1, "ed": 3, "text": "hyperspectral image"}, {"st": 7, "ed": 9, "text": "spatial information"}, {"st": 36, "ed": 38, "text": "extracted features"}, {"st": 57, "ed": 59, "text": "machine learning"}, {"st": 63, "ed": 65, "text": "gaussian kernel"}, {"st": 93, "ed": 95, "text": "special case"}, {"st": 98, "ed": 100, "text": "experiments conducted"}]
[{"st": 0, "ed": 2, "text": "deep learning"}, {"st": 3, "ed": 7, "text": "multi layer neural networks"}, {"st": 13, "ed": 16, "text": "modern machine learning"}, {"st": 19, "ed": 22, "text": "deep neural networks"}, {"st": 43, "ed": 45, "text": "training process"}, {"st": 63, "ed": 66, "text": "deep neural network"}, {"st": 96, "ed": 99, "text": "deep neural networks"}, {"st": 108, "ed": 111, "text": "proof of concept"}, {"st": 114, "ed": 116, "text": "proposed method"}, {"st": 119, "ed": 121, "text": "sparse linear"}, {"st": 122, "ed": 124, "text": "deep autoencoder"}]
[{"st": 32, "ed": 34, "text": "highly correlated"}, {"st": 40, "ed": 42, "text": "low dimensional"}, {"st": 50, "ed": 52, "text": "high dimensional"}, {"st": 140, "ed": 142, "text": "recognition performance"}]
[{"st": 19, "ed": 21, "text": "training data"}, {"st": 53, "ed": 57, "text": "convolutional neural network cnn"}, {"st": 59, "ed": 61, "text": "training data"}, {"st": 97, "ed": 99, "text": "feature extraction"}, {"st": 100, "ed": 102, "text": "fine tuning"}, {"st": 108, "ed": 110, "text": "multitask learning"}, {"st": 132, "ed": 134, "text": "fine tuning"}]
[{"st": 4, "ed": 6, "text": "computer vision"}, {"st": 25, "ed": 28, "text": "deep generative models"}, {"st": 78, "ed": 82, "text": "trained end to end"}]
[{"st": 2, "ed": 4, "text": "machine learning"}, {"st": 18, "ed": 20, "text": "input data"}, {"st": 35, "ed": 37, "text": "machine learning"}, {"st": 69, "ed": 71, "text": "adversarial examples"}, {"st": 84, "ed": 86, "text": "machine learning"}, {"st": 119, "ed": 121, "text": "machine learning"}, {"st": 160, "ed": 162, "text": "machine learning"}, {"st": 173, "ed": 175, "text": "adversarial images"}, {"st": 188, "ed": 190, "text": "classification accuracy"}, {"st": 200, "ed": 202, "text": "adversarial examples"}]
[{"st": 6, "ed": 8, "text": "multi label"}, {"st": 9, "ed": 11, "text": "multi class"}, {"st": 11, "ed": 13, "text": "image classification"}, {"st": 25, "ed": 28, "text": "cross entropy loss"}, {"st": 28, "ed": 30, "text": "logistic regression"}, {"st": 53, "ed": 55, "text": "classification problem"}, {"st": 85, "ed": 87, "text": "faster convergence"}, {"st": 90, "ed": 92, "text": "a 7"}]
[{"st": 2, "ed": 5, "text": "deep neural networks"}, {"st": 15, "ed": 18, "text": "efficient and accurate"}, {"st": 47, "ed": 49, "text": "error function"}, {"st": 51, "ed": 54, "text": "deep neural network"}, {"st": 73, "ed": 75, "text": "error function"}, {"st": 94, "ed": 96, "text": "random matrix"}, {"st": 104, "ed": 106, "text": "error function"}, {"st": 117, "ed": 119, "text": "theoretical results"}]
[{"st": 15, "ed": 17, "text": "joint distributions"}, {"st": 63, "ed": 65, "text": "joint probability"}, {"st": 67, "ed": 69, "text": "main result"}, {"st": 71, "ed": 73, "text": "directly applied"}, {"st": 82, "ed": 85, "text": "training and test"}, {"st": 88, "ed": 90, "text": "learning framework"}]
[{"st": 2, "ed": 4, "text": "magnetic resonance"}, {"st": 5, "ed": 7, "text": "fully automatic"}, {"st": 24, "ed": 26, "text": "mr images"}, {"st": 37, "ed": 40, "text": "fully convolutional network"}, {"st": 43, "ed": 45, "text": "image representations"}, {"st": 69, "ed": 72, "text": "detection and segmentation"}, {"st": 78, "ed": 82, "text": "trained end to end"}, {"st": 121, "ed": 123, "text": "fully convolutional"}, {"st": 126, "ed": 129, "text": "restricted boltzmann machines"}, {"st": 152, "ed": 154, "text": "substantially improve"}]
[{"st": 26, "ed": 28, "text": "applications including"}, {"st": 42, "ed": 44, "text": "variational autoencoders"}, {"st": 45, "ed": 47, "text": "user interface"}, {"st": 50, "ed": 52, "text": "latent space"}, {"st": 58, "ed": 62, "text": "deep convolutional neural network"}, {"st": 75, "ed": 77, "text": "empirically evaluate"}, {"st": 88, "ed": 90, "text": "relative improvement"}]
[{"st": 14, "ed": 17, "text": "online feature selection"}, {"st": 24, "ed": 26, "text": "underlying structure"}, {"st": 32, "ed": 34, "text": "image analysis"}, {"st": 53, "ed": 55, "text": "feature selection"}, {"st": 111, "ed": 114, "text": "online feature selection"}, {"st": 125, "ed": 128, "text": "feature selection method"}, {"st": 131, "ed": 133, "text": "proposed approach"}, {"st": 139, "ed": 141, "text": "group selection"}, {"st": 149, "ed": 151, "text": "group selection"}, {"st": 157, "ed": 159, "text": "spectral analysis"}, {"st": 161, "ed": 163, "text": "discriminative features"}, {"st": 169, "ed": 171, "text": "group selection"}, {"st": 174, "ed": 176, "text": "linear regression"}, {"st": 212, "ed": 214, "text": "multiple tasks"}, {"st": 215, "ed": 217, "text": "image classification"}, {"st": 217, "ed": 219, "text": "face verification"}, {"st": 222, "ed": 224, "text": "extensive empirical"}, {"st": 227, "ed": 229, "text": "real world"}, {"st": 230, "ed": 233, "text": "benchmark data sets"}, {"st": 236, "ed": 238, "text": "method outperforms"}, {"st": 243, "ed": 246, "text": "online feature selection"}]
[{"st": 30, "ed": 33, "text": "event related potential"}, {"st": 43, "ed": 46, "text": "linear discriminant analysis"}, {"st": 55, "ed": 57, "text": "classification problems"}, {"st": 88, "ed": 90, "text": "kullback leibler"}, {"st": 102, "ed": 104, "text": "feature selection"}, {"st": 119, "ed": 121, "text": "kullback leibler"}, {"st": 136, "ed": 138, "text": "numerical experiments"}, {"st": 148, "ed": 150, "text": "method outperforms"}]
[{"st": 1, "ed": 3, "text": "recent works"}, {"st": 14, "ed": 16, "text": "worst case"}, {"st": 17, "ed": 19, "text": "adversarial perturbations"}, {"st": 51, "ed": 53, "text": "random noise"}, {"st": 60, "ed": 62, "text": "worst case"}, {"st": 68, "ed": 70, "text": "quantitative analysis"}, {"st": 84, "ed": 86, "text": "theoretical bounds"}, {"st": 140, "ed": 142, "text": "random noise"}, {"st": 152, "ed": 154, "text": "worst case"}, {"st": 155, "ed": 157, "text": "random noise"}, {"st": 169, "ed": 171, "text": "accurate estimates"}, {"st": 179, "ed": 182, "text": "deep neural networks"}, {"st": 194, "ed": 196, "text": "decision boundaries"}]
[{"st": 11, "ed": 13, "text": "stochastic optimization"}, {"st": 18, "ed": 20, "text": "optimization process"}, {"st": 38, "ed": 40, "text": "optimization method"}, {"st": 41, "ed": 43, "text": "large scale"}, {"st": 62, "ed": 64, "text": "optimization method"}, {"st": 124, "ed": 126, "text": "hyper parameters"}, {"st": 132, "ed": 134, "text": "baseline method"}, {"st": 145, "ed": 147, "text": "deep learning"}]
[{"st": 2, "ed": 4, "text": "classification techniques"}, {"st": 8, "ed": 10, "text": "traditional approaches"}, {"st": 82, "ed": 84, "text": "binary classification"}]
[{"st": 1, "ed": 3, "text": "based methods"}, {"st": 6, "ed": 8, "text": "promising results"}, {"st": 12, "ed": 14, "text": "depth estimation"}, {"st": 19, "ed": 21, "text": "existing approaches"}, {"st": 38, "ed": 40, "text": "ground truth"}, {"st": 64, "ed": 66, "text": "existing approaches"}, {"st": 86, "ed": 88, "text": "training objective"}, {"st": 91, "ed": 94, "text": "convolutional neural network"}, {"st": 100, "ed": 102, "text": "depth estimation"}, {"st": 106, "ed": 108, "text": "ground truth"}, {"st": 124, "ed": 126, "text": "image reconstruction"}, {"st": 132, "ed": 134, "text": "image reconstruction"}, {"st": 149, "ed": 151, "text": "training loss"}, {"st": 168, "ed": 170, "text": "improved performance"}, {"st": 177, "ed": 179, "text": "method produces"}, {"st": 186, "ed": 188, "text": "depth estimation"}, {"st": 195, "ed": 197, "text": "supervised methods"}, {"st": 202, "ed": 204, "text": "ground truth"}]
[{"st": 1, "ed": 4, "text": "nonnegative matrix factorization"}, {"st": 14, "ed": 16, "text": "clustering algorithms"}, {"st": 19, "ed": 21, "text": "spectral clustering"}, {"st": 34, "ed": 36, "text": "clustering quality"}, {"st": 40, "ed": 42, "text": "clustering algorithms"}, {"st": 46, "ed": 50, "text": "synthetic and real world"}, {"st": 87, "ed": 89, "text": "proximal gradient"}, {"st": 98, "ed": 104, "text": "alternating direction method of multipliers admm"}, {"st": 117, "ed": 119, "text": "convergence guarantee"}, {"st": 149, "ed": 151, "text": "extensive experiments"}, {"st": 188, "ed": 191, "text": "orders of magnitude"}, {"st": 207, "ed": 209, "text": "spectral clustering"}]
[{"st": 0, "ed": 4, "text": "recurrent neural networks rnns"}, {"st": 8, "ed": 10, "text": "time series"}, {"st": 27, "ed": 30, "text": "short term memory"}, {"st": 31, "ed": 33, "text": "lstm rnns"}]
[{"st": 0, "ed": 3, "text": "image super resolution"}, {"st": 7, "ed": 9, "text": "inverse problem"}, {"st": 31, "ed": 33, "text": "empirical risk"}, {"st": 39, "ed": 43, "text": "mean squared error mse"}, {"st": 67, "ed": 70, "text": "a posteriori map"}, {"st": 89, "ed": 91, "text": "map estimation"}, {"st": 94, "ed": 96, "text": "non trivial"}, {"st": 111, "ed": 113, "text": "map inference"}, {"st": 119, "ed": 121, "text": "iterative algorithms"}, {"st": 122, "ed": 124, "text": "don t"}, {"st": 130, "ed": 132, "text": "neural network"}, {"st": 141, "ed": 143, "text": "map inference"}, {"st": 160, "ed": 162, "text": "neural network"}, {"st": 189, "ed": 191, "text": "low resolution"}, {"st": 200, "ed": 202, "text": "map inference"}, {"st": 207, "ed": 209, "text": "cross entropy"}, {"st": 227, "ed": 230, "text": "generative adversarial networks"}, {"st": 248, "ed": 250, "text": "baseline method"}, {"st": 252, "ed": 254, "text": "maximum likelihood"}, {"st": 263, "ed": 265, "text": "based approach"}, {"st": 280, "ed": 282, "text": "variational inference"}]
[{"st": 20, "ed": 22, "text": "large scale"}, {"st": 74, "ed": 76, "text": "machine learning"}, {"st": 78, "ed": 81, "text": "convolutional neural networks"}, {"st": 82, "ed": 85, "text": "support vector machines"}, {"st": 93, "ed": 96, "text": "change point detection"}, {"st": 98, "ed": 101, "text": "mean squared error"}, {"st": 104, "ed": 107, "text": "hidden markov models"}, {"st": 108, "ed": 110, "text": "maximum likelihood"}, {"st": 134, "ed": 138, "text": "los angeles police department"}]
[{"st": 39, "ed": 41, "text": "missing data"}, {"st": 50, "ed": 52, "text": "missing data"}, {"st": 62, "ed": 65, "text": "low rank tensor"}, {"st": 67, "ed": 69, "text": "highly correlated"}, {"st": 105, "ed": 107, "text": "missing data"}, {"st": 122, "ed": 124, "text": "matrix completion"}, {"st": 145, "ed": 147, "text": "proposed algorithm"}]
[{"st": 29, "ed": 31, "text": "np hard"}, {"st": 32, "ed": 34, "text": "existing methods"}, {"st": 44, "ed": 46, "text": "empirical performance"}, {"st": 71, "ed": 73, "text": "undirected graph"}, {"st": 84, "ed": 86, "text": "spanning tree"}, {"st": 101, "ed": 103, "text": "optimal solution"}, {"st": 112, "ed": 115, "text": "significantly outperforms existing"}, {"st": 118, "ed": 120, "text": "benchmark tasks"}]
[{"st": 0, "ed": 2, "text": "adversarial examples"}, {"st": 8, "ed": 10, "text": "machine learning"}, {"st": 23, "ed": 25, "text": "black box"}, {"st": 34, "ed": 36, "text": "adversarial training"}, {"st": 45, "ed": 47, "text": "adversarial examples"}, {"st": 67, "ed": 69, "text": "adversarial training"}, {"st": 81, "ed": 83, "text": "adversarial training"}, {"st": 95, "ed": 97, "text": "adversarial training"}, {"st": 106, "ed": 108, "text": "adversarial training"}, {"st": 119, "ed": 121, "text": "multi step"}, {"st": 141, "ed": 143, "text": "black box"}, {"st": 155, "ed": 157, "text": "trained models"}, {"st": 161, "ed": 163, "text": "adversarial examples"}, {"st": 175, "ed": 177, "text": "true label"}]
[{"st": 9, "ed": 11, "text": "low rank"}, {"st": 18, "ed": 20, "text": "spectral graph"}, {"st": 34, "ed": 36, "text": "low rank"}, {"st": 41, "ed": 43, "text": "low rank"}, {"st": 64, "ed": 66, "text": "convex optimization"}, {"st": 70, "ed": 73, "text": "low rank tensor"}, {"st": 73, "ed": 75, "text": "inverse problems"}, {"st": 87, "ed": 89, "text": "low rank"}, {"st": 95, "ed": 97, "text": "theoretical analysis"}, {"st": 108, "ed": 110, "text": "multi dimensional"}, {"st": 116, "ed": 118, "text": "approximation error"}, {"st": 131, "ed": 133, "text": "wide variety"}, {"st": 160, "ed": 163, "text": "orders of magnitude"}]
[{"st": 0, "ed": 2, "text": "variational autoencoders"}, {"st": 7, "ed": 10, "text": "deep neural networks"}, {"st": 14, "ed": 16, "text": "generative models"}, {"st": 25, "ed": 27, "text": "variational autoencoders"}, {"st": 64, "ed": 66, "text": "real valued"}]
[{"st": 0, "ed": 3, "text": "deep generative models"}, {"st": 11, "ed": 13, "text": "complex data"}, {"st": 17, "ed": 19, "text": "input data"}, {"st": 43, "ed": 45, "text": "max margin"}, {"st": 45, "ed": 48, "text": "deep generative models"}, {"st": 51, "ed": 53, "text": "class conditional"}, {"st": 62, "ed": 64, "text": "max margin"}, {"st": 68, "ed": 70, "text": "predictive performance"}, {"st": 74, "ed": 78, "text": "supervised and semi supervised"}, {"st": 85, "ed": 88, "text": "semi supervised learning"}, {"st": 94, "ed": 96, "text": "max margin"}, {"st": 105, "ed": 107, "text": "posterior inference"}, {"st": 113, "ed": 115, "text": "max margin"}, {"st": 118, "ed": 120, "text": "regularization terms"}, {"st": 121, "ed": 123, "text": "unlabeled data"}, {"st": 141, "ed": 143, "text": "empirical results"}, {"st": 149, "ed": 151, "text": "max margin"}, {"st": 153, "ed": 155, "text": "significantly improve"}, {"st": 156, "ed": 158, "text": "prediction performance"}, {"st": 168, "ed": 170, "text": "supervised learning"}, {"st": 181, "ed": 184, "text": "convolutional neural networks"}, {"st": 193, "ed": 196, "text": "semi supervised learning"}, {"st": 199, "ed": 201, "text": "efficient inference"}, {"st": 207, "ed": 209, "text": "classification results"}]
[{"st": 6, "ed": 10, "text": "convolutional neural network cnn"}, {"st": 27, "ed": 29, "text": "approach called"}, {"st": 37, "ed": 39, "text": "class specific"}, {"st": 75, "ed": 77, "text": "models including"}, {"st": 77, "ed": 79, "text": "image captioning"}, {"st": 80, "ed": 84, "text": "visual question answering vqa"}]
[{"st": 14, "ed": 16, "text": "variational autoencoders"}, {"st": 39, "ed": 41, "text": "structural constraints"}, {"st": 44, "ed": 46, "text": "graphical model"}, {"st": 48, "ed": 51, "text": "deep generative models"}, {"st": 75, "ed": 78, "text": "end to end"}, {"st": 85, "ed": 89, "text": "unsupervised and semi supervised"}]
[{"st": 2, "ed": 4, "text": "open problem"}, {"st": 8, "ed": 10, "text": "artificial intelligence"}, {"st": 15, "ed": 17, "text": "learning systems"}, {"st": 38, "ed": 40, "text": "training strategy"}, {"st": 52, "ed": 54, "text": "training data"}, {"st": 102, "ed": 104, "text": "deep learning"}, {"st": 110, "ed": 112, "text": "cifar 100"}]
[{"st": 20, "ed": 23, "text": "stochastic gradient descent"}, {"st": 63, "ed": 65, "text": "method called"}, {"st": 87, "ed": 89, "text": "cifar 10"}]
[{"st": 7, "ed": 9, "text": "artificial intelligence"}, {"st": 27, "ed": 31, "text": "convolutional neural network cnn"}, {"st": 37, "ed": 40, "text": "artificial neural network"}, {"st": 48, "ed": 52, "text": "scale invariant feature transform"}]
[{"st": 1, "ed": 3, "text": "deep learning"}, {"st": 12, "ed": 14, "text": "deep learning"}, {"st": 23, "ed": 25, "text": "labeled samples"}, {"st": 30, "ed": 32, "text": "deep learning"}, {"st": 38, "ed": 40, "text": "labeled samples"}, {"st": 44, "ed": 46, "text": "remote sensing"}, {"st": 48, "ed": 50, "text": "active learning"}, {"st": 64, "ed": 66, "text": "proposed algorithm"}, {"st": 67, "ed": 69, "text": "training samples"}, {"st": 82, "ed": 84, "text": "deep network"}, {"st": 88, "ed": 90, "text": "training samples"}, {"st": 94, "ed": 96, "text": "proposed algorithm"}, {"st": 108, "ed": 110, "text": "classification algorithms"}, {"st": 118, "ed": 120, "text": "proposed algorithm"}, {"st": 121, "ed": 124, "text": "efficient and effective"}]
[{"st": 27, "ed": 29, "text": "hash functions"}, {"st": 41, "ed": 43, "text": "objective functions"}, {"st": 63, "ed": 65, "text": "hash functions"}, {"st": 66, "ed": 69, "text": "minimum description length"}, {"st": 74, "ed": 76, "text": "hash codes"}, {"st": 93, "ed": 95, "text": "efficient learning"}, {"st": 119, "ed": 121, "text": "hash function"}, {"st": 126, "ed": 128, "text": "extensive experiments"}, {"st": 132, "ed": 134, "text": "large scale"}, {"st": 138, "ed": 140, "text": "proposed method"}]
[{"st": 7, "ed": 9, "text": "linear regression"}, {"st": 86, "ed": 88, "text": "optimization problem"}, {"st": 100, "ed": 102, "text": "synthetic data"}, {"st": 111, "ed": 113, "text": "multivariate regression"}, {"st": 124, "ed": 126, "text": "promising results"}, {"st": 129, "ed": 131, "text": "real world"}, {"st": 147, "ed": 149, "text": "social network"}, {"st": 158, "ed": 160, "text": "pose estimation"}, {"st": 185, "ed": 187, "text": "open source"}]
[{"st": 5, "ed": 7, "text": "loss function"}, {"st": 17, "ed": 19, "text": "loss functions"}, {"st": 21, "ed": 23, "text": "robust statistics"}, {"st": 33, "ed": 35, "text": "loss functions"}, {"st": 47, "ed": 49, "text": "loss functions"}, {"st": 57, "ed": 59, "text": "negative log"}]
[{"st": 53, "ed": 55, "text": "attention based"}, {"st": 66, "ed": 68, "text": "x ray"}, {"st": 90, "ed": 94, "text": "recurrent neural network rnn"}, {"st": 101, "ed": 103, "text": "x ray"}, {"st": 126, "ed": 128, "text": "100 000"}, {"st": 142, "ed": 144, "text": "reinforcement learning"}, {"st": 147, "ed": 149, "text": "task specific"}]
[{"st": 1, "ed": 3, "text": "activity recognition"}, {"st": 27, "ed": 29, "text": "pattern recognition"}, {"st": 48, "ed": 50, "text": "activity recognition"}, {"st": 69, "ed": 71, "text": "machine learning"}, {"st": 73, "ed": 75, "text": "unsupervised clustering"}, {"st": 93, "ed": 95, "text": "semi supervised"}, {"st": 107, "ed": 109, "text": "multi user"}]
[{"st": 8, "ed": 10, "text": "challenging task"}, {"st": 17, "ed": 19, "text": "complex models"}, {"st": 100, "ed": 103, "text": "image to image"}, {"st": 120, "ed": 122, "text": "generated images"}]
[{"st": 3, "ed": 5, "text": "data model"}, {"st": 23, "ed": 25, "text": "low rank"}, {"st": 30, "ed": 32, "text": "sparse matrix"}, {"st": 46, "ed": 48, "text": "robust pca"}, {"st": 63, "ed": 65, "text": "existing algorithms"}, {"st": 80, "ed": 82, "text": "robust pca"}, {"st": 164, "ed": 167, "text": "ell 1 norm"}]
[{"st": 56, "ed": 58, "text": "inter class"}, {"st": 148, "ed": 150, "text": "proposed approach"}, {"st": 152, "ed": 154, "text": "classification accuracy"}]
[{"st": 3, "ed": 6, "text": "canonical correlation analysis"}, {"st": 9, "ed": 11, "text": "successfully applied"}, {"st": 43, "ed": 45, "text": "recognition tasks"}, {"st": 56, "ed": 58, "text": "probabilistic framework"}, {"st": 77, "ed": 81, "text": "synthetic and real data"}, {"st": 94, "ed": 96, "text": "real data"}, {"st": 100, "ed": 102, "text": "face database"}, {"st": 106, "ed": 108, "text": "face database"}, {"st": 113, "ed": 115, "text": "proposed algorithm"}, {"st": 116, "ed": 118, "text": "face recognition"}, {"st": 123, "ed": 125, "text": "facial expressions"}]
[{"st": 8, "ed": 11, "text": "supervised machine learning"}, {"st": 14, "ed": 16, "text": "multi scale"}, {"st": 16, "ed": 18, "text": "geometric properties"}, {"st": 32, "ed": 34, "text": "training set"}, {"st": 79, "ed": 81, "text": "feature extraction"}, {"st": 87, "ed": 89, "text": "theoretical properties"}]
[{"st": 14, "ed": 16, "text": "machine learning"}, {"st": 18, "ed": 20, "text": "image classification"}, {"st": 32, "ed": 34, "text": "input image"}, {"st": 34, "ed": 36, "text": "significantly improves"}, {"st": 87, "ed": 89, "text": "real world"}, {"st": 102, "ed": 104, "text": "image classification"}, {"st": 112, "ed": 114, "text": "technique called"}, {"st": 226, "ed": 228, "text": "similar accuracy"}]
[{"st": 16, "ed": 18, "text": "large scale"}, {"st": 20, "ed": 22, "text": "empirical results"}, {"st": 25, "ed": 27, "text": "neural networks"}, {"st": 28, "ed": 30, "text": "weight matrices"}, {"st": 36, "ed": 38, "text": "neural networks"}, {"st": 40, "ed": 42, "text": "significant reduction"}, {"st": 45, "ed": 47, "text": "computational complexity"}, {"st": 68, "ed": 70, "text": "neural networks"}, {"st": 83, "ed": 85, "text": "error bounds"}, {"st": 87, "ed": 89, "text": "neural networks"}, {"st": 94, "ed": 96, "text": "neural networks"}, {"st": 107, "ed": 109, "text": "back propagation"}]
[{"st": 50, "ed": 52, "text": "neural network"}, {"st": 57, "ed": 60, "text": "human pose estimation"}, {"st": 94, "ed": 96, "text": "multi scale"}, {"st": 101, "ed": 103, "text": "performance improvement"}, {"st": 161, "ed": 164, "text": "human pose estimation"}, {"st": 165, "ed": 167, "text": "face alignment"}]
[{"st": 8, "ed": 10, "text": "training data"}, {"st": 12, "ed": 14, "text": "neural network"}, {"st": 16, "ed": 18, "text": "approximate bayesian"}, {"st": 25, "ed": 27, "text": "neural network"}, {"st": 28, "ed": 30, "text": "synthetic data"}, {"st": 36, "ed": 38, "text": "proposal distribution"}, {"st": 40, "ed": 42, "text": "approximate inference"}, {"st": 44, "ed": 46, "text": "synthetic data"}, {"st": 54, "ed": 56, "text": "recognition task"}, {"st": 68, "ed": 70, "text": "synthetic data"}, {"st": 82, "ed": 84, "text": "task specific"}, {"st": 88, "ed": 90, "text": "neural network"}, {"st": 99, "ed": 101, "text": "real world"}, {"st": 111, "ed": 113, "text": "empirical results"}, {"st": 124, "ed": 126, "text": "synthetic data"}, {"st": 134, "ed": 136, "text": "neural network"}]
[{"st": 0, "ed": 4, "text": "restricted boltzmann machines rbms"}, {"st": 28, "ed": 30, "text": "contrastive divergence"}, {"st": 34, "ed": 36, "text": "belief propagation"}, {"st": 44, "ed": 46, "text": "structured prediction"}, {"st": 80, "ed": 82, "text": "belief propagation"}, {"st": 90, "ed": 93, "text": "tens of thousands"}, {"st": 103, "ed": 105, "text": "maximum likelihood"}, {"st": 106, "ed": 108, "text": "max margin"}, {"st": 132, "ed": 134, "text": "structured prediction"}, {"st": 152, "ed": 155, "text": "learning and inference"}]
[{"st": 0, "ed": 2, "text": "unsupervised learning"}, {"st": 16, "ed": 18, "text": "unlabelled data"}, {"st": 28, "ed": 30, "text": "generative model"}, {"st": 38, "ed": 40, "text": "input data"}, {"st": 42, "ed": 44, "text": "latent representation"}, {"st": 60, "ed": 62, "text": "input samples"}, {"st": 100, "ed": 102, "text": "latent space"}, {"st": 157, "ed": 159, "text": "classification performance"}, {"st": 169, "ed": 171, "text": "input data"}]
[{"st": 5, "ed": 7, "text": "noisy labels"}, {"st": 12, "ed": 14, "text": "visual recognition"}, {"st": 22, "ed": 24, "text": "noisy labels"}, {"st": 60, "ed": 62, "text": "real world"}, {"st": 62, "ed": 64, "text": "noisy labels"}, {"st": 70, "ed": 72, "text": "true labels"}, {"st": 90, "ed": 92, "text": "side information"}, {"st": 101, "ed": 103, "text": "knowledge graph"}, {"st": 115, "ed": 117, "text": "traditional approaches"}, {"st": 129, "ed": 131, "text": "benchmark datasets"}, {"st": 144, "ed": 146, "text": "noisy labels"}, {"st": 151, "ed": 153, "text": "empirical study"}, {"st": 158, "ed": 160, "text": "proposed method"}]
[{"st": 2, "ed": 4, "text": "active learning"}, {"st": 9, "ed": 11, "text": "machine learning"}, {"st": 11, "ed": 13, "text": "deep learning"}, {"st": 19, "ed": 21, "text": "deep learning"}, {"st": 28, "ed": 30, "text": "active learning"}, {"st": 32, "ed": 34, "text": "active learning"}, {"st": 51, "ed": 53, "text": "recent advances"}, {"st": 54, "ed": 56, "text": "deep learning"}, {"st": 80, "ed": 82, "text": "deep learning"}, {"st": 93, "ed": 95, "text": "recent advances"}, {"st": 97, "ed": 99, "text": "deep learning"}, {"st": 101, "ed": 103, "text": "active learning"}, {"st": 111, "ed": 113, "text": "active learning"}, {"st": 132, "ed": 135, "text": "taking advantage of"}, {"st": 140, "ed": 143, "text": "convolutional neural networks"}, {"st": 146, "ed": 148, "text": "active learning"}, {"st": 150, "ed": 152, "text": "image data"}, {"st": 154, "ed": 156, "text": "significant improvement"}, {"st": 158, "ed": 160, "text": "active learning"}, {"st": 167, "ed": 169, "text": "mnist dataset"}, {"st": 173, "ed": 175, "text": "skin cancer"}]
[{"st": 4, "ed": 6, "text": "learning task"}, {"st": 12, "ed": 14, "text": "input space"}, {"st": 18, "ed": 21, "text": "visual object recognition"}, {"st": 65, "ed": 67, "text": "weakly supervised"}, {"st": 123, "ed": 125, "text": "distance metric"}, {"st": 138, "ed": 140, "text": "face verification"}, {"st": 162, "ed": 164, "text": "weakly supervised"}]
[{"st": 7, "ed": 9, "text": "machine learning"}, {"st": 20, "ed": 22, "text": "practical problems"}, {"st": 35, "ed": 37, "text": "machine learning"}]
[{"st": 0, "ed": 4, "text": "convolutional neural networks cnns"}, {"st": 15, "ed": 17, "text": "computer vision"}, {"st": 20, "ed": 22, "text": "classification problems"}, {"st": 95, "ed": 97, "text": "significantly lower"}, {"st": 131, "ed": 133, "text": "adversarial examples"}]
[{"st": 0, "ed": 2, "text": "dnn based"}, {"st": 2, "ed": 4, "text": "cross modal"}, {"st": 24, "ed": 26, "text": "existing methods"}, {"st": 59, "ed": 61, "text": "cross modal"}, {"st": 62, "ed": 64, "text": "metric learning"}, {"st": 65, "ed": 67, "text": "multi task"}, {"st": 72, "ed": 74, "text": "ranking loss"}, {"st": 75, "ed": 77, "text": "semi supervised"}, {"st": 81, "ed": 83, "text": "cross modal"}, {"st": 88, "ed": 91, "text": "multi task learning"}, {"st": 94, "ed": 96, "text": "ranking loss"}, {"st": 106, "ed": 108, "text": "cross modal"}, {"st": 113, "ed": 115, "text": "semi supervised"}, {"st": 126, "ed": 129, "text": "labeled and unlabeled"}, {"st": 133, "ed": 135, "text": "existing methods"}, {"st": 146, "ed": 148, "text": "cross modal"}, {"st": 152, "ed": 154, "text": "cross modal"}]
[{"st": 0, "ed": 2, "text": "recent advances"}, {"st": 3, "ed": 5, "text": "deep learning"}, {"st": 6, "ed": 8, "text": "natural images"}, {"st": 33, "ed": 37, "text": "deep convolutional neural network"}, {"st": 68, "ed": 70, "text": "natural images"}, {"st": 83, "ed": 85, "text": "network architectures"}, {"st": 87, "ed": 89, "text": "natural images"}, {"st": 143, "ed": 145, "text": "multi view"}, {"st": 145, "ed": 149, "text": "deep convolutional neural network"}, {"st": 162, "ed": 164, "text": "large scale"}, {"st": 166, "ed": 168, "text": "breast cancer"}, {"st": 183, "ed": 185, "text": "training set"}, {"st": 203, "ed": 205, "text": "training set"}, {"st": 225, "ed": 227, "text": "deep learning"}]
[{"st": 0, "ed": 2, "text": "current approaches"}, {"st": 9, "ed": 11, "text": "training data"}, {"st": 15, "ed": 17, "text": "training data"}, {"st": 32, "ed": 34, "text": "image classification"}, {"st": 47, "ed": 49, "text": "training data"}, {"st": 79, "ed": 81, "text": "examples include"}, {"st": 87, "ed": 90, "text": "mnist and cifar"}]
[{"st": 5, "ed": 7, "text": "active learning"}, {"st": 20, "ed": 22, "text": "unlabeled data"}, {"st": 46, "ed": 48, "text": "cifar 10"}]
[{"st": 18, "ed": 20, "text": "time consuming"}, {"st": 85, "ed": 87, "text": "receptive field"}, {"st": 112, "ed": 114, "text": "fully convolutional"}, {"st": 180, "ed": 184, "text": "deep neural network architecture"}, {"st": 201, "ed": 203, "text": "a 20"}, {"st": 203, "ed": 205, "text": "relative improvement"}]
[{"st": 4, "ed": 8, "text": "convolutional neural network cnn"}, {"st": 37, "ed": 39, "text": "traditional methods"}, {"st": 45, "ed": 48, "text": "hand crafted features"}, {"st": 62, "ed": 64, "text": "raw image"}, {"st": 74, "ed": 76, "text": "deep cnn"}, {"st": 98, "ed": 100, "text": "competitive results"}, {"st": 102, "ed": 104, "text": "deep architectures"}, {"st": 117, "ed": 119, "text": "proposed method"}]
[{"st": 0, "ed": 3, "text": "artificial neural networks"}, {"st": 5, "ed": 7, "text": "successfully applied"}, {"st": 11, "ed": 14, "text": "machine learning tasks"}, {"st": 15, "ed": 17, "text": "image recognition"}, {"st": 40, "ed": 42, "text": "ensemble methods"}, {"st": 45, "ed": 47, "text": "majority voting"}, {"st": 48, "ed": 50, "text": "bayes optimal"}, {"st": 57, "ed": 59, "text": "image recognition"}, {"st": 61, "ed": 64, "text": "deep neural networks"}, {"st": 78, "ed": 80, "text": "network structure"}, {"st": 87, "ed": 89, "text": "training process"}, {"st": 95, "ed": 97, "text": "multiple times"}, {"st": 114, "ed": 116, "text": "neural networks"}, {"st": 139, "ed": 141, "text": "ensemble methods"}]
[{"st": 32, "ed": 34, "text": "unlike previous"}, {"st": 38, "ed": 40, "text": "adversarial networks"}, {"st": 80, "ed": 82, "text": "generated data"}, {"st": 85, "ed": 87, "text": "prior distribution"}, {"st": 119, "ed": 121, "text": "recently proposed"}]
[{"st": 31, "ed": 36, "text": "long short term memory lstm"}, {"st": 39, "ed": 41, "text": "feed forward"}, {"st": 41, "ed": 43, "text": "deep architecture"}, {"st": 71, "ed": 73, "text": "recurrent network"}]
[{"st": 0, "ed": 2, "text": "feature extraction"}, {"st": 12, "ed": 15, "text": "classification and regression"}, {"st": 25, "ed": 27, "text": "machine learning"}, {"st": 28, "ed": 30, "text": "feature extraction"}, {"st": 43, "ed": 45, "text": "multi scale"}, {"st": 46, "ed": 48, "text": "multi layer"}, {"st": 54, "ed": 57, "text": "fully convolutional network"}, {"st": 62, "ed": 64, "text": "feature extraction"}, {"st": 66, "ed": 70, "text": "trained end to end"}, {"st": 101, "ed": 103, "text": "hand crafted"}, {"st": 169, "ed": 171, "text": "feature extraction"}]
[{"st": 0, "ed": 4, "text": "generative adversarial networks gans"}, {"st": 5, "ed": 7, "text": "highly effective"}, {"st": 7, "ed": 9, "text": "unsupervised learning"}, {"st": 42, "ed": 44, "text": "mode collapse"}, {"st": 48, "ed": 50, "text": "batch normalization"}, {"st": 98, "ed": 100, "text": "gan training"}, {"st": 127, "ed": 129, "text": "gan training"}, {"st": 130, "ed": 132, "text": "significantly improves"}, {"st": 191, "ed": 193, "text": "cifar 10"}, {"st": 209, "ed": 211, "text": "squared loss"}, {"st": 216, "ed": 218, "text": "qualitative results"}, {"st": 244, "ed": 248, "text": "available at https github.com"}]
[{"st": 0, "ed": 2, "text": "domain adaptation"}, {"st": 3, "ed": 5, "text": "transfer learning"}, {"st": 13, "ed": 16, "text": "training and testing"}, {"st": 21, "ed": 23, "text": "previous research"}, {"st": 30, "ed": 32, "text": "feature representation"}, {"st": 33, "ed": 37, "text": "source and target domains"}, {"st": 54, "ed": 56, "text": "domain adaptation"}, {"st": 62, "ed": 64, "text": "latent feature"}, {"st": 74, "ed": 77, "text": "source and target"}, {"st": 85, "ed": 87, "text": "conditional probability"}, {"st": 89, "ed": 92, "text": "maximum mean discrepancy"}, {"st": 144, "ed": 146, "text": "discriminative power"}, {"st": 162, "ed": 164, "text": "geometric structure"}, {"st": 173, "ed": 175, "text": "geometric structure"}, {"st": 179, "ed": 181, "text": "extensive experiments"}, {"st": 185, "ed": 187, "text": "cross domain"}, {"st": 187, "ed": 189, "text": "image classification"}, {"st": 200, "ed": 202, "text": "proposed method"}, {"st": 202, "ed": 204, "text": "consistently outperforms"}]
[{"st": 0, "ed": 3, "text": "convolutional neural networks"}, {"st": 4, "ed": 6, "text": "visual features"}, {"st": 12, "ed": 14, "text": "computer vision"}, {"st": 28, "ed": 30, "text": "generic framework"}, {"st": 32, "ed": 34, "text": "deep networks"}, {"st": 34, "ed": 37, "text": "end to end"}, {"st": 58, "ed": 60, "text": "deep features"}, {"st": 71, "ed": 73, "text": "unsupervised learning"}, {"st": 92, "ed": 94, "text": "loss function"}, {"st": 101, "ed": 103, "text": "proposed approach"}, {"st": 114, "ed": 116, "text": "unsupervised methods"}]
[{"st": 12, "ed": 14, "text": "feature selection"}, {"st": 16, "ed": 18, "text": "central role"}, {"st": 38, "ed": 40, "text": "graph based"}, {"st": 42, "ed": 44, "text": "feature selection"}, {"st": 129, "ed": 132, "text": "point of view"}, {"st": 149, "ed": 151, "text": "recent literature"}, {"st": 155, "ed": 157, "text": "object recognition"}]
[{"st": 1, "ed": 3, "text": "autoregressive models"}, {"st": 72, "ed": 74, "text": "key idea"}, {"st": 77, "ed": 79, "text": "hidden states"}]
[{"st": 0, "ed": 4, "text": "magnetic resonance imaging mri"}, {"st": 23, "ed": 25, "text": "mr images"}, {"st": 28, "ed": 30, "text": "time consuming"}, {"st": 58, "ed": 63, "text": "deep convolutional neural networks cnns"}, {"st": 67, "ed": 70, "text": "institutional review board"}, {"st": 73, "ed": 75, "text": "informed consent"}, {"st": 85, "ed": 87, "text": "mr images"}, {"st": 114, "ed": 116, "text": "feature maps"}, {"st": 135, "ed": 137, "text": "automatic segmentation"}, {"st": 198, "ed": 200, "text": "clinical practice"}]
[{"st": 18, "ed": 20, "text": "anomaly detection"}, {"st": 41, "ed": 43, "text": "linear subspace"}, {"st": 55, "ed": 57, "text": "robust pca"}, {"st": 140, "ed": 142, "text": "recent advances"}, {"st": 154, "ed": 156, "text": "real world"}]
[{"st": 1, "ed": 3, "text": "similarity based"}, {"st": 3, "ed": 5, "text": "clustering methods"}, {"st": 11, "ed": 13, "text": "similarity matrix"}, {"st": 35, "ed": 37, "text": "similarity metric"}, {"st": 48, "ed": 50, "text": "similarity matrix"}, {"st": 69, "ed": 72, "text": "real world data"}, {"st": 93, "ed": 95, "text": "simultaneously learn"}, {"st": 113, "ed": 116, "text": "kernel k means"}, {"st": 116, "ed": 118, "text": "k means"}, {"st": 119, "ed": 121, "text": "spectral clustering"}, {"st": 148, "ed": 151, "text": "multiple kernel learning"}, {"st": 211, "ed": 213, "text": "extensive experiments"}]
[{"st": 0, "ed": 2, "text": "machine learning"}, {"st": 6, "ed": 8, "text": "deep architectures"}, {"st": 32, "ed": 34, "text": "adversarial samples"}, {"st": 41, "ed": 43, "text": "adversarial samples"}, {"st": 52, "ed": 54, "text": "adversarial samples"}, {"st": 84, "ed": 86, "text": "proposed method"}, {"st": 92, "ed": 94, "text": "multi channel"}, {"st": 113, "ed": 115, "text": "generating adversarial"}]
[{"st": 3, "ed": 5, "text": "image classification"}, {"st": 77, "ed": 79, "text": "don t"}, {"st": 83, "ed": 85, "text": "ell 0"}, {"st": 87, "ed": 89, "text": "ell 1"}, {"st": 94, "ed": 96, "text": "computational efficiency"}, {"st": 99, "ed": 101, "text": "discriminative power"}, {"st": 119, "ed": 121, "text": "extensive experiments"}, {"st": 126, "ed": 128, "text": "face recognition"}, {"st": 128, "ed": 130, "text": "object categorization"}, {"st": 130, "ed": 132, "text": "scene classification"}, {"st": 146, "ed": 148, "text": "proposed approach"}, {"st": 158, "ed": 160, "text": "recognition accuracy"}]
[{"st": 0, "ed": 2, "text": "large datasets"}, {"st": 13, "ed": 15, "text": "mechanical turk"}, {"st": 16, "ed": 18, "text": "social media"}, {"st": 49, "ed": 51, "text": "deep network"}, {"st": 65, "ed": 67, "text": "deep network"}, {"st": 72, "ed": 75, "text": "end to end"}, {"st": 75, "ed": 78, "text": "stochastic gradient descent"}, {"st": 97, "ed": 99, "text": "non trivial"}, {"st": 103, "ed": 105, "text": "dropout regularization"}, {"st": 114, "ed": 116, "text": "numerical experiments"}, {"st": 121, "ed": 123, "text": "cifar 10"}, {"st": 124, "ed": 126, "text": "mnist datasets"}]
[{"st": 3, "ed": 5, "text": "machine learning"}, {"st": 5, "ed": 7, "text": "based approach"}, {"st": 9, "ed": 11, "text": "image compression"}, {"st": 108, "ed": 110, "text": "adversarial training"}]
[{"st": 0, "ed": 2, "text": "adversarial training"}, {"st": 7, "ed": 10, "text": "deep neural networks"}, {"st": 24, "ed": 26, "text": "deep state"}, {"st": 45, "ed": 47, "text": "adversarial training"}, {"st": 71, "ed": 73, "text": "deep models"}, {"st": 107, "ed": 112, "text": "cifar 10 and cifar 100"}, {"st": 119, "ed": 121, "text": "adversarial training"}, {"st": 131, "ed": 133, "text": "significant improvement"}, {"st": 134, "ed": 136, "text": "classification accuracy"}]
[{"st": 11, "ed": 13, "text": "nearest neighbor"}, {"st": 19, "ed": 22, "text": "learning to rank"}, {"st": 27, "ed": 29, "text": "directly optimizing"}, {"st": 29, "ed": 31, "text": "ranking based"}, {"st": 31, "ed": 33, "text": "evaluation metrics"}, {"st": 52, "ed": 54, "text": "hamming distance"}, {"st": 89, "ed": 91, "text": "gradient based"}, {"st": 106, "ed": 108, "text": "image retrieval"}]
[{"st": 4, "ed": 6, "text": "unsupervised learning"}, {"st": 16, "ed": 19, "text": "fully connected layers"}, {"st": 37, "ed": 39, "text": "cost function"}, {"st": 53, "ed": 55, "text": "neural network"}, {"st": 96, "ed": 98, "text": "semi supervised"}, {"st": 101, "ed": 103, "text": "closed form"}, {"st": 106, "ed": 109, "text": "online learning algorithm"}, {"st": 118, "ed": 122, "text": "synthetic and real world"}, {"st": 126, "ed": 128, "text": "neural networks"}, {"st": 137, "ed": 140, "text": "online learning algorithm"}, {"st": 156, "ed": 158, "text": "potential applications"}, {"st": 162, "ed": 165, "text": "online learning algorithm"}]
[{"st": 15, "ed": 17, "text": "unified framework"}, {"st": 44, "ed": 46, "text": "back propagation"}, {"st": 65, "ed": 67, "text": "training phase"}, {"st": 98, "ed": 100, "text": "back propagation"}, {"st": 112, "ed": 114, "text": "discrete state"}, {"st": 123, "ed": 125, "text": "discrete space"}, {"st": 136, "ed": 138, "text": "unified framework"}, {"st": 156, "ed": 159, "text": "weights and activations"}, {"st": 205, "ed": 207, "text": "event driven"}, {"st": 235, "ed": 237, "text": "discrete space"}]
[{"st": 6, "ed": 10, "text": "generative adversarial networks gans"}, {"st": 57, "ed": 59, "text": "face image"}, {"st": 68, "ed": 70, "text": "low resolution"}, {"st": 70, "ed": 72, "text": "face image"}, {"st": 77, "ed": 79, "text": "face image"}, {"st": 111, "ed": 113, "text": "face image"}, {"st": 118, "ed": 120, "text": "low resolution"}, {"st": 135, "ed": 137, "text": "feature vector"}, {"st": 139, "ed": 141, "text": "face verification"}, {"st": 154, "ed": 156, "text": "face image"}, {"st": 167, "ed": 169, "text": "face image"}, {"st": 180, "ed": 182, "text": "automatically generated"}]
[{"st": 48, "ed": 51, "text": "variational auto encoders"}, {"st": 60, "ed": 62, "text": "training objective"}, {"st": 68, "ed": 70, "text": "inference network"}, {"st": 75, "ed": 77, "text": "abstract concepts"}, {"st": 92, "ed": 94, "text": "evaluation metrics"}]
[{"st": 0, "ed": 2, "text": "magnetic resonance"}, {"st": 8, "ed": 10, "text": "ill posed"}, {"st": 39, "ed": 41, "text": "compressed sensing"}, {"st": 69, "ed": 72, "text": "generative adversarial networks"}, {"st": 76, "ed": 79, "text": "low dimensional manifold"}, {"st": 82, "ed": 84, "text": "mr images"}, {"st": 91, "ed": 93, "text": "least squares"}, {"st": 98, "ed": 100, "text": "ell 1"}, {"st": 102, "ed": 104, "text": "deep residual"}, {"st": 106, "ed": 108, "text": "skip connections"}, {"st": 132, "ed": 134, "text": "ell 1"}, {"st": 141, "ed": 144, "text": "convolutional neural network"}, {"st": 146, "ed": 148, "text": "jointly trained"}, {"st": 162, "ed": 164, "text": "feed forward"}, {"st": 167, "ed": 169, "text": "generator network"}, {"st": 226, "ed": 229, "text": "orders of magnitude"}]
[{"st": 3, "ed": 5, "text": "theoretical analysis"}, {"st": 7, "ed": 9, "text": "recently proposed"}, {"st": 10, "ed": 12, "text": "subspace learning"}, {"st": 15, "ed": 17, "text": "principal component"}, {"st": 34, "ed": 36, "text": "gain insight"}, {"st": 41, "ed": 43, "text": "ell 1"}, {"st": 52, "ed": 54, "text": "geometric analysis"}, {"st": 56, "ed": 58, "text": "closely related"}, {"st": 58, "ed": 60, "text": "continuous optimization"}, {"st": 125, "ed": 127, "text": "learning algorithm"}, {"st": 155, "ed": 157, "text": "synthetic data"}]
[{"st": 48, "ed": 51, "text": "convolutional neural network"}, {"st": 58, "ed": 60, "text": "semi supervised"}, {"st": 81, "ed": 83, "text": "recent advances"}, {"st": 84, "ed": 86, "text": "large scale"}, {"st": 86, "ed": 88, "text": "similarity graph"}]
[{"st": 1, "ed": 4, "text": "deep neural networks"}, {"st": 7, "ed": 9, "text": "low power"}, {"st": 26, "ed": 28, "text": "low precision"}, {"st": 30, "ed": 32, "text": "efficient inference"}, {"st": 88, "ed": 90, "text": "neural networks"}, {"st": 116, "ed": 118, "text": "convex problems"}, {"st": 131, "ed": 133, "text": "greedy search"}, {"st": 147, "ed": 149, "text": "low precision"}]
[{"st": 0, "ed": 2, "text": "deep learning"}, {"st": 190, "ed": 192, "text": "open source"}]
[{"st": 5, "ed": 7, "text": "deep network"}, {"st": 32, "ed": 34, "text": "starting point"}, {"st": 43, "ed": 45, "text": "score function"}]
[{"st": 28, "ed": 30, "text": "frame level"}]
[{"st": 31, "ed": 33, "text": "exponential loss"}, {"st": 41, "ed": 43, "text": "random noise"}, {"st": 46, "ed": 48, "text": "boosting algorithms"}, {"st": 64, "ed": 66, "text": "exponential loss"}, {"st": 90, "ed": 93, "text": "self paced learning"}, {"st": 103, "ed": 105, "text": "boosting algorithm"}, {"st": 124, "ed": 126, "text": "extensive experiments"}]
[{"st": 12, "ed": 14, "text": "deep learning"}, {"st": 19, "ed": 21, "text": "speech recognition"}, {"st": 30, "ed": 35, "text": "deep convolutional neural network cnn"}, {"st": 36, "ed": 38, "text": "image classification"}, {"st": 43, "ed": 45, "text": "deep cnn"}, {"st": 49, "ed": 51, "text": "chemical properties"}, {"st": 95, "ed": 97, "text": "neural network"}, {"st": 120, "ed": 124, "text": "multi layer perceptron mlp"}, {"st": 124, "ed": 127, "text": "deep neural networks"}, {"st": 154, "ed": 156, "text": "deep learning"}, {"st": 164, "ed": 167, "text": "deep neural networks"}, {"st": 170, "ed": 172, "text": "computational chemistry"}, {"st": 175, "ed": 177, "text": "feature engineering"}, {"st": 183, "ed": 185, "text": "deep learning"}]
[{"st": 0, "ed": 3, "text": "deep neural networks"}, {"st": 13, "ed": 15, "text": "complex tasks"}, {"st": 90, "ed": 92, "text": "developmental psychology"}, {"st": 115, "ed": 117, "text": "cognitive psychology"}, {"st": 187, "ed": 189, "text": "cognitive psychology"}]
[{"st": 7, "ed": 9, "text": "domain adaptation"}, {"st": 9, "ed": 11, "text": "active learning"}, {"st": 13, "ed": 15, "text": "covariate shift"}, {"st": 15, "ed": 18, "text": "semi supervised learning"}, {"st": 20, "ed": 22, "text": "concept drift"}, {"st": 62, "ed": 64, "text": "surrogate loss"}, {"st": 85, "ed": 87, "text": "classification error"}, {"st": 102, "ed": 104, "text": "error rates"}, {"st": 112, "ed": 114, "text": "empirical results"}, {"st": 122, "ed": 125, "text": "semi supervised learning"}, {"st": 129, "ed": 131, "text": "performance improvements"}, {"st": 135, "ed": 137, "text": "unlabeled data"}, {"st": 153, "ed": 155, "text": "classification error"}]
[{"st": 4, "ed": 6, "text": "machine learning"}, {"st": 26, "ed": 28, "text": "adversarial perturbations"}, {"st": 36, "ed": 38, "text": "machine learning"}, {"st": 71, "ed": 73, "text": "adversarial attack"}, {"st": 83, "ed": 85, "text": "hyperparameter tuning"}, {"st": 97, "ed": 99, "text": "deep learning"}, {"st": 138, "ed": 142, "text": "available at https github.com"}]
[{"st": 0, "ed": 4, "text": "generative adversarial networks gans"}, {"st": 15, "ed": 17, "text": "gan training"}, {"st": 20, "ed": 22, "text": "saddle point"}, {"st": 22, "ed": 24, "text": "optimization problem"}, {"st": 76, "ed": 78, "text": "optimization procedure"}, {"st": 126, "ed": 128, "text": "desirable properties"}, {"st": 132, "ed": 134, "text": "large data"}]
[{"st": 0, "ed": 2, "text": "machine learning"}, {"st": 5, "ed": 7, "text": "increasingly popular"}, {"st": 14, "ed": 18, "text": "functional magnetic resonance imaging"}, {"st": 25, "ed": 27, "text": "convolutional networks"}, {"st": 44, "ed": 47, "text": "convolutional neural network"}, {"st": 54, "ed": 57, "text": "convolutional neural network"}, {"st": 71, "ed": 74, "text": "mild cognitive impairment"}, {"st": 92, "ed": 94, "text": "convolutional network"}, {"st": 101, "ed": 103, "text": "functional connectivity"}]
[{"st": 1, "ed": 3, "text": "dimensionality reduction"}, {"st": 20, "ed": 22, "text": "low dimensional"}, {"st": 72, "ed": 74, "text": "dimensionality reduction"}, {"st": 99, "ed": 101, "text": "method generates"}, {"st": 102, "ed": 104, "text": "network structure"}]
[{"st": 1, "ed": 3, "text": "neural network"}, {"st": 3, "ed": 5, "text": "image classification"}, {"st": 49, "ed": 51, "text": "building block"}, {"st": 65, "ed": 67, "text": "key contribution"}, {"st": 73, "ed": 75, "text": "search space"}, {"st": 86, "ed": 88, "text": "convolutional layer"}, {"st": 92, "ed": 94, "text": "cifar 10"}, {"st": 102, "ed": 104, "text": "imagenet dataset"}, {"st": 150, "ed": 152, "text": "top 5"}, {"st": 196, "ed": 198, "text": "computational cost"}, {"st": 245, "ed": 247, "text": "cifar 10"}, {"st": 256, "ed": 258, "text": "error rate"}, {"st": 267, "ed": 269, "text": "image features"}, {"st": 271, "ed": 273, "text": "image classification"}, {"st": 279, "ed": 281, "text": "computer vision"}, {"st": 286, "ed": 288, "text": "object detection"}, {"st": 289, "ed": 291, "text": "learned features"}]
[{"st": 4, "ed": 6, "text": "generative framework"}, {"st": 10, "ed": 12, "text": "previously unseen"}, {"st": 19, "ed": 21, "text": "class conditional"}, {"st": 25, "ed": 27, "text": "class conditional"}, {"st": 30, "ed": 32, "text": "exponential family"}, {"st": 75, "ed": 77, "text": "class conditional"}, {"st": 84, "ed": 86, "text": "existing methods"}, {"st": 87, "ed": 90, "text": "zero shot learning"}, {"st": 101, "ed": 103, "text": "generative model"}, {"st": 121, "ed": 123, "text": "unlabeled data"}, {"st": 124, "ed": 126, "text": "unseen classes"}, {"st": 132, "ed": 134, "text": "class conditional"}, {"st": 145, "ed": 148, "text": "few shot learning"}, {"st": 174, "ed": 177, "text": "benchmark data sets"}]
[{"st": 0, "ed": 2, "text": "unsupervised learning"}, {"st": 4, "ed": 6, "text": "computer vision"}, {"st": 9, "ed": 11, "text": "latent representations"}, {"st": 13, "ed": 15, "text": "low dimensional"}, {"st": 36, "ed": 38, "text": "main goal"}, {"st": 66, "ed": 68, "text": "robust pca"}, {"st": 81, "ed": 83, "text": "spectral clustering"}, {"st": 101, "ed": 103, "text": "real world"}]
[{"st": 1, "ed": 4, "text": "recurrent neural networks"}, {"st": 24, "ed": 26, "text": "time series"}, {"st": 30, "ed": 33, "text": "recurrent neural networks"}, {"st": 90, "ed": 92, "text": "neural networks"}, {"st": 96, "ed": 98, "text": "time series"}, {"st": 108, "ed": 111, "text": "recurrent neural network"}]
[{"st": 0, "ed": 4, "text": "convolutional neural networks cnns"}, {"st": 6, "ed": 8, "text": "successfully applied"}, {"st": 20, "ed": 22, "text": "deep model"}, {"st": 44, "ed": 46, "text": "labeled images"}, {"st": 73, "ed": 75, "text": "active learning"}, {"st": 77, "ed": 79, "text": "empirical study"}, {"st": 84, "ed": 86, "text": "active learning"}, {"st": 109, "ed": 111, "text": "active learning"}, {"st": 140, "ed": 142, "text": "theoretical result"}, {"st": 157, "ed": 159, "text": "active learning"}, {"st": 180, "ed": 182, "text": "proposed method"}, {"st": 182, "ed": 185, "text": "significantly outperforms existing"}, {"st": 187, "ed": 189, "text": "image classification"}]
[{"st": 3, "ed": 5, "text": "recently introduced"}, {"st": 9, "ed": 13, "text": "generative adversarial network gan"}, {"st": 17, "ed": 19, "text": "promising results"}, {"st": 20, "ed": 22, "text": "generate realistic"}, {"st": 30, "ed": 32, "text": "generated samples"}, {"st": 33, "ed": 35, "text": "input variables"}, {"st": 50, "ed": 52, "text": "generated samples"}, {"st": 82, "ed": 84, "text": "existing models"}, {"st": 87, "ed": 89, "text": "face image"}, {"st": 128, "ed": 130, "text": "generated samples"}, {"st": 163, "ed": 165, "text": "neural network"}]
[{"st": 9, "ed": 11, "text": "probabilistic models"}, {"st": 17, "ed": 19, "text": "probabilistic models"}, {"st": 52, "ed": 54, "text": "internal structure"}, {"st": 90, "ed": 92, "text": "latent variable"}, {"st": 94, "ed": 96, "text": "matrix variate"}, {"st": 98, "ed": 102, "text": "canonical correlation analysis cca"}, {"st": 112, "ed": 114, "text": "maximum likelihood"}, {"st": 133, "ed": 135, "text": "synthetic data"}, {"st": 144, "ed": 146, "text": "real data"}]
[{"st": 0, "ed": 3, "text": "person re identification"}, {"st": 28, "ed": 31, "text": "spatial and temporal"}, {"st": 38, "ed": 41, "text": "person re identification"}, {"st": 44, "ed": 46, "text": "feature extractor"}, {"st": 77, "ed": 79, "text": "pooling layer"}, {"st": 127, "ed": 129, "text": "approach outperforms"}]
[{"st": 1, "ed": 3, "text": "nystr om"}, {"st": 15, "ed": 17, "text": "kernel matrices"}, {"st": 48, "ed": 50, "text": "nystr om"}, {"st": 73, "ed": 75, "text": "nystr om"}, {"st": 78, "ed": 80, "text": "poor performance"}, {"st": 99, "ed": 101, "text": "nystr om"}, {"st": 102, "ed": 104, "text": "theoretical analysis"}, {"st": 105, "ed": 107, "text": "numerical experiments"}, {"st": 120, "ed": 122, "text": "nystr om"}, {"st": 143, "ed": 145, "text": "computational complexity"}, {"st": 152, "ed": 154, "text": "improved accuracy"}]
[{"st": 17, "ed": 19, "text": "deep learning"}, {"st": 58, "ed": 60, "text": "preliminary results"}, {"st": 63, "ed": 65, "text": "deep learning"}, {"st": 77, "ed": 79, "text": "ensemble methods"}]
[{"st": 18, "ed": 20, "text": "machine learning"}, {"st": 51, "ed": 53, "text": "image data"}, {"st": 57, "ed": 59, "text": "based approach"}]
[{"st": 9, "ed": 12, "text": "generative adversarial networks"}, {"st": 24, "ed": 26, "text": "natural images"}, {"st": 91, "ed": 93, "text": "multi channel"}, {"st": 123, "ed": 125, "text": "latent space"}]
[{"st": 0, "ed": 3, "text": "person re identification"}, {"st": 20, "ed": 22, "text": "challenging problem"}, {"st": 28, "ed": 30, "text": "content based"}, {"st": 61, "ed": 63, "text": "conventional methods"}, {"st": 66, "ed": 68, "text": "pairwise similarity"}, {"st": 86, "ed": 88, "text": "deep learning"}, {"st": 109, "ed": 111, "text": "intra class"}, {"st": 122, "ed": 124, "text": "intra class"}, {"st": 126, "ed": 128, "text": "inter class"}, {"st": 131, "ed": 133, "text": "distance metric"}, {"st": 157, "ed": 159, "text": "intra class"}, {"st": 175, "ed": 177, "text": "intra class"}, {"st": 180, "ed": 182, "text": "inter class"}, {"st": 189, "ed": 191, "text": "regularization term"}, {"st": 195, "ed": 200, "text": "deep convolutional neural network cnn"}, {"st": 207, "ed": 209, "text": "deep model"}, {"st": 241, "ed": 243, "text": "benchmark datasets"}]
[{"st": 27, "ed": 29, "text": "training set"}, {"st": 56, "ed": 58, "text": "mnist dataset"}, {"st": 60, "ed": 62, "text": "machine learning"}, {"st": 76, "ed": 79, "text": "training and testing"}, {"st": 84, "ed": 88, "text": "available at https github.com"}]
[{"st": 0, "ed": 5, "text": "deep convolutional neural networks cnns"}, {"st": 36, "ed": 39, "text": "simple yet effective"}, {"st": 40, "ed": 42, "text": "softmax loss"}, {"st": 55, "ed": 57, "text": "softmax loss"}, {"st": 107, "ed": 110, "text": "end to end"}, {"st": 124, "ed": 126, "text": "pre trained"}, {"st": 126, "ed": 128, "text": "deep cnn"}, {"st": 130, "ed": 132, "text": "fine tune"}, {"st": 135, "ed": 137, "text": "promising results"}, {"st": 141, "ed": 143, "text": "extensive experiments"}, {"st": 148, "ed": 150, "text": "loss function"}]
[{"st": 2, "ed": 4, "text": "real world"}, {"st": 5, "ed": 7, "text": "visual recognition"}, {"st": 26, "ed": 28, "text": "semantically meaningful"}, {"st": 46, "ed": 48, "text": "knowledge graph"}, {"st": 64, "ed": 66, "text": "image representation"}, {"st": 93, "ed": 95, "text": "open world"}, {"st": 115, "ed": 117, "text": "knowledge graph"}, {"st": 124, "ed": 126, "text": "learning framework"}, {"st": 133, "ed": 135, "text": "prior knowledge"}, {"st": 148, "ed": 150, "text": "visual recognition"}]
[{"st": 0, "ed": 2, "text": "compressed sensing"}, {"st": 44, "ed": 46, "text": "recovery guarantees"}, {"st": 68, "ed": 70, "text": "deep learning"}, {"st": 81, "ed": 83, "text": "deep learning"}, {"st": 84, "ed": 86, "text": "jointly learns"}, {"st": 90, "ed": 92, "text": "projection matrix"}, {"st": 98, "ed": 101, "text": "end to end"}, {"st": 106, "ed": 108, "text": "real images"}, {"st": 113, "ed": 115, "text": "proposed approach"}]
[{"st": 0, "ed": 2, "text": "x ray"}, {"st": 2, "ed": 4, "text": "computed tomography"}, {"st": 40, "ed": 42, "text": "deep learning"}, {"st": 45, "ed": 47, "text": "receptive field"}, {"st": 47, "ed": 49, "text": "neural networks"}, {"st": 51, "ed": 53, "text": "u net"}, {"st": 55, "ed": 57, "text": "impressive performance"}, {"st": 63, "ed": 65, "text": "theoretical justification"}, {"st": 74, "ed": 76, "text": "deep convolutional"}, {"st": 78, "ed": 80, "text": "main goal"}, {"st": 90, "ed": 92, "text": "u net"}, {"st": 95, "ed": 97, "text": "multi resolution"}, {"st": 97, "ed": 99, "text": "deep learning"}, {"st": 107, "ed": 109, "text": "u net"}, {"st": 142, "ed": 144, "text": "extensive experiments"}, {"st": 146, "ed": 148, "text": "patient data"}, {"st": 154, "ed": 156, "text": "network architectures"}]
[{"st": 0, "ed": 2, "text": "denoising autoencoders"}, {"st": 5, "ed": 7, "text": "deep learning"}, {"st": 10, "ed": 12, "text": "feature extraction"}, {"st": 22, "ed": 25, "text": "encoder and decoder"}, {"st": 33, "ed": 35, "text": "loss function"}, {"st": 52, "ed": 54, "text": "loss functions"}, {"st": 61, "ed": 65, "text": "mean squared error mse"}, {"st": 68, "ed": 70, "text": "cross entropy"}, {"st": 76, "ed": 78, "text": "image data"}, {"st": 82, "ed": 84, "text": "loss function"}, {"st": 201, "ed": 203, "text": "training data"}]
[{"st": 1, "ed": 3, "text": "recently proposed"}, {"st": 3, "ed": 5, "text": "multi layer"}, {"st": 6, "ed": 8, "text": "sparse coding"}, {"st": 24, "ed": 28, "text": "convolutional neural networks cnns"}, {"st": 36, "ed": 38, "text": "forward pass"}, {"st": 52, "ed": 54, "text": "sparse representation"}, {"st": 56, "ed": 58, "text": "feature maps"}, {"st": 120, "ed": 122, "text": "remains unclear"}, {"st": 137, "ed": 139, "text": "convolutional filters"}, {"st": 200, "ed": 202, "text": "non trivial"}, {"st": 210, "ed": 212, "text": "online algorithm"}, {"st": 217, "ed": 219, "text": "real data"}, {"st": 255, "ed": 257, "text": "matrix factorization"}, {"st": 257, "ed": 260, "text": "sparse dictionary learning"}, {"st": 262, "ed": 264, "text": "auto encoders"}]
[{"st": 1, "ed": 3, "text": "existing approaches"}, {"st": 4, "ed": 6, "text": "multi view"}, {"st": 6, "ed": 8, "text": "subspace clustering"}, {"st": 12, "ed": 14, "text": "affinity matrix"}, {"st": 24, "ed": 26, "text": "spectral clustering"}, {"st": 38, "ed": 40, "text": "multi view"}, {"st": 40, "ed": 42, "text": "subspace clustering"}, {"st": 50, "ed": 52, "text": "affinity matrix"}, {"st": 62, "ed": 64, "text": "low rank"}, {"st": 72, "ed": 74, "text": "affinity matrix"}, {"st": 100, "ed": 102, "text": "low rank"}, {"st": 104, "ed": 107, "text": "constrained optimization problem"}, {"st": 137, "ed": 140, "text": "a reproducing kernel"}, {"st": 143, "ed": 145, "text": "proposed algorithm"}, {"st": 150, "ed": 152, "text": "multi view"}, {"st": 152, "ed": 154, "text": "subspace clustering"}, {"st": 160, "ed": 162, "text": "real world"}]
[{"st": 53, "ed": 55, "text": "neural networks"}, {"st": 118, "ed": 120, "text": "semantically meaningful"}]
[{"st": 11, "ed": 13, "text": "labeled training"}, {"st": 16, "ed": 18, "text": "task specific"}, {"st": 30, "ed": 32, "text": "domain experts"}, {"st": 54, "ed": 56, "text": "time consuming"}, {"st": 81, "ed": 83, "text": "generative adversarial"}, {"st": 136, "ed": 139, "text": "image and text"}, {"st": 147, "ed": 149, "text": "cifar 10"}, {"st": 155, "ed": 157, "text": "relation extraction"}, {"st": 164, "ed": 166, "text": "domain specific"}]
[{"st": 5, "ed": 7, "text": "deep learning"}, {"st": 12, "ed": 14, "text": "machine learning"}, {"st": 31, "ed": 34, "text": "artificial neural networks"}, {"st": 53, "ed": 55, "text": "cost effective"}, {"st": 90, "ed": 93, "text": "deep neural networks"}, {"st": 100, "ed": 102, "text": "em algorithm"}, {"st": 103, "ed": 105, "text": "jointly learning"}, {"st": 130, "ed": 133, "text": "deep neural networks"}, {"st": 133, "ed": 136, "text": "end to end"}, {"st": 139, "ed": 141, "text": "noisy labels"}, {"st": 152, "ed": 154, "text": "proposed approach"}, {"st": 182, "ed": 184, "text": "classification regression"}]
[{"st": 3, "ed": 5, "text": "missing entries"}, {"st": 11, "ed": 13, "text": "pattern recognition"}, {"st": 46, "ed": 48, "text": "clustering techniques"}, {"st": 74, "ed": 76, "text": "challenging problem"}, {"st": 77, "ed": 80, "text": "provide theoretical guarantees"}, {"st": 84, "ed": 86, "text": "ell 0"}, {"st": 88, "ed": 90, "text": "based optimization"}, {"st": 136, "ed": 138, "text": "proposed method"}, {"st": 158, "ed": 160, "text": "proposed method"}, {"st": 163, "ed": 165, "text": "clustering technique"}]
[{"st": 34, "ed": 36, "text": "update rule"}, {"st": 126, "ed": 128, "text": "computational complexity"}]
[{"st": 5, "ed": 7, "text": "dimensionality reduction"}, {"st": 27, "ed": 29, "text": "projection matrix"}, {"st": 62, "ed": 64, "text": "high dimensional"}, {"st": 82, "ed": 84, "text": "projection matrix"}, {"st": 122, "ed": 124, "text": "low dimensional"}, {"st": 130, "ed": 132, "text": "projection matrix"}, {"st": 156, "ed": 158, "text": "regularization parameter"}, {"st": 173, "ed": 175, "text": "regularization parameter"}, {"st": 181, "ed": 183, "text": "low dimensional"}, {"st": 197, "ed": 199, "text": "mnist datasets"}]
[{"st": 0, "ed": 2, "text": "deep learning"}, {"st": 13, "ed": 15, "text": "classification tasks"}, {"st": 51, "ed": 54, "text": "deep neural network"}, {"st": 65, "ed": 67, "text": "training data"}, {"st": 71, "ed": 73, "text": "unlabeled instances"}, {"st": 98, "ed": 101, "text": "deep neural network"}, {"st": 105, "ed": 107, "text": "supervised learning"}, {"st": 117, "ed": 119, "text": "deep learning"}, {"st": 136, "ed": 139, "text": "image and text"}, {"st": 177, "ed": 179, "text": "social media"}, {"st": 180, "ed": 182, "text": "public health"}, {"st": 185, "ed": 187, "text": "conduct experiments"}, {"st": 214, "ed": 216, "text": "approach outperforms"}, {"st": 219, "ed": 222, "text": "text and image"}, {"st": 232, "ed": 235, "text": "image and text"}, {"st": 247, "ed": 250, "text": "text and image"}]
[{"st": 7, "ed": 9, "text": "subspace clustering"}, {"st": 88, "ed": 90, "text": "subspace clustering"}, {"st": 94, "ed": 96, "text": "synthetic data"}, {"st": 131, "ed": 133, "text": "benchmark datasets"}]
[{"st": 5, "ed": 9, "text": "robust principal component analysis"}, {"st": 22, "ed": 25, "text": "non convex optimization"}, {"st": 36, "ed": 38, "text": "low rank"}, {"st": 45, "ed": 47, "text": "theoretical analysis"}, {"st": 49, "ed": 51, "text": "proposed algorithm"}, {"st": 54, "ed": 56, "text": "recovery guarantees"}, {"st": 62, "ed": 64, "text": "synthetic experiments"}, {"st": 112, "ed": 114, "text": "real world"}, {"st": 116, "ed": 118, "text": "image classification"}]
[{"st": 0, "ed": 4, "text": "generative adversarial networks gans"}, {"st": 10, "ed": 12, "text": "class label"}, {"st": 18, "ed": 20, "text": "conditional gan"}, {"st": 27, "ed": 29, "text": "recently proposed"}, {"st": 29, "ed": 31, "text": "wasserstein gan"}, {"st": 34, "ed": 36, "text": "adversarial training"}, {"st": 41, "ed": 43, "text": "network architectures"}, {"st": 54, "ed": 56, "text": "conditional gan"}, {"st": 69, "ed": 71, "text": "representation space"}, {"st": 95, "ed": 97, "text": "generated samples"}, {"st": 105, "ed": 107, "text": "cifar 10"}, {"st": 113, "ed": 116, "text": "supervised and unsupervised"}]
[{"st": 5, "ed": 8, "text": "unsupervised domain adaptation"}, {"st": 10, "ed": 12, "text": "labeled data"}, {"st": 29, "ed": 32, "text": "source and target"}, {"st": 69, "ed": 71, "text": "exponential family"}, {"st": 119, "ed": 121, "text": "benchmark datasets"}, {"st": 147, "ed": 149, "text": "recently proposed"}]
[{"st": 9, "ed": 11, "text": "challenging problem"}, {"st": 48, "ed": 50, "text": "deep cnn"}]
[{"st": 2, "ed": 4, "text": "point clouds"}, {"st": 52, "ed": 54, "text": "local information"}, {"st": 58, "ed": 60, "text": "time consuming"}, {"st": 65, "ed": 67, "text": "multiple scales"}, {"st": 90, "ed": 93, "text": "fully convolutional network"}, {"st": 108, "ed": 110, "text": "point wise"}, {"st": 118, "ed": 121, "text": "end to end"}, {"st": 156, "ed": 158, "text": "point clouds"}, {"st": 164, "ed": 166, "text": "point clouds"}, {"st": 185, "ed": 187, "text": "pooling layer"}, {"st": 245, "ed": 247, "text": "point clouds"}]
[{"st": 0, "ed": 2, "text": "feature representations"}, {"st": 3, "ed": 5, "text": "pre trained"}, {"st": 5, "ed": 8, "text": "deep neural networks"}, {"st": 23, "ed": 25, "text": "fine tuning"}, {"st": 42, "ed": 44, "text": "feature representations"}, {"st": 54, "ed": 56, "text": "fine tuning"}, {"st": 106, "ed": 109, "text": "simple yet effective"}, {"st": 112, "ed": 114, "text": "fine tuning"}, {"st": 114, "ed": 116, "text": "pre trained"}, {"st": 116, "ed": 118, "text": "deep networks"}, {"st": 126, "ed": 128, "text": "prevent overfitting"}, {"st": 170, "ed": 172, "text": "deep network"}, {"st": 183, "ed": 185, "text": "fine tuned"}, {"st": 187, "ed": 189, "text": "classification task"}, {"st": 215, "ed": 217, "text": "non trivial"}, {"st": 237, "ed": 239, "text": "input data"}, {"st": 242, "ed": 244, "text": "reinforcement learning"}, {"st": 247, "ed": 249, "text": "recurrent networks"}, {"st": 272, "ed": 275, "text": "convolutional neural networks"}, {"st": 283, "ed": 285, "text": "fine tuning"}]
[{"st": 35, "ed": 37, "text": "deep learning"}]
[{"st": 18, "ed": 20, "text": "auto encoders"}, {"st": 35, "ed": 37, "text": "auto encoders"}, {"st": 115, "ed": 117, "text": "local optimum"}, {"st": 152, "ed": 154, "text": "auto encoders"}, {"st": 194, "ed": 196, "text": "auto encoders"}, {"st": 208, "ed": 210, "text": "representation learning"}]
[{"st": 21, "ed": 23, "text": "source data"}, {"st": 34, "ed": 36, "text": "source data"}, {"st": 44, "ed": 46, "text": "previous approaches"}, {"st": 46, "ed": 48, "text": "covariate shift"}, {"st": 53, "ed": 55, "text": "marginal distributions"}, {"st": 62, "ed": 64, "text": "conditional distributions"}, {"st": 77, "ed": 79, "text": "objective function"}, {"st": 80, "ed": 82, "text": "simultaneously learns"}, {"st": 83, "ed": 85, "text": "common space"}, {"st": 89, "ed": 91, "text": "conditional distributions"}, {"st": 110, "ed": 113, "text": "source and target"}, {"st": 122, "ed": 124, "text": "objective function"}, {"st": 129, "ed": 131, "text": "source data"}, {"st": 137, "ed": 139, "text": "proposed approach"}]
[{"st": 71, "ed": 73, "text": "prediction accuracy"}, {"st": 144, "ed": 147, "text": "deep neural network"}, {"st": 170, "ed": 172, "text": "substantially improve"}]
[{"st": 0, "ed": 2, "text": "recent research"}, {"st": 4, "ed": 7, "text": "training and inference"}, {"st": 8, "ed": 11, "text": "deep neural networks"}, {"st": 16, "ed": 18, "text": "low precision"}, {"st": 38, "ed": 40, "text": "compact representations"}, {"st": 45, "ed": 47, "text": "significant reduction"}, {"st": 53, "ed": 55, "text": "dnn training"}, {"st": 57, "ed": 59, "text": "neural network"}, {"st": 77, "ed": 79, "text": "floating point"}, {"st": 90, "ed": 92, "text": "proposed approach"}]
[{"st": 1, "ed": 3, "text": "wasserstein distance"}, {"st": 13, "ed": 15, "text": "machine learning"}, {"st": 26, "ed": 28, "text": "numerous applications"}, {"st": 34, "ed": 36, "text": "domain adaptation"}, {"st": 36, "ed": 38, "text": "dimensionality reduction"}, {"st": 103, "ed": 105, "text": "decoder network"}, {"st": 111, "ed": 113, "text": "embedding space"}, {"st": 116, "ed": 118, "text": "original input"}, {"st": 126, "ed": 128, "text": "optimization problems"}, {"st": 144, "ed": 146, "text": "numerical experiments"}, {"st": 152, "ed": 154, "text": "image datasets"}]
[{"st": 23, "ed": 26, "text": "convolutional neural network"}, {"st": 75, "ed": 77, "text": "power consumption"}, {"st": 148, "ed": 150, "text": "image classification"}, {"st": 157, "ed": 161, "text": "mnist and cifar 10"}, {"st": 200, "ed": 202, "text": "vgg 16"}, {"st": 218, "ed": 220, "text": "cifar 10"}]
[{"st": 0, "ed": 2, "text": "deep convolutional"}, {"st": 6, "ed": 8, "text": "doesn t"}, {"st": 12, "ed": 14, "text": "local minimum"}, {"st": 19, "ed": 21, "text": "pre trained"}, {"st": 38, "ed": 41, "text": "end to end"}, {"st": 56, "ed": 58, "text": "proposed approach"}, {"st": 58, "ed": 60, "text": "simultaneously learn"}, {"st": 71, "ed": 73, "text": "pre trained"}, {"st": 98, "ed": 100, "text": "faster convergence"}, {"st": 116, "ed": 118, "text": "diabetic retinopathy"}]
[{"st": 0, "ed": 2, "text": "recent research"}, {"st": 8, "ed": 11, "text": "deep neural networks"}, {"st": 19, "ed": 21, "text": "small perturbations"}, {"st": 55, "ed": 57, "text": "adversarial perturbations"}, {"st": 80, "ed": 82, "text": "natural images"}, {"st": 111, "ed": 113, "text": "machine learning"}]
[{"st": 4, "ed": 6, "text": "supervised classification"}, {"st": 49, "ed": 51, "text": "class label"}, {"st": 56, "ed": 58, "text": "pattern recognition"}, {"st": 59, "ed": 61, "text": "machine learning"}, {"st": 68, "ed": 70, "text": "main idea"}, {"st": 71, "ed": 73, "text": "supervised methods"}, {"st": 84, "ed": 86, "text": "input output"}, {"st": 102, "ed": 104, "text": "feature vector"}, {"st": 124, "ed": 126, "text": "supervised classification"}, {"st": 136, "ed": 138, "text": "classification techniques"}, {"st": 157, "ed": 159, "text": "supervised classification"}]
[{"st": 0, "ed": 2, "text": "conjugate gradient"}, {"st": 24, "ed": 26, "text": "conjugate gradient"}, {"st": 34, "ed": 36, "text": "linear convergence"}, {"st": 43, "ed": 45, "text": "strongly convex"}, {"st": 49, "ed": 51, "text": "experimentally demonstrate"}, {"st": 62, "ed": 64, "text": "large scale"}, {"st": 64, "ed": 66, "text": "optimization problems"}, {"st": 95, "ed": 97, "text": "significant improvement"}]
[{"st": 28, "ed": 30, "text": "existing solutions"}, {"st": 47, "ed": 49, "text": "deep learning"}, {"st": 68, "ed": 71, "text": "extensive experimental results"}, {"st": 84, "ed": 86, "text": "proposed method"}, {"st": 92, "ed": 94, "text": "without sacrificing"}]
[{"st": 5, "ed": 8, "text": "deep neural network"}, {"st": 60, "ed": 62, "text": "numerical analysis"}, {"st": 81, "ed": 83, "text": "multi step"}, {"st": 92, "ed": 94, "text": "multi step"}, {"st": 142, "ed": 144, "text": "higher accuracy"}, {"st": 150, "ed": 153, "text": "cifar and imagenet"}, {"st": 163, "ed": 166, "text": "cifar and imagenet"}, {"st": 213, "ed": 215, "text": "training process"}, {"st": 227, "ed": 229, "text": "training strategy"}, {"st": 258, "ed": 260, "text": "significant improvement"}]
[{"st": 0, "ed": 4, "text": "nonnegative matrix factorization nmf"}, {"st": 5, "ed": 7, "text": "dimensionality reduction"}, {"st": 8, "ed": 10, "text": "factor analysis"}, {"st": 13, "ed": 15, "text": "special case"}, {"st": 20, "ed": 22, "text": "low rank"}]
[{"st": 7, "ed": 9, "text": "random forests"}, {"st": 12, "ed": 14, "text": "random forests"}, {"st": 35, "ed": 37, "text": "random forests"}, {"st": 63, "ed": 65, "text": "main idea"}, {"st": 87, "ed": 89, "text": "proposed method"}, {"st": 95, "ed": 97, "text": "denoising autoencoders"}, {"st": 101, "ed": 103, "text": "denoising autoencoder"}, {"st": 114, "ed": 116, "text": "input samples"}, {"st": 131, "ed": 134, "text": "input and output"}]
[{"st": 8, "ed": 10, "text": "computer vision"}, {"st": 11, "ed": 15, "text": "feed forward neural networks"}, {"st": 17, "ed": 20, "text": "probabilistic graphical models"}, {"st": 31, "ed": 33, "text": "recent works"}, {"st": 37, "ed": 39, "text": "mean field"}, {"st": 90, "ed": 92, "text": "learning algorithm"}, {"st": 101, "ed": 103, "text": "classification problem"}, {"st": 104, "ed": 106, "text": "cifar 10"}, {"st": 109, "ed": 111, "text": "binary image"}, {"st": 122, "ed": 124, "text": "improve performance"}, {"st": 131, "ed": 133, "text": "generalization capability"}]
[{"st": 4, "ed": 8, "text": "deep convolutional neural network"}, {"st": 12, "ed": 14, "text": "building blocks"}, {"st": 53, "ed": 55, "text": "residual connections"}, {"st": 56, "ed": 58, "text": "benchmark datasets"}, {"st": 100, "ed": 104, "text": "deep convolutional neural networks"}, {"st": 142, "ed": 146, "text": "deep convolutional neural network"}, {"st": 164, "ed": 166, "text": "training procedure"}]
[{"st": 6, "ed": 9, "text": "sparse subspace clustering"}, {"st": 10, "ed": 12, "text": "spectral clustering"}, {"st": 14, "ed": 16, "text": "similarity matrix"}, {"st": 20, "ed": 23, "text": "each data point"}, {"st": 37, "ed": 39, "text": "matching pursuit"}, {"st": 42, "ed": 44, "text": "based methods"}, {"st": 82, "ed": 84, "text": "least squares"}, {"st": 97, "ed": 99, "text": "proposed algorithm"}, {"st": 109, "ed": 111, "text": "proposed method"}, {"st": 111, "ed": 114, "text": "compares favorably with"}, {"st": 115, "ed": 117, "text": "based method"}]
[{"st": 2, "ed": 4, "text": "common practice"}, {"st": 18, "ed": 20, "text": "learning curve"}, {"st": 22, "ed": 25, "text": "training and test"}, {"st": 30, "ed": 32, "text": "batch size"}, {"st": 39, "ed": 42, "text": "stochastic gradient descent"}, {"st": 60, "ed": 62, "text": "training epochs"}, {"st": 87, "ed": 89, "text": "learning rate"}, {"st": 93, "ed": 95, "text": "batch size"}, {"st": 144, "ed": 146, "text": "resnet 50"}]
[{"st": 2, "ed": 4, "text": "learned models"}, {"st": 23, "ed": 25, "text": "training data"}, {"st": 48, "ed": 50, "text": "neural networks"}, {"st": 74, "ed": 76, "text": "neural networks"}, {"st": 92, "ed": 94, "text": "adversarial examples"}]
[{"st": 0, "ed": 2, "text": "generative models"}, {"st": 4, "ed": 7, "text": "variational auto encoders"}, {"st": 9, "ed": 13, "text": "generative adversarial networks gans"}, {"st": 19, "ed": 21, "text": "prior distribution"}, {"st": 23, "ed": 25, "text": "latent space"}, {"st": 32, "ed": 34, "text": "trained model"}, {"st": 81, "ed": 83, "text": "latent space"}, {"st": 100, "ed": 102, "text": "prior distribution"}, {"st": 114, "ed": 116, "text": "distribution matching"}, {"st": 122, "ed": 124, "text": "latent space"}, {"st": 127, "ed": 129, "text": "prior distribution"}, {"st": 144, "ed": 146, "text": "higher quality"}]
[{"st": 4, "ed": 6, "text": "deep learning"}, {"st": 7, "ed": 9, "text": "image analysis"}, {"st": 12, "ed": 14, "text": "reconstructed images"}, {"st": 65, "ed": 68, "text": "end to end"}, {"st": 84, "ed": 86, "text": "iterative reconstruction"}, {"st": 98, "ed": 102, "text": "convolutional neural network cnn"}, {"st": 114, "ed": 116, "text": "trained jointly"}, {"st": 125, "ed": 127, "text": "detection task"}, {"st": 137, "ed": 139, "text": "nodule detection"}, {"st": 140, "ed": 142, "text": "low dose"}, {"st": 146, "ed": 149, "text": "end to end"}, {"st": 175, "ed": 177, "text": "significant reduction"}, {"st": 178, "ed": 181, "text": "false positive rate"}, {"st": 195, "ed": 197, "text": "low dose"}, {"st": 206, "ed": 209, "text": "end to end"}]
[{"st": 0, "ed": 4, "text": "convolutional neural networks cnns"}, {"st": 12, "ed": 14, "text": "wide variety"}, {"st": 58, "ed": 60, "text": "recently proposed"}, {"st": 62, "ed": 64, "text": "compression scheme"}, {"st": 82, "ed": 84, "text": "classification accuracy"}, {"st": 134, "ed": 136, "text": "object recognition"}, {"st": 201, "ed": 203, "text": "class labels"}]
[{"st": 5, "ed": 7, "text": "machine learning"}, {"st": 11, "ed": 13, "text": "neural network"}, {"st": 85, "ed": 87, "text": "trained model"}]
[{"st": 2, "ed": 4, "text": "inner product"}, {"st": 10, "ed": 14, "text": "convolutional neural networks cnns"}, {"st": 18, "ed": 21, "text": "end to end"}, {"st": 21, "ed": 23, "text": "visual representation"}, {"st": 44, "ed": 46, "text": "parameter space"}, {"st": 68, "ed": 70, "text": "learning framework"}, {"st": 88, "ed": 90, "text": "inner product"}, {"st": 109, "ed": 111, "text": "softmax loss"}, {"st": 134, "ed": 136, "text": "faster convergence"}, {"st": 140, "ed": 142, "text": "classification accuracy"}, {"st": 149, "ed": 151, "text": "theoretical insights"}]
[{"st": 10, "ed": 12, "text": "developing countries"}, {"st": 72, "ed": 74, "text": "african continent"}]
[{"st": 8, "ed": 10, "text": "breast cancer"}, {"st": 24, "ed": 26, "text": "learning algorithms"}, {"st": 66, "ed": 68, "text": "breast cancer"}, {"st": 80, "ed": 83, "text": "convolutional neural network"}]
[{"st": 6, "ed": 8, "text": "subspace clustering"}, {"st": 58, "ed": 60, "text": "noise free"}, {"st": 77, "ed": 79, "text": "subspace clustering"}, {"st": 88, "ed": 92, "text": "synthetic and real data"}]
[{"st": 1, "ed": 4, "text": "convolutional neural networks"}, {"st": 10, "ed": 12, "text": "object recognition"}, {"st": 23, "ed": 25, "text": "decision making"}, {"st": 69, "ed": 71, "text": "efficient learning"}, {"st": 99, "ed": 101, "text": "local features"}, {"st": 103, "ed": 105, "text": "decision making"}, {"st": 152, "ed": 154, "text": "decision support"}]
[{"st": 23, "ed": 26, "text": "modern machine learning"}, {"st": 61, "ed": 64, "text": "convolutional neural network"}, {"st": 77, "ed": 79, "text": "natural image"}, {"st": 131, "ed": 133, "text": "neural network"}]
[{"st": 13, "ed": 15, "text": "a level"}, {"st": 25, "ed": 28, "text": "convolutional neural network"}, {"st": 38, "ed": 40, "text": "x ray"}, {"st": 43, "ed": 45, "text": "100 000"}, {"st": 47, "ed": 49, "text": "x ray"}]
[{"st": 0, "ed": 3, "text": "gaussian mixture models"}, {"st": 12, "ed": 14, "text": "machine learning"}, {"st": 17, "ed": 20, "text": "expectation maximization em"}, {"st": 37, "ed": 39, "text": "stationary point"}, {"st": 41, "ed": 43, "text": "log likelihood"}, {"st": 59, "ed": 61, "text": "negative log"}, {"st": 61, "ed": 63, "text": "likelihood function"}, {"st": 65, "ed": 68, "text": "kullback leibler kl"}, {"st": 82, "ed": 84, "text": "wasserstein distance"}, {"st": 97, "ed": 99, "text": "wasserstein distance"}, {"st": 101, "ed": 103, "text": "mixture model"}, {"st": 125, "ed": 127, "text": "wasserstein distance"}, {"st": 137, "ed": 140, "text": "stochastic gradient descent"}, {"st": 154, "ed": 156, "text": "parameter estimates"}]
[{"st": 8, "ed": 10, "text": "machine learning"}, {"st": 22, "ed": 24, "text": "existing solutions"}, {"st": 101, "ed": 103, "text": "task specific"}, {"st": 110, "ed": 112, "text": "outperforms existing"}]
[{"st": 3, "ed": 5, "text": "framework named"}, {"st": 10, "ed": 12, "text": "dissimilarity measure"}, {"st": 16, "ed": 19, "text": "hidden markov models"}, {"st": 21, "ed": 23, "text": "conditional distributions"}, {"st": 29, "ed": 31, "text": "marginal distribution"}, {"st": 37, "ed": 39, "text": "gaussian mixture"}, {"st": 59, "ed": 62, "text": "gaussian mixture model"}, {"st": 77, "ed": 79, "text": "optimal transport"}, {"st": 80, "ed": 83, "text": "the wasserstein metric"}, {"st": 97, "ed": 99, "text": "optimal transport"}, {"st": 106, "ed": 109, "text": "the wasserstein metric"}, {"st": 116, "ed": 118, "text": "optimization problem"}, {"st": 123, "ed": 126, "text": "the wasserstein metric"}, {"st": 132, "ed": 134, "text": "wasserstein distance"}, {"st": 144, "ed": 146, "text": "monte carlo"}, {"st": 218, "ed": 220, "text": "t sne"}, {"st": 227, "ed": 231, "text": "synthetic and real data"}]
[{"st": 17, "ed": 20, "text": "deep convolutional network"}, {"st": 27, "ed": 29, "text": "method produces"}, {"st": 53, "ed": 55, "text": "computationally efficient"}, {"st": 64, "ed": 66, "text": "numerical results"}]
[{"st": 2, "ed": 4, "text": "global optimality"}, {"st": 5, "ed": 7, "text": "deep learning"}, {"st": 38, "ed": 40, "text": "approximation algorithm"}, {"st": 43, "ed": 45, "text": "deep models"}, {"st": 72, "ed": 74, "text": "step size"}, {"st": 107, "ed": 109, "text": "global optimality"}, {"st": 142, "ed": 144, "text": "object recognition"}]
[{"st": 1, "ed": 3, "text": "domain adaptation"}, {"st": 12, "ed": 14, "text": "previous approaches"}, {"st": 19, "ed": 22, "text": "deep neural networks"}, {"st": 39, "ed": 41, "text": "labeled data"}, {"st": 60, "ed": 63, "text": "synthetic and real"}, {"st": 66, "ed": 68, "text": "previous approaches"}, {"st": 87, "ed": 91, "text": "generative adversarial networks gans"}, {"st": 150, "ed": 153, "text": "source and target"}]
[{"st": 7, "ed": 9, "text": "higher dimensional"}, {"st": 19, "ed": 21, "text": "feed forward"}, {"st": 21, "ed": 24, "text": "deep neural networks"}, {"st": 32, "ed": 35, "text": "block coordinate descent"}, {"st": 55, "ed": 57, "text": "convex analysis"}, {"st": 68, "ed": 70, "text": "stationary point"}, {"st": 72, "ed": 75, "text": "linear convergence rate"}, {"st": 95, "ed": 97, "text": "error rates"}, {"st": 105, "ed": 108, "text": "stochastic gradient descent"}]
[{"st": 0, "ed": 3, "text": "semi supervised learning"}, {"st": 16, "ed": 18, "text": "labeled dataset"}, {"st": 39, "ed": 41, "text": "deep learning"}, {"st": 48, "ed": 50, "text": "adversarial training"}, {"st": 80, "ed": 82, "text": "mnist dataset"}, {"st": 93, "ed": 95, "text": "layer wise"}, {"st": 96, "ed": 98, "text": "adversarial noise"}, {"st": 104, "ed": 106, "text": "error rate"}, {"st": 122, "ed": 124, "text": "adversarial examples"}, {"st": 141, "ed": 143, "text": "error rate"}, {"st": 151, "ed": 153, "text": "ladder network"}]
[{"st": 4, "ed": 7, "text": "artificial neural network"}, {"st": 18, "ed": 20, "text": "transfer function"}, {"st": 25, "ed": 28, "text": "principal component analysis"}, {"st": 34, "ed": 36, "text": "weight vectors"}, {"st": 37, "ed": 39, "text": "linear transformations"}, {"st": 71, "ed": 73, "text": "hidden nodes"}, {"st": 75, "ed": 77, "text": "class distributions"}, {"st": 78, "ed": 80, "text": "latent representations"}, {"st": 86, "ed": 88, "text": "latent representations"}, {"st": 131, "ed": 133, "text": "real data"}]
[{"st": 0, "ed": 3, "text": "deep generative models"}, {"st": 8, "ed": 11, "text": "low dimensional latent"}, {"st": 37, "ed": 39, "text": "riemannian geometry"}, {"st": 46, "ed": 48, "text": "efficient algorithms"}, {"st": 74, "ed": 76, "text": "tangent vector"}, {"st": 115, "ed": 117, "text": "image data"}, {"st": 123, "ed": 126, "text": "deep generative models"}, {"st": 143, "ed": 145, "text": "latent space"}, {"st": 181, "ed": 183, "text": "riemannian geometry"}, {"st": 184, "ed": 187, "text": "deep generative models"}]
[{"st": 0, "ed": 4, "text": "convolutional neural networks cnns"}, {"st": 55, "ed": 57, "text": "annotated data"}, {"st": 89, "ed": 91, "text": "interpretable models"}, {"st": 108, "ed": 110, "text": "visual concepts"}, {"st": 114, "ed": 116, "text": "visual cues"}, {"st": 119, "ed": 121, "text": "feature vectors"}, {"st": 132, "ed": 134, "text": "few shot"}, {"st": 175, "ed": 177, "text": "achieve competitive"}, {"st": 190, "ed": 193, "text": "few shot learning"}]
[{"st": 0, "ed": 2, "text": "kernel methods"}, {"st": 3, "ed": 5, "text": "powerful tools"}, {"st": 16, "ed": 18, "text": "infinite dimensional"}, {"st": 22, "ed": 26, "text": "reproducing kernel hilbert space"}, {"st": 38, "ed": 40, "text": "kernel methods"}, {"st": 48, "ed": 50, "text": "representational power"}, {"st": 58, "ed": 61, "text": "deep neural networks"}, {"st": 64, "ed": 66, "text": "multi layer"}, {"st": 66, "ed": 68, "text": "hierarchical representations"}, {"st": 99, "ed": 101, "text": "linear transformations"}, {"st": 117, "ed": 119, "text": "multiple layers"}, {"st": 120, "ed": 122, "text": "hidden units"}, {"st": 132, "ed": 134, "text": "finite dimensional"}, {"st": 154, "ed": 156, "text": "random fourier"}, {"st": 168, "ed": 170, "text": "convolutional network"}, {"st": 196, "ed": 198, "text": "finite dimensional"}]
[{"st": 4, "ed": 6, "text": "deep learning"}, {"st": 8, "ed": 10, "text": "supervised learning"}, {"st": 20, "ed": 22, "text": "artificial intelligence"}, {"st": 35, "ed": 37, "text": "deep learning"}, {"st": 71, "ed": 73, "text": "deep learning"}, {"st": 143, "ed": 146, "text": "deep neural network"}, {"st": 164, "ed": 166, "text": "multi dimensional"}, {"st": 171, "ed": 173, "text": "visualization techniques"}, {"st": 199, "ed": 201, "text": "deep learning"}, {"st": 229, "ed": 231, "text": "image classification"}, {"st": 242, "ed": 244, "text": "deep model"}, {"st": 257, "ed": 259, "text": "deep models"}, {"st": 264, "ed": 266, "text": "initial results"}]
[{"st": 47, "ed": 49, "text": "neural network"}, {"st": 78, "ed": 80, "text": "computed tomography"}, {"st": 83, "ed": 85, "text": "estimation error"}, {"st": 133, "ed": 136, "text": "trained neural network"}]
[{"st": 10, "ed": 12, "text": "image classification"}, {"st": 73, "ed": 75, "text": "generalization ability"}, {"st": 93, "ed": 95, "text": "image data"}, {"st": 106, "ed": 109, "text": "convolutional neural networks"}, {"st": 114, "ed": 116, "text": "input data"}, {"st": 141, "ed": 143, "text": "significantly improve"}, {"st": 179, "ed": 181, "text": "cifar 10"}]
[{"st": 6, "ed": 8, "text": "point cloud"}, {"st": 10, "ed": 12, "text": "automatic segmentation"}, {"st": 31, "ed": 33, "text": "post processing"}, {"st": 42, "ed": 44, "text": "classification algorithm"}, {"st": 78, "ed": 80, "text": "point cloud"}, {"st": 102, "ed": 104, "text": "deep learning"}, {"st": 111, "ed": 113, "text": "automatic segmentation"}]
[{"st": 13, "ed": 15, "text": "previous approaches"}, {"st": 18, "ed": 20, "text": "reinforcement learning"}, {"st": 22, "ed": 24, "text": "genetic algorithms"}, {"st": 29, "ed": 31, "text": "model based"}, {"st": 43, "ed": 45, "text": "increasing complexity"}, {"st": 61, "ed": 63, "text": "cifar 10"}, {"st": 73, "ed": 75, "text": "classification accuracy"}, {"st": 76, "ed": 78, "text": "error rate"}, {"st": 89, "ed": 91, "text": "times faster"}, {"st": 118, "ed": 120, "text": "error rate"}, {"st": 160, "ed": 162, "text": "top 5"}]
[{"st": 5, "ed": 7, "text": "deep networks"}, {"st": 32, "ed": 34, "text": "regularization technique"}, {"st": 53, "ed": 55, "text": "extracted features"}, {"st": 69, "ed": 71, "text": "regularization technique"}, {"st": 93, "ed": 95, "text": "proposed method"}, {"st": 105, "ed": 107, "text": "special case"}, {"st": 140, "ed": 142, "text": "numerical experiments"}, {"st": 167, "ed": 169, "text": "performance gains"}]
[{"st": 0, "ed": 3, "text": "deep neural networks"}, {"st": 13, "ed": 16, "text": "cross entropy loss"}, {"st": 28, "ed": 30, "text": "intra class"}, {"st": 32, "ed": 34, "text": "inter class"}, {"st": 76, "ed": 78, "text": "computational burden"}, {"st": 99, "ed": 101, "text": "deep networks"}, {"st": 104, "ed": 106, "text": "intra class"}, {"st": 109, "ed": 111, "text": "inter class"}, {"st": 124, "ed": 126, "text": "deep features"}, {"st": 131, "ed": 133, "text": "linear subspace"}, {"st": 138, "ed": 140, "text": "inter class"}, {"st": 152, "ed": 154, "text": "low rank"}, {"st": 174, "ed": 176, "text": "classification loss"}, {"st": 181, "ed": 183, "text": "metric learning"}, {"st": 199, "ed": 201, "text": "deep networks"}, {"st": 212, "ed": 214, "text": "classification performance"}, {"st": 216, "ed": 218, "text": "object recognition"}, {"st": 242, "ed": 244, "text": "data model"}]
[{"st": 0, "ed": 2, "text": "large scale"}, {"st": 2, "ed": 4, "text": "distributed training"}, {"st": 32, "ed": 34, "text": "distributed training"}, {"st": 105, "ed": 107, "text": "image classification"}, {"st": 107, "ed": 109, "text": "speech recognition"}, {"st": 113, "ed": 115, "text": "multiple datasets"}, {"st": 132, "ed": 134, "text": "compression ratio"}, {"st": 146, "ed": 148, "text": "resnet 50"}, {"st": 163, "ed": 165, "text": "large scale"}, {"st": 165, "ed": 167, "text": "distributed training"}, {"st": 174, "ed": 176, "text": "distributed training"}]
[{"st": 1, "ed": 4, "text": "deep neural networks"}, {"st": 5, "ed": 8, "text": "stochastic gradient descent"}, {"st": 16, "ed": 18, "text": "learning rate"}, {"st": 23, "ed": 25, "text": "batch sizes"}, {"st": 29, "ed": 31, "text": "training epochs"}, {"st": 32, "ed": 34, "text": "batch sizes"}, {"st": 56, "ed": 58, "text": "batch size"}, {"st": 64, "ed": 66, "text": "batch size"}, {"st": 74, "ed": 76, "text": "convergence rate"}, {"st": 78, "ed": 80, "text": "batch sizes"}, {"st": 104, "ed": 108, "text": "cifar 10 cifar 100"}, {"st": 118, "ed": 120, "text": "batch sizes"}, {"st": 121, "ed": 123, "text": "improve performance"}, {"st": 131, "ed": 133, "text": "nvidia tesla"}]
[{"st": 6, "ed": 8, "text": "deep learning"}, {"st": 10, "ed": 13, "text": "convolutional neural networks"}, {"st": 31, "ed": 33, "text": "deep learning"}, {"st": 50, "ed": 53, "text": "easy to implement"}]
[{"st": 10, "ed": 12, "text": "kernel methods"}, {"st": 21, "ed": 23, "text": "computation cost"}, {"st": 85, "ed": 87, "text": "expectation maximization"}, {"st": 116, "ed": 118, "text": "random fourier"}, {"st": 145, "ed": 147, "text": "empirical studies"}, {"st": 152, "ed": 154, "text": "real world"}, {"st": 160, "ed": 162, "text": "significantly improves"}, {"st": 165, "ed": 167, "text": "kernel methods"}, {"st": 179, "ed": 181, "text": "matrix multiplication"}]
[{"st": 11, "ed": 13, "text": "covariate shift"}, {"st": 98, "ed": 100, "text": "activation function"}, {"st": 150, "ed": 152, "text": "covariate shift"}, {"st": 169, "ed": 171, "text": "large networks"}, {"st": 188, "ed": 190, "text": "batch normalization"}]
[{"st": 1, "ed": 3, "text": "decision trees"}, {"st": 17, "ed": 20, "text": "ability to learn"}, {"st": 35, "ed": 37, "text": "deep learning"}, {"st": 41, "ed": 44, "text": "end to end"}, {"st": 59, "ed": 61, "text": "supervised learning"}, {"st": 81, "ed": 83, "text": "decision trees"}, {"st": 104, "ed": 106, "text": "expectation maximization"}, {"st": 106, "ed": 108, "text": "training scheme"}, {"st": 109, "ed": 111, "text": "decision trees"}, {"st": 138, "ed": 140, "text": "image datasets"}, {"st": 143, "ed": 145, "text": "neural networks"}, {"st": 158, "ed": 161, "text": "end to end"}, {"st": 161, "ed": 163, "text": "learning scheme"}, {"st": 165, "ed": 167, "text": "decision trees"}, {"st": 179, "ed": 181, "text": "decision tree"}]
[{"st": 72, "ed": 74, "text": "generated image"}, {"st": 81, "ed": 83, "text": "adversarial attacks"}, {"st": 87, "ed": 89, "text": "training procedure"}, {"st": 94, "ed": 96, "text": "adversarial examples"}, {"st": 108, "ed": 111, "text": "vulnerable to adversarial"}]
[{"st": 0, "ed": 2, "text": "matrix decomposition"}, {"st": 9, "ed": 11, "text": "machine learning"}, {"st": 17, "ed": 19, "text": "successfully applied"}, {"st": 23, "ed": 25, "text": "matrix decomposition"}, {"st": 52, "ed": 54, "text": "matrix decomposition"}, {"st": 60, "ed": 63, "text": "multi view data"}, {"st": 81, "ed": 83, "text": "multi view"}, {"st": 95, "ed": 97, "text": "matrix decomposition"}, {"st": 106, "ed": 108, "text": "gaussian distribution"}, {"st": 123, "ed": 125, "text": "variational bayesian"}, {"st": 125, "ed": 127, "text": "inference algorithm"}, {"st": 133, "ed": 135, "text": "posterior distribution"}, {"st": 152, "ed": 154, "text": "extensive experiments"}, {"st": 155, "ed": 160, "text": "synthetic and real world datasets"}]
[{"st": 3, "ed": 5, "text": "generative framework"}, {"st": 7, "ed": 10, "text": "zero shot learning"}, {"st": 12, "ed": 15, "text": "training and test"}, {"st": 23, "ed": 25, "text": "variational autoencoder"}, {"st": 45, "ed": 47, "text": "unseen classes"}, {"st": 73, "ed": 76, "text": "encoder decoder architecture"}, {"st": 115, "ed": 117, "text": "unseen classes"}, {"st": 134, "ed": 137, "text": "zero shot learning"}, {"st": 158, "ed": 160, "text": "benchmark datasets"}]
[{"st": 36, "ed": 38, "text": "conditional gan"}, {"st": 75, "ed": 77, "text": "unlike previous"}, {"st": 77, "ed": 79, "text": "conditional gan"}, {"st": 104, "ed": 106, "text": "photo realistic"}, {"st": 135, "ed": 137, "text": "benchmark dataset"}]
[{"st": 30, "ed": 32, "text": "machine learning"}, {"st": 57, "ed": 61, "text": "generative adversarial networks gans"}, {"st": 66, "ed": 68, "text": "multi modal"}, {"st": 75, "ed": 77, "text": "mode collapse"}, {"st": 114, "ed": 116, "text": "latent space"}, {"st": 134, "ed": 136, "text": "gan training"}, {"st": 137, "ed": 139, "text": "cifar 10"}, {"st": 163, "ed": 165, "text": "multi modal"}]
[{"st": 127, "ed": 129, "text": "fully convolutional"}, {"st": 129, "ed": 131, "text": "deep neural"}]
[{"st": 10, "ed": 12, "text": "computer vision"}, {"st": 25, "ed": 27, "text": "internal representations"}, {"st": 34, "ed": 36, "text": "existing methods"}, {"st": 70, "ed": 72, "text": "automatically identify"}, {"st": 111, "ed": 113, "text": "class label"}, {"st": 156, "ed": 158, "text": "proposed method"}]
[{"st": 0, "ed": 2, "text": "unsupervised learning"}, {"st": 3, "ed": 7, "text": "generative adversarial networks gans"}, {"st": 22, "ed": 25, "text": "cross entropy loss"}, {"st": 31, "ed": 33, "text": "loss function"}, {"st": 37, "ed": 39, "text": "vanishing gradients"}, {"st": 55, "ed": 57, "text": "least squares"}, {"st": 57, "ed": 60, "text": "generative adversarial networks"}, {"st": 64, "ed": 66, "text": "least squares"}, {"st": 66, "ed": 68, "text": "loss function"}, {"st": 76, "ed": 78, "text": "objective function"}, {"st": 91, "ed": 93, "text": "theoretical analysis"}, {"st": 117, "ed": 119, "text": "higher quality"}, {"st": 135, "ed": 137, "text": "image quality"}, {"st": 198, "ed": 200, "text": "gaussian mixture"}, {"st": 206, "ed": 208, "text": "proposed method"}]
[{"st": 9, "ed": 11, "text": "challenging task"}, {"st": 23, "ed": 25, "text": "framework named"}, {"st": 27, "ed": 30, "text": "variational auto encoder"}, {"st": 52, "ed": 54, "text": "latent features"}, {"st": 74, "ed": 76, "text": "prior knowledge"}, {"st": 92, "ed": 94, "text": "generalization ability"}, {"st": 132, "ed": 134, "text": "extensive experiments"}, {"st": 146, "ed": 148, "text": "feature vectors"}, {"st": 160, "ed": 162, "text": "real world"}]
[{"st": 12, "ed": 14, "text": "deep learning"}, {"st": 23, "ed": 26, "text": "deep neural networks"}, {"st": 34, "ed": 36, "text": "input samples"}, {"st": 38, "ed": 40, "text": "adversarial examples"}, {"st": 41, "ed": 43, "text": "adversarial examples"}, {"st": 51, "ed": 54, "text": "deep neural networks"}, {"st": 62, "ed": 64, "text": "adversarial examples"}, {"st": 72, "ed": 75, "text": "deep neural networks"}, {"st": 85, "ed": 87, "text": "adversarial examples"}, {"st": 98, "ed": 100, "text": "adversarial examples"}, {"st": 101, "ed": 104, "text": "deep neural networks"}, {"st": 108, "ed": 110, "text": "generating adversarial"}, {"st": 125, "ed": 127, "text": "adversarial examples"}, {"st": 133, "ed": 135, "text": "adversarial examples"}]
[{"st": 1, "ed": 3, "text": "neural network"}, {"st": 9, "ed": 11, "text": "adversarial examples"}, {"st": 29, "ed": 31, "text": "previous methods"}, {"st": 36, "ed": 38, "text": "finite difference"}, {"st": 59, "ed": 61, "text": "generating adversarial"}, {"st": 78, "ed": 80, "text": "black box"}, {"st": 85, "ed": 88, "text": "orders of magnitude"}, {"st": 102, "ed": 104, "text": "adversarial attacks"}, {"st": 106, "ed": 108, "text": "partial information"}, {"st": 131, "ed": 133, "text": "adversarial attack"}, {"st": 137, "ed": 139, "text": "machine learning"}, {"st": 147, "ed": 149, "text": "partial information"}]
[{"st": 0, "ed": 3, "text": "convolutional neural network"}, {"st": 14, "ed": 16, "text": "machine learning"}, {"st": 24, "ed": 26, "text": "crucial step"}, {"st": 54, "ed": 56, "text": "convolutional layers"}, {"st": 58, "ed": 60, "text": "fully connected"}, {"st": 113, "ed": 115, "text": "random search"}, {"st": 163, "ed": 165, "text": "convolutional network"}, {"st": 174, "ed": 176, "text": "cifar 10"}]
[{"st": 3, "ed": 5, "text": "neural network"}, {"st": 18, "ed": 20, "text": "deep learning"}, {"st": 25, "ed": 27, "text": "deep learning"}, {"st": 37, "ed": 39, "text": "neural network"}, {"st": 42, "ed": 44, "text": "recently proposed"}, {"st": 82, "ed": 84, "text": "monte carlo"}, {"st": 91, "ed": 94, "text": "upper confidence bound"}, {"st": 112, "ed": 114, "text": "network architecture"}, {"st": 131, "ed": 133, "text": "empirical study"}, {"st": 148, "ed": 150, "text": "mnist svhn"}, {"st": 151, "ed": 153, "text": "cifar 10"}]
[{"st": 42, "ed": 44, "text": "cost effective"}, {"st": 50, "ed": 52, "text": "neural network"}, {"st": 53, "ed": 55, "text": "image segmentation"}, {"st": 66, "ed": 69, "text": "convolutional neural network"}]
[{"st": 0, "ed": 3, "text": "deep neural network"}, {"st": 12, "ed": 14, "text": "image classification"}, {"st": 72, "ed": 74, "text": "diabetic retinopathy"}, {"st": 104, "ed": 106, "text": "input space"}]
[{"st": 0, "ed": 3, "text": "deep neural networks"}, {"st": 5, "ed": 7, "text": "excellent performance"}, {"st": 16, "ed": 20, "text": "vulnerable to adversarial examples"}, {"st": 45, "ed": 47, "text": "generative model"}, {"st": 53, "ed": 55, "text": "pre processing"}, {"st": 63, "ed": 65, "text": "generative model"}, {"st": 96, "ed": 98, "text": "min max"}, {"st": 105, "ed": 107, "text": "adversarial examples"}, {"st": 121, "ed": 123, "text": "decision boundary"}, {"st": 134, "ed": 136, "text": "adversarial training"}, {"st": 165, "ed": 167, "text": "pre trained"}, {"st": 167, "ed": 169, "text": "generative model"}, {"st": 172, "ed": 174, "text": "method called"}]
[{"st": 3, "ed": 5, "text": "multi class"}, {"st": 59, "ed": 61, "text": "scoring functions"}, {"st": 77, "ed": 79, "text": "k 1"}, {"st": 107, "ed": 109, "text": "facial recognition"}]
[{"st": 0, "ed": 2, "text": "neural network"}, {"st": 23, "ed": 25, "text": "network architecture"}, {"st": 27, "ed": 29, "text": "skip connections"}, {"st": 30, "ed": 32, "text": "loss functions"}, {"st": 40, "ed": 42, "text": "batch size"}, {"st": 42, "ed": 44, "text": "learning rate"}, {"st": 77, "ed": 79, "text": "loss functions"}, {"st": 105, "ed": 107, "text": "loss function"}, {"st": 128, "ed": 130, "text": "network architecture"}]
[{"st": 0, "ed": 2, "text": "transfer learning"}, {"st": 10, "ed": 12, "text": "simultaneously learns"}, {"st": 20, "ed": 22, "text": "labeled data"}, {"st": 33, "ed": 35, "text": "labeled data"}, {"st": 44, "ed": 46, "text": "transfer learning"}, {"st": 49, "ed": 53, "text": "source and target domains"}, {"st": 96, "ed": 100, "text": "source and target domains"}, {"st": 112, "ed": 114, "text": "source domain"}, {"st": 130, "ed": 132, "text": "multivariate statistics"}, {"st": 153, "ed": 155, "text": "closed form"}, {"st": 159, "ed": 161, "text": "transfer learning"}, {"st": 167, "ed": 171, "text": "synthetic and real world"}, {"st": 188, "ed": 190, "text": "transfer learning"}, {"st": 191, "ed": 193, "text": "domain adaptation"}]
[{"st": 1, "ed": 3, "text": "neural networks"}, {"st": 29, "ed": 31, "text": "adversarial examples"}, {"st": 36, "ed": 38, "text": "generalization capabilities"}, {"st": 48, "ed": 50, "text": "weight matrix"}, {"st": 53, "ed": 55, "text": "neural networks"}, {"st": 60, "ed": 62, "text": "weight matrix"}, {"st": 69, "ed": 71, "text": "neural network"}, {"st": 81, "ed": 83, "text": "weight matrix"}, {"st": 84, "ed": 86, "text": "condition number"}]
[{"st": 0, "ed": 2, "text": "feature selection"}, {"st": 4, "ed": 6, "text": "dimensionality reduction"}, {"st": 25, "ed": 27, "text": "feature selection"}, {"st": 43, "ed": 45, "text": "feature selection"}, {"st": 58, "ed": 60, "text": "low dimensional"}, {"st": 111, "ed": 113, "text": "feature selection"}, {"st": 125, "ed": 127, "text": "feature selection"}, {"st": 138, "ed": 140, "text": "weight matrix"}, {"st": 146, "ed": 148, "text": "hidden layer"}, {"st": 155, "ed": 157, "text": "spectral graph"}, {"st": 164, "ed": 166, "text": "learning process"}, {"st": 179, "ed": 181, "text": "low dimensional"}, {"st": 183, "ed": 185, "text": "extensive experiments"}]
[{"st": 18, "ed": 20, "text": "computed tomography"}, {"st": 32, "ed": 34, "text": "labeled data"}, {"st": 40, "ed": 42, "text": "labeled data"}, {"st": 47, "ed": 49, "text": "u net"}, {"st": 73, "ed": 75, "text": "ct images"}, {"st": 111, "ed": 113, "text": "success rate"}, {"st": 116, "ed": 118, "text": "classification accuracy"}]
[{"st": 5, "ed": 8, "text": "deep neural networks"}, {"st": 18, "ed": 20, "text": "optimal solutions"}, {"st": 40, "ed": 43, "text": "deep neural networks"}, {"st": 58, "ed": 60, "text": "generative network"}, {"st": 63, "ed": 65, "text": "synthetic data"}, {"st": 90, "ed": 92, "text": "decision boundaries"}, {"st": 111, "ed": 113, "text": "cifar 10"}, {"st": 126, "ed": 128, "text": "generative network"}, {"st": 130, "ed": 132, "text": "catastrophic forgetting"}, {"st": 149, "ed": 151, "text": "generative network"}, {"st": 171, "ed": 173, "text": "catastrophic forgetting"}, {"st": 187, "ed": 189, "text": "higher dimensional"}]
[{"st": 9, "ed": 11, "text": "machine learning"}, {"st": 14, "ed": 16, "text": "image classification"}, {"st": 21, "ed": 23, "text": "dataset size"}, {"st": 31, "ed": 33, "text": "image classification"}, {"st": 40, "ed": 42, "text": "training data"}, {"st": 76, "ed": 78, "text": "image classification"}, {"st": 96, "ed": 98, "text": "randomly chosen"}, {"st": 100, "ed": 102, "text": "training data"}, {"st": 117, "ed": 119, "text": "randomly selected"}, {"st": 121, "ed": 123, "text": "training set"}, {"st": 139, "ed": 141, "text": "significantly improved"}, {"st": 141, "ed": 143, "text": "classification accuracy"}, {"st": 153, "ed": 155, "text": "error rate"}, {"st": 175, "ed": 177, "text": "cifar 10"}, {"st": 186, "ed": 188, "text": "improved accuracy"}, {"st": 195, "ed": 197, "text": "training set"}, {"st": 213, "ed": 215, "text": "training data"}]
[{"st": 0, "ed": 2, "text": "neural networks"}, {"st": 11, "ed": 13, "text": "wide variety"}, {"st": 18, "ed": 20, "text": "affine transformation"}, {"st": 101, "ed": 103, "text": "hadamard matrix"}, {"st": 118, "ed": 120, "text": "neural network"}]
[{"st": 3, "ed": 5, "text": "complex data"}, {"st": 33, "ed": 35, "text": "optimization process"}, {"st": 38, "ed": 40, "text": "expert knowledge"}]
[{"st": 2, "ed": 4, "text": "meta learning"}, {"st": 24, "ed": 26, "text": "meta learning"}, {"st": 61, "ed": 63, "text": "task specific"}, {"st": 70, "ed": 72, "text": "task specific"}, {"st": 87, "ed": 89, "text": "distance metric"}, {"st": 115, "ed": 117, "text": "task specific"}, {"st": 134, "ed": 136, "text": "learning rates"}, {"st": 140, "ed": 142, "text": "meta learning"}, {"st": 144, "ed": 146, "text": "method achieves"}, {"st": 151, "ed": 153, "text": "comparable performance"}, {"st": 154, "ed": 156, "text": "few shot"}, {"st": 156, "ed": 159, "text": "classification and regression"}]
[{"st": 29, "ed": 31, "text": "optimal transport"}, {"st": 50, "ed": 52, "text": "machine learning"}, {"st": 55, "ed": 57, "text": "natural language"}, {"st": 59, "ed": 61, "text": "computer vision"}, {"st": 87, "ed": 89, "text": "wasserstein distance"}, {"st": 128, "ed": 130, "text": "wasserstein distance"}]
[{"st": 3, "ed": 5, "text": "component analysis"}, {"st": 10, "ed": 12, "text": "compact representations"}, {"st": 19, "ed": 21, "text": "feature extraction"}, {"st": 21, "ed": 23, "text": "dimensionality reduction"}, {"st": 46, "ed": 48, "text": "k svd"}, {"st": 52, "ed": 54, "text": "sparse coding"}, {"st": 55, "ed": 59, "text": "singular value decomposition svd"}, {"st": 69, "ed": 71, "text": "visual data"}, {"st": 80, "ed": 82, "text": "component analysis"}, {"st": 88, "ed": 90, "text": "component analysis"}, {"st": 95, "ed": 97, "text": "low complexity"}, {"st": 98, "ed": 100, "text": "low rank"}, {"st": 140, "ed": 142, "text": "component analysis"}, {"st": 147, "ed": 149, "text": "component analysis"}, {"st": 154, "ed": 157, "text": "sparse dictionary learning"}, {"st": 176, "ed": 178, "text": "low rank"}, {"st": 184, "ed": 186, "text": "significantly smaller"}, {"st": 190, "ed": 192, "text": "efficient learning"}, {"st": 215, "ed": 217, "text": "proposed approach"}, {"st": 220, "ed": 222, "text": "real world"}, {"st": 227, "ed": 229, "text": "image denoising"}]
[{"st": 10, "ed": 12, "text": "efficiently compute"}, {"st": 50, "ed": 52, "text": "objective functions"}, {"st": 53, "ed": 55, "text": "linear programming"}, {"st": 60, "ed": 62, "text": "map estimation"}, {"st": 70, "ed": 72, "text": "worst case"}, {"st": 78, "ed": 80, "text": "potts model"}, {"st": 87, "ed": 89, "text": "worst case"}, {"st": 102, "ed": 104, "text": "efficiently compute"}, {"st": 118, "ed": 120, "text": "recently proposed"}, {"st": 124, "ed": 128, "text": "synthetic and real data"}, {"st": 139, "ed": 142, "text": "log partition function"}, {"st": 148, "ed": 150, "text": "message passing"}]
[{"st": 7, "ed": 9, "text": "surrogate models"}, {"st": 10, "ed": 12, "text": "uncertainty quantification"}, {"st": 22, "ed": 24, "text": "deep convolutional"}, {"st": 24, "ed": 26, "text": "encoder decoder"}, {"st": 35, "ed": 37, "text": "deep learning"}, {"st": 38, "ed": 41, "text": "image to image"}, {"st": 45, "ed": 47, "text": "neural networks"}, {"st": 61, "ed": 63, "text": "convolutional neural"}, {"st": 65, "ed": 67, "text": "recently introduced"}, {"st": 79, "ed": 82, "text": "deep convolutional networks"}, {"st": 84, "ed": 86, "text": "approximate bayesian"}, {"st": 94, "ed": 96, "text": "approach achieves"}, {"st": 104, "ed": 106, "text": "predictive accuracy"}, {"st": 107, "ed": 109, "text": "uncertainty quantification"}, {"st": 116, "ed": 118, "text": "neural networks"}, {"st": 124, "ed": 126, "text": "gaussian processes"}, {"st": 127, "ed": 129, "text": "ensemble methods"}, {"st": 132, "ed": 134, "text": "training data"}, {"st": 148, "ed": 150, "text": "uncertainty quantification"}, {"st": 150, "ed": 152, "text": "benchmark problems"}, {"st": 161, "ed": 163, "text": "limited data"}, {"st": 170, "ed": 172, "text": "surrogate model"}, {"st": 181, "ed": 183, "text": "underlying structure"}, {"st": 200, "ed": 203, "text": "image to image"}, {"st": 207, "ed": 209, "text": "computer vision"}, {"st": 226, "ed": 228, "text": "uncertainty quantification"}, {"st": 239, "ed": 241, "text": "bayesian statistics"}, {"st": 247, "ed": 249, "text": "monte carlo"}]
[{"st": 0, "ed": 2, "text": "activation functions"}, {"st": 7, "ed": 9, "text": "neural networks"}, {"st": 11, "ed": 14, "text": "training and testing"}, {"st": 24, "ed": 26, "text": "activation function"}, {"st": 28, "ed": 32, "text": "rectified linear unit relu"}, {"st": 40, "ed": 42, "text": "activation function"}, {"st": 42, "ed": 44, "text": "closely related"}, {"st": 132, "ed": 134, "text": "https github.com"}]
[{"st": 1, "ed": 4, "text": "deep neural networks"}, {"st": 10, "ed": 12, "text": "artificial intelligence"}, {"st": 16, "ed": 18, "text": "training instances"}, {"st": 37, "ed": 40, "text": "massive amounts of"}, {"st": 48, "ed": 51, "text": "deep neural network"}, {"st": 89, "ed": 91, "text": "image recognition"}, {"st": 102, "ed": 104, "text": "relative error"}, {"st": 117, "ed": 119, "text": "deep residual"}, {"st": 125, "ed": 128, "text": "partial differential equation"}]
[{"st": 4, "ed": 8, "text": "convolutional neural networks cnns"}, {"st": 9, "ed": 11, "text": "computer vision"}, {"st": 50, "ed": 52, "text": "time consuming"}, {"st": 79, "ed": 81, "text": "active learning"}, {"st": 82, "ed": 84, "text": "transfer learning"}, {"st": 84, "ed": 86, "text": "fine tuning"}, {"st": 89, "ed": 91, "text": "framework called"}, {"st": 97, "ed": 99, "text": "pre trained"}, {"st": 110, "ed": 112, "text": "fine tuned"}, {"st": 177, "ed": 180, "text": "computer aided diagnosis"}, {"st": 192, "ed": 194, "text": "computer vision"}, {"st": 195, "ed": 197, "text": "image analysis"}]
[{"st": 1, "ed": 3, "text": "neural networks"}, {"st": 4, "ed": 6, "text": "decision trees"}, {"st": 8, "ed": 10, "text": "machine learning"}, {"st": 27, "ed": 29, "text": "base classifiers"}, {"st": 44, "ed": 46, "text": "decision tree"}, {"st": 49, "ed": 51, "text": "linear classifier"}, {"st": 63, "ed": 65, "text": "neural network"}, {"st": 82, "ed": 84, "text": "closed form"}, {"st": 91, "ed": 93, "text": "neural network"}, {"st": 121, "ed": 123, "text": "decision trees"}, {"st": 129, "ed": 131, "text": "proposed method"}, {"st": 157, "ed": 159, "text": "fine grained"}, {"st": 174, "ed": 176, "text": "multi class"}]
[{"st": 5, "ed": 7, "text": "neural networks"}, {"st": 9, "ed": 12, "text": "vulnerable to adversarial"}, {"st": 34, "ed": 36, "text": "training objective"}, {"st": 47, "ed": 49, "text": "network architectures"}, {"st": 53, "ed": 55, "text": "ell 1"}, {"st": 62, "ed": 64, "text": "square root"}, {"st": 112, "ed": 114, "text": "regularization scheme"}, {"st": 132, "ed": 134, "text": "pooling layers"}, {"st": 144, "ed": 146, "text": "weight distribution"}, {"st": 149, "ed": 151, "text": "extensive experiments"}]
[{"st": 9, "ed": 11, "text": "great importance"}, {"st": 15, "ed": 17, "text": "existing approaches"}, {"st": 78, "ed": 81, "text": "deep neural networks"}, {"st": 118, "ed": 120, "text": "real datasets"}]
[{"st": 42, "ed": 44, "text": "100 000"}, {"st": 70, "ed": 72, "text": "deep learning"}, {"st": 80, "ed": 82, "text": "small sample"}, {"st": 98, "ed": 100, "text": "ground truth"}, {"st": 120, "ed": 122, "text": "weakly supervised"}, {"st": 125, "ed": 127, "text": "image level"}, {"st": 166, "ed": 168, "text": "pre trained"}, {"st": 168, "ed": 171, "text": "deep convolutional networks"}, {"st": 171, "ed": 173, "text": "feature embedding"}, {"st": 184, "ed": 187, "text": "multiple instance learning"}]
[{"st": 6, "ed": 8, "text": "radio telescopes"}, {"st": 111, "ed": 115, "text": "generative adversarial network gan"}, {"st": 129, "ed": 131, "text": "loss function"}, {"st": 135, "ed": 137, "text": "adversarial loss"}, {"st": 142, "ed": 144, "text": "adversarial loss"}, {"st": 149, "ed": 151, "text": "natural image"}]
[{"st": 3, "ed": 6, "text": "generative adversarial networks"}, {"st": 8, "ed": 10, "text": "difficult task"}, {"st": 59, "ed": 62, "text": "ability to learn"}, {"st": 78, "ed": 80, "text": "mode collapse"}]
[{"st": 4, "ed": 6, "text": "batch normalization"}, {"st": 13, "ed": 16, "text": "deep neural networks"}, {"st": 26, "ed": 28, "text": "internal representation"}, {"st": 75, "ed": 77, "text": "method called"}, {"st": 99, "ed": 101, "text": "existing solutions"}, {"st": 103, "ed": 105, "text": "hidden layer"}, {"st": 156, "ed": 158, "text": "faster convergence"}, {"st": 187, "ed": 189, "text": "image classification"}, {"st": 219, "ed": 221, "text": "comparable accuracy"}, {"st": 224, "ed": 226, "text": "batch size"}, {"st": 232, "ed": 234, "text": "cifar 10"}]
[{"st": 4, "ed": 6, "text": "deep learning"}, {"st": 34, "ed": 36, "text": "primary care"}, {"st": 72, "ed": 74, "text": "machine learning"}, {"st": 89, "ed": 91, "text": "supervised classification"}, {"st": 109, "ed": 111, "text": "deep learning"}]
[{"st": 3, "ed": 5, "text": "neural networks"}, {"st": 26, "ed": 28, "text": "strong assumptions"}, {"st": 29, "ed": 31, "text": "network structures"}, {"st": 33, "ed": 35, "text": "computational costs"}, {"st": 57, "ed": 59, "text": "computationally efficient"}, {"st": 66, "ed": 68, "text": "adversarial perturbations"}, {"st": 79, "ed": 81, "text": "training procedure"}]
[{"st": 3, "ed": 7, "text": "convolutional neural network cnn"}, {"st": 23, "ed": 25, "text": "convolutional filters"}, {"st": 38, "ed": 40, "text": "convolutional filters"}, {"st": 59, "ed": 61, "text": "trainable parameters"}, {"st": 72, "ed": 74, "text": "extensive experiments"}, {"st": 82, "ed": 84, "text": "image classification"}, {"st": 87, "ed": 89, "text": "significant reduction"}]
[{"st": 63, "ed": 65, "text": "accurately predict"}, {"st": 144, "ed": 146, "text": "real world"}]
[{"st": 0, "ed": 2, "text": "structured prediction"}, {"st": 20, "ed": 22, "text": "score function"}, {"st": 32, "ed": 34, "text": "neural networks"}, {"st": 38, "ed": 40, "text": "score function"}, {"st": 52, "ed": 54, "text": "neural network"}, {"st": 57, "ed": 59, "text": "structured output"}, {"st": 74, "ed": 76, "text": "desired properties"}, {"st": 117, "ed": 119, "text": "deep structured"}, {"st": 126, "ed": 128, "text": "challenging problem"}, {"st": 132, "ed": 134, "text": "scene graph"}]
[{"st": 67, "ed": 69, "text": "significantly improve"}, {"st": 73, "ed": 75, "text": "class conditional"}, {"st": 75, "ed": 77, "text": "image generation"}, {"st": 82, "ed": 84, "text": "image dataset"}, {"st": 115, "ed": 117, "text": "super resolution"}, {"st": 123, "ed": 125, "text": "super resolution"}, {"st": 142, "ed": 144, "text": "batch normalization"}]
[{"st": 8, "ed": 11, "text": "generative adversarial networks"}, {"st": 26, "ed": 28, "text": "technique called"}]
[{"st": 10, "ed": 12, "text": "conventional approaches"}, {"st": 13, "ed": 15, "text": "machine teaching"}, {"st": 21, "ed": 23, "text": "instance level"}, {"st": 43, "ed": 45, "text": "significantly improve"}, {"st": 48, "ed": 51, "text": "ability to learn"}, {"st": 89, "ed": 91, "text": "automatically generate"}]
[{"st": 8, "ed": 11, "text": "deep convolutional networks"}, {"st": 140, "ed": 142, "text": "learned representations"}, {"st": 150, "ed": 152, "text": "deep networks"}, {"st": 180, "ed": 182, "text": "natural image"}]
[{"st": 25, "ed": 29, "text": "convolutional neural networks cnn"}, {"st": 34, "ed": 36, "text": "great potential"}, {"st": 57, "ed": 60, "text": "block coordinate descent"}, {"st": 61, "ed": 63, "text": "optimization method"}, {"st": 76, "ed": 81, "text": "alternating direction method of multipliers"}, {"st": 87, "ed": 89, "text": "numerical experiments"}, {"st": 96, "ed": 100, "text": "signal to noise ratio"}, {"st": 105, "ed": 107, "text": "magnetic resonance"}]
[{"st": 7, "ed": 9, "text": "continuous relaxation"}, {"st": 10, "ed": 12, "text": "map inference"}, {"st": 14, "ed": 18, "text": "markov random fields mrfs"}, {"st": 32, "ed": 34, "text": "stationary point"}, {"st": 43, "ed": 46, "text": "block coordinate descent"}, {"st": 75, "ed": 81, "text": "alternating direction method of multipliers admm"}, {"st": 85, "ed": 87, "text": "real world"}, {"st": 93, "ed": 95, "text": "significantly outperforms"}, {"st": 98, "ed": 100, "text": "based methods"}, {"st": 101, "ed": 104, "text": "compares favorably with"}, {"st": 109, "ed": 111, "text": "optimization algorithms"}]
[{"st": 10, "ed": 12, "text": "random noise"}, {"st": 31, "ed": 33, "text": "gaussian noise"}, {"st": 43, "ed": 45, "text": "random noise"}, {"st": 52, "ed": 54, "text": "decision boundary"}, {"st": 61, "ed": 63, "text": "linear classifiers"}, {"st": 71, "ed": 73, "text": "decision boundaries"}]
[{"st": 0, "ed": 2, "text": "machine learning"}, {"st": 4, "ed": 8, "text": "vulnerable to adversarial examples"}, {"st": 14, "ed": 16, "text": "computer vision"}, {"st": 24, "ed": 26, "text": "school bus"}, {"st": 34, "ed": 36, "text": "open question"}, {"st": 48, "ed": 50, "text": "adversarial examples"}, {"st": 60, "ed": 62, "text": "adversarial examples"}, {"st": 63, "ed": 65, "text": "computer vision"}, {"st": 98, "ed": 100, "text": "adversarial examples"}, {"st": 104, "ed": 106, "text": "computer vision"}]
[{"st": 1, "ed": 3, "text": "batch size"}, {"st": 5, "ed": 7, "text": "neural networks"}, {"st": 12, "ed": 14, "text": "accuracy loss"}, {"st": 35, "ed": 37, "text": "batch size"}, {"st": 69, "ed": 71, "text": "batch size"}, {"st": 95, "ed": 97, "text": "at large"}, {"st": 97, "ed": 99, "text": "batch sizes"}, {"st": 107, "ed": 109, "text": "parameter space"}, {"st": 129, "ed": 131, "text": "batch size"}, {"st": 156, "ed": 158, "text": "adversarial attacks"}, {"st": 171, "ed": 173, "text": "theoretical result"}, {"st": 181, "ed": 183, "text": "adversarial perturbation"}, {"st": 192, "ed": 194, "text": "empirical results"}, {"st": 197, "ed": 199, "text": "adversarial training"}, {"st": 213, "ed": 215, "text": "network architectures"}, {"st": 217, "ed": 220, "text": "mnist cifar 10"}, {"st": 221, "ed": 223, "text": "cifar 100"}]
[{"st": 14, "ed": 16, "text": "deep networks"}, {"st": 24, "ed": 26, "text": "adversarial perturbations"}, {"st": 41, "ed": 43, "text": "generative model"}, {"st": 60, "ed": 62, "text": "adversarial perturbations"}, {"st": 64, "ed": 66, "text": "latent space"}, {"st": 67, "ed": 69, "text": "sufficiently large"}, {"st": 71, "ed": 73, "text": "generative model"}, {"st": 84, "ed": 86, "text": "adversarial perturbations"}]
[{"st": 0, "ed": 2, "text": "deep learning"}, {"st": 6, "ed": 8, "text": "artificial intelligence"}, {"st": 27, "ed": 29, "text": "deep learning"}, {"st": 75, "ed": 77, "text": "deep learning"}, {"st": 105, "ed": 107, "text": "deep learning"}, {"st": 114, "ed": 116, "text": "deep learning"}, {"st": 129, "ed": 131, "text": "deep learning"}, {"st": 156, "ed": 158, "text": "deep learning"}, {"st": 168, "ed": 170, "text": "deep learning"}]
[{"st": 0, "ed": 2, "text": "domain adaptation"}, {"st": 8, "ed": 10, "text": "labeled data"}, {"st": 12, "ed": 14, "text": "source domain"}, {"st": 43, "ed": 45, "text": "domain adversarial"}, {"st": 54, "ed": 56, "text": "feature extractor"}, {"st": 59, "ed": 62, "text": "source and target"}, {"st": 69, "ed": 71, "text": "domain adversarial"}, {"st": 79, "ed": 81, "text": "feature extraction"}, {"st": 87, "ed": 89, "text": "distribution matching"}, {"st": 97, "ed": 99, "text": "domain adaptation"}, {"st": 109, "ed": 113, "text": "source and target domains"}, {"st": 121, "ed": 123, "text": "source domain"}, {"st": 144, "ed": 146, "text": "decision boundaries"}, {"st": 159, "ed": 161, "text": "models 1"}, {"st": 164, "ed": 166, "text": "domain adaptation"}, {"st": 170, "ed": 172, "text": "domain adversarial"}, {"st": 175, "ed": 177, "text": "penalty term"}, {"st": 186, "ed": 188, "text": "decision boundary"}, {"st": 206, "ed": 208, "text": "natural gradient"}, {"st": 216, "ed": 218, "text": "extensive empirical"}, {"st": 227, "ed": 229, "text": "significantly improve"}, {"st": 238, "ed": 240, "text": "traffic sign"}, {"st": 241, "ed": 243, "text": "wi fi"}, {"st": 244, "ed": 246, "text": "domain adaptation"}]
[{"st": 0, "ed": 3, "text": "deep neural networks"}, {"st": 14, "ed": 16, "text": "real world"}, {"st": 21, "ed": 23, "text": "performance gains"}, {"st": 82, "ed": 85, "text": "fully connected layers"}, {"st": 87, "ed": 89, "text": "convolutional layers"}, {"st": 132, "ed": 134, "text": "image classification"}, {"st": 137, "ed": 139, "text": "compression scheme"}, {"st": 145, "ed": 147, "text": "deep learning"}, {"st": 150, "ed": 152, "text": "resource constrained"}]
[{"st": 4, "ed": 6, "text": "active learning"}, {"st": 29, "ed": 31, "text": "active learning"}, {"st": 34, "ed": 36, "text": "deep networks"}, {"st": 64, "ed": 66, "text": "active learning"}, {"st": 87, "ed": 89, "text": "decision boundaries"}, {"st": 119, "ed": 121, "text": "input space"}, {"st": 131, "ed": 133, "text": "demonstrate empirically"}, {"st": 138, "ed": 140, "text": "faster convergence"}]
[{"st": 15, "ed": 18, "text": "stochastic gradient descent"}, {"st": 18, "ed": 20, "text": "vr sgd"}, {"st": 41, "ed": 43, "text": "vr sgd"}, {"st": 64, "ed": 66, "text": "learning rates"}, {"st": 70, "ed": 72, "text": "convergence analysis"}, {"st": 79, "ed": 81, "text": "update rules"}, {"st": 86, "ed": 88, "text": "objective functions"}, {"st": 92, "ed": 94, "text": "vr sgd"}, {"st": 101, "ed": 103, "text": "strongly convex"}, {"st": 113, "ed": 115, "text": "convergence properties"}, {"st": 116, "ed": 118, "text": "vr sgd"}, {"st": 119, "ed": 121, "text": "strongly convex"}, {"st": 125, "ed": 127, "text": "vr sgd"}, {"st": 137, "ed": 139, "text": "convergence guarantees"}, {"st": 141, "ed": 143, "text": "strongly convex"}, {"st": 148, "ed": 150, "text": "convergence guarantees"}, {"st": 151, "ed": 153, "text": "vr sgd"}, {"st": 160, "ed": 162, "text": "vr sgd"}, {"st": 164, "ed": 166, "text": "learning rates"}, {"st": 167, "ed": 169, "text": "similar performance"}, {"st": 178, "ed": 180, "text": "convergence rate"}, {"st": 180, "ed": 182, "text": "mathcal o"}, {"st": 189, "ed": 191, "text": "vr sgd"}, {"st": 194, "ed": 196, "text": "machine learning"}, {"st": 203, "ed": 206, "text": "empirical risk minimization"}, {"st": 216, "ed": 218, "text": "vr sgd"}, {"st": 219, "ed": 221, "text": "significantly faster"}]
[{"st": 13, "ed": 15, "text": "dimensionality reduction"}, {"st": 26, "ed": 28, "text": "clustering methods"}, {"st": 134, "ed": 136, "text": "synthetic data"}]
[{"st": 1, "ed": 3, "text": "few shot"}, {"st": 8, "ed": 10, "text": "learning algorithms"}, {"st": 21, "ed": 23, "text": "recent progress"}, {"st": 24, "ed": 26, "text": "few shot"}, {"st": 29, "ed": 31, "text": "meta learning"}, {"st": 38, "ed": 40, "text": "learning algorithm"}, {"st": 48, "ed": 50, "text": "classification problems"}, {"st": 54, "ed": 56, "text": "labeled training"}, {"st": 68, "ed": 70, "text": "few shot"}, {"st": 76, "ed": 78, "text": "unlabeled examples"}, {"st": 91, "ed": 93, "text": "unlabeled examples"}, {"st": 105, "ed": 107, "text": "labeled examples"}, {"st": 135, "ed": 137, "text": "prototypical networks"}, {"st": 149, "ed": 151, "text": "unlabeled examples"}, {"st": 160, "ed": 163, "text": "end to end"}, {"st": 171, "ed": 173, "text": "unlabeled examples"}, {"st": 215, "ed": 217, "text": "experiments confirm"}, {"st": 219, "ed": 221, "text": "prototypical networks"}, {"st": 229, "ed": 231, "text": "unlabeled examples"}, {"st": 234, "ed": 236, "text": "semi supervised"}]
[{"st": 23, "ed": 25, "text": "semi supervised"}, {"st": 25, "ed": 27, "text": "deep learning"}, {"st": 28, "ed": 30, "text": "achieve high"}, {"st": 30, "ed": 32, "text": "generalization performance"}, {"st": 34, "ed": 38, "text": "deep convolutional neural network"}, {"st": 53, "ed": 55, "text": "labeled data"}, {"st": 62, "ed": 64, "text": "unlabeled data"}, {"st": 71, "ed": 73, "text": "labeled data"}, {"st": 80, "ed": 83, "text": "expectation maximization algorithm"}, {"st": 91, "ed": 93, "text": "unlabeled data"}, {"st": 130, "ed": 132, "text": "active learning"}, {"st": 154, "ed": 156, "text": "mnist dataset"}, {"st": 159, "ed": 161, "text": "error rate"}, {"st": 180, "ed": 182, "text": "network architecture"}]
[{"st": 0, "ed": 2, "text": "deep learning"}, {"st": 6, "ed": 10, "text": "convolutional neural networks cnns"}, {"st": 21, "ed": 23, "text": "computer vision"}, {"st": 27, "ed": 29, "text": "large scale"}, {"st": 56, "ed": 58, "text": "deep learning"}, {"st": 58, "ed": 62, "text": "generative adversarial networks gans"}, {"st": 74, "ed": 76, "text": "synthetic data"}, {"st": 97, "ed": 100, "text": "computed tomography ct"}, {"st": 148, "ed": 150, "text": "synthetic data"}, {"st": 170, "ed": 172, "text": "classification performance"}, {"st": 186, "ed": 188, "text": "synthetic data"}, {"st": 204, "ed": 206, "text": "synthetic data"}]
[{"st": 11, "ed": 14, "text": "generative adversarial nets"}, {"st": 50, "ed": 52, "text": "wasserstein gan"}, {"st": 77, "ed": 79, "text": "training procedure"}, {"st": 91, "ed": 94, "text": "semi supervised learning"}, {"st": 105, "ed": 107, "text": "photo realistic"}, {"st": 110, "ed": 112, "text": "previous methods"}, {"st": 118, "ed": 121, "text": "semi supervised learning"}, {"st": 130, "ed": 132, "text": "inception score"}, {"st": 140, "ed": 142, "text": "cifar 10"}, {"st": 155, "ed": 157, "text": "cifar 10"}, {"st": 162, "ed": 164, "text": "labeled images"}]
[{"st": 0, "ed": 3, "text": "distance metric learning"}, {"st": 13, "ed": 15, "text": "nearest neighbor"}, {"st": 48, "ed": 51, "text": "distance metric learning"}, {"st": 54, "ed": 56, "text": "mahalanobis distance"}, {"st": 66, "ed": 68, "text": "nearest neighbor"}, {"st": 77, "ed": 79, "text": "distance metrics"}, {"st": 120, "ed": 122, "text": "highly nonlinear"}, {"st": 132, "ed": 134, "text": "proposed method"}]
[{"st": 11, "ed": 13, "text": "time consuming"}, {"st": 21, "ed": 23, "text": "active learning"}, {"st": 57, "ed": 59, "text": "active learning"}, {"st": 99, "ed": 101, "text": "multi class"}, {"st": 101, "ed": 103, "text": "active learning"}, {"st": 155, "ed": 158, "text": "empirical risk minimization"}, {"st": 161, "ed": 163, "text": "active learning"}, {"st": 166, "ed": 168, "text": "proposed method"}, {"st": 178, "ed": 180, "text": "active learning"}, {"st": 188, "ed": 190, "text": "proposed method"}, {"st": 219, "ed": 221, "text": "proposed method"}]
[{"st": 2, "ed": 4, "text": "remarkable success"}, {"st": 7, "ed": 11, "text": "convolutional neural networks cnns"}, {"st": 12, "ed": 14, "text": "object recognition"}, {"st": 15, "ed": 17, "text": "deep learning"}, {"st": 23, "ed": 25, "text": "computer vision"}, {"st": 27, "ed": 29, "text": "metric learning"}, {"st": 31, "ed": 33, "text": "deep learning"}, {"st": 35, "ed": 37, "text": "metric learning"}, {"st": 56, "ed": 58, "text": "method called"}, {"st": 89, "ed": 91, "text": "large margin"}, {"st": 110, "ed": 112, "text": "intra class"}, {"st": 114, "ed": 116, "text": "inter class"}, {"st": 127, "ed": 129, "text": "object recognition"}, {"st": 135, "ed": 137, "text": "comparative study"}, {"st": 153, "ed": 155, "text": "extensive experiments"}, {"st": 157, "ed": 159, "text": "object recognition"}, {"st": 161, "ed": 163, "text": "two face"}]
[{"st": 9, "ed": 11, "text": "machine learning"}, {"st": 62, "ed": 64, "text": "monte carlo"}]
[{"st": 66, "ed": 68, "text": "real life"}, {"st": 112, "ed": 114, "text": "probabilistic model"}, {"st": 117, "ed": 119, "text": "multiple modalities"}, {"st": 132, "ed": 134, "text": "missing entries"}, {"st": 165, "ed": 167, "text": "approximate bayesian"}, {"st": 194, "ed": 196, "text": "fine tune"}, {"st": 231, "ed": 233, "text": "empirical analysis"}, {"st": 234, "ed": 236, "text": "prediction performance"}, {"st": 258, "ed": 260, "text": "temporal dynamics"}]
[{"st": 59, "ed": 61, "text": "text based"}, {"st": 64, "ed": 66, "text": "extracted features"}]
[{"st": 0, "ed": 2, "text": "subspace clustering"}, {"st": 8, "ed": 10, "text": "computer vision"}, {"st": 14, "ed": 16, "text": "intrinsic dimension"}, {"st": 27, "ed": 29, "text": "spectral clustering"}, {"st": 36, "ed": 38, "text": "subspace clustering"}, {"st": 42, "ed": 44, "text": "sparse representation"}, {"st": 46, "ed": 48, "text": "low rank"}, {"st": 61, "ed": 63, "text": "existing methods"}, {"st": 66, "ed": 68, "text": "large scale"}, {"st": 68, "ed": 70, "text": "convex optimization"}, {"st": 78, "ed": 80, "text": "computational complexity"}, {"st": 90, "ed": 93, "text": "efficiency and scalability"}, {"st": 95, "ed": 97, "text": "spectral clustering"}, {"st": 103, "ed": 105, "text": "large scale"}, {"st": 112, "ed": 114, "text": "subspace clustering"}, {"st": 131, "ed": 133, "text": "diagonal matrix"}, {"st": 141, "ed": 143, "text": "diagonal matrix"}, {"st": 145, "ed": 147, "text": "efficiently learn"}, {"st": 166, "ed": 168, "text": "significantly reduces"}, {"st": 169, "ed": 171, "text": "computational complexity"}, {"st": 192, "ed": 194, "text": "subspace clustering"}, {"st": 205, "ed": 207, "text": "significantly improves"}, {"st": 220, "ed": 222, "text": "conducted experiments"}, {"st": 223, "ed": 225, "text": "synthetic data"}, {"st": 233, "ed": 235, "text": "large scale"}]
[{"st": 4, "ed": 6, "text": "theoretical understanding"}, {"st": 6, "ed": 9, "text": "deep neural networks"}, {"st": 24, "ed": 26, "text": "representation learning"}, {"st": 27, "ed": 29, "text": "component analysis"}, {"st": 50, "ed": 52, "text": "component analysis"}, {"st": 60, "ed": 62, "text": "hierarchical structure"}, {"st": 65, "ed": 67, "text": "latent variables"}, {"st": 76, "ed": 78, "text": "optimization algorithm"}, {"st": 81, "ed": 83, "text": "alternating direction"}, {"st": 83, "ed": 85, "text": "neural networks"}, {"st": 88, "ed": 90, "text": "parameter learning"}, {"st": 95, "ed": 97, "text": "feed forward"}, {"st": 130, "ed": 132, "text": "performance improvements"}, {"st": 136, "ed": 138, "text": "tasks including"}]
[{"st": 15, "ed": 17, "text": "deep architectures"}, {"st": 67, "ed": 70, "text": "stochastic gradient descent"}, {"st": 75, "ed": 78, "text": "deep neural networks"}, {"st": 135, "ed": 137, "text": "convergence guarantees"}, {"st": 165, "ed": 167, "text": "faster training"}, {"st": 187, "ed": 189, "text": "recently proposed"}]
[{"st": 5, "ed": 7, "text": "data fusion"}, {"st": 9, "ed": 12, "text": "multi view data"}, {"st": 20, "ed": 23, "text": "takes into account"}, {"st": 25, "ed": 27, "text": "multi view"}, {"st": 41, "ed": 43, "text": "data fusion"}, {"st": 56, "ed": 58, "text": "neural network"}, {"st": 64, "ed": 67, "text": "task of classifying"}, {"st": 113, "ed": 115, "text": "multi view"}, {"st": 133, "ed": 135, "text": "method outperforms"}]
[{"st": 12, "ed": 14, "text": "gaussian process"}, {"st": 29, "ed": 31, "text": "training data"}, {"st": 71, "ed": 73, "text": "kernel space"}, {"st": 85, "ed": 87, "text": "multi modal"}, {"st": 102, "ed": 104, "text": "demonstrate empirically"}]
[{"st": 1, "ed": 4, "text": "convolutional neural networks"}, {"st": 13, "ed": 15, "text": "vision tasks"}, {"st": 18, "ed": 22, "text": "vulnerable to adversarial examples"}, {"st": 34, "ed": 36, "text": "adversarial examples"}, {"st": 53, "ed": 55, "text": "safety critical"}, {"st": 61, "ed": 63, "text": "adversarial attacks"}, {"st": 97, "ed": 99, "text": "adversarial examples"}, {"st": 113, "ed": 115, "text": "adversarial examples"}, {"st": 122, "ed": 124, "text": "input images"}, {"st": 127, "ed": 129, "text": "extensive experiments"}, {"st": 139, "ed": 141, "text": "adversarial examples"}, {"st": 160, "ed": 162, "text": "recently proposed"}, {"st": 198, "ed": 200, "text": "success rate"}, {"st": 215, "ed": 217, "text": "large margin"}, {"st": 253, "ed": 257, "text": "available at https github.com"}]
[{"st": 0, "ed": 2, "text": "lung cancer"}, {"st": 17, "ed": 19, "text": "computed tomography"}, {"st": 52, "ed": 54, "text": "ct images"}, {"st": 74, "ed": 78, "text": "convolutional neural networks cnn"}, {"st": 87, "ed": 89, "text": "multi output"}, {"st": 109, "ed": 111, "text": "lidc idri"}, {"st": 121, "ed": 123, "text": "multi output"}, {"st": 131, "ed": 133, "text": "classification accuracy"}, {"st": 137, "ed": 140, "text": "end to end"}, {"st": 150, "ed": 152, "text": "lidc idri"}, {"st": 172, "ed": 174, "text": "prediction accuracy"}]
[{"st": 0, "ed": 2, "text": "adversarial attack"}, {"st": 30, "ed": 32, "text": "adversarial perturbations"}, {"st": 37, "ed": 39, "text": "input signal"}, {"st": 63, "ed": 65, "text": "training scheme"}, {"st": 70, "ed": 72, "text": "fine tuning"}, {"st": 144, "ed": 146, "text": "vgg 16"}, {"st": 146, "ed": 148, "text": "resnet 50"}, {"st": 157, "ed": 159, "text": "vgg 16"}, {"st": 167, "ed": 169, "text": "resnet 50"}]
[{"st": 34, "ed": 36, "text": "time consuming"}, {"st": 59, "ed": 61, "text": "challenging problem"}, {"st": 152, "ed": 155, "text": "qualitative and quantitative"}]
[{"st": 38, "ed": 40, "text": "case study"}]
[{"st": 73, "ed": 75, "text": "anecdotal evidence"}, {"st": 108, "ed": 110, "text": "task oriented"}]
[{"st": 29, "ed": 31, "text": "strategy game"}, {"st": 51, "ed": 53, "text": "temporal patterns"}]
[{"st": 6, "ed": 9, "text": "formal concept analysis"}, {"st": 90, "ed": 92, "text": "machine learning"}, {"st": 92, "ed": 94, "text": "data mining"}, {"st": 97, "ed": 99, "text": "text mining"}]
[{"st": 10, "ed": 12, "text": "e commerce"}, {"st": 46, "ed": 48, "text": "automatically identify"}, {"st": 62, "ed": 64, "text": "previous works"}, {"st": 74, "ed": 76, "text": "level features"}, {"st": 113, "ed": 115, "text": "user generated"}, {"st": 133, "ed": 135, "text": "latent factors"}, {"st": 145, "ed": 147, "text": "fine grained"}, {"st": 161, "ed": 164, "text": "hidden markov model"}, {"st": 164, "ed": 167, "text": "latent dirichlet allocation"}, {"st": 168, "ed": 170, "text": "lda based"}, {"st": 184, "ed": 186, "text": "large scale"}, {"st": 189, "ed": 191, "text": "real world"}, {"st": 195, "ed": 197, "text": "significant improvement"}]
[{"st": 1, "ed": 3, "text": "recommender systems"}, {"st": 29, "ed": 31, "text": "prior methods"}, {"st": 64, "ed": 66, "text": "user experience"}, {"st": 146, "ed": 149, "text": "hidden markov model"}, {"st": 178, "ed": 181, "text": "latent dirichlet allocation"}, {"st": 200, "ed": 202, "text": "real world"}, {"st": 228, "ed": 230, "text": "case study"}, {"st": 239, "ed": 241, "text": "user experience"}]
[{"st": 15, "ed": 17, "text": "user generated"}, {"st": 38, "ed": 40, "text": "user generated"}, {"st": 53, "ed": 55, "text": "distant supervision"}, {"st": 64, "ed": 67, "text": "probabilistic graphical model"}, {"st": 68, "ed": 70, "text": "jointly learns"}, {"st": 101, "ed": 103, "text": "large scale"}]
[{"st": 115, "ed": 118, "text": "probabilistic graphical model"}, {"st": 155, "ed": 157, "text": "real valued"}, {"st": 163, "ed": 165, "text": "fine grained"}]
[{"st": 43, "ed": 45, "text": "prior works"}, {"st": 89, "ed": 91, "text": "latent topic"}, {"st": 146, "ed": 148, "text": "real world"}]
[{"st": 23, "ed": 25, "text": "recommender systems"}, {"st": 66, "ed": 68, "text": "user experience"}, {"st": 86, "ed": 89, "text": "geometric brownian motion"}, {"st": 89, "ed": 91, "text": "brownian motion"}, {"st": 92, "ed": 95, "text": "latent dirichlet allocation"}, {"st": 102, "ed": 104, "text": "user experience"}, {"st": 130, "ed": 132, "text": "extensive experiments"}, {"st": 134, "ed": 136, "text": "real world"}]
[{"st": 27, "ed": 29, "text": "prior works"}, {"st": 41, "ed": 43, "text": "strong assumptions"}, {"st": 67, "ed": 70, "text": "probabilistic graphical models"}, {"st": 83, "ed": 85, "text": "user interactions"}, {"st": 121, "ed": 124, "text": "conditional random fields"}, {"st": 130, "ed": 132, "text": "expert knowledge"}, {"st": 133, "ed": 136, "text": "semi supervised learning"}, {"st": 146, "ed": 148, "text": "fine grained"}, {"st": 196, "ed": 198, "text": "generative models"}, {"st": 200, "ed": 203, "text": "hidden markov model"}, {"st": 203, "ed": 206, "text": "latent dirichlet allocation"}, {"st": 207, "ed": 209, "text": "brownian motion"}, {"st": 241, "ed": 243, "text": "recommender systems"}]
[{"st": 2, "ed": 4, "text": "neural networks"}, {"st": 11, "ed": 13, "text": "large vocabulary"}, {"st": 52, "ed": 54, "text": "network depth"}, {"st": 71, "ed": 74, "text": "fully connected layers"}, {"st": 108, "ed": 112, "text": "convolutional neural network cnn"}, {"st": 146, "ed": 148, "text": "bi directional"}, {"st": 167, "ed": 171, "text": "word error rate wer"}]
[{"st": 39, "ed": 41, "text": "domain specific"}, {"st": 41, "ed": 43, "text": "problem solving"}, {"st": 129, "ed": 131, "text": "domain knowledge"}, {"st": 132, "ed": 134, "text": "problem solving"}, {"st": 160, "ed": 162, "text": "digital media"}, {"st": 217, "ed": 219, "text": "spatio temporal"}, {"st": 225, "ed": 227, "text": "problem solving"}, {"st": 229, "ed": 231, "text": "application areas"}]
[{"st": 60, "ed": 62, "text": "visual memory"}, {"st": 107, "ed": 110, "text": "images and videos"}, {"st": 111, "ed": 113, "text": "spatio temporal"}, {"st": 113, "ed": 115, "text": "natural language"}, {"st": 155, "ed": 157, "text": "spatial relations"}, {"st": 158, "ed": 160, "text": "natural language"}, {"st": 176, "ed": 178, "text": "online learning"}]
[{"st": 1, "ed": 3, "text": "natural language"}, {"st": 44, "ed": 46, "text": "natural language"}, {"st": 77, "ed": 79, "text": "natural language"}, {"st": 83, "ed": 85, "text": "artificial intelligence"}]
[{"st": 15, "ed": 17, "text": "great success"}, {"st": 62, "ed": 64, "text": "co occurrence"}, {"st": 107, "ed": 109, "text": "co occurrence"}, {"st": 131, "ed": 133, "text": "asymptotic analysis"}, {"st": 213, "ed": 215, "text": "open source"}, {"st": 218, "ed": 220, "text": "https github.com"}]
[{"st": 6, "ed": 8, "text": "fully convolutional"}, {"st": 8, "ed": 10, "text": "attention based"}, {"st": 11, "ed": 14, "text": "text to speech"}, {"st": 25, "ed": 27, "text": "speech synthesis"}, {"st": 69, "ed": 71, "text": "attention based"}, {"st": 71, "ed": 73, "text": "speech synthesis"}]
[{"st": 0, "ed": 3, "text": "words and phrases"}, {"st": 49, "ed": 52, "text": "words and phrases"}, {"st": 65, "ed": 68, "text": "world wide web"}, {"st": 82, "ed": 84, "text": "search engines"}, {"st": 104, "ed": 107, "text": "words and phrases"}, {"st": 109, "ed": 112, "text": "world wide web"}, {"st": 117, "ed": 120, "text": "world wide web"}, {"st": 128, "ed": 130, "text": "context information"}, {"st": 149, "ed": 151, "text": "hierarchical clustering"}, {"st": 229, "ed": 232, "text": "support vector machines"}]
[{"st": 3, "ed": 5, "text": "unified framework"}, {"st": 8, "ed": 10, "text": "natural language"}, {"st": 62, "ed": 64, "text": "natural language"}]
[{"st": 212, "ed": 214, "text": "monte carlo"}]
[{"st": 8, "ed": 10, "text": "web search"}]
[{"st": 7, "ed": 9, "text": "sentiment analysis"}, {"st": 11, "ed": 13, "text": "based method"}, {"st": 14, "ed": 16, "text": "machine learning"}, {"st": 33, "ed": 35, "text": "sentiment classification"}, {"st": 41, "ed": 43, "text": "comparative study"}, {"st": 57, "ed": 59, "text": "social media"}, {"st": 78, "ed": 80, "text": "feature selection"}, {"st": 82, "ed": 84, "text": "machine learning"}, {"st": 93, "ed": 95, "text": "sentiment analysis"}, {"st": 119, "ed": 121, "text": "machine learning"}, {"st": 126, "ed": 128, "text": "naive bayes"}, {"st": 137, "ed": 139, "text": "ensemble method"}, {"st": 151, "ed": 153, "text": "machine learning"}, {"st": 169, "ed": 171, "text": "cost sensitive"}, {"st": 180, "ed": 182, "text": "sentiment classification"}]
[{"st": 52, "ed": 55, "text": "deep neural networks"}, {"st": 63, "ed": 65, "text": "feature extraction"}]
[{"st": 5, "ed": 7, "text": "co occurrences"}, {"st": 39, "ed": 41, "text": "classification task"}, {"st": 44, "ed": 46, "text": "theoretical analysis"}, {"st": 72, "ed": 74, "text": "conditional probability"}, {"st": 77, "ed": 79, "text": "text corpus"}, {"st": 85, "ed": 87, "text": "co occurrence"}, {"st": 112, "ed": 114, "text": "co occurrences"}, {"st": 139, "ed": 141, "text": "feature learning"}, {"st": 142, "ed": 144, "text": "machine learning"}]
[{"st": 5, "ed": 7, "text": "variational autoencoder"}, {"st": 14, "ed": 16, "text": "sequential data"}, {"st": 22, "ed": 24, "text": "multi scale"}, {"st": 28, "ed": 30, "text": "sequential data"}, {"st": 38, "ed": 40, "text": "graphical model"}, {"st": 79, "ed": 81, "text": "latent variables"}, {"st": 97, "ed": 100, "text": "word error rate"}, {"st": 111, "ed": 114, "text": "automatic speech recognition"}]
[{"st": 1, "ed": 4, "text": "bag of words"}, {"st": 75, "ed": 77, "text": "text data"}, {"st": 121, "ed": 123, "text": "fuzzy clustering"}, {"st": 139, "ed": 141, "text": "lower dimensional"}, {"st": 152, "ed": 154, "text": "quantitative evaluation"}, {"st": 156, "ed": 158, "text": "fuzzy clustering"}, {"st": 164, "ed": 167, "text": "principal components analysis"}, {"st": 169, "ed": 173, "text": "singular value decomposition svd"}]
[{"st": 3, "ed": 7, "text": "automatic speech recognition asr"}, {"st": 13, "ed": 15, "text": "previously unseen"}, {"st": 24, "ed": 27, "text": "training and testing"}, {"st": 36, "ed": 38, "text": "domain invariant"}, {"st": 46, "ed": 48, "text": "asr systems"}, {"st": 62, "ed": 64, "text": "variational autoencoder"}, {"st": 78, "ed": 80, "text": "latent variables"}, {"st": 88, "ed": 90, "text": "latent variables"}, {"st": 98, "ed": 100, "text": "domain invariant"}, {"st": 118, "ed": 121, "text": "word error rate"}]
[{"st": 0, "ed": 2, "text": "deep learning"}, {"st": 15, "ed": 19, "text": "deep convolutional neural network"}, {"st": 49, "ed": 51, "text": "feature extractor"}, {"st": 56, "ed": 58, "text": "cosine similarity"}, {"st": 62, "ed": 64, "text": "key feature"}, {"st": 70, "ed": 72, "text": "feature map"}, {"st": 72, "ed": 74, "text": "activation function"}, {"st": 83, "ed": 85, "text": "multitask learning"}, {"st": 91, "ed": 93, "text": "feature extractor"}, {"st": 107, "ed": 109, "text": "achieved impressive"}]
[{"st": 4, "ed": 6, "text": "social interaction"}, {"st": 80, "ed": 82, "text": "differential privacy"}, {"st": 118, "ed": 120, "text": "differential privacy"}, {"st": 128, "ed": 130, "text": "case studies"}, {"st": 132, "ed": 134, "text": "social networks"}, {"st": 137, "ed": 139, "text": "text corpora"}, {"st": 148, "ed": 150, "text": "posterior distribution"}, {"st": 151, "ed": 153, "text": "latent variables"}]
[{"st": 73, "ed": 75, "text": "input features"}, {"st": 91, "ed": 93, "text": "hand written"}, {"st": 110, "ed": 112, "text": "higher order"}, {"st": 114, "ed": 116, "text": "input features"}, {"st": 124, "ed": 126, "text": "fully connected"}, {"st": 126, "ed": 128, "text": "feed forward"}, {"st": 138, "ed": 140, "text": "previously proposed"}, {"st": 150, "ed": 155, "text": "long short term memory lstm"}, {"st": 161, "ed": 163, "text": "feed forward"}]
[{"st": 6, "ed": 8, "text": "natural language"}, {"st": 45, "ed": 47, "text": "multi agent"}]
[{"st": 14, "ed": 17, "text": "optical character recognition"}, {"st": 30, "ed": 32, "text": "active learning"}, {"st": 35, "ed": 37, "text": "significantly reduce"}, {"st": 40, "ed": 42, "text": "labeled samples"}, {"st": 62, "ed": 64, "text": "word level"}, {"st": 122, "ed": 124, "text": "predictive accuracy"}, {"st": 128, "ed": 130, "text": "correctly classified"}]
[{"st": 21, "ed": 23, "text": "recently introduced"}, {"st": 53, "ed": 55, "text": "recent developments"}, {"st": 56, "ed": 58, "text": "image analysis"}, {"st": 59, "ed": 62, "text": "convolutional neural networks"}]
[{"st": 7, "ed": 9, "text": "policy iteration"}, {"st": 9, "ed": 11, "text": "method called"}, {"st": 18, "ed": 20, "text": "optimal policy"}, {"st": 22, "ed": 24, "text": "infinite horizon"}, {"st": 37, "ed": 39, "text": "performance loss"}, {"st": 82, "ed": 84, "text": "value iteration"}, {"st": 88, "ed": 90, "text": "policy iteration"}, {"st": 114, "ed": 116, "text": "monte carlo"}, {"st": 124, "ed": 126, "text": "theoretical results"}, {"st": 140, "ed": 142, "text": "reinforcement learning"}]
[{"st": 10, "ed": 12, "text": "least squares"}, {"st": 15, "ed": 17, "text": "ridge regression"}, {"st": 23, "ed": 26, "text": "under mild assumptions"}, {"st": 43, "ed": 45, "text": "prediction error"}, {"st": 90, "ed": 92, "text": "main results"}, {"st": 104, "ed": 106, "text": "random vectors"}]
[{"st": 9, "ed": 11, "text": "latent variables"}, {"st": 51, "ed": 53, "text": "special case"}, {"st": 55, "ed": 57, "text": "ising model"}, {"st": 153, "ed": 155, "text": "proposed method"}, {"st": 167, "ed": 169, "text": "latent variables"}]
[{"st": 18, "ed": 20, "text": "decision making"}, {"st": 38, "ed": 40, "text": "recent developments"}, {"st": 48, "ed": 50, "text": "neural networks"}, {"st": 65, "ed": 67, "text": "non trivial"}, {"st": 96, "ed": 98, "text": "learning systems"}, {"st": 100, "ed": 102, "text": "decision makers"}]
[{"st": 10, "ed": 12, "text": "linear classification"}, {"st": 14, "ed": 16, "text": "special case"}, {"st": 53, "ed": 55, "text": "large margin"}, {"st": 57, "ed": 59, "text": "machine learning"}, {"st": 137, "ed": 139, "text": "convergence rates"}, {"st": 153, "ed": 155, "text": "margin based"}, {"st": 158, "ed": 160, "text": "convergence rates"}]
[{"st": 108, "ed": 110, "text": "precision recall"}]
[{"st": 3, "ed": 5, "text": "approximate posterior"}, {"st": 18, "ed": 20, "text": "variational inference"}, {"st": 31, "ed": 33, "text": "efficient inference"}, {"st": 35, "ed": 37, "text": "mean field"}, {"st": 69, "ed": 71, "text": "approximate posterior"}, {"st": 113, "ed": 115, "text": "normalizing flows"}]
[{"st": 0, "ed": 2, "text": "submodular functions"}, {"st": 25, "ed": 27, "text": "submodular function"}, {"st": 32, "ed": 35, "text": "complete bipartite graph"}, {"st": 68, "ed": 70, "text": "submodular function"}, {"st": 77, "ed": 79, "text": "complete graph"}, {"st": 97, "ed": 99, "text": "matching problem"}, {"st": 111, "ed": 113, "text": "np hard"}, {"st": 134, "ed": 136, "text": "main result"}, {"st": 141, "ed": 143, "text": "approximation algorithm"}, {"st": 156, "ed": 158, "text": "submodular function"}, {"st": 172, "ed": 174, "text": "approximation algorithm"}, {"st": 182, "ed": 184, "text": "a 4"}, {"st": 185, "ed": 187, "text": "approximate solution"}]
[{"st": 6, "ed": 8, "text": "map estimation"}, {"st": 16, "ed": 18, "text": "posterior distribution"}, {"st": 31, "ed": 33, "text": "inference problem"}, {"st": 40, "ed": 42, "text": "hidden variables"}, {"st": 46, "ed": 48, "text": "marginal map"}, {"st": 50, "ed": 52, "text": "np hard"}, {"st": 79, "ed": 81, "text": "marginal map"}, {"st": 93, "ed": 95, "text": "optimization problem"}, {"st": 119, "ed": 121, "text": "message passing"}, {"st": 123, "ed": 125, "text": "marginal map"}, {"st": 133, "ed": 135, "text": "sum product"}, {"st": 160, "ed": 162, "text": "marginal map"}, {"st": 182, "ed": 184, "text": "optimal solutions"}, {"st": 199, "ed": 201, "text": "significantly outperform"}, {"st": 202, "ed": 204, "text": "existing approaches"}, {"st": 213, "ed": 215, "text": "local search"}]
[{"st": 4, "ed": 7, "text": "deep neural networks"}, {"st": 8, "ed": 10, "text": "approximate bayesian"}, {"st": 19, "ed": 21, "text": "generative models"}, {"st": 39, "ed": 41, "text": "approximate posterior"}, {"st": 55, "ed": 57, "text": "back propagation"}, {"st": 59, "ed": 61, "text": "back propagation"}, {"st": 90, "ed": 93, "text": "real world data"}, {"st": 104, "ed": 106, "text": "missing data"}]
[{"st": 19, "ed": 21, "text": "open ended"}, {"st": 27, "ed": 30, "text": "hundreds of thousands"}, {"st": 64, "ed": 66, "text": "significant improvement"}, {"st": 71, "ed": 73, "text": "real data"}]
[{"st": 0, "ed": 3, "text": "approximate dynamic programming"}, {"st": 14, "ed": 16, "text": "large scale"}, {"st": 50, "ed": 52, "text": "operations research"}, {"st": 54, "ed": 56, "text": "computationally intensive"}, {"st": 92, "ed": 94, "text": "prediction error"}, {"st": 127, "ed": 129, "text": "faster convergence"}]
[{"st": 8, "ed": 10, "text": "numerical methods"}, {"st": 13, "ed": 15, "text": "tasks including"}, {"st": 15, "ed": 17, "text": "linear algebra"}, {"st": 21, "ed": 23, "text": "differential equations"}, {"st": 75, "ed": 77, "text": "complex data"}, {"st": 95, "ed": 97, "text": "numerical methods"}, {"st": 150, "ed": 152, "text": "open problems"}, {"st": 161, "ed": 163, "text": "numerical methods"}, {"st": 185, "ed": 187, "text": "differential equation"}]
[{"st": 18, "ed": 20, "text": "probabilistic method"}, {"st": 86, "ed": 88, "text": "probabilistic modeling"}, {"st": 106, "ed": 109, "text": "maximum likelihood estimation"}, {"st": 142, "ed": 144, "text": "neural networks"}, {"st": 145, "ed": 148, "text": "simulated and real"}, {"st": 150, "ed": 152, "text": "methods outperform"}, {"st": 152, "ed": 154, "text": "previous approaches"}]
[{"st": 4, "ed": 6, "text": "variational inference"}, {"st": 46, "ed": 48, "text": "inference algorithms"}, {"st": 68, "ed": 70, "text": "exponential family"}, {"st": 104, "ed": 106, "text": "recently proposed"}, {"st": 124, "ed": 126, "text": "models including"}]
[{"st": 145, "ed": 147, "text": "worst case"}, {"st": 177, "ed": 179, "text": "worst case"}, {"st": 185, "ed": 187, "text": "approximation algorithm"}]
[{"st": 0, "ed": 2, "text": "marginal map"}, {"st": 11, "ed": 13, "text": "latent variables"}, {"st": 59, "ed": 61, "text": "marginal map"}, {"st": 76, "ed": 79, "text": "block coordinate descent"}, {"st": 88, "ed": 91, "text": "guaranteed to converge"}, {"st": 102, "ed": 104, "text": "marginal map"}, {"st": 107, "ed": 109, "text": "real world"}, {"st": 113, "ed": 115, "text": "approximate inference"}]
[{"st": 5, "ed": 7, "text": "off policy"}, {"st": 10, "ed": 12, "text": "reinforcement learning"}, {"st": 43, "ed": 45, "text": "real world"}, {"st": 72, "ed": 75, "text": "sequential decision making"}, {"st": 99, "ed": 101, "text": "importance sampling"}, {"st": 110, "ed": 112, "text": "benchmark problems"}, {"st": 125, "ed": 127, "text": "provide theoretical"}]
[{"st": 5, "ed": 7, "text": "deep learning"}, {"st": 13, "ed": 15, "text": "generative models"}, {"st": 27, "ed": 29, "text": "objective functions"}, {"st": 69, "ed": 71, "text": "image captioning"}, {"st": 81, "ed": 83, "text": "empirical performance"}, {"st": 84, "ed": 86, "text": "objective function"}, {"st": 118, "ed": 120, "text": "maximum likelihood"}, {"st": 123, "ed": 125, "text": "training objective"}, {"st": 142, "ed": 144, "text": "objective function"}, {"st": 155, "ed": 157, "text": "adversarial training"}, {"st": 165, "ed": 167, "text": "maximum likelihood"}, {"st": 180, "ed": 182, "text": "theoretical analysis"}, {"st": 185, "ed": 187, "text": "adversarial training"}]
[{"st": 0, "ed": 2, "text": "existing methods"}, {"st": 68, "ed": 70, "text": "intrinsic dimensionality"}, {"st": 94, "ed": 96, "text": "proposed algorithm"}, {"st": 97, "ed": 99, "text": "fine grained"}, {"st": 101, "ed": 104, "text": "accuracy and speed"}, {"st": 130, "ed": 132, "text": "theoretical properties"}, {"st": 133, "ed": 135, "text": "demonstrate empirically"}, {"st": 137, "ed": 139, "text": "proposed algorithm"}]
[{"st": 54, "ed": 56, "text": "statistical power"}, {"st": 63, "ed": 66, "text": "false positive rate"}, {"st": 77, "ed": 79, "text": "unseen data"}, {"st": 163, "ed": 165, "text": "classification accuracy"}, {"st": 232, "ed": 234, "text": "constant factor"}, {"st": 243, "ed": 245, "text": "prediction methods"}]
[{"st": 6, "ed": 8, "text": "neural networks"}, {"st": 44, "ed": 46, "text": "training objective"}, {"st": 52, "ed": 54, "text": "worst case"}, {"st": 60, "ed": 62, "text": "starting point"}, {"st": 75, "ed": 77, "text": "neural networks"}]
[{"st": 4, "ed": 6, "text": "reinforcement learning"}, {"st": 8, "ed": 12, "text": "partially observable markov decision"}, {"st": 20, "ed": 22, "text": "spectral methods"}, {"st": 31, "ed": 33, "text": "latent variable"}, {"st": 36, "ed": 39, "text": "hidden markov models"}, {"st": 62, "ed": 64, "text": "learning algorithm"}, {"st": 105, "ed": 107, "text": "expected reward"}, {"st": 117, "ed": 119, "text": "optimal regret"}]
[{"st": 59, "ed": 61, "text": "worst case"}, {"st": 78, "ed": 80, "text": "instance specific"}, {"st": 89, "ed": 91, "text": "main results"}, {"st": 109, "ed": 111, "text": "least squares"}, {"st": 141, "ed": 143, "text": "computationally efficient"}, {"st": 160, "ed": 162, "text": "least squares"}, {"st": 188, "ed": 190, "text": "least squares"}, {"st": 200, "ed": 202, "text": "closely related"}]
[{"st": 12, "ed": 14, "text": "social network"}, {"st": 16, "ed": 18, "text": "bandit feedback"}, {"st": 35, "ed": 37, "text": "learning agent"}, {"st": 68, "ed": 70, "text": "computationally efficient"}, {"st": 78, "ed": 80, "text": "regret bounds"}, {"st": 136, "ed": 138, "text": "large scale"}, {"st": 166, "ed": 168, "text": "significantly reduce"}, {"st": 170, "ed": 172, "text": "real world"}]
[{"st": 5, "ed": 9, "text": "expectation maximization em algorithm"}, {"st": 30, "ed": 32, "text": "em algorithm"}, {"st": 46, "ed": 48, "text": "differentially private"}, {"st": 57, "ed": 59, "text": "recently developed"}, {"st": 78, "ed": 80, "text": "differential privacy"}, {"st": 88, "ed": 91, "text": "moment generating function"}, {"st": 95, "ed": 97, "text": "random variable"}, {"st": 113, "ed": 115, "text": "empirical results"}, {"st": 124, "ed": 126, "text": "similar performance"}, {"st": 149, "ed": 151, "text": "learning algorithms"}]
[{"st": 11, "ed": 13, "text": "prior knowledge"}, {"st": 75, "ed": 77, "text": "importance sampling"}, {"st": 135, "ed": 137, "text": "inference algorithms"}, {"st": 145, "ed": 147, "text": "prior information"}, {"st": 152, "ed": 154, "text": "theoretical guarantees"}]
[{"st": 30, "ed": 32, "text": "randomly chosen"}, {"st": 56, "ed": 58, "text": "special cases"}, {"st": 105, "ed": 107, "text": "confidence intervals"}, {"st": 144, "ed": 146, "text": "structural properties"}, {"st": 161, "ed": 163, "text": "ranking based"}, {"st": 164, "ed": 166, "text": "parametric models"}, {"st": 178, "ed": 181, "text": "a long standing"}, {"st": 181, "ed": 183, "text": "open question"}]
[{"st": 11, "ed": 14, "text": "deep neural networks"}, {"st": 16, "ed": 19, "text": "rectified linear units"}, {"st": 32, "ed": 34, "text": "hidden layer"}, {"st": 35, "ed": 37, "text": "global optimality"}, {"st": 109, "ed": 111, "text": "natural number"}, {"st": 124, "ed": 126, "text": "hidden layers"}, {"st": 140, "ed": 142, "text": "hidden layers"}, {"st": 146, "ed": 148, "text": "frac 1"}, {"st": 150, "ed": 152, "text": "k 1"}, {"st": 160, "ed": 162, "text": "mathbb r"}, {"st": 164, "ed": 166, "text": "mathbb r"}, {"st": 192, "ed": 194, "text": "network architecture"}]
[{"st": 3, "ed": 5, "text": "compressive sensing"}, {"st": 14, "ed": 17, "text": "real world data"}, {"st": 68, "ed": 70, "text": "signal recovery"}, {"st": 88, "ed": 91, "text": "deep convolutional network"}, {"st": 151, "ed": 153, "text": "times faster"}, {"st": 165, "ed": 167, "text": "computationally intensive"}, {"st": 167, "ed": 169, "text": "off line"}, {"st": 169, "ed": 171, "text": "training procedure"}, {"st": 193, "ed": 195, "text": "sparse recovery"}]
[{"st": 4, "ed": 7, "text": "k nearest neighbour"}, {"st": 11, "ed": 14, "text": "curse of dimensionality"}, {"st": 51, "ed": 53, "text": "intrinsic dimensionality"}, {"st": 91, "ed": 93, "text": "intrinsic dimensionality"}, {"st": 119, "ed": 121, "text": "demonstrate empirically"}, {"st": 124, "ed": 126, "text": "significantly outperforms"}, {"st": 132, "ed": 135, "text": "locality sensitive hashing"}]
[{"st": 17, "ed": 19, "text": "low rank"}]
[{"st": 15, "ed": 17, "text": "causal discovery"}, {"st": 67, "ed": 69, "text": "null hypothesis"}]
[{"st": 12, "ed": 14, "text": "higher education"}, {"st": 19, "ed": 22, "text": "at risk students"}, {"st": 36, "ed": 38, "text": "unsupervised clustering"}, {"st": 51, "ed": 55, "text": "california state university northridge"}]
[{"st": 7, "ed": 9, "text": "computational models"}, {"st": 12, "ed": 14, "text": "reinforcement learning"}, {"st": 35, "ed": 37, "text": "decision making"}, {"st": 54, "ed": 56, "text": "decision making"}, {"st": 77, "ed": 79, "text": "machine learning"}, {"st": 191, "ed": 193, "text": "evaluation criteria"}, {"st": 202, "ed": 204, "text": "intrinsic motivation"}, {"st": 231, "ed": 234, "text": "directions for future"}]
[{"st": 6, "ed": 8, "text": "collaborative filtering"}, {"st": 21, "ed": 23, "text": "online fashion"}, {"st": 67, "ed": 69, "text": "sample complexity"}, {"st": 83, "ed": 86, "text": "positive and negative"}, {"st": 129, "ed": 131, "text": "cold start"}, {"st": 168, "ed": 170, "text": "cold start"}, {"st": 199, "ed": 202, "text": "positive and negative"}]
[{"st": 0, "ed": 2, "text": "submodular functions"}, {"st": 51, "ed": 53, "text": "submodular functions"}, {"st": 66, "ed": 68, "text": "submodular functions"}, {"st": 86, "ed": 88, "text": "approximation ratio"}, {"st": 90, "ed": 92, "text": "greedy algorithm"}, {"st": 129, "ed": 131, "text": "greedy algorithm"}, {"st": 133, "ed": 135, "text": "non trivial"}, {"st": 135, "ed": 137, "text": "approximation ratio"}, {"st": 151, "ed": 153, "text": "greedy algorithm"}, {"st": 166, "ed": 168, "text": "approximation ratio"}, {"st": 179, "ed": 181, "text": "submodular function"}, {"st": 212, "ed": 214, "text": "greedy algorithm"}, {"st": 215, "ed": 217, "text": "real world"}, {"st": 246, "ed": 248, "text": "non trivial"}, {"st": 254, "ed": 256, "text": "submodular function"}]
[{"st": 6, "ed": 8, "text": "domains including"}, {"st": 10, "ed": 12, "text": "web search"}, {"st": 112, "ed": 114, "text": "worst case"}]
[{"st": 32, "ed": 34, "text": "widely studied"}, {"st": 44, "ed": 46, "text": "bayes optimal"}, {"st": 158, "ed": 160, "text": "optimization process"}, {"st": 166, "ed": 168, "text": "numerical experiments"}, {"st": 180, "ed": 182, "text": "significantly faster"}, {"st": 184, "ed": 186, "text": "bayesian optimization"}, {"st": 193, "ed": 195, "text": "real world"}]
[{"st": 3, "ed": 5, "text": "adversarial training"}, {"st": 11, "ed": 13, "text": "generative model"}, {"st": 21, "ed": 23, "text": "adversarial training"}, {"st": 29, "ed": 31, "text": "generative model"}, {"st": 62, "ed": 64, "text": "dependency structure"}, {"st": 83, "ed": 85, "text": "generative model"}, {"st": 105, "ed": 107, "text": "generative model"}, {"st": 112, "ed": 114, "text": "neural network"}, {"st": 117, "ed": 119, "text": "causal graph"}, {"st": 139, "ed": 141, "text": "conditional gan"}, {"st": 170, "ed": 172, "text": "conditional gan"}, {"st": 178, "ed": 180, "text": "generative model"}, {"st": 188, "ed": 190, "text": "generative model"}]
[{"st": 62, "ed": 64, "text": "partial observability"}, {"st": 88, "ed": 90, "text": "policy evaluation"}, {"st": 93, "ed": 95, "text": "reinforcement learning"}, {"st": 131, "ed": 133, "text": "control policies"}, {"st": 168, "ed": 171, "text": "deep reinforcement learning"}]
[{"st": 7, "ed": 9, "text": "conditional independence"}, {"st": 21, "ed": 23, "text": "joint distribution"}, {"st": 29, "ed": 31, "text": "random vectors"}, {"st": 49, "ed": 51, "text": "conditional independence"}, {"st": 66, "ed": 68, "text": "boosted trees"}, {"st": 77, "ed": 79, "text": "probability distributions"}, {"st": 105, "ed": 107, "text": "classification problem"}, {"st": 131, "ed": 133, "text": "joint distribution"}, {"st": 151, "ed": 153, "text": "joint distribution"}, {"st": 166, "ed": 168, "text": "nearest neighbor"}, {"st": 175, "ed": 177, "text": "generated samples"}, {"st": 192, "ed": 194, "text": "theoretical results"}, {"st": 196, "ed": 198, "text": "generalization bounds"}, {"st": 206, "ed": 208, "text": "error bounds"}, {"st": 231, "ed": 233, "text": "empirically validate"}, {"st": 239, "ed": 242, "text": "simulated and real"}, {"st": 245, "ed": 247, "text": "performance gains"}]
[{"st": 59, "ed": 61, "text": "machine learning"}, {"st": 119, "ed": 121, "text": "machine learning"}, {"st": 124, "ed": 126, "text": "numerical simulations"}]
[{"st": 4, "ed": 6, "text": "neural networks"}, {"st": 6, "ed": 8, "text": "neural networks"}, {"st": 13, "ed": 15, "text": "supervised learning"}, {"st": 86, "ed": 88, "text": "neural networks"}, {"st": 124, "ed": 127, "text": "partial differential equations"}, {"st": 131, "ed": 133, "text": "surrogate models"}, {"st": 135, "ed": 137, "text": "fully differentiable"}]
[{"st": 4, "ed": 6, "text": "neural networks"}, {"st": 6, "ed": 8, "text": "neural networks"}, {"st": 13, "ed": 15, "text": "supervised learning"}, {"st": 95, "ed": 97, "text": "benchmark problems"}, {"st": 98, "ed": 100, "text": "mathematical physics"}]
[{"st": 9, "ed": 11, "text": "joint distribution"}, {"st": 52, "ed": 54, "text": "conditional distributions"}, {"st": 68, "ed": 70, "text": "clustering algorithms"}, {"st": 72, "ed": 74, "text": "k means"}, {"st": 75, "ed": 78, "text": "gaussian mixture models"}, {"st": 84, "ed": 86, "text": "observed data"}, {"st": 88, "ed": 90, "text": "mathbf x"}, {"st": 156, "ed": 159, "text": "number of clusters"}, {"st": 174, "ed": 176, "text": "clustering problems"}, {"st": 206, "ed": 208, "text": "k means"}, {"st": 210, "ed": 212, "text": "large data"}]
[{"st": 1, "ed": 3, "text": "decision making"}, {"st": 9, "ed": 11, "text": "real world"}, {"st": 20, "ed": 22, "text": "decision rules"}, {"st": 27, "ed": 29, "text": "training error"}, {"st": 105, "ed": 107, "text": "supervised learning"}, {"st": 184, "ed": 186, "text": "decision rule"}]
[{"st": 1, "ed": 3, "text": "human brain"}, {"st": 40, "ed": 42, "text": "deep neural"}, {"st": 50, "ed": 52, "text": "unsupervised learning"}, {"st": 69, "ed": 71, "text": "deep learning"}]
[{"st": 15, "ed": 17, "text": "equivalence class"}, {"st": 40, "ed": 42, "text": "proposed approach"}]
[{"st": 15, "ed": 17, "text": "mixture models"}, {"st": 57, "ed": 59, "text": "recent literature"}, {"st": 68, "ed": 70, "text": "mixture components"}, {"st": 90, "ed": 92, "text": "based clustering"}, {"st": 99, "ed": 101, "text": "bayes optimal"}, {"st": 104, "ed": 106, "text": "model based"}, {"st": 145, "ed": 147, "text": "probability distributions"}, {"st": 148, "ed": 150, "text": "metric spaces"}, {"st": 154, "ed": 156, "text": "optimal transport"}]
[{"st": 8, "ed": 10, "text": "multi agent"}, {"st": 10, "ed": 12, "text": "reinforcement learning"}, {"st": 32, "ed": 34, "text": "reward functions"}, {"st": 115, "ed": 117, "text": "large scale"}, {"st": 208, "ed": 210, "text": "nonlinear function"}]
[{"st": 0, "ed": 2, "text": "recent advances"}, {"st": 6, "ed": 9, "text": "inverse reinforcement learning"}, {"st": 34, "ed": 36, "text": "demonstration data"}, {"st": 102, "ed": 104, "text": "compact representation"}, {"st": 152, "ed": 154, "text": "significantly outperform"}, {"st": 193, "ed": 195, "text": "active learning"}]
[{"st": 2, "ed": 4, "text": "learning rate"}, {"st": 24, "ed": 26, "text": "loss function"}, {"st": 30, "ed": 32, "text": "strong convexity"}, {"st": 36, "ed": 38, "text": "learning rate"}, {"st": 52, "ed": 54, "text": "loss function"}, {"st": 63, "ed": 65, "text": "recently proposed"}, {"st": 65, "ed": 67, "text": "batch normalization"}, {"st": 69, "ed": 71, "text": "widely adopted"}, {"st": 73, "ed": 75, "text": "neural network"}, {"st": 94, "ed": 96, "text": "loss function"}, {"st": 102, "ed": 104, "text": "learning rate"}, {"st": 108, "ed": 110, "text": "batch normalization"}, {"st": 115, "ed": 117, "text": "update rule"}, {"st": 119, "ed": 121, "text": "learning rate"}, {"st": 124, "ed": 127, "text": "stochastic gradient descent"}, {"st": 130, "ed": 132, "text": "learning rate"}, {"st": 151, "ed": 153, "text": "proposed method"}, {"st": 163, "ed": 165, "text": "learning rate"}, {"st": 170, "ed": 172, "text": "near optimal"}, {"st": 172, "ed": 174, "text": "convergence rates"}, {"st": 189, "ed": 191, "text": "batch setting"}, {"st": 194, "ed": 196, "text": "sqrt t"}, {"st": 197, "ed": 199, "text": "convex loss"}, {"st": 215, "ed": 217, "text": "proposed method"}, {"st": 225, "ed": 227, "text": "loss function"}, {"st": 228, "ed": 230, "text": "deep learning"}, {"st": 235, "ed": 237, "text": "theoretical understanding"}, {"st": 242, "ed": 244, "text": "batch normalization"}]
[{"st": 0, "ed": 2, "text": "machine learning"}, {"st": 14, "ed": 16, "text": "reinforcement learning"}, {"st": 68, "ed": 70, "text": "quantum state"}, {"st": 149, "ed": 151, "text": "quantum state"}, {"st": 203, "ed": 205, "text": "d dimensional"}, {"st": 242, "ed": 244, "text": "reinforcement learning"}]
[{"st": 16, "ed": 18, "text": "deep learning"}, {"st": 26, "ed": 28, "text": "input data"}, {"st": 46, "ed": 48, "text": "fixed points"}, {"st": 55, "ed": 57, "text": "input data"}, {"st": 74, "ed": 76, "text": "monte carlo"}, {"st": 101, "ed": 103, "text": "deep learning"}, {"st": 121, "ed": 123, "text": "input data"}]
[]
[{"st": 2, "ed": 4, "text": "decision making"}, {"st": 11, "ed": 13, "text": "partially observed"}, {"st": 31, "ed": 33, "text": "contextual bandits"}, {"st": 35, "ed": 37, "text": "wide variety"}, {"st": 38, "ed": 40, "text": "applications including"}, {"st": 66, "ed": 68, "text": "key challenge"}, {"st": 86, "ed": 88, "text": "previous approaches"}, {"st": 140, "ed": 142, "text": "policy evaluation"}, {"st": 148, "ed": 150, "text": "approach yields"}, {"st": 177, "ed": 179, "text": "extensive empirical"}, {"st": 189, "ed": 191, "text": "existing techniques"}]
[{"st": 4, "ed": 6, "text": "group action"}, {"st": 17, "ed": 19, "text": "exponential family"}, {"st": 21, "ed": 23, "text": "graphical model"}, {"st": 51, "ed": 53, "text": "group action"}, {"st": 57, "ed": 59, "text": "random variables"}, {"st": 74, "ed": 76, "text": "inference problem"}, {"st": 114, "ed": 116, "text": "variational approximation"}, {"st": 117, "ed": 119, "text": "map inference"}, {"st": 151, "ed": 153, "text": "map inference"}, {"st": 166, "ed": 168, "text": "objective function"}]
[{"st": 1, "ed": 3, "text": "active learning"}, {"st": 31, "ed": 33, "text": "active learning"}, {"st": 92, "ed": 94, "text": "active learning"}, {"st": 254, "ed": 256, "text": "active learning"}, {"st": 258, "ed": 260, "text": "significant improvement"}]
[{"st": 0, "ed": 2, "text": "marginal map"}, {"st": 4, "ed": 6, "text": "notoriously difficult"}, {"st": 18, "ed": 20, "text": "marginal map"}, {"st": 32, "ed": 34, "text": "mean field"}, {"st": 40, "ed": 42, "text": "message passing"}, {"st": 68, "ed": 70, "text": "local optimum"}, {"st": 96, "ed": 98, "text": "special case"}]
[{"st": 20, "ed": 22, "text": "deep learning"}, {"st": 22, "ed": 24, "text": "pre training"}, {"st": 25, "ed": 27, "text": "unsupervised learning"}, {"st": 28, "ed": 30, "text": "fine tuning"}, {"st": 41, "ed": 43, "text": "training data"}, {"st": 51, "ed": 53, "text": "feature vector"}, {"st": 65, "ed": 67, "text": "labelled data"}, {"st": 76, "ed": 78, "text": "single layer"}, {"st": 85, "ed": 87, "text": "multi layer"}, {"st": 87, "ed": 89, "text": "neural network"}, {"st": 93, "ed": 95, "text": "phase transition"}, {"st": 102, "ed": 104, "text": "unlabelled data"}, {"st": 106, "ed": 108, "text": "generalization error"}, {"st": 121, "ed": 123, "text": "unsupervised learning"}, {"st": 151, "ed": 153, "text": "deep learning"}, {"st": 156, "ed": 158, "text": "iterative algorithm"}, {"st": 161, "ed": 163, "text": "weight vector"}]
[{"st": 73, "ed": 75, "text": "low dimensional"}, {"st": 87, "ed": 89, "text": "predictive performance"}, {"st": 111, "ed": 113, "text": "maximum likelihood"}, {"st": 128, "ed": 130, "text": "latent factors"}, {"st": 157, "ed": 159, "text": "latent factor"}]
[{"st": 7, "ed": 9, "text": "pairwise comparisons"}, {"st": 53, "ed": 55, "text": "pairwise comparisons"}, {"st": 65, "ed": 67, "text": "computational efficiency"}, {"st": 69, "ed": 71, "text": "speed ups"}, {"st": 73, "ed": 76, "text": "orders of magnitude"}, {"st": 90, "ed": 92, "text": "theoretical guarantees"}]
[{"st": 75, "ed": 77, "text": "inference algorithm"}, {"st": 80, "ed": 82, "text": "predictive performance"}, {"st": 92, "ed": 94, "text": "latent structure"}]
[{"st": 6, "ed": 8, "text": "labeled data"}, {"st": 36, "ed": 38, "text": "labeled data"}, {"st": 83, "ed": 86, "text": "rates of convergence"}, {"st": 127, "ed": 129, "text": "computationally efficient"}, {"st": 129, "ed": 131, "text": "method called"}]
[{"st": 15, "ed": 17, "text": "machine learning"}, {"st": 48, "ed": 50, "text": "convergence guarantees"}, {"st": 110, "ed": 112, "text": "machine learning"}, {"st": 132, "ed": 134, "text": "continuous function"}, {"st": 144, "ed": 146, "text": "worst case"}, {"st": 146, "ed": 148, "text": "error bound"}, {"st": 167, "ed": 169, "text": "adaptive control"}, {"st": 180, "ed": 182, "text": "performance metrics"}, {"st": 183, "ed": 185, "text": "approach outperforms"}, {"st": 185, "ed": 187, "text": "recently proposed"}, {"st": 192, "ed": 194, "text": "gaussian processes"}, {"st": 217, "ed": 219, "text": "online learning"}]
[{"st": 6, "ed": 8, "text": "probabilistic programming"}, {"st": 14, "ed": 16, "text": "random variables"}, {"st": 34, "ed": 36, "text": "probabilistic programming"}, {"st": 41, "ed": 43, "text": "computationally efficient"}, {"st": 70, "ed": 72, "text": "variational inference"}, {"st": 105, "ed": 107, "text": "significant speedups"}, {"st": 118, "ed": 120, "text": "logistic regression"}]
[{"st": 0, "ed": 2, "text": "policy evaluation"}, {"st": 4, "ed": 6, "text": "crucial step"}, {"st": 8, "ed": 10, "text": "reinforcement learning"}, {"st": 32, "ed": 34, "text": "policy evaluation"}, {"st": 35, "ed": 37, "text": "linear function"}, {"st": 47, "ed": 49, "text": "policy evaluation"}, {"st": 55, "ed": 57, "text": "saddle point"}, {"st": 62, "ed": 64, "text": "primal dual"}, {"st": 85, "ed": 87, "text": "sample size"}, {"st": 93, "ed": 95, "text": "linear convergence"}, {"st": 98, "ed": 100, "text": "saddle point"}, {"st": 111, "ed": 113, "text": "strong convexity"}, {"st": 117, "ed": 119, "text": "numerical experiments"}, {"st": 120, "ed": 122, "text": "benchmark problems"}]
[{"st": 5, "ed": 7, "text": "machine learning"}, {"st": 22, "ed": 24, "text": "machine learning"}, {"st": 39, "ed": 41, "text": "machine learning"}, {"st": 61, "ed": 63, "text": "machine learning"}, {"st": 99, "ed": 101, "text": "machine learning"}, {"st": 113, "ed": 115, "text": "machine teaching"}, {"st": 124, "ed": 126, "text": "machine teaching"}, {"st": 139, "ed": 141, "text": "software engineering"}, {"st": 185, "ed": 187, "text": "machine teaching"}, {"st": 190, "ed": 192, "text": "machine teaching"}, {"st": 201, "ed": 203, "text": "machine learning"}, {"st": 220, "ed": 222, "text": "machine learning"}]
[{"st": 58, "ed": 60, "text": "spectral radius"}, {"st": 82, "ed": 84, "text": "convex relaxation"}]
[{"st": 4, "ed": 6, "text": "challenging problem"}, {"st": 7, "ed": 9, "text": "reinforcement learning"}, {"st": 17, "ed": 19, "text": "control policies"}, {"st": 36, "ed": 38, "text": "recent advances"}, {"st": 39, "ed": 41, "text": "deep rl"}, {"st": 44, "ed": 47, "text": "multi agent systems"}, {"st": 64, "ed": 66, "text": "deep rl"}, {"st": 90, "ed": 92, "text": "prior knowledge"}, {"st": 112, "ed": 115, "text": "deep reinforcement learning"}, {"st": 118, "ed": 120, "text": "control policies"}, {"st": 135, "ed": 137, "text": "local neighborhood"}, {"st": 145, "ed": 147, "text": "task specific"}, {"st": 167, "ed": 169, "text": "trust region"}, {"st": 169, "ed": 171, "text": "policy optimization"}]
[{"st": 8, "ed": 10, "text": "meta learning"}, {"st": 28, "ed": 30, "text": "binary classification"}, {"st": 68, "ed": 70, "text": "error rate"}, {"st": 75, "ed": 77, "text": "spanning tree"}, {"st": 88, "ed": 90, "text": "meta learning"}, {"st": 100, "ed": 102, "text": "error rate"}, {"st": 114, "ed": 116, "text": "extensive simulations"}, {"st": 117, "ed": 119, "text": "benchmark datasets"}, {"st": 132, "ed": 134, "text": "competing methods"}]
[{"st": 4, "ed": 6, "text": "machine learning"}, {"st": 64, "ed": 66, "text": "randomly chosen"}, {"st": 72, "ed": 74, "text": "typically requires"}, {"st": 128, "ed": 130, "text": "confidence intervals"}, {"st": 165, "ed": 167, "text": "numerical results"}]
[{"st": 33, "ed": 35, "text": "deep lstm"}, {"st": 68, "ed": 71, "text": "accuracy and robustness"}, {"st": 76, "ed": 78, "text": "kalman filter"}]
[{"st": 3, "ed": 5, "text": "monte carlo"}, {"st": 95, "ed": 97, "text": "computationally efficient"}, {"st": 137, "ed": 139, "text": "cosine similarity"}, {"st": 164, "ed": 167, "text": "locality sensitive hashing"}, {"st": 177, "ed": 179, "text": "computationally efficient"}, {"st": 196, "ed": 198, "text": "significantly fewer"}, {"st": 233, "ed": 235, "text": "sampling methods"}, {"st": 237, "ed": 239, "text": "synthetic datasets"}, {"st": 242, "ed": 244, "text": "real datasets"}]
[{"st": 5, "ed": 7, "text": "contextual bandits"}, {"st": 18, "ed": 20, "text": "contextual bandit"}, {"st": 41, "ed": 43, "text": "conditional distribution"}, {"st": 51, "ed": 54, "text": "upper confidence bound"}, {"st": 63, "ed": 65, "text": "importance sampling"}, {"st": 105, "ed": 107, "text": "regret bounds"}, {"st": 108, "ed": 110, "text": "mathcal o"}, {"st": 173, "ed": 175, "text": "mathcal o"}, {"st": 187, "ed": 189, "text": "cost sensitive"}, {"st": 194, "ed": 196, "text": "empirical performance"}, {"st": 197, "ed": 199, "text": "real world"}, {"st": 208, "ed": 210, "text": "contextual bandit"}]
[{"st": 50, "ed": 52, "text": "genetic algorithms"}, {"st": 52, "ed": 54, "text": "genetic programming"}]
[{"st": 14, "ed": 16, "text": "partially observable"}, {"st": 29, "ed": 31, "text": "partially observable"}, {"st": 61, "ed": 64, "text": "hidden markov models"}]
[]
[{"st": 3, "ed": 5, "text": "practical problems"}, {"st": 30, "ed": 32, "text": "mutual information"}, {"st": 45, "ed": 47, "text": "higher order"}]
[{"st": 89, "ed": 91, "text": "statistical methods"}, {"st": 91, "ed": 93, "text": "neural networks"}, {"st": 94, "ed": 96, "text": "finite state"}, {"st": 116, "ed": 118, "text": "visual attention"}, {"st": 126, "ed": 128, "text": "speech recognition"}, {"st": 138, "ed": 140, "text": "multi modal"}, {"st": 140, "ed": 142, "text": "task oriented"}]
[{"st": 51, "ed": 53, "text": "classification algorithms"}, {"st": 74, "ed": 77, "text": "hidden markov models"}, {"st": 152, "ed": 154, "text": "visual cues"}, {"st": 172, "ed": 174, "text": "feed forward"}, {"st": 174, "ed": 177, "text": "hidden markov model"}, {"st": 196, "ed": 198, "text": "feed forward"}, {"st": 225, "ed": 227, "text": "computational intelligence"}, {"st": 252, "ed": 254, "text": "feed forward"}]
[{"st": 4, "ed": 8, "text": "content based image retrieval"}, {"st": 15, "ed": 17, "text": "image content"}, {"st": 65, "ed": 67, "text": "feature extraction"}, {"st": 73, "ed": 75, "text": "co occurrence"}, {"st": 134, "ed": 136, "text": "image retrieval"}, {"st": 167, "ed": 169, "text": "image retrieval"}]
[{"st": 6, "ed": 8, "text": "zero shot"}, {"st": 11, "ed": 13, "text": "key contribution"}, {"st": 15, "ed": 17, "text": "proposed approach"}, {"st": 31, "ed": 34, "text": "zero shot learning"}, {"st": 39, "ed": 41, "text": "metric learning"}, {"st": 67, "ed": 70, "text": "zero shot learning"}, {"st": 78, "ed": 80, "text": "training phase"}, {"st": 126, "ed": 128, "text": "proposed approach"}, {"st": 140, "ed": 142, "text": "zero shot"}]
[{"st": 1, "ed": 3, "text": "deep learning"}, {"st": 8, "ed": 10, "text": "computer vision"}, {"st": 15, "ed": 17, "text": "visual data"}, {"st": 18, "ed": 20, "text": "sufficiently large"}, {"st": 20, "ed": 22, "text": "real world"}, {"st": 43, "ed": 46, "text": "deep reinforcement learning"}, {"st": 56, "ed": 58, "text": "real world"}, {"st": 79, "ed": 81, "text": "fine tuned"}, {"st": 83, "ed": 86, "text": "end to end"}, {"st": 103, "ed": 105, "text": "fine tuned"}, {"st": 112, "ed": 114, "text": "significant improvement"}, {"st": 135, "ed": 137, "text": "efficient learning"}, {"st": 152, "ed": 154, "text": "real world"}]
[{"st": 5, "ed": 8, "text": "convolutional neural networks"}, {"st": 50, "ed": 53, "text": "propose and evaluate"}, {"st": 73, "ed": 75, "text": "spatio temporal"}, {"st": 98, "ed": 100, "text": "accuracy loss"}]
[{"st": 4, "ed": 7, "text": "end to end"}, {"st": 7, "ed": 9, "text": "fine tuning"}, {"st": 34, "ed": 36, "text": "fine tuning"}, {"st": 37, "ed": 39, "text": "significantly improves"}]
[{"st": 110, "ed": 112, "text": "non invasive"}, {"st": 140, "ed": 142, "text": "real world"}, {"st": 149, "ed": 151, "text": "tasks involving"}, {"st": 196, "ed": 198, "text": "non invasive"}]
[{"st": 16, "ed": 18, "text": "real world"}, {"st": 43, "ed": 45, "text": "a 7"}, {"st": 86, "ed": 88, "text": "success rate"}]
[{"st": 24, "ed": 26, "text": "large datasets"}, {"st": 43, "ed": 45, "text": "deep learning"}, {"st": 75, "ed": 78, "text": "deep reinforcement learning"}, {"st": 101, "ed": 103, "text": "open source"}, {"st": 104, "ed": 107, "text": "the research community"}]
[{"st": 8, "ed": 11, "text": "deep neural networks"}, {"st": 41, "ed": 43, "text": "training data"}]
[{"st": 18, "ed": 20, "text": "deep linear"}, {"st": 20, "ed": 22, "text": "neural networks"}, {"st": 63, "ed": 65, "text": "quadratic loss"}, {"st": 82, "ed": 85, "text": "number of iterations"}, {"st": 157, "ed": 159, "text": "positive definite"}, {"st": 184, "ed": 186, "text": "condition number"}]
[{"st": 5, "ed": 7, "text": "optimization problems"}, {"st": 8, "ed": 10, "text": "machine learning"}, {"st": 38, "ed": 41, "text": "support vector machine"}, {"st": 58, "ed": 60, "text": "regularization parameter"}, {"st": 87, "ed": 89, "text": "linear complexity"}, {"st": 98, "ed": 101, "text": "support vector machine"}, {"st": 110, "ed": 112, "text": "training points"}, {"st": 148, "ed": 150, "text": "support vectors"}, {"st": 153, "ed": 155, "text": "regularization parameter"}]
[{"st": 11, "ed": 13, "text": "computer vision"}, {"st": 71, "ed": 73, "text": "neural network"}, {"st": 75, "ed": 77, "text": "novelty detection"}, {"st": 95, "ed": 97, "text": "novelty detection"}, {"st": 133, "ed": 135, "text": "novelty detection"}, {"st": 148, "ed": 152, "text": "mars desert research station"}, {"st": 155, "ed": 157, "text": "systems engineering"}, {"st": 169, "ed": 171, "text": "computer vision"}, {"st": 189, "ed": 191, "text": "semi arid"}]
[{"st": 31, "ed": 33, "text": "feature set"}, {"st": 71, "ed": 73, "text": "decision trees"}, {"st": 81, "ed": 83, "text": "decision trees"}, {"st": 89, "ed": 91, "text": "assignment problem"}, {"st": 101, "ed": 103, "text": "modified version"}]
[{"st": 4, "ed": 6, "text": "dimensionality reduction"}, {"st": 7, "ed": 9, "text": "matrix decomposition"}, {"st": 12, "ed": 14, "text": "low dimensional"}, {"st": 108, "ed": 110, "text": "sparse representation"}, {"st": 120, "ed": 122, "text": "sufficient conditions"}, {"st": 127, "ed": 129, "text": "low rank"}, {"st": 150, "ed": 152, "text": "matrix approximation"}, {"st": 161, "ed": 163, "text": "real world"}, {"st": 172, "ed": 174, "text": "low complexity"}, {"st": 177, "ed": 179, "text": "sparse matrix"}, {"st": 183, "ed": 185, "text": "sparse pca"}]
[{"st": 0, "ed": 3, "text": "linear dimensionality reduction"}, {"st": 5, "ed": 7, "text": "powerful tools"}, {"st": 8, "ed": 10, "text": "image analysis"}, {"st": 16, "ed": 18, "text": "important features"}, {"st": 24, "ed": 28, "text": "nonnegative matrix factorization nmf"}, {"st": 53, "ed": 55, "text": "nonnegative matrix"}, {"st": 59, "ed": 61, "text": "closely related"}, {"st": 85, "ed": 87, "text": "image analysis"}, {"st": 91, "ed": 93, "text": "spatial information"}, {"st": 96, "ed": 99, "text": "takes into account"}, {"st": 146, "ed": 148, "text": "hyperspectral image"}]
[{"st": 0, "ed": 2, "text": "signal processing"}, {"st": 9, "ed": 12, "text": "mean square error"}, {"st": 56, "ed": 58, "text": "kernel based"}, {"st": 62, "ed": 64, "text": "sparse linear"}, {"st": 65, "ed": 67, "text": "nuclear norm"}, {"st": 92, "ed": 94, "text": "multi kernel"}, {"st": 104, "ed": 106, "text": "signal processing"}, {"st": 113, "ed": 115, "text": "cognitive radio"}]
[{"st": 21, "ed": 23, "text": "temporal dynamics"}, {"st": 24, "ed": 26, "text": "motor learning"}]
[{"st": 26, "ed": 28, "text": "linear models"}, {"st": 63, "ed": 65, "text": "open source"}, {"st": 71, "ed": 75, "text": "available at https github.com"}]
[{"st": 7, "ed": 9, "text": "compressive sensing"}, {"st": 11, "ed": 13, "text": "multi channel"}, {"st": 66, "ed": 68, "text": "compressive sensing"}, {"st": 75, "ed": 77, "text": "mathcal o"}, {"st": 85, "ed": 87, "text": "multi channel"}, {"st": 117, "ed": 119, "text": "mathcal o"}, {"st": 127, "ed": 129, "text": "mathcal o"}, {"st": 141, "ed": 143, "text": "mathcal o"}, {"st": 175, "ed": 177, "text": "result shows"}, {"st": 221, "ed": 223, "text": "extensive experiments"}, {"st": 225, "ed": 228, "text": "effectiveness and efficiency"}]
[{"st": 1, "ed": 3, "text": "computer vision"}, {"st": 12, "ed": 14, "text": "large scale"}, {"st": 28, "ed": 30, "text": "efficient learning"}, {"st": 64, "ed": 66, "text": "big data"}, {"st": 77, "ed": 79, "text": "loss function"}, {"st": 102, "ed": 105, "text": "provide theoretical guarantees"}, {"st": 139, "ed": 143, "text": "real and synthetic data"}, {"st": 146, "ed": 148, "text": "proposed method"}]
[{"st": 4, "ed": 6, "text": "finite sample"}, {"st": 9, "ed": 11, "text": "linear convergence"}, {"st": 12, "ed": 15, "text": "stochastic gradient descent"}, {"st": 18, "ed": 20, "text": "strongly convex"}, {"st": 44, "ed": 46, "text": "strong convexity"}, {"st": 60, "ed": 62, "text": "sampling distribution"}, {"st": 63, "ed": 65, "text": "importance sampling"}, {"st": 88, "ed": 90, "text": "importance sampling"}, {"st": 168, "ed": 170, "text": "least squares"}, {"st": 175, "ed": 177, "text": "least squares"}, {"st": 195, "ed": 197, "text": "least squares"}]
[{"st": 5, "ed": 7, "text": "image segmentation"}, {"st": 53, "ed": 56, "text": "statistical machine learning"}, {"st": 96, "ed": 98, "text": "maximum entropy"}, {"st": 102, "ed": 104, "text": "maximum likelihood"}, {"st": 112, "ed": 114, "text": "phase transitions"}]
[{"st": 1, "ed": 3, "text": "signal processing"}, {"st": 4, "ed": 6, "text": "machine learning"}, {"st": 25, "ed": 27, "text": "kernel based"}, {"st": 40, "ed": 42, "text": "online learning"}, {"st": 106, "ed": 108, "text": "linear independence"}, {"st": 129, "ed": 131, "text": "dual space"}]
[{"st": 8, "ed": 10, "text": "unsupervised clustering"}, {"st": 24, "ed": 27, "text": "sparse subspace clustering"}, {"st": 107, "ed": 109, "text": "special case"}, {"st": 171, "ed": 173, "text": "inner product"}, {"st": 179, "ed": 182, "text": "provide theoretical guarantees"}, {"st": 194, "ed": 196, "text": "achieves higher"}, {"st": 219, "ed": 221, "text": "proposed method"}, {"st": 223, "ed": 225, "text": "face database"}, {"st": 229, "ed": 231, "text": "face database"}, {"st": 233, "ed": 235, "text": "mnist handwritten"}]
[{"st": 0, "ed": 2, "text": "multi output"}, {"st": 2, "ed": 4, "text": "inference tasks"}, {"st": 6, "ed": 9, "text": "multi label classification"}, {"st": 11, "ed": 13, "text": "increasingly important"}, {"st": 20, "ed": 23, "text": "multi label classification"}, {"st": 39, "ed": 42, "text": "taking into account"}, {"st": 79, "ed": 81, "text": "larger datasets"}, {"st": 125, "ed": 127, "text": "recently proposed"}, {"st": 140, "ed": 142, "text": "highly competitive"}, {"st": 144, "ed": 146, "text": "multi label"}, {"st": 157, "ed": 160, "text": "tens of thousands"}]
[{"st": 0, "ed": 3, "text": "non convex optimization"}, {"st": 16, "ed": 18, "text": "convex functions"}, {"st": 42, "ed": 44, "text": "objective function"}, {"st": 76, "ed": 78, "text": "optimization algorithms"}, {"st": 104, "ed": 106, "text": "optimization procedure"}, {"st": 118, "ed": 120, "text": "latent variable"}, {"st": 125, "ed": 127, "text": "consistently outperform"}]
[{"st": 0, "ed": 3, "text": "stochastic gradient descent"}, {"st": 13, "ed": 15, "text": "online optimization"}, {"st": 33, "ed": 35, "text": "training examples"}, {"st": 52, "ed": 54, "text": "sampling distribution"}, {"st": 56, "ed": 58, "text": "importance sampling"}, {"st": 64, "ed": 66, "text": "sampling distribution"}, {"st": 114, "ed": 116, "text": "faster convergence"}, {"st": 121, "ed": 123, "text": "image classification"}, {"st": 124, "ed": 126, "text": "deep features"}, {"st": 136, "ed": 138, "text": "matrix factorization"}, {"st": 148, "ed": 150, "text": "reinforcement learning"}, {"st": 168, "ed": 170, "text": "off policy"}]
[{"st": 17, "ed": 19, "text": "wide variety"}, {"st": 40, "ed": 42, "text": "real world"}, {"st": 44, "ed": 46, "text": "side information"}, {"st": 56, "ed": 58, "text": "neurological disorder"}, {"st": 165, "ed": 168, "text": "branch and bound"}, {"st": 191, "ed": 193, "text": "empirical studies"}, {"st": 195, "ed": 197, "text": "classification tasks"}, {"st": 198, "ed": 200, "text": "neurological disorders"}]
[{"st": 12, "ed": 14, "text": "mixture components"}, {"st": 22, "ed": 25, "text": "kullback leibler divergence"}, {"st": 37, "ed": 39, "text": "based method"}, {"st": 87, "ed": 89, "text": "computationally efficient"}, {"st": 91, "ed": 93, "text": "proposed algorithm"}, {"st": 102, "ed": 104, "text": "numerical examples"}, {"st": 118, "ed": 120, "text": "computational complexity"}, {"st": 122, "ed": 124, "text": "proposed approach"}]
[{"st": 3, "ed": 5, "text": "nonnegative matrix"}, {"st": 7, "ed": 10, "text": "nonnegative matrix factorization"}, {"st": 17, "ed": 19, "text": "nonnegative matrix"}, {"st": 57, "ed": 59, "text": "coordinate descent"}, {"st": 81, "ed": 85, "text": "synthetic and real world"}]
[{"st": 0, "ed": 3, "text": "modern machine learning"}, {"st": 7, "ed": 9, "text": "large scale"}, {"st": 98, "ed": 100, "text": "machine learning"}, {"st": 121, "ed": 123, "text": "outlier detection"}, {"st": 134, "ed": 136, "text": "component analysis"}, {"st": 147, "ed": 150, "text": "labeled and unlabeled"}, {"st": 156, "ed": 158, "text": "synthetic experiments"}, {"st": 164, "ed": 166, "text": "real world"}, {"st": 166, "ed": 168, "text": "classification tasks"}]
[{"st": 4, "ed": 6, "text": "clustering algorithms"}, {"st": 18, "ed": 20, "text": "successfully applied"}, {"st": 29, "ed": 31, "text": "speech recognition"}, {"st": 43, "ed": 45, "text": "clustering algorithm"}, {"st": 69, "ed": 71, "text": "similarity measure"}, {"st": 82, "ed": 84, "text": "clustering algorithms"}, {"st": 97, "ed": 99, "text": "clustering algorithms"}, {"st": 107, "ed": 109, "text": "evaluation metrics"}]
[{"st": 2, "ed": 4, "text": "based clustering"}, {"st": 22, "ed": 24, "text": "challenging problem"}, {"st": 93, "ed": 95, "text": "hierarchical clustering"}, {"st": 95, "ed": 97, "text": "method called"}, {"st": 99, "ed": 101, "text": "nearest neighbor"}, {"st": 111, "ed": 114, "text": "layer by layer"}]
[{"st": 12, "ed": 14, "text": "unsupervised clustering"}, {"st": 58, "ed": 60, "text": "subspace clustering"}, {"st": 71, "ed": 73, "text": "image data"}, {"st": 79, "ed": 81, "text": "highly competitive"}, {"st": 82, "ed": 84, "text": "existing algorithms"}, {"st": 91, "ed": 93, "text": "clustering performance"}]
[{"st": 3, "ed": 5, "text": "multi class"}, {"st": 57, "ed": 59, "text": "naive bayes"}, {"st": 75, "ed": 77, "text": "k 1"}, {"st": 94, "ed": 96, "text": "theoretical foundation"}, {"st": 123, "ed": 126, "text": "optical character recognition"}]
[{"st": 0, "ed": 3, "text": "deep neural networks"}, {"st": 19, "ed": 21, "text": "objective function"}, {"st": 91, "ed": 93, "text": "learning algorithms"}, {"st": 97, "ed": 99, "text": "human perception"}, {"st": 114, "ed": 116, "text": "associative memory"}, {"st": 127, "ed": 129, "text": "higher order"}, {"st": 154, "ed": 156, "text": "sufficiently large"}, {"st": 168, "ed": 170, "text": "objective function"}, {"st": 181, "ed": 183, "text": "semantically meaningful"}, {"st": 191, "ed": 193, "text": "decision boundary"}, {"st": 212, "ed": 214, "text": "adversarial images"}, {"st": 230, "ed": 233, "text": "rectified linear units"}, {"st": 243, "ed": 245, "text": "higher order"}, {"st": 253, "ed": 255, "text": "higher order"}, {"st": 270, "ed": 272, "text": "higher order"}, {"st": 278, "ed": 280, "text": "visual perception"}]
[{"st": 0, "ed": 2, "text": "robust pca"}, {"st": 30, "ed": 32, "text": "efficient online"}, {"st": 32, "ed": 35, "text": "robust principal component"}, {"st": 40, "ed": 44, "text": "robust principal component analysis"}, {"st": 46, "ed": 48, "text": "unlike existing"}]
[{"st": 11, "ed": 15, "text": "nonnegative matrix factorization nmf"}, {"st": 58, "ed": 61, "text": "low rank matrix"}]
[{"st": 0, "ed": 2, "text": "coordinate descent"}, {"st": 16, "ed": 18, "text": "convex optimization"}, {"st": 50, "ed": 52, "text": "primal dual"}, {"st": 91, "ed": 93, "text": "hinge loss"}, {"st": 93, "ed": 96, "text": "support vector machines"}]
[{"st": 0, "ed": 2, "text": "machine learning"}, {"st": 26, "ed": 28, "text": "local minima"}, {"st": 36, "ed": 38, "text": "machine learning"}, {"st": 45, "ed": 47, "text": "potential energy"}, {"st": 53, "ed": 55, "text": "machine learning"}, {"st": 82, "ed": 84, "text": "molecular structure"}]
[{"st": 5, "ed": 7, "text": "spectral clustering"}, {"st": 7, "ed": 9, "text": "based approach"}, {"st": 11, "ed": 13, "text": "subspace clustering"}, {"st": 16, "ed": 18, "text": "proposed method"}, {"st": 28, "ed": 31, "text": "each data point"}, {"st": 73, "ed": 78, "text": "alternating direction method of multipliers"}, {"st": 82, "ed": 84, "text": "efficiently solve"}, {"st": 89, "ed": 91, "text": "proposed method"}, {"st": 98, "ed": 100, "text": "subspace clustering"}]
[{"st": 1, "ed": 3, "text": "deep learning"}, {"st": 6, "ed": 8, "text": "network architectures"}, {"st": 11, "ed": 13, "text": "performance improvement"}, {"st": 15, "ed": 17, "text": "iterative reconstruction"}, {"st": 29, "ed": 31, "text": "deep learning"}, {"st": 77, "ed": 79, "text": "low rank"}, {"st": 83, "ed": 85, "text": "inverse problems"}, {"st": 98, "ed": 101, "text": "deep neural network"}, {"st": 111, "ed": 113, "text": "linear unit"}, {"st": 123, "ed": 125, "text": "deep network"}, {"st": 154, "ed": 156, "text": "high pass"}, {"st": 192, "ed": 194, "text": "deep convolutional"}, {"st": 198, "ed": 200, "text": "numerical experiments"}, {"st": 202, "ed": 204, "text": "inverse problems"}, {"st": 225, "ed": 227, "text": "deep learning"}, {"st": 262, "ed": 264, "text": "natural extension"}, {"st": 266, "ed": 268, "text": "signal processing"}]
[{"st": 31, "ed": 33, "text": "recently proposed"}, {"st": 42, "ed": 45, "text": "singular value decomposition"}, {"st": 85, "ed": 87, "text": "nuclear norm"}, {"st": 88, "ed": 90, "text": "convex surrogate"}, {"st": 96, "ed": 98, "text": "ell 1"}, {"st": 130, "ed": 132, "text": "recovery guarantees"}, {"st": 141, "ed": 144, "text": "principal component analysis"}, {"st": 154, "ed": 160, "text": "alternating direction method of multipliers admm"}, {"st": 168, "ed": 170, "text": "numerical experiments"}, {"st": 174, "ed": 176, "text": "real world"}]
[{"st": 0, "ed": 3, "text": "sparse subspace clustering"}, {"st": 22, "ed": 24, "text": "low dimensional"}, {"st": 27, "ed": 29, "text": "ell 1"}, {"st": 36, "ed": 38, "text": "computational complexity"}, {"st": 45, "ed": 47, "text": "matching pursuit"}, {"st": 52, "ed": 54, "text": "clustering accuracy"}, {"st": 72, "ed": 74, "text": "clustering accuracy"}, {"st": 96, "ed": 98, "text": "computational complexity"}, {"st": 124, "ed": 126, "text": "numerical results"}, {"st": 128, "ed": 130, "text": "synthetic data"}, {"st": 131, "ed": 134, "text": "real world data"}]
[{"st": 1, "ed": 3, "text": "sparse representations"}, {"st": 7, "ed": 9, "text": "sparse representation"}, {"st": 17, "ed": 20, "text": "dictionary learning algorithms"}, {"st": 35, "ed": 37, "text": "learning process"}, {"st": 48, "ed": 50, "text": "training data"}, {"st": 68, "ed": 71, "text": "dictionary learning algorithms"}, {"st": 79, "ed": 81, "text": "computational cost"}, {"st": 82, "ed": 84, "text": "training set"}, {"st": 123, "ed": 125, "text": "incomplete data"}, {"st": 129, "ed": 131, "text": "theoretical analysis"}]
[{"st": 89, "ed": 91, "text": "neural nets"}, {"st": 126, "ed": 128, "text": "real world"}, {"st": 133, "ed": 135, "text": "data mining"}]
[{"st": 55, "ed": 57, "text": "open source"}, {"st": 65, "ed": 67, "text": "reinforcement learning"}, {"st": 73, "ed": 75, "text": "multi agent"}]
[{"st": 91, "ed": 93, "text": "natural language"}]
[{"st": 23, "ed": 25, "text": "significantly improve"}, {"st": 51, "ed": 53, "text": "previous approaches"}, {"st": 83, "ed": 85, "text": "kernel learning"}, {"st": 96, "ed": 98, "text": "dependency structure"}, {"st": 105, "ed": 107, "text": "approach yields"}, {"st": 123, "ed": 126, "text": "training and test"}]
[{"st": 19, "ed": 21, "text": "social web"}, {"st": 28, "ed": 30, "text": "user generated"}, {"st": 63, "ed": 65, "text": "real life"}, {"st": 81, "ed": 83, "text": "valuable information"}, {"st": 102, "ed": 104, "text": "social media"}, {"st": 109, "ed": 112, "text": "statistical machine learning"}, {"st": 183, "ed": 185, "text": "social web"}, {"st": 198, "ed": 200, "text": "real world"}]
[{"st": 14, "ed": 16, "text": "multi modal"}, {"st": 16, "ed": 18, "text": "multi step"}, {"st": 44, "ed": 46, "text": "multi modal"}, {"st": 46, "ed": 48, "text": "multi step"}, {"st": 59, "ed": 61, "text": "natural language"}]
[{"st": 6, "ed": 9, "text": "recurrent neural network"}, {"st": 24, "ed": 26, "text": "published results"}, {"st": 42, "ed": 45, "text": "recurrent neural network"}, {"st": 50, "ed": 53, "text": "recurrent neural network"}, {"st": 55, "ed": 57, "text": "error rate"}]
[{"st": 30, "ed": 32, "text": "important role"}, {"st": 36, "ed": 38, "text": "natural language"}, {"st": 41, "ed": 43, "text": "machine translation"}, {"st": 43, "ed": 47, "text": "part of speech tagging"}, {"st": 49, "ed": 51, "text": "question answering"}, {"st": 52, "ed": 54, "text": "feature selection"}, {"st": 65, "ed": 68, "text": "conditional random field"}, {"st": 89, "ed": 91, "text": "genetic algorithm"}, {"st": 114, "ed": 116, "text": "feature selection"}, {"st": 119, "ed": 122, "text": "fold cross validation"}, {"st": 138, "ed": 140, "text": "f measure"}]
[{"st": 5, "ed": 8, "text": "short term memory"}, {"st": 8, "ed": 11, "text": "recurrent neural networks"}, {"st": 49, "ed": 51, "text": "real valued"}, {"st": 84, "ed": 86, "text": "wide variety"}]
[{"st": 8, "ed": 10, "text": "back propagation"}, {"st": 10, "ed": 12, "text": "neural network"}, {"st": 62, "ed": 64, "text": "back propagation"}, {"st": 127, "ed": 129, "text": "speech signal"}, {"st": 135, "ed": 137, "text": "recognition rate"}]
[{"st": 0, "ed": 4, "text": "recurrent neural networks rnns"}, {"st": 11, "ed": 14, "text": "end to end"}, {"st": 33, "ed": 35, "text": "input output"}, {"st": 46, "ed": 49, "text": "short term memory"}, {"st": 49, "ed": 51, "text": "rnn architecture"}, {"st": 69, "ed": 71, "text": "speech recognition"}, {"st": 88, "ed": 92, "text": "deep recurrent neural networks"}, {"st": 95, "ed": 97, "text": "multiple levels"}, {"st": 105, "ed": 107, "text": "deep networks"}, {"st": 119, "ed": 123, "text": "trained end to end"}, {"st": 131, "ed": 134, "text": "short term memory"}]
[{"st": 107, "ed": 109, "text": "success rate"}]
[{"st": 25, "ed": 27, "text": "encoder decoder"}, {"st": 43, "ed": 45, "text": "jointly learn"}, {"st": 87, "ed": 89, "text": "attention mechanism"}]
[{"st": 35, "ed": 37, "text": "cross product"}]
[{"st": 8, "ed": 12, "text": "convolutional neural networks cnn"}, {"st": 16, "ed": 18, "text": "pre trained"}, {"st": 18, "ed": 20, "text": "word vectors"}, {"st": 21, "ed": 23, "text": "sentence level"}, {"st": 33, "ed": 35, "text": "hyperparameter tuning"}, {"st": 39, "ed": 41, "text": "excellent results"}, {"st": 44, "ed": 46, "text": "learning task"}, {"st": 49, "ed": 51, "text": "fine tuning"}, {"st": 72, "ed": 74, "text": "task specific"}, {"st": 97, "ed": 99, "text": "sentiment analysis"}]
[{"st": 1, "ed": 4, "text": "statistical machine translation"}, {"st": 9, "ed": 11, "text": "linear combination"}, {"st": 22, "ed": 24, "text": "linear combination"}, {"st": 52, "ed": 54, "text": "expressive power"}, {"st": 85, "ed": 87, "text": "neural networks"}, {"st": 95, "ed": 97, "text": "learning framework"}, {"st": 113, "ed": 115, "text": "network structure"}, {"st": 134, "ed": 136, "text": "phrase based"}, {"st": 136, "ed": 138, "text": "machine translation"}]
[{"st": 3, "ed": 5, "text": "neural network"}, {"st": 8, "ed": 10, "text": "attention model"}, {"st": 37, "ed": 41, "text": "trained end to end"}, {"st": 96, "ed": 98, "text": "question answering"}, {"st": 129, "ed": 131, "text": "comparable performance"}]
[]
[{"st": 1, "ed": 4, "text": "short term memory"}, {"st": 22, "ed": 24, "text": "speech recognition"}, {"st": 29, "ed": 31, "text": "performance improvement"}, {"st": 58, "ed": 60, "text": "previous research"}, {"st": 62, "ed": 66, "text": "deep recurrent neural networks"}, {"st": 68, "ed": 70, "text": "deep lstm"}, {"st": 74, "ed": 76, "text": "empirically evaluated"}, {"st": 78, "ed": 80, "text": "large vocabulary"}, {"st": 82, "ed": 84, "text": "speech recognition"}, {"st": 92, "ed": 94, "text": "training process"}, {"st": 95, "ed": 97, "text": "lstm networks"}, {"st": 106, "ed": 108, "text": "deep lstm"}]
[{"st": 0, "ed": 2, "text": "mixed language"}, {"st": 13, "ed": 15, "text": "natural language"}, {"st": 21, "ed": 23, "text": "machine translation"}, {"st": 24, "ed": 26, "text": "sentiment analysis"}, {"st": 43, "ed": 45, "text": "multiple languages"}, {"st": 52, "ed": 54, "text": "code switching"}, {"st": 79, "ed": 83, "text": "part of speech tagging"}, {"st": 87, "ed": 89, "text": "named entity"}, {"st": 96, "ed": 98, "text": "machine learning"}, {"st": 104, "ed": 107, "text": "recurrent neural networks"}, {"st": 113, "ed": 115, "text": "word embedding"}, {"st": 116, "ed": 118, "text": "automatically learn"}, {"st": 123, "ed": 125, "text": "mixed language"}, {"st": 143, "ed": 145, "text": "code switching"}, {"st": 154, "ed": 156, "text": "error rate"}]
[{"st": 10, "ed": 12, "text": "dynamic programming"}, {"st": 21, "ed": 23, "text": "neural net"}, {"st": 86, "ed": 88, "text": "sufficient statistics"}, {"st": 102, "ed": 104, "text": "strong baseline"}]
[{"st": 10, "ed": 13, "text": "short term memory"}, {"st": 14, "ed": 16, "text": "neural networks"}, {"st": 79, "ed": 81, "text": "conducted experiments"}, {"st": 93, "ed": 95, "text": "machine translation"}]
[{"st": 0, "ed": 4, "text": "convolutional neural networks cnns"}, {"st": 15, "ed": 17, "text": "large vocabulary"}, {"st": 17, "ed": 20, "text": "continuous speech recognition"}, {"st": 31, "ed": 33, "text": "recent advances"}, {"st": 38, "ed": 40, "text": "neural networks"}, {"st": 62, "ed": 65, "text": "deep convolutional network"}, {"st": 75, "ed": 77, "text": "convolutional layers"}, {"st": 79, "ed": 81, "text": "pooling layer"}, {"st": 104, "ed": 106, "text": "multi scale"}, {"st": 106, "ed": 108, "text": "input features"}, {"st": 129, "ed": 131, "text": "speech recognition"}, {"st": 159, "ed": 161, "text": "deep cnns"}, {"st": 173, "ed": 175, "text": "training data"}, {"st": 177, "ed": 180, "text": "word error rate"}, {"st": 183, "ed": 185, "text": "cross entropy"}]
[{"st": 0, "ed": 2, "text": "question answering"}, {"st": 88, "ed": 90, "text": "pre trained"}, {"st": 92, "ed": 94, "text": "encoder decoder"}, {"st": 108, "ed": 110, "text": "fine tuned"}, {"st": 115, "ed": 117, "text": "evaluation demonstrates"}, {"st": 130, "ed": 133, "text": "neural network architectures"}]
[{"st": 1, "ed": 5, "text": "recurrent neural networks rnns"}, {"st": 20, "ed": 22, "text": "speech synthesis"}, {"st": 25, "ed": 30, "text": "long short term memory lstm"}, {"st": 38, "ed": 41, "text": "vanishing gradient problem"}, {"st": 50, "ed": 52, "text": "recent studies"}, {"st": 65, "ed": 69, "text": "feed forward neural networks"}, {"st": 128, "ed": 130, "text": "significantly fewer"}]
[{"st": 22, "ed": 24, "text": "machine reading"}, {"st": 46, "ed": 49, "text": "short term memory"}, {"st": 69, "ed": 71, "text": "neural attention"}, {"st": 107, "ed": 109, "text": "sentiment analysis"}, {"st": 110, "ed": 112, "text": "natural language"}]
[{"st": 2, "ed": 5, "text": "recurrent neural network"}, {"st": 6, "ed": 8, "text": "probabilistic models"}, {"st": 16, "ed": 18, "text": "efficient inference"}, {"st": 40, "ed": 42, "text": "previously published"}, {"st": 43, "ed": 45, "text": "generative model"}]
[{"st": 20, "ed": 22, "text": "neural network"}, {"st": 26, "ed": 28, "text": "output sequence"}, {"st": 79, "ed": 81, "text": "natural language"}, {"st": 97, "ed": 99, "text": "trading card"}]
[{"st": 31, "ed": 33, "text": "dnn based"}, {"st": 41, "ed": 43, "text": "recently proposed"}, {"st": 115, "ed": 117, "text": "dnn based"}]
[{"st": 7, "ed": 9, "text": "neural network"}, {"st": 82, "ed": 84, "text": "speech recognition"}, {"st": 86, "ed": 88, "text": "significant improvement"}, {"st": 95, "ed": 97, "text": "n gram"}]
[{"st": 4, "ed": 6, "text": "neural attention"}, {"st": 23, "ed": 25, "text": "unlike previous"}, {"st": 42, "ed": 44, "text": "attention mechanism"}, {"st": 47, "ed": 49, "text": "fine grained"}, {"st": 72, "ed": 74, "text": "cnn news"}]
[{"st": 93, "ed": 95, "text": "sentiment analysis"}, {"st": 96, "ed": 98, "text": "manually annotated"}, {"st": 101, "ed": 103, "text": "approach outperforms"}, {"st": 103, "ed": 105, "text": "attention based"}]
[{"st": 12, "ed": 14, "text": "open problem"}, {"st": 48, "ed": 50, "text": "word representations"}, {"st": 64, "ed": 66, "text": "machine translation"}, {"st": 68, "ed": 70, "text": "jointly learns"}, {"st": 94, "ed": 96, "text": "strong baseline"}]
[{"st": 8, "ed": 10, "text": "question answering"}, {"st": 26, "ed": 30, "text": "recurrent neural network rnn"}, {"st": 101, "ed": 103, "text": "goal oriented"}]
[{"st": 11, "ed": 13, "text": "attention mechanism"}, {"st": 18, "ed": 22, "text": "neural machine translation nmt"}, {"st": 44, "ed": 46, "text": "real life"}, {"st": 46, "ed": 48, "text": "e commerce"}, {"st": 73, "ed": 75, "text": "meta data"}, {"st": 86, "ed": 88, "text": "significantly improve"}, {"st": 133, "ed": 135, "text": "domain adaptation"}, {"st": 142, "ed": 144, "text": "e commerce"}, {"st": 167, "ed": 169, "text": "phrase based"}]
[{"st": 50, "ed": 52, "text": "fairy tale"}, {"st": 59, "ed": 61, "text": "attention based"}, {"st": 61, "ed": 63, "text": "neural network"}, {"st": 91, "ed": 93, "text": "neural network"}, {"st": 93, "ed": 95, "text": "significantly outperforms"}]
[{"st": 4, "ed": 6, "text": "neural architecture"}, {"st": 83, "ed": 85, "text": "benchmark datasets"}, {"st": 97, "ed": 99, "text": "qualitative analysis"}]
[{"st": 21, "ed": 23, "text": "neural network"}, {"st": 55, "ed": 57, "text": "attention mechanism"}, {"st": 71, "ed": 73, "text": "previous works"}, {"st": 74, "ed": 76, "text": "neural network"}, {"st": 79, "ed": 81, "text": "pre defined"}, {"st": 81, "ed": 83, "text": "hyper parameters"}, {"st": 98, "ed": 100, "text": "attention model"}, {"st": 100, "ed": 102, "text": "significantly outperforms"}, {"st": 110, "ed": 112, "text": "large margin"}]
[{"st": 0, "ed": 3, "text": "deep neural networks"}, {"st": 10, "ed": 12, "text": "related tasks"}, {"st": 20, "ed": 22, "text": "rnn based"}, {"st": 40, "ed": 42, "text": "sensitivity analysis"}, {"st": 51, "ed": 54, "text": "quantitative and qualitative"}]
[{"st": 36, "ed": 38, "text": "attention model"}, {"st": 44, "ed": 46, "text": "explicitly modeling"}, {"st": 59, "ed": 61, "text": "recurrent network"}]
[{"st": 4, "ed": 6, "text": "speech recognition"}, {"st": 52, "ed": 55, "text": "automatic gain control"}, {"st": 75, "ed": 77, "text": "keyword spotting"}, {"st": 92, "ed": 94, "text": "significantly improves"}, {"st": 101, "ed": 103, "text": "neural network"}, {"st": 113, "ed": 115, "text": "keyword spotting"}]
[{"st": 9, "ed": 11, "text": "neural network"}, {"st": 16, "ed": 18, "text": "error detection"}, {"st": 35, "ed": 37, "text": "error detection"}]
[{"st": 1, "ed": 3, "text": "error detection"}, {"st": 22, "ed": 24, "text": "current approaches"}, {"st": 54, "ed": 56, "text": "recurrent neural"}, {"st": 66, "ed": 68, "text": "background knowledge"}, {"st": 86, "ed": 88, "text": "error correction"}]
[{"st": 1, "ed": 3, "text": "attention mechanism"}, {"st": 9, "ed": 13, "text": "neural machine translation nmt"}, {"st": 24, "ed": 26, "text": "fixed length"}, {"st": 56, "ed": 58, "text": "attention mechanism"}, {"st": 66, "ed": 68, "text": "natural language"}, {"st": 83, "ed": 85, "text": "attention mechanism"}, {"st": 89, "ed": 91, "text": "image captioning"}]
[{"st": 1, "ed": 4, "text": "short term memory"}, {"st": 5, "ed": 9, "text": "recurrent neural networks rnns"}, {"st": 21, "ed": 23, "text": "speech recognition"}, {"st": 45, "ed": 49, "text": "convolutional neural networks cnns"}, {"st": 51, "ed": 53, "text": "significant improvements"}, {"st": 55, "ed": 59, "text": "feed forward neural networks"}, {"st": 77, "ed": 79, "text": "network architecture"}, {"st": 82, "ed": 85, "text": "recurrent neural network"}, {"st": 112, "ed": 114, "text": "local feature"}, {"st": 122, "ed": 124, "text": "lstm network"}, {"st": 139, "ed": 141, "text": "lstm rnns"}, {"st": 164, "ed": 166, "text": "speech recognition"}]
[{"st": 1, "ed": 3, "text": "neural networks"}, {"st": 4, "ed": 7, "text": "achieved great success"}, {"st": 8, "ed": 10, "text": "sentiment classification"}, {"st": 32, "ed": 34, "text": "sentiment classification"}, {"st": 36, "ed": 38, "text": "recurrent architecture"}, {"st": 55, "ed": 58, "text": "short term memory"}, {"st": 58, "ed": 60, "text": "neural networks"}, {"st": 115, "ed": 117, "text": "sentiment analysis"}]
[{"st": 11, "ed": 13, "text": "neural attention"}, {"st": 33, "ed": 35, "text": "based approaches"}, {"st": 138, "ed": 140, "text": "graph based"}]
[{"st": 6, "ed": 8, "text": "neural network"}, {"st": 31, "ed": 33, "text": "structural information"}]
[{"st": 21, "ed": 23, "text": "deep learning"}, {"st": 39, "ed": 41, "text": "raw data"}]
[{"st": 5, "ed": 7, "text": "response generation"}, {"st": 22, "ed": 24, "text": "weakly supervised"}, {"st": 37, "ed": 39, "text": "encoder network"}, {"st": 42, "ed": 44, "text": "neural network"}, {"st": 106, "ed": 108, "text": "significantly higher"}, {"st": 114, "ed": 116, "text": "tf idf"}]
[{"st": 22, "ed": 24, "text": "multiple source"}, {"st": 37, "ed": 39, "text": "attention mechanisms"}, {"st": 52, "ed": 54, "text": "existing techniques"}, {"st": 79, "ed": 82, "text": "achieve competitive results"}]
[{"st": 50, "ed": 52, "text": "machine translation"}, {"st": 75, "ed": 77, "text": "natural language"}, {"st": 96, "ed": 98, "text": "theoretical guarantees"}, {"st": 99, "ed": 101, "text": "empirical evidence"}]
[{"st": 3, "ed": 5, "text": "neural network"}, {"st": 7, "ed": 9, "text": "jointly learns"}, {"st": 9, "ed": 11, "text": "distributed representations"}, {"st": 89, "ed": 91, "text": "question answering"}, {"st": 93, "ed": 96, "text": "unsupervised and supervised"}, {"st": 116, "ed": 118, "text": "trained models"}]
[{"st": 10, "ed": 13, "text": "encoder decoder architecture"}, {"st": 18, "ed": 20, "text": "character level"}, {"st": 35, "ed": 38, "text": "source and target"}, {"st": 76, "ed": 79, "text": "end to end"}, {"st": 81, "ed": 83, "text": "fully differentiable"}, {"st": 90, "ed": 92, "text": "strong baseline"}, {"st": 94, "ed": 96, "text": "character level"}, {"st": 97, "ed": 100, "text": "neural machine translation"}]
[{"st": 31, "ed": 33, "text": "image caption"}, {"st": 57, "ed": 59, "text": "parallel corpora"}, {"st": 69, "ed": 71, "text": "cross lingual"}, {"st": 71, "ed": 73, "text": "image captioning"}]
[{"st": 6, "ed": 8, "text": "empirical comparison"}, {"st": 14, "ed": 16, "text": "attention based"}, {"st": 19, "ed": 22, "text": "end to end"}, {"st": 80, "ed": 82, "text": "speech recognition"}, {"st": 92, "ed": 94, "text": "neural network"}]
[{"st": 3, "ed": 5, "text": "compositional structure"}, {"st": 11, "ed": 13, "text": "deep network"}]
[{"st": 6, "ed": 10, "text": "feed forward neural networks"}, {"st": 35, "ed": 37, "text": "computational requirements"}, {"st": 43, "ed": 45, "text": "resource constrained"}, {"st": 47, "ed": 49, "text": "mobile phones"}, {"st": 57, "ed": 59, "text": "neural network"}]
[{"st": 0, "ed": 4, "text": "recurrent neural networks rnns"}, {"st": 8, "ed": 10, "text": "building block"}, {"st": 15, "ed": 17, "text": "natural language"}, {"st": 18, "ed": 20, "text": "recent research"}, {"st": 44, "ed": 46, "text": "machine learning"}, {"st": 56, "ed": 58, "text": "regularization techniques"}, {"st": 59, "ed": 61, "text": "l2 regularization"}, {"st": 69, "ed": 71, "text": "hidden states"}, {"st": 92, "ed": 94, "text": "rnn architectures"}, {"st": 97, "ed": 99, "text": "performance improvements"}, {"st": 105, "ed": 107, "text": "regularization techniques"}, {"st": 112, "ed": 114, "text": "regularization techniques"}]
[{"st": 1, "ed": 4, "text": "recurrent neural network"}, {"st": 24, "ed": 26, "text": "recurrent unit"}, {"st": 29, "ed": 31, "text": "recurrent unit"}, {"st": 64, "ed": 66, "text": "convolutional layer"}, {"st": 83, "ed": 85, "text": "applications including"}, {"st": 86, "ed": 88, "text": "question answering"}, {"st": 95, "ed": 97, "text": "experiments demonstrate"}, {"st": 112, "ed": 114, "text": "open source"}]
[{"st": 37, "ed": 39, "text": "outperforms existing"}, {"st": 52, "ed": 54, "text": "word level"}, {"st": 74, "ed": 76, "text": "character level"}]
[{"st": 8, "ed": 10, "text": "natural language"}, {"st": 30, "ed": 32, "text": "background knowledge"}, {"st": 41, "ed": 43, "text": "control policies"}, {"st": 53, "ed": 55, "text": "natural language"}, {"st": 58, "ed": 60, "text": "parameter space"}, {"st": 89, "ed": 91, "text": "natural language"}, {"st": 143, "ed": 145, "text": "image classification"}, {"st": 148, "ed": 150, "text": "reinforcement learning"}]
[{"st": 3, "ed": 5, "text": "challenging problem"}, {"st": 11, "ed": 13, "text": "fine grained"}, {"st": 48, "ed": 50, "text": "fine grained"}, {"st": 124, "ed": 126, "text": "future research"}]
[{"st": 19, "ed": 21, "text": "skip connections"}, {"st": 28, "ed": 30, "text": "skip connections"}, {"st": 57, "ed": 59, "text": "skip connections"}, {"st": 76, "ed": 78, "text": "frame rate"}, {"st": 82, "ed": 85, "text": "speech recognition tasks"}, {"st": 95, "ed": 97, "text": "consistently outperform"}, {"st": 124, "ed": 127, "text": "word error rate"}, {"st": 133, "ed": 135, "text": "cross entropy"}, {"st": 159, "ed": 161, "text": "recognition task"}, {"st": 170, "ed": 172, "text": "relative improvement"}]
[{"st": 19, "ed": 21, "text": "observed data"}, {"st": 61, "ed": 63, "text": "variable stars"}, {"st": 156, "ed": 158, "text": "data stream"}, {"st": 202, "ed": 204, "text": "applications including"}, {"st": 235, "ed": 237, "text": "domain knowledge"}]
[{"st": 0, "ed": 3, "text": "principal components analysis"}, {"st": 8, "ed": 10, "text": "auto encoder"}, {"st": 24, "ed": 26, "text": "principal components"}, {"st": 44, "ed": 46, "text": "sparse linear"}, {"st": 58, "ed": 60, "text": "a level"}, {"st": 76, "ed": 78, "text": "low order"}, {"st": 105, "ed": 107, "text": "low order"}, {"st": 116, "ed": 118, "text": "auto encoders"}]
[{"st": 73, "ed": 75, "text": "mean shift"}, {"st": 78, "ed": 80, "text": "main contribution"}, {"st": 102, "ed": 104, "text": "mean shift"}, {"st": 120, "ed": 123, "text": "maximum mean discrepancy"}, {"st": 126, "ed": 128, "text": "gaussian kernel"}, {"st": 132, "ed": 134, "text": "sample size"}, {"st": 158, "ed": 162, "text": "signal to noise ratio"}]
[{"st": 5, "ed": 7, "text": "linear models"}, {"st": 46, "ed": 48, "text": "sample size"}, {"st": 103, "ed": 106, "text": "mean squared error"}, {"st": 127, "ed": 129, "text": "message passing"}, {"st": 153, "ed": 156, "text": "mean squared error"}, {"st": 231, "ed": 233, "text": "least square"}, {"st": 237, "ed": 239, "text": "heavy tailed"}]
[{"st": 6, "ed": 8, "text": "decision theoretic"}, {"st": 15, "ed": 17, "text": "random variables"}, {"st": 89, "ed": 93, "text": "reproducing kernel hilbert spaces"}, {"st": 106, "ed": 108, "text": "statistical power"}, {"st": 112, "ed": 114, "text": "distance based"}, {"st": 144, "ed": 147, "text": "maximum mean discrepancy"}, {"st": 149, "ed": 151, "text": "gaussian kernel"}, {"st": 217, "ed": 219, "text": "asymptotically optimal"}, {"st": 241, "ed": 243, "text": "kernel bandwidth"}]
[{"st": 10, "ed": 13, "text": "convex loss functions"}, {"st": 14, "ed": 16, "text": "binary classification"}, {"st": 25, "ed": 28, "text": "simple and effective"}, {"st": 28, "ed": 30, "text": "boosting algorithm"}, {"st": 48, "ed": 50, "text": "convex loss"}, {"st": 51, "ed": 53, "text": "prediction accuracy"}, {"st": 132, "ed": 134, "text": "classification algorithm"}, {"st": 159, "ed": 161, "text": "boosting algorithms"}, {"st": 205, "ed": 207, "text": "theoretical results"}, {"st": 226, "ed": 228, "text": "boosting algorithms"}, {"st": 234, "ed": 236, "text": "extensive numerical"}, {"st": 242, "ed": 244, "text": "theoretical properties"}]
[{"st": 0, "ed": 4, "text": "deep convolutional neural networks"}, {"st": 12, "ed": 14, "text": "machine learning"}, {"st": 30, "ed": 32, "text": "atari games"}, {"st": 34, "ed": 36, "text": "board game"}, {"st": 46, "ed": 48, "text": "feature extraction"}, {"st": 62, "ed": 66, "text": "deep convolutional neural networks"}, {"st": 67, "ed": 69, "text": "feature extraction"}, {"st": 90, "ed": 92, "text": "non linearity"}, {"st": 94, "ed": 96, "text": "network layer"}, {"st": 104, "ed": 106, "text": "scale parameter"}, {"st": 150, "ed": 152, "text": "lipschitz continuous"}, {"st": 152, "ed": 154, "text": "non linearities"}, {"st": 155, "ed": 158, "text": "rectified linear units"}, {"st": 168, "ed": 170, "text": "lipschitz continuous"}, {"st": 194, "ed": 196, "text": "feature extractor"}, {"st": 218, "ed": 220, "text": "network depth"}]
[{"st": 21, "ed": 23, "text": "main contributions"}, {"st": 68, "ed": 70, "text": "recently introduced"}, {"st": 70, "ed": 72, "text": "posterior sampling"}, {"st": 128, "ed": 130, "text": "linear regression"}]
[{"st": 5, "ed": 7, "text": "unsupervised learning"}, {"st": 14, "ed": 16, "text": "large scale"}, {"st": 27, "ed": 29, "text": "mutual information"}, {"st": 70, "ed": 72, "text": "objective function"}, {"st": 83, "ed": 85, "text": "method works"}, {"st": 94, "ed": 96, "text": "numerical experiments"}, {"st": 101, "ed": 103, "text": "highly efficient"}, {"st": 105, "ed": 107, "text": "salient features"}, {"st": 114, "ed": 116, "text": "existing methods"}, {"st": 125, "ed": 127, "text": "training speed"}, {"st": 136, "ed": 138, "text": "proposed method"}]
[{"st": 7, "ed": 9, "text": "statistical inference"}, {"st": 15, "ed": 18, "text": "stochastic gradient descent"}, {"st": 22, "ed": 24, "text": "step size"}, {"st": 37, "ed": 39, "text": "statistical inference"}, {"st": 64, "ed": 66, "text": "inference procedure"}, {"st": 76, "ed": 78, "text": "large scale"}, {"st": 88, "ed": 91, "text": "synthetic and real"}, {"st": 101, "ed": 103, "text": "statistical methods"}]
[{"st": 9, "ed": 11, "text": "polyphonic music"}, {"st": 26, "ed": 28, "text": "chord progression"}, {"st": 38, "ed": 40, "text": "polyphonic music"}]
[{"st": 0, "ed": 3, "text": "wireless sensor networks"}, {"st": 9, "ed": 11, "text": "recent years"}, {"st": 80, "ed": 82, "text": "applications including"}, {"st": 91, "ed": 93, "text": "least squares"}, {"st": 95, "ed": 97, "text": "message passing"}, {"st": 130, "ed": 132, "text": "numerical simulations"}]
[]
[{"st": 2, "ed": 4, "text": "real world"}, {"st": 22, "ed": 24, "text": "dna microarray"}, {"st": 34, "ed": 36, "text": "low dimensional"}, {"st": 57, "ed": 60, "text": "sparse subspace clustering"}, {"st": 71, "ed": 73, "text": "low dimensional"}, {"st": 75, "ed": 77, "text": "key idea"}, {"st": 94, "ed": 96, "text": "sparse representation"}, {"st": 119, "ed": 121, "text": "spectral clustering"}, {"st": 139, "ed": 141, "text": "np hard"}, {"st": 144, "ed": 146, "text": "convex relaxation"}, {"st": 174, "ed": 176, "text": "proposed algorithm"}, {"st": 178, "ed": 180, "text": "solved efficiently"}, {"st": 195, "ed": 197, "text": "proposed algorithm"}, {"st": 220, "ed": 222, "text": "missing entries"}, {"st": 241, "ed": 243, "text": "proposed algorithm"}, {"st": 246, "ed": 248, "text": "synthetic data"}, {"st": 253, "ed": 255, "text": "real world"}, {"st": 257, "ed": 259, "text": "motion segmentation"}]
[{"st": 68, "ed": 70, "text": "ground truth"}, {"st": 72, "ed": 74, "text": "recent advances"}, {"st": 80, "ed": 82, "text": "recovery guarantees"}, {"st": 92, "ed": 94, "text": "existing methods"}, {"st": 142, "ed": 144, "text": "multiple objects"}, {"st": 164, "ed": 166, "text": "ground truth"}, {"st": 169, "ed": 171, "text": "parameter free"}, {"st": 183, "ed": 185, "text": "total number"}, {"st": 194, "ed": 196, "text": "near optimal"}, {"st": 196, "ed": 198, "text": "error correction"}, {"st": 216, "ed": 218, "text": "left frac"}, {"st": 259, "ed": 261, "text": "proposed algorithm"}, {"st": 263, "ed": 266, "text": "benchmark data sets"}, {"st": 270, "ed": 272, "text": "real world"}]
[{"st": 1, "ed": 4, "text": "a posteriori map"}, {"st": 7, "ed": 10, "text": "markov random fields"}, {"st": 19, "ed": 21, "text": "real world"}, {"st": 27, "ed": 29, "text": "np hard"}, {"st": 59, "ed": 64, "text": "alternating direction method of multipliers"}, {"st": 88, "ed": 90, "text": "large scale"}, {"st": 98, "ed": 101, "text": "hundreds of thousands"}, {"st": 119, "ed": 121, "text": "comparable accuracy"}, {"st": 141, "ed": 143, "text": "small scale"}, {"st": 154, "ed": 156, "text": "benchmark datasets"}]
[{"st": 1, "ed": 3, "text": "pattern recognition"}, {"st": 17, "ed": 19, "text": "central moment"}, {"st": 25, "ed": 28, "text": "principal component analysis"}, {"st": 33, "ed": 35, "text": "least squares"}, {"st": 36, "ed": 39, "text": "canonical correlation analysis"}, {"st": 60, "ed": 63, "text": "singular value decomposition"}, {"st": 68, "ed": 70, "text": "component analysis"}, {"st": 74, "ed": 76, "text": "learning framework"}, {"st": 116, "ed": 118, "text": "machine learning"}, {"st": 127, "ed": 129, "text": "kernel based"}, {"st": 136, "ed": 138, "text": "inner product"}, {"st": 159, "ed": 161, "text": "outer product"}, {"st": 193, "ed": 195, "text": "mean shift"}, {"st": 203, "ed": 205, "text": "experiments conducted"}, {"st": 206, "ed": 209, "text": "simulated and real"}]
[{"st": 16, "ed": 18, "text": "mathbb r"}, {"st": 36, "ed": 38, "text": "sparse recovery"}, {"st": 43, "ed": 46, "text": "sparse dictionary learning"}, {"st": 46, "ed": 48, "text": "sparse pca"}, {"st": 53, "ed": 55, "text": "signal processing"}, {"st": 88, "ed": 90, "text": "recovery problem"}, {"st": 161, "ed": 163, "text": "proposed algorithm"}]
[{"st": 3, "ed": 6, "text": "problem of recovering"}, {"st": 11, "ed": 13, "text": "invertible matrix"}, {"st": 20, "ed": 22, "text": "mathbb r"}, {"st": 31, "ed": 33, "text": "mathbf x"}, {"st": 35, "ed": 37, "text": "mathbf x"}, {"st": 42, "ed": 44, "text": "recovery problem"}, {"st": 48, "ed": 50, "text": "theoretical understanding"}, {"st": 56, "ed": 58, "text": "sparse representation"}, {"st": 62, "ed": 64, "text": "input signals"}, {"st": 66, "ed": 68, "text": "numerous applications"}, {"st": 70, "ed": 72, "text": "signal processing"}, {"st": 88, "ed": 90, "text": "mathbf x"}, {"st": 102, "ed": 104, "text": "mathbf x"}, {"st": 112, "ed": 114, "text": "efficient algorithms"}, {"st": 115, "ed": 117, "text": "recovery guarantees"}, {"st": 118, "ed": 120, "text": "mathbf x"}, {"st": 146, "ed": 148, "text": "nonconvex optimization"}, {"st": 198, "ed": 200, "text": "geometric structure"}, {"st": 206, "ed": 208, "text": "trust region"}]
[{"st": 10, "ed": 12, "text": "application areas"}, {"st": 14, "ed": 16, "text": "signal processing"}, {"st": 19, "ed": 21, "text": "nuclear norm"}, {"st": 23, "ed": 25, "text": "convex relaxation"}, {"st": 44, "ed": 46, "text": "real world"}, {"st": 47, "ed": 49, "text": "nuclear norm"}, {"st": 68, "ed": 70, "text": "higher accuracy"}, {"st": 72, "ed": 74, "text": "nuclear norm"}, {"st": 80, "ed": 82, "text": "rank approximation"}, {"st": 90, "ed": 92, "text": "rank approximation"}, {"st": 93, "ed": 95, "text": "subspace clustering"}, {"st": 107, "ed": 109, "text": "optimization strategy"}, {"st": 112, "ed": 114, "text": "theoretical guarantee"}, {"st": 121, "ed": 123, "text": "proposed method"}, {"st": 124, "ed": 126, "text": "promising results"}, {"st": 130, "ed": 132, "text": "motion segmentation"}, {"st": 140, "ed": 142, "text": "subspace clustering"}]
[{"st": 1, "ed": 3, "text": "subspace clustering"}, {"st": 35, "ed": 37, "text": "subspace clustering"}, {"st": 74, "ed": 76, "text": "iterative method"}, {"st": 87, "ed": 89, "text": "linear optimization"}, {"st": 128, "ed": 130, "text": "sufficient conditions"}, {"st": 138, "ed": 140, "text": "proposed approach"}, {"st": 187, "ed": 189, "text": "spectral clustering"}, {"st": 195, "ed": 197, "text": "spectral clustering"}, {"st": 200, "ed": 202, "text": "numerical simulations"}, {"st": 204, "ed": 208, "text": "real and synthetic data"}, {"st": 219, "ed": 221, "text": "subspace clustering"}, {"st": 232, "ed": 234, "text": "significantly improves"}, {"st": 241, "ed": 243, "text": "subspace segmentation"}]
[{"st": 34, "ed": 36, "text": "maximum likelihood"}, {"st": 60, "ed": 63, "text": "problem of recovering"}, {"st": 64, "ed": 66, "text": "discrete variables"}, {"st": 71, "ed": 73, "text": "m 1"}, {"st": 78, "ed": 80, "text": "noisy observations"}, {"st": 95, "ed": 97, "text": "low complexity"}, {"st": 152, "ed": 154, "text": "statistical models"}, {"st": 167, "ed": 169, "text": "maximum likelihood"}, {"st": 174, "ed": 176, "text": "numerical experiments"}, {"st": 182, "ed": 186, "text": "synthetic and real data"}, {"st": 196, "ed": 198, "text": "algorithmic framework"}]
[{"st": 0, "ed": 5, "text": "deep convolutional neural networks cnns"}, {"st": 23, "ed": 25, "text": "significant computational"}, {"st": 76, "ed": 78, "text": "low power"}, {"st": 93, "ed": 96, "text": "depth and width"}, {"st": 100, "ed": 102, "text": "feature extraction"}, {"st": 123, "ed": 125, "text": "non linearity"}, {"st": 130, "ed": 132, "text": "feature map"}, {"st": 143, "ed": 145, "text": "mathcal o"}, {"st": 193, "ed": 195, "text": "input signal"}, {"st": 226, "ed": 228, "text": "input signals"}, {"st": 232, "ed": 234, "text": "feature map"}, {"st": 253, "ed": 255, "text": "network depth"}]
[{"st": 19, "ed": 21, "text": "classical mechanics"}, {"st": 39, "ed": 41, "text": "metric spaces"}, {"st": 51, "ed": 55, "text": "reproducing kernel hilbert spaces"}, {"st": 62, "ed": 64, "text": "clustering problem"}, {"st": 101, "ed": 104, "text": "kernel k means"}, {"st": 104, "ed": 106, "text": "optimization problem"}, {"st": 119, "ed": 122, "text": "kernel k means"}, {"st": 160, "ed": 162, "text": "iterative algorithm"}, {"st": 174, "ed": 176, "text": "computational cost"}, {"st": 177, "ed": 180, "text": "kernel k means"}, {"st": 194, "ed": 196, "text": "carefully designed"}, {"st": 196, "ed": 198, "text": "numerical experiments"}, {"st": 203, "ed": 205, "text": "proposed method"}, {"st": 207, "ed": 210, "text": "kernel k means"}, {"st": 210, "ed": 212, "text": "spectral clustering"}, {"st": 213, "ed": 215, "text": "k means"}, {"st": 216, "ed": 219, "text": "gaussian mixture models"}]
[{"st": 6, "ed": 8, "text": "feature extractor"}, {"st": 10, "ed": 12, "text": "deep model"}, {"st": 44, "ed": 46, "text": "sensitive information"}, {"st": 82, "ed": 84, "text": "sensitive information"}, {"st": 118, "ed": 120, "text": "image datasets"}, {"st": 128, "ed": 130, "text": "achieve high"}]
[{"st": 71, "ed": 73, "text": "human brain"}, {"st": 74, "ed": 76, "text": "self organizing"}, {"st": 76, "ed": 78, "text": "neural network"}, {"st": 122, "ed": 124, "text": "output layer"}, {"st": 126, "ed": 128, "text": "neural network"}]
[{"st": 2, "ed": 4, "text": "kerala state"}, {"st": 30, "ed": 32, "text": "highly nonlinear"}, {"st": 53, "ed": 55, "text": "neural network"}, {"st": 57, "ed": 59, "text": "time series"}]
[{"st": 6, "ed": 8, "text": "genetic algorithm"}, {"st": 139, "ed": 141, "text": "cellular automata"}, {"st": 145, "ed": 147, "text": "classification problem"}]
[{"st": 1, "ed": 3, "text": "article presents"}, {"st": 48, "ed": 50, "text": "search space"}, {"st": 56, "ed": 58, "text": "compositional structure"}, {"st": 61, "ed": 63, "text": "pattern matching"}, {"st": 108, "ed": 110, "text": "positive feedback"}]
[{"st": 1, "ed": 3, "text": "recently introduced"}, {"st": 14, "ed": 16, "text": "optimization problems"}, {"st": 23, "ed": 25, "text": "recent progress"}, {"st": 35, "ed": 38, "text": "self organized criticality"}, {"st": 49, "ed": 51, "text": "method called"}, {"st": 65, "ed": 67, "text": "optimal solution"}, {"st": 77, "ed": 79, "text": "cost function"}, {"st": 89, "ed": 91, "text": "local optima"}, {"st": 96, "ed": 98, "text": "configuration space"}, {"st": 127, "ed": 129, "text": "statistical physics"}, {"st": 132, "ed": 134, "text": "simulated annealing"}, {"st": 179, "ed": 181, "text": "constrained optimization"}]
[]
[{"st": 51, "ed": 54, "text": "pieces of evidence"}, {"st": 107, "ed": 111, "text": "royal institute of technology"}, {"st": 156, "ed": 158, "text": "posterior probability"}]
[{"st": 160, "ed": 162, "text": "posterior probability"}]
[]
[{"st": 43, "ed": 46, "text": "pieces of evidence"}, {"st": 216, "ed": 219, "text": "pieces of evidence"}, {"st": 230, "ed": 232, "text": "evidence based"}]
[{"st": 7, "ed": 10, "text": "pieces of evidence"}, {"st": 42, "ed": 45, "text": "pieces of evidence"}, {"st": 66, "ed": 68, "text": "computational complexity"}, {"st": 100, "ed": 102, "text": "computational complexity"}, {"st": 105, "ed": 107, "text": "total number"}]
[{"st": 15, "ed": 18, "text": "pieces of evidence"}, {"st": 47, "ed": 49, "text": "large scale"}, {"st": 70, "ed": 72, "text": "iterative optimization"}, {"st": 88, "ed": 90, "text": "iterative optimization"}, {"st": 91, "ed": 93, "text": "medium sized"}, {"st": 110, "ed": 112, "text": "global minimum"}]
[{"st": 18, "ed": 20, "text": "neural network"}, {"st": 43, "ed": 46, "text": "pieces of evidence"}, {"st": 52, "ed": 54, "text": "neural network"}, {"st": 84, "ed": 86, "text": "iterative optimization"}, {"st": 88, "ed": 90, "text": "clustering performance"}, {"st": 130, "ed": 132, "text": "iterative optimization"}]
[{"st": 18, "ed": 20, "text": "neural network"}, {"st": 40, "ed": 43, "text": "pieces of evidence"}, {"st": 48, "ed": 51, "text": "number of clusters"}, {"st": 74, "ed": 77, "text": "number of clusters"}, {"st": 96, "ed": 99, "text": "pieces of evidence"}, {"st": 113, "ed": 116, "text": "number of clusters"}]
[{"st": 15, "ed": 18, "text": "pieces of evidence"}, {"st": 47, "ed": 49, "text": "large scale"}, {"st": 70, "ed": 72, "text": "iterative optimization"}, {"st": 88, "ed": 90, "text": "iterative optimization"}, {"st": 91, "ed": 93, "text": "medium sized"}, {"st": 110, "ed": 112, "text": "global minimum"}]
[{"st": 114, "ed": 116, "text": "front end"}]
[{"st": 15, "ed": 18, "text": "pieces of evidence"}, {"st": 104, "ed": 106, "text": "optimal solutions"}, {"st": 120, "ed": 123, "text": "o n 2"}, {"st": 139, "ed": 141, "text": "optimization problem"}]
[]
[{"st": 22, "ed": 25, "text": "taking into account"}, {"st": 46, "ed": 49, "text": "pieces of evidence"}, {"st": 52, "ed": 54, "text": "clustering process"}, {"st": 84, "ed": 86, "text": "computational performance"}, {"st": 118, "ed": 120, "text": "computational complexity"}, {"st": 129, "ed": 132, "text": "o n 2"}, {"st": 143, "ed": 145, "text": "mean field"}]
[{"st": 37, "ed": 39, "text": "clustering process"}]
[{"st": 19, "ed": 21, "text": "neural networks"}]
[{"st": 200, "ed": 202, "text": "unsupervised learning"}]
[{"st": 13, "ed": 16, "text": "artificial neural networks"}, {"st": 76, "ed": 78, "text": "differential calculus"}, {"st": 94, "ed": 96, "text": "high school"}]
[{"st": 118, "ed": 120, "text": "input data"}, {"st": 138, "ed": 140, "text": "input data"}, {"st": 190, "ed": 192, "text": "working memory"}]
[{"st": 3, "ed": 5, "text": "computational complexity"}, {"st": 17, "ed": 19, "text": "constraint satisfaction"}, {"st": 32, "ed": 34, "text": "spin glass"}, {"st": 52, "ed": 54, "text": "computational complexity"}, {"st": 57, "ed": 59, "text": "spin glass"}, {"st": 71, "ed": 73, "text": "spin glass"}, {"st": 81, "ed": 83, "text": "spin glass"}, {"st": 85, "ed": 87, "text": "closely related"}, {"st": 99, "ed": 101, "text": "hierarchical bayesian"}, {"st": 101, "ed": 103, "text": "optimization algorithm"}, {"st": 136, "ed": 138, "text": "local search"}, {"st": 156, "ed": 158, "text": "performance measures"}, {"st": 159, "ed": 161, "text": "optimization algorithms"}, {"st": 181, "ed": 183, "text": "constraint satisfaction"}]
[{"st": 3, "ed": 5, "text": "hierarchical bayesian"}, {"st": 5, "ed": 7, "text": "optimization algorithm"}, {"st": 17, "ed": 19, "text": "tuning parameters"}, {"st": 33, "ed": 35, "text": "selection pressure"}, {"st": 50, "ed": 52, "text": "selection pressure"}, {"st": 62, "ed": 64, "text": "low order"}]
[{"st": 53, "ed": 55, "text": "neural networks"}]
[{"st": 12, "ed": 14, "text": "subsumption architecture"}, {"st": 15, "ed": 17, "text": "evolutionary robotics"}]
[{"st": 0, "ed": 2, "text": "multi dimensional"}, {"st": 8, "ed": 10, "text": "challenging problem"}, {"st": 15, "ed": 17, "text": "neural networks"}, {"st": 25, "ed": 27, "text": "multi dimensional"}, {"st": 47, "ed": 49, "text": "neural networks"}, {"st": 58, "ed": 61, "text": "multi layer perceptron"}, {"st": 63, "ed": 65, "text": "supervised learning"}, {"st": 76, "ed": 78, "text": "unsupervised learning"}]
[{"st": 70, "ed": 72, "text": "cognitive neuroscience"}]
[{"st": 2, "ed": 4, "text": "e commerce"}, {"st": 90, "ed": 92, "text": "traffic flow"}, {"st": 104, "ed": 106, "text": "self organizing"}, {"st": 115, "ed": 118, "text": "decision support systems"}, {"st": 137, "ed": 139, "text": "control problems"}, {"st": 148, "ed": 150, "text": "clustering algorithm"}, {"st": 160, "ed": 162, "text": "genetic programming"}, {"st": 168, "ed": 170, "text": "empirical results"}, {"st": 182, "ed": 185, "text": "self organizing map"}, {"st": 203, "ed": 205, "text": "fuzzy clustering"}, {"st": 212, "ed": 214, "text": "swarm intelligence"}]
[{"st": 6, "ed": 9, "text": "exploratory data analysis"}, {"st": 50, "ed": 52, "text": "data mining"}, {"st": 58, "ed": 60, "text": "web applications"}, {"st": 72, "ed": 74, "text": "recent approaches"}, {"st": 75, "ed": 77, "text": "self organizing"}, {"st": 87, "ed": 89, "text": "recently proposed"}, {"st": 128, "ed": 130, "text": "swarm intelligence"}, {"st": 135, "ed": 138, "text": "exploratory data analysis"}, {"st": 138, "ed": 140, "text": "image retrieval"}]
[{"st": 124, "ed": 126, "text": "faster convergence"}, {"st": 130, "ed": 132, "text": "genetic algorithms"}, {"st": 147, "ed": 149, "text": "case study"}, {"st": 150, "ed": 152, "text": "genetic algorithm"}, {"st": 156, "ed": 158, "text": "image segmentation"}, {"st": 163, "ed": 165, "text": "unsupervised clustering"}, {"st": 170, "ed": 172, "text": "evolutionary algorithm"}, {"st": 221, "ed": 223, "text": "genetic algorithms"}, {"st": 234, "ed": 236, "text": "genetic algorithms"}, {"st": 241, "ed": 243, "text": "faster convergence"}, {"st": 244, "ed": 246, "text": "image segmentation"}]
[{"st": 124, "ed": 126, "text": "faster convergence"}, {"st": 130, "ed": 132, "text": "genetic algorithms"}, {"st": 147, "ed": 149, "text": "case study"}, {"st": 150, "ed": 152, "text": "genetic algorithm"}, {"st": 156, "ed": 158, "text": "image segmentation"}, {"st": 163, "ed": 165, "text": "unsupervised clustering"}, {"st": 170, "ed": 172, "text": "evolutionary algorithm"}, {"st": 221, "ed": 223, "text": "genetic algorithms"}, {"st": 234, "ed": 236, "text": "genetic algorithms"}, {"st": 241, "ed": 243, "text": "faster convergence"}, {"st": 244, "ed": 246, "text": "image segmentation"}]
[{"st": 63, "ed": 65, "text": "real world"}, {"st": 100, "ed": 102, "text": "genetic algorithms"}, {"st": 116, "ed": 118, "text": "k means"}, {"st": 118, "ed": 120, "text": "unsupervised clustering"}, {"st": 122, "ed": 124, "text": "genetic algorithms"}, {"st": 129, "ed": 131, "text": "evolutionary algorithm"}, {"st": 150, "ed": 152, "text": "non trivial"}, {"st": 156, "ed": 158, "text": "np complete"}, {"st": 187, "ed": 189, "text": "genetic algorithms"}, {"st": 207, "ed": 209, "text": "genetic algorithms"}, {"st": 214, "ed": 216, "text": "faster convergence"}, {"st": 217, "ed": 219, "text": "image segmentation"}]
[{"st": 2, "ed": 5, "text": "artificial neural networks"}, {"st": 7, "ed": 9, "text": "generalization capabilities"}]
[{"st": 8, "ed": 10, "text": "genetic programming"}, {"st": 18, "ed": 20, "text": "genetic algorithms"}, {"st": 28, "ed": 30, "text": "building block"}, {"st": 30, "ed": 32, "text": "decision making"}, {"st": 75, "ed": 77, "text": "building block"}, {"st": 119, "ed": 121, "text": "building block"}]
[{"st": 0, "ed": 2, "text": "genetic algorithms"}, {"st": 17, "ed": 19, "text": "fitness landscape"}, {"st": 28, "ed": 30, "text": "non stationary"}, {"st": 45, "ed": 47, "text": "non stationary"}, {"st": 60, "ed": 63, "text": "minimum description length"}, {"st": 95, "ed": 97, "text": "non stationary"}, {"st": 105, "ed": 107, "text": "non stationary"}, {"st": 107, "ed": 109, "text": "optimization problems"}, {"st": 113, "ed": 115, "text": "search algorithm"}]
[{"st": 3, "ed": 5, "text": "genetic algorithm"}, {"st": 39, "ed": 41, "text": "fitness landscape"}, {"st": 50, "ed": 52, "text": "non stationary"}, {"st": 100, "ed": 102, "text": "space complexity"}]
[{"st": 36, "ed": 38, "text": "proposed method"}, {"st": 78, "ed": 80, "text": "hierarchical bayesian"}, {"st": 80, "ed": 82, "text": "optimization algorithm"}, {"st": 130, "ed": 132, "text": "market share"}]
[{"st": 6, "ed": 8, "text": "genetic programming"}, {"st": 129, "ed": 131, "text": "genetic algorithms"}]
[{"st": 14, "ed": 16, "text": "hierarchical bayesian"}, {"st": 16, "ed": 18, "text": "optimization algorithm"}, {"st": 23, "ed": 25, "text": "genetic algorithm"}, {"st": 135, "ed": 137, "text": "evolutionary algorithms"}]
[]
[{"st": 4, "ed": 6, "text": "evolutionary algorithm"}, {"st": 19, "ed": 22, "text": "travelling salesman problem"}, {"st": 56, "ed": 58, "text": "structural properties"}]
[{"st": 40, "ed": 42, "text": "local optimum"}, {"st": 123, "ed": 125, "text": "optimisation problems"}, {"st": 131, "ed": 133, "text": "optimisation problems"}]
[{"st": 31, "ed": 33, "text": "classification models"}, {"st": 71, "ed": 73, "text": "machine learning"}]
[{"st": 2, "ed": 4, "text": "learning algorithm"}, {"st": 7, "ed": 9, "text": "neural networks"}, {"st": 30, "ed": 32, "text": "hidden neurons"}, {"st": 45, "ed": 47, "text": "hidden neurons"}, {"st": 54, "ed": 56, "text": "successfully applied"}, {"st": 80, "ed": 82, "text": "correctly classified"}, {"st": 94, "ed": 96, "text": "fully connected"}]
[{"st": 3, "ed": 5, "text": "self organizing"}, {"st": 6, "ed": 8, "text": "neural networks"}, {"st": 21, "ed": 23, "text": "self organizing"}, {"st": 24, "ed": 26, "text": "multi layered"}, {"st": 26, "ed": 28, "text": "neural networks"}, {"st": 36, "ed": 38, "text": "neural networks"}]
[{"st": 1, "ed": 3, "text": "neural networks"}, {"st": 14, "ed": 16, "text": "neural networks"}, {"st": 17, "ed": 19, "text": "correctly classified"}, {"st": 51, "ed": 53, "text": "neural networks"}, {"st": 56, "ed": 58, "text": "neural networks"}, {"st": 97, "ed": 99, "text": "decision rules"}]
[{"st": 1, "ed": 3, "text": "neural network"}, {"st": 14, "ed": 16, "text": "classification rules"}, {"st": 24, "ed": 26, "text": "classification rules"}, {"st": 34, "ed": 36, "text": "neural networks"}, {"st": 48, "ed": 50, "text": "classification rules"}]
[{"st": 3, "ed": 5, "text": "multi class"}, {"st": 14, "ed": 16, "text": "neural network"}, {"st": 16, "ed": 18, "text": "decision tree"}, {"st": 38, "ed": 40, "text": "classification models"}, {"st": 77, "ed": 79, "text": "classification accuracy"}, {"st": 120, "ed": 122, "text": "correctly classified"}, {"st": 134, "ed": 136, "text": "correctly classified"}]
[{"st": 2, "ed": 4, "text": "neural networks"}, {"st": 47, "ed": 49, "text": "hidden neurons"}, {"st": 81, "ed": 83, "text": "neural network"}]
[{"st": 4, "ed": 6, "text": "neural networks"}, {"st": 10, "ed": 12, "text": "self organization"}]
[{"st": 14, "ed": 16, "text": "swarm intelligence"}, {"st": 37, "ed": 39, "text": "exploration exploitation"}, {"st": 99, "ed": 101, "text": "control problems"}, {"st": 112, "ed": 114, "text": "genetic algorithms"}, {"st": 182, "ed": 185, "text": "times faster than"}]
[{"st": 16, "ed": 18, "text": "statistical mechanics"}, {"st": 24, "ed": 26, "text": "learning algorithm"}, {"st": 66, "ed": 68, "text": "weight decay"}, {"st": 73, "ed": 75, "text": "learning algorithm"}, {"st": 125, "ed": 127, "text": "convergence speed"}, {"st": 132, "ed": 134, "text": "learning algorithm"}, {"st": 143, "ed": 145, "text": "learning scheme"}, {"st": 166, "ed": 168, "text": "convergence speed"}]
[{"st": 108, "ed": 110, "text": "modus operandi"}, {"st": 148, "ed": 150, "text": "genetic programming"}, {"st": 163, "ed": 165, "text": "genetic engineering"}]
[{"st": 8, "ed": 10, "text": "neural networks"}, {"st": 48, "ed": 50, "text": "artificial intelligence"}]
[{"st": 17, "ed": 19, "text": "probabilistic model"}, {"st": 47, "ed": 50, "text": "number of iterations"}, {"st": 52, "ed": 54, "text": "global convergence"}, {"st": 59, "ed": 62, "text": "number of iterations"}]
[{"st": 25, "ed": 28, "text": "recurrent neural networks"}]
[{"st": 3, "ed": 5, "text": "genetic programming"}, {"st": 22, "ed": 24, "text": "molecular dynamics"}, {"st": 25, "ed": 27, "text": "unlike previous"}, {"st": 74, "ed": 76, "text": "functional form"}, {"st": 103, "ed": 105, "text": "training set"}, {"st": 129, "ed": 131, "text": "functional form"}, {"st": 142, "ed": 144, "text": "genetic programming"}, {"st": 151, "ed": 153, "text": "based optimization"}, {"st": 208, "ed": 210, "text": "based approach"}, {"st": 210, "ed": 212, "text": "significantly improves"}]
[{"st": 7, "ed": 9, "text": "associative memory"}, {"st": 47, "ed": 49, "text": "associative memory"}, {"st": 62, "ed": 64, "text": "distributed memory"}, {"st": 113, "ed": 115, "text": "numerical simulations"}]
[{"st": 43, "ed": 45, "text": "evolutionary algorithms"}, {"st": 55, "ed": 57, "text": "evolutionary algorithms"}, {"st": 82, "ed": 84, "text": "natural extension"}]
[{"st": 12, "ed": 14, "text": "associative memory"}, {"st": 41, "ed": 43, "text": "learning rule"}, {"st": 70, "ed": 72, "text": "activity patterns"}, {"st": 90, "ed": 92, "text": "activity patterns"}]
[{"st": 63, "ed": 65, "text": "evolutionary algorithm"}, {"st": 119, "ed": 121, "text": "evolutionary algorithm"}, {"st": 150, "ed": 152, "text": "real world"}]
[{"st": 1, "ed": 3, "text": "building block"}, {"st": 6, "ed": 8, "text": "genetic algorithms"}, {"st": 43, "ed": 45, "text": "building block"}, {"st": 55, "ed": 57, "text": "building block"}, {"st": 88, "ed": 90, "text": "hierarchical structure"}, {"st": 115, "ed": 117, "text": "empirical results"}, {"st": 120, "ed": 122, "text": "proposed method"}]
[{"st": 6, "ed": 8, "text": "fuzzy set"}, {"st": 44, "ed": 46, "text": "neural networks"}, {"st": 51, "ed": 54, "text": "accuracy and computational"}]
[{"st": 4, "ed": 6, "text": "systems architecture"}, {"st": 12, "ed": 14, "text": "multi agent"}, {"st": 54, "ed": 56, "text": "multi agent"}, {"st": 64, "ed": 66, "text": "agents learn"}, {"st": 85, "ed": 87, "text": "agents learn"}, {"st": 91, "ed": 94, "text": "multi layer perceptron"}]
[{"st": 6, "ed": 8, "text": "neural networks"}, {"st": 14, "ed": 16, "text": "genetic algorithms"}, {"st": 20, "ed": 22, "text": "missing data"}, {"st": 60, "ed": 63, "text": "accuracy and computational"}, {"st": 73, "ed": 75, "text": "missing data"}]
[{"st": 3, "ed": 5, "text": "computational complexity"}, {"st": 30, "ed": 32, "text": "linear programming"}, {"st": 121, "ed": 123, "text": "special cases"}]
[{"st": 8, "ed": 10, "text": "evolutionary algorithms"}, {"st": 11, "ed": 13, "text": "randomly generated"}, {"st": 28, "ed": 30, "text": "problem instances"}, {"st": 40, "ed": 42, "text": "global optimum"}, {"st": 49, "ed": 52, "text": "branch and bound"}, {"st": 55, "ed": 57, "text": "hierarchical bayesian"}, {"st": 57, "ed": 59, "text": "optimization algorithm"}, {"st": 62, "ed": 64, "text": "marginal distribution"}, {"st": 69, "ed": 71, "text": "genetic algorithm"}]
[{"st": 5, "ed": 8, "text": "bayesian optimization algorithm"}]
[]
[{"st": 41, "ed": 43, "text": "collaborative filtering"}]
[{"st": 9, "ed": 11, "text": "genetic algorithm"}, {"st": 28, "ed": 30, "text": "higher level"}, {"st": 39, "ed": 41, "text": "higher level"}, {"st": 46, "ed": 48, "text": "search space"}, {"st": 60, "ed": 62, "text": "search space"}, {"st": 82, "ed": 84, "text": "multiple choice"}]
[{"st": 10, "ed": 12, "text": "collaborative filtering"}]
[{"st": 9, "ed": 11, "text": "decision making"}, {"st": 18, "ed": 20, "text": "evolutionary algorithms"}, {"st": 30, "ed": 32, "text": "computational intelligence"}, {"st": 81, "ed": 83, "text": "real world"}]
[{"st": 22, "ed": 25, "text": "self organizing map"}, {"st": 28, "ed": 30, "text": "fuzzy inference"}, {"st": 80, "ed": 82, "text": "rough set"}, {"st": 97, "ed": 99, "text": "large data"}, {"st": 120, "ed": 122, "text": "proposed algorithm"}]
[{"st": 6, "ed": 8, "text": "rough set"}, {"st": 24, "ed": 27, "text": "self organizing map"}, {"st": 30, "ed": 32, "text": "pre processing"}, {"st": 74, "ed": 76, "text": "fuzzy inference"}]
[{"st": 9, "ed": 11, "text": "recommender systems"}, {"st": 129, "ed": 131, "text": "similarity measure"}]
[{"st": 4, "ed": 6, "text": "np hard"}, {"st": 25, "ed": 27, "text": "previous approaches"}, {"st": 28, "ed": 30, "text": "problem specific"}, {"st": 56, "ed": 58, "text": "genetic algorithms"}, {"st": 60, "ed": 63, "text": "attracted much attention"}, {"st": 92, "ed": 94, "text": "real world"}]
[{"st": 6, "ed": 8, "text": "fuzzy inference"}, {"st": 11, "ed": 13, "text": "self organizing"}, {"st": 13, "ed": 15, "text": "feature map"}, {"st": 15, "ed": 17, "text": "neural networks"}]
[{"st": 10, "ed": 12, "text": "associative memory"}, {"st": 26, "ed": 28, "text": "cognitive architecture"}]
[{"st": 63, "ed": 65, "text": "based approach"}]
[{"st": 27, "ed": 29, "text": "constrained optimization"}, {"st": 54, "ed": 57, "text": "joint probability distribution"}, {"st": 107, "ed": 109, "text": "joint distribution"}, {"st": 144, "ed": 146, "text": "optimization problem"}, {"st": 185, "ed": 187, "text": "mixture distribution"}, {"st": 198, "ed": 200, "text": "constraint satisfaction"}]
[{"st": 11, "ed": 13, "text": "artificial life"}, {"st": 54, "ed": 56, "text": "unified framework"}, {"st": 96, "ed": 98, "text": "undirected graph"}]
[{"st": 13, "ed": 15, "text": "optimal solution"}, {"st": 18, "ed": 20, "text": "genetic algorithm"}, {"st": 25, "ed": 27, "text": "incremental learning"}, {"st": 32, "ed": 35, "text": "a sufficient condition"}, {"st": 42, "ed": 44, "text": "optimal solution"}, {"st": 63, "ed": 65, "text": "optimal solution"}]
[{"st": 2, "ed": 4, "text": "genetic algorithm"}, {"st": 19, "ed": 21, "text": "probabilistic model"}, {"st": 56, "ed": 58, "text": "theoretical framework"}, {"st": 65, "ed": 68, "text": "point of view"}, {"st": 84, "ed": 87, "text": "ordinary differential equation"}, {"st": 98, "ed": 100, "text": "local optima"}, {"st": 113, "ed": 115, "text": "local optima"}]
[{"st": 8, "ed": 10, "text": "human brain"}, {"st": 24, "ed": 26, "text": "external memory"}, {"st": 177, "ed": 179, "text": "neural networks"}]
[{"st": 6, "ed": 8, "text": "evolutionary algorithms"}, {"st": 11, "ed": 13, "text": "global optimization"}, {"st": 36, "ed": 38, "text": "genetic algorithm"}, {"st": 78, "ed": 80, "text": "differential evolution"}, {"st": 101, "ed": 103, "text": "genetic algorithm"}, {"st": 115, "ed": 117, "text": "method called"}, {"st": 140, "ed": 142, "text": "objective functions"}, {"st": 197, "ed": 200, "text": "point of view"}]
[{"st": 6, "ed": 8, "text": "stochastic optimization"}, {"st": 14, "ed": 16, "text": "previous works"}, {"st": 28, "ed": 30, "text": "optimization methods"}, {"st": 34, "ed": 36, "text": "simulated annealing"}, {"st": 41, "ed": 43, "text": "simulated annealing"}, {"st": 45, "ed": 47, "text": "differential evolution"}, {"st": 64, "ed": 66, "text": "genetic algorithm"}, {"st": 77, "ed": 79, "text": "optimization problem"}, {"st": 103, "ed": 105, "text": "unit cell"}, {"st": 109, "ed": 111, "text": "extensive numerical"}]
[{"st": 6, "ed": 10, "text": "feed forward neural networks"}, {"st": 36, "ed": 38, "text": "training sets"}, {"st": 43, "ed": 45, "text": "sensitivity analysis"}, {"st": 47, "ed": 49, "text": "genetic algorithm"}, {"st": 53, "ed": 55, "text": "neural network"}, {"st": 59, "ed": 62, "text": "advantages and disadvantages"}]
[{"st": 4, "ed": 6, "text": "mind map"}, {"st": 111, "ed": 113, "text": "search engines"}, {"st": 120, "ed": 122, "text": "natural language"}]
[{"st": 2, "ed": 4, "text": "incremental learning"}, {"st": 6, "ed": 8, "text": "classification tasks"}, {"st": 16, "ed": 18, "text": "real valued"}, {"st": 26, "ed": 29, "text": "feedforward neural networks"}, {"st": 31, "ed": 33, "text": "hidden layer"}, {"st": 49, "ed": 53, "text": "number of hidden units"}, {"st": 58, "ed": 60, "text": "real valued"}, {"st": 79, "ed": 81, "text": "generalization error"}, {"st": 93, "ed": 95, "text": "published results"}, {"st": 100, "ed": 102, "text": "early stopping"}]
[{"st": 28, "ed": 30, "text": "error rate"}, {"st": 53, "ed": 55, "text": "neural networks"}]
[]
[{"st": 6, "ed": 8, "text": "genetic algorithm"}, {"st": 67, "ed": 69, "text": "fitness function"}, {"st": 95, "ed": 97, "text": "building block"}, {"st": 133, "ed": 135, "text": "randomly generated"}]
[{"st": 24, "ed": 26, "text": "evolutionary algorithm"}, {"st": 42, "ed": 44, "text": "optimization process"}, {"st": 55, "ed": 57, "text": "evolutionary algorithms"}, {"st": 153, "ed": 155, "text": "search spaces"}]
[]
[{"st": 61, "ed": 63, "text": "sensory input"}]
[{"st": 8, "ed": 10, "text": "shortest path"}, {"st": 15, "ed": 17, "text": "neural networks"}, {"st": 21, "ed": 23, "text": "shortest path"}, {"st": 23, "ed": 25, "text": "optimization problems"}, {"st": 31, "ed": 33, "text": "neural networks"}, {"st": 37, "ed": 39, "text": "packet switched"}, {"st": 48, "ed": 50, "text": "combinatorial optimization"}, {"st": 53, "ed": 55, "text": "shortest path"}, {"st": 64, "ed": 66, "text": "neural networks"}, {"st": 75, "ed": 77, "text": "shortest path"}]
[]
[{"st": 10, "ed": 12, "text": "donald knuth"}, {"st": 28, "ed": 30, "text": "operational research"}, {"st": 67, "ed": 69, "text": "exhaustive search"}, {"st": 81, "ed": 83, "text": "current approaches"}, {"st": 143, "ed": 145, "text": "fitness function"}, {"st": 147, "ed": 149, "text": "evolutionary algorithm"}, {"st": 173, "ed": 175, "text": "evolutionary algorithms"}]
[{"st": 21, "ed": 23, "text": "differential evolution"}, {"st": 39, "ed": 41, "text": "integer programming"}, {"st": 58, "ed": 60, "text": "proposed method"}, {"st": 67, "ed": 69, "text": "differential evolution"}]
[{"st": 13, "ed": 15, "text": "innate immunity"}, {"st": 26, "ed": 28, "text": "conceptual framework"}]
[{"st": 1, "ed": 3, "text": "recent years"}, {"st": 3, "ed": 5, "text": "genetic algorithms"}, {"st": 44, "ed": 46, "text": "genetic algorithm"}, {"st": 69, "ed": 71, "text": "problem specific"}, {"st": 89, "ed": 91, "text": "genetic algorithms"}, {"st": 201, "ed": 203, "text": "genetic algorithms"}, {"st": 206, "ed": 208, "text": "genetic algorithm"}, {"st": 318, "ed": 320, "text": "hill climbing"}, {"st": 351, "ed": 353, "text": "real data"}, {"st": 376, "ed": 378, "text": "tabu search"}]
[{"st": 0, "ed": 2, "text": "dendritic cells"}, {"st": 83, "ed": 85, "text": "preliminary results"}]
[{"st": 15, "ed": 17, "text": "dendritic cell"}, {"st": 26, "ed": 28, "text": "dendritic cells"}, {"st": 39, "ed": 41, "text": "training data"}, {"st": 44, "ed": 46, "text": "expert knowledge"}, {"st": 53, "ed": 55, "text": "input signals"}, {"st": 103, "ed": 106, "text": "principal component analysis"}, {"st": 111, "ed": 113, "text": "input data"}]
[{"st": 15, "ed": 17, "text": "time series"}, {"st": 46, "ed": 49, "text": "long term memory"}, {"st": 88, "ed": 90, "text": "crude oil"}]
[{"st": 12, "ed": 14, "text": "machine learning"}, {"st": 40, "ed": 42, "text": "expert knowledge"}, {"st": 47, "ed": 49, "text": "unlabelled data"}, {"st": 61, "ed": 63, "text": "expert knowledge"}, {"st": 152, "ed": 155, "text": "exploratory data analysis"}]
[{"st": 6, "ed": 8, "text": "inverse problem"}, {"st": 97, "ed": 99, "text": "least square"}, {"st": 133, "ed": 135, "text": "autonomous robot"}]
[{"st": 7, "ed": 9, "text": "neurological disorders"}, {"st": 200, "ed": 202, "text": "neural network"}, {"st": 210, "ed": 212, "text": "cross validation"}]
[{"st": 5, "ed": 7, "text": "classification algorithms"}, {"st": 15, "ed": 17, "text": "machine vision"}, {"st": 72, "ed": 74, "text": "classification problem"}]
[{"st": 1, "ed": 3, "text": "image classification"}, {"st": 25, "ed": 27, "text": "ground truth"}]
[{"st": 28, "ed": 30, "text": "comparative study"}]
[{"st": 71, "ed": 73, "text": "multilayer perceptron"}, {"st": 73, "ed": 76, "text": "taking into account"}]
[{"st": 58, "ed": 60, "text": "real applications"}, {"st": 69, "ed": 71, "text": "final results"}]
[{"st": 6, "ed": 8, "text": "image processing"}, {"st": 59, "ed": 62, "text": "swedish space corporation"}, {"st": 124, "ed": 126, "text": "navigational aid"}]
[{"st": 9, "ed": 11, "text": "objective functions"}, {"st": 27, "ed": 29, "text": "starting point"}, {"st": 49, "ed": 51, "text": "image compression"}, {"st": 93, "ed": 95, "text": "real life"}, {"st": 117, "ed": 119, "text": "practical applications"}]
[{"st": 125, "ed": 127, "text": "neural networks"}, {"st": 135, "ed": 137, "text": "feature extraction"}, {"st": 163, "ed": 165, "text": "recognition accuracy"}]
[{"st": 43, "ed": 45, "text": "face recognition"}, {"st": 51, "ed": 53, "text": "face recognition"}, {"st": 61, "ed": 63, "text": "sift features"}, {"st": 70, "ed": 73, "text": "global and local"}, {"st": 91, "ed": 93, "text": "sift features"}, {"st": 112, "ed": 114, "text": "sift features"}, {"st": 132, "ed": 134, "text": "decision theory"}, {"st": 166, "ed": 168, "text": "face recognition"}]
[{"st": 14, "ed": 16, "text": "sift features"}, {"st": 21, "ed": 23, "text": "sift features"}, {"st": 29, "ed": 31, "text": "object detection"}, {"st": 57, "ed": 59, "text": "sift features"}, {"st": 84, "ed": 86, "text": "similarity function"}, {"st": 86, "ed": 89, "text": "taking into account"}, {"st": 122, "ed": 124, "text": "feature set"}]
[{"st": 52, "ed": 56, "text": "scale invariant feature transform"}, {"st": 58, "ed": 60, "text": "feature descriptor"}, {"st": 91, "ed": 93, "text": "feature extraction"}, {"st": 111, "ed": 114, "text": "gaussian mixture model"}, {"st": 176, "ed": 178, "text": "sift features"}, {"st": 213, "ed": 215, "text": "recognition accuracy"}, {"st": 216, "ed": 218, "text": "invariant features"}]
[{"st": 12, "ed": 16, "text": "scale invariant feature transform"}, {"st": 19, "ed": 21, "text": "feature sets"}, {"st": 70, "ed": 72, "text": "feature set"}]
[{"st": 11, "ed": 13, "text": "feature extraction"}, {"st": 20, "ed": 22, "text": "proposed approach"}, {"st": 54, "ed": 57, "text": "curse of dimensionality"}, {"st": 67, "ed": 69, "text": "reduction techniques"}, {"st": 107, "ed": 109, "text": "pattern matching"}, {"st": 133, "ed": 135, "text": "feature extraction"}]
[{"st": 8, "ed": 12, "text": "gaussian mixture model gmm"}, {"st": 79, "ed": 83, "text": "expectation maximization em algorithm"}, {"st": 96, "ed": 98, "text": "feature vectors"}]
[{"st": 22, "ed": 24, "text": "feature extraction"}, {"st": 67, "ed": 69, "text": "majority voting"}, {"st": 75, "ed": 77, "text": "classification decision"}, {"st": 80, "ed": 84, "text": "multi layer perceptron mlp"}, {"st": 96, "ed": 98, "text": "recognition rate"}, {"st": 114, "ed": 116, "text": "recent methods"}, {"st": 119, "ed": 121, "text": "character recognition"}, {"st": 131, "ed": 133, "text": "success rate"}]
[{"st": 3, "ed": 5, "text": "vision systems"}, {"st": 8, "ed": 10, "text": "local features"}, {"st": 17, "ed": 19, "text": "face recognition"}, {"st": 24, "ed": 26, "text": "feature selection"}, {"st": 50, "ed": 52, "text": "feature selection"}, {"st": 65, "ed": 67, "text": "penalty term"}, {"st": 70, "ed": 72, "text": "squared error"}, {"st": 77, "ed": 79, "text": "feature selection"}, {"st": 94, "ed": 96, "text": "convex relaxation"}, {"st": 140, "ed": 142, "text": "face images"}]
[{"st": 13, "ed": 15, "text": "feature descriptor"}, {"st": 16, "ed": 19, "text": "multi task learning"}, {"st": 23, "ed": 25, "text": "feature descriptor"}, {"st": 32, "ed": 34, "text": "feature descriptor"}, {"st": 43, "ed": 45, "text": "spatial information"}, {"st": 68, "ed": 70, "text": "estimation problem"}, {"st": 91, "ed": 93, "text": "improve performance"}, {"st": 110, "ed": 112, "text": "proposed method"}, {"st": 113, "ed": 115, "text": "achieve comparable"}, {"st": 117, "ed": 119, "text": "previous approaches"}]
[{"st": 98, "ed": 100, "text": "arabic language"}]
[{"st": 16, "ed": 18, "text": "existing methods"}, {"st": 88, "ed": 90, "text": "real images"}]
[]
[{"st": 7, "ed": 9, "text": "face detection"}, {"st": 25, "ed": 27, "text": "time consuming"}, {"st": 27, "ed": 29, "text": "pre processing"}, {"st": 36, "ed": 38, "text": "face images"}, {"st": 80, "ed": 82, "text": "face detection"}, {"st": 110, "ed": 114, "text": "scale invariant feature transform"}, {"st": 122, "ed": 124, "text": "proposed algorithm"}, {"st": 137, "ed": 139, "text": "face images"}]
[{"st": 44, "ed": 46, "text": "unlike previous"}, {"st": 52, "ed": 54, "text": "additional assumptions"}]
[{"st": 8, "ed": 10, "text": "recently introduced"}, {"st": 35, "ed": 37, "text": "feature descriptor"}]
[{"st": 0, "ed": 2, "text": "feature selection"}, {"st": 14, "ed": 16, "text": "feature selection"}, {"st": 57, "ed": 59, "text": "inter class"}, {"st": 60, "ed": 62, "text": "intra class"}, {"st": 125, "ed": 127, "text": "pattern classification"}, {"st": 132, "ed": 134, "text": "feature selection"}, {"st": 137, "ed": 139, "text": "nearest neighbour"}, {"st": 150, "ed": 152, "text": "feature vectors"}, {"st": 154, "ed": 156, "text": "problems involving"}]
[{"st": 32, "ed": 34, "text": "biologically inspired"}, {"st": 34, "ed": 36, "text": "local binary"}, {"st": 38, "ed": 40, "text": "similarity based"}, {"st": 40, "ed": 42, "text": "face recognition"}, {"st": 55, "ed": 57, "text": "based approach"}, {"st": 67, "ed": 69, "text": "training samples"}, {"st": 102, "ed": 104, "text": "face recognition"}, {"st": 113, "ed": 115, "text": "face images"}]
[{"st": 9, "ed": 13, "text": "multi layer perceptron mlp"}, {"st": 39, "ed": 41, "text": "indian subcontinent"}, {"st": 51, "ed": 53, "text": "feature set"}, {"st": 85, "ed": 87, "text": "recognition rate"}, {"st": 92, "ed": 95, "text": "fold cross validation"}]
[{"st": 9, "ed": 13, "text": "multi layer perceptron mlp"}, {"st": 25, "ed": 27, "text": "feature set"}, {"st": 38, "ed": 40, "text": "indian subcontinent"}, {"st": 50, "ed": 52, "text": "feature set"}, {"st": 82, "ed": 84, "text": "feature set"}]
[{"st": 25, "ed": 27, "text": "wide variety"}, {"st": 75, "ed": 77, "text": "higher order"}, {"st": 92, "ed": 94, "text": "object class"}, {"st": 112, "ed": 114, "text": "computationally efficient"}, {"st": 116, "ed": 118, "text": "approximate inference"}, {"st": 135, "ed": 137, "text": "higher order"}, {"st": 143, "ed": 146, "text": "tens of thousands"}, {"st": 156, "ed": 158, "text": "linear programming"}, {"st": 199, "ed": 201, "text": "higher order"}]
[{"st": 0, "ed": 2, "text": "texture classification"}, {"st": 21, "ed": 23, "text": "texture classification"}, {"st": 37, "ed": 39, "text": "pattern recognition"}, {"st": 39, "ed": 41, "text": "object tracking"}, {"st": 94, "ed": 96, "text": "local binary"}, {"st": 100, "ed": 102, "text": "co occurrence"}, {"st": 106, "ed": 108, "text": "edge detection"}, {"st": 154, "ed": 156, "text": "previous approaches"}]
[{"st": 23, "ed": 25, "text": "current methods"}, {"st": 51, "ed": 53, "text": "computer vision"}, {"st": 56, "ed": 58, "text": "image segmentation"}, {"st": 59, "ed": 61, "text": "gesture recognition"}, {"st": 96, "ed": 98, "text": "pose estimation"}, {"st": 133, "ed": 135, "text": "multi dimensional"}]
[{"st": 2, "ed": 4, "text": "content based"}, {"st": 4, "ed": 6, "text": "image classification"}, {"st": 33, "ed": 35, "text": "k means"}, {"st": 92, "ed": 94, "text": "k means"}, {"st": 97, "ed": 99, "text": "low level"}]
[]
[{"st": 2, "ed": 4, "text": "internal structure"}, {"st": 8, "ed": 10, "text": "object detection"}, {"st": 14, "ed": 16, "text": "event recognition"}, {"st": 34, "ed": 36, "text": "object detection"}, {"st": 40, "ed": 42, "text": "event recognition"}, {"st": 43, "ed": 45, "text": "event recognition"}]
[{"st": 45, "ed": 47, "text": "spatial relations"}, {"st": 78, "ed": 80, "text": "event recognition"}]
[{"st": 10, "ed": 12, "text": "english verbs"}, {"st": 47, "ed": 49, "text": "object class"}, {"st": 51, "ed": 53, "text": "fine grained"}, {"st": 60, "ed": 62, "text": "coarse grained"}, {"st": 93, "ed": 95, "text": "classification accuracy"}, {"st": 134, "ed": 136, "text": "time series"}, {"st": 156, "ed": 158, "text": "bounding boxes"}, {"st": 173, "ed": 175, "text": "event recognition"}]
[{"st": 8, "ed": 10, "text": "multiple modalities"}, {"st": 21, "ed": 23, "text": "data analysis"}, {"st": 38, "ed": 41, "text": "synthetic and real"}, {"st": 61, "ed": 63, "text": "multi modal"}, {"st": 69, "ed": 71, "text": "previous attempts"}, {"st": 74, "ed": 76, "text": "spectral clustering"}]
[{"st": 22, "ed": 24, "text": "graphical model"}, {"st": 31, "ed": 33, "text": "approximate inference"}, {"st": 34, "ed": 36, "text": "belief propagation"}, {"st": 49, "ed": 52, "text": "guaranteed to converge"}, {"st": 55, "ed": 58, "text": "number of iterations"}, {"st": 65, "ed": 67, "text": "prior methods"}, {"st": 70, "ed": 73, "text": "accuracy and computational"}]
[{"st": 4, "ed": 6, "text": "approximate inference"}, {"st": 51, "ed": 53, "text": "monte carlo"}]
[{"st": 170, "ed": 172, "text": "object appearance"}]
[{"st": 8, "ed": 10, "text": "machine vision"}, {"st": 40, "ed": 42, "text": "machine vision"}, {"st": 46, "ed": 48, "text": "hierarchical bayesian"}, {"st": 118, "ed": 120, "text": "random variables"}]
[{"st": 17, "ed": 19, "text": "real world"}, {"st": 19, "ed": 21, "text": "image analysis"}]
[{"st": 15, "ed": 17, "text": "remote sensing"}, {"st": 31, "ed": 33, "text": "classification results"}, {"st": 37, "ed": 39, "text": "previous results"}, {"st": 42, "ed": 44, "text": "context free"}, {"st": 48, "ed": 50, "text": "dynamic programming"}]
[{"st": 56, "ed": 58, "text": "level set"}]
[{"st": 0, "ed": 2, "text": "electrical impedance"}, {"st": 6, "ed": 8, "text": "functional imaging"}, {"st": 39, "ed": 41, "text": "temporal resolution"}, {"st": 58, "ed": 60, "text": "animal model"}, {"st": 105, "ed": 107, "text": "image segmentation"}, {"st": 136, "ed": 138, "text": "ct scan"}, {"st": 158, "ed": 160, "text": "ct scan"}]
[{"st": 11, "ed": 13, "text": "dimensionality reduction"}, {"st": 16, "ed": 21, "text": "non negative matrix factorization nmf"}, {"st": 23, "ed": 26, "text": "exploratory data analysis"}, {"st": 34, "ed": 36, "text": "optimization problem"}, {"st": 44, "ed": 46, "text": "non trivial"}, {"st": 53, "ed": 55, "text": "optimization problem"}, {"st": 74, "ed": 76, "text": "globally optimal"}, {"st": 95, "ed": 97, "text": "non negativity"}, {"st": 119, "ed": 121, "text": "optimization problem"}]
[{"st": 77, "ed": 79, "text": "inertial navigation"}, {"st": 81, "ed": 83, "text": "kalman filter"}, {"st": 89, "ed": 91, "text": "inertial navigation"}, {"st": 94, "ed": 96, "text": "vision based"}]
[{"st": 63, "ed": 66, "text": "field of view"}, {"st": 84, "ed": 86, "text": "closed form"}]
[{"st": 27, "ed": 29, "text": "powerful tool"}, {"st": 80, "ed": 82, "text": "important information"}, {"st": 115, "ed": 117, "text": "relevant information"}, {"st": 139, "ed": 141, "text": "artificial intelligence"}]
[{"st": 6, "ed": 8, "text": "evidence based"}]
[{"st": 9, "ed": 11, "text": "belief propagation"}, {"st": 14, "ed": 16, "text": "wide variety"}, {"st": 98, "ed": 100, "text": "computational complexity"}, {"st": 154, "ed": 156, "text": "sum product"}, {"st": 168, "ed": 170, "text": "marginal probabilities"}, {"st": 174, "ed": 176, "text": "maximum likelihood"}]
[{"st": 30, "ed": 32, "text": "probability distributions"}, {"st": 63, "ed": 65, "text": "np hard"}, {"st": 92, "ed": 94, "text": "conditional independence"}, {"st": 107, "ed": 109, "text": "structure learning"}, {"st": 120, "ed": 123, "text": "real world data"}, {"st": 144, "ed": 146, "text": "structure learning"}, {"st": 177, "ed": 179, "text": "theoretical analysis"}, {"st": 195, "ed": 197, "text": "random variables"}]
[{"st": 2, "ed": 5, "text": "number of clusters"}, {"st": 15, "ed": 17, "text": "a level"}, {"st": 36, "ed": 38, "text": "learning algorithm"}, {"st": 44, "ed": 46, "text": "clustering problem"}, {"st": 80, "ed": 83, "text": "number of clusters"}, {"st": 88, "ed": 90, "text": "error rates"}]
[{"st": 206, "ed": 208, "text": "pattern recognition"}]
[{"st": 0, "ed": 2, "text": "object detection"}, {"st": 7, "ed": 9, "text": "computer vision"}, {"st": 23, "ed": 25, "text": "object detection"}, {"st": 27, "ed": 31, "text": "scale invariant feature transform"}, {"st": 34, "ed": 36, "text": "automatic segmentation"}, {"st": 136, "ed": 138, "text": "proposed approach"}, {"st": 141, "ed": 143, "text": "object detection"}]
[{"st": 0, "ed": 2, "text": "texture classification"}, {"st": 15, "ed": 17, "text": "image processing"}, {"st": 136, "ed": 138, "text": "proposed approach"}]
[{"st": 0, "ed": 2, "text": "sparse coding"}, {"st": 8, "ed": 10, "text": "compact representations"}, {"st": 13, "ed": 15, "text": "sparse coding"}, {"st": 70, "ed": 73, "text": "factors of variation"}, {"st": 92, "ed": 94, "text": "sparse codes"}, {"st": 95, "ed": 97, "text": "natural images"}, {"st": 128, "ed": 130, "text": "statistical efficiency"}, {"st": 153, "ed": 155, "text": "sparse coding"}]
[{"st": 6, "ed": 8, "text": "neural networks"}, {"st": 15, "ed": 17, "text": "hand written"}, {"st": 23, "ed": 25, "text": "english alphabet"}, {"st": 38, "ed": 40, "text": "feature extraction"}, {"st": 47, "ed": 49, "text": "neural network"}]
[{"st": 11, "ed": 13, "text": "low level"}, {"st": 20, "ed": 22, "text": "computer vision"}, {"st": 56, "ed": 58, "text": "higher level"}, {"st": 62, "ed": 64, "text": "object recognition"}, {"st": 116, "ed": 118, "text": "posterior distributions"}]
[{"st": 69, "ed": 71, "text": "low level"}, {"st": 97, "ed": 100, "text": "run length encoding"}, {"st": 135, "ed": 137, "text": "multiple levels"}, {"st": 145, "ed": 147, "text": "unsupervised learning"}]
[{"st": 6, "ed": 8, "text": "fully automatic"}, {"st": 8, "ed": 10, "text": "brain tumor"}, {"st": 14, "ed": 17, "text": "deep neural networks"}, {"st": 64, "ed": 66, "text": "machine learning"}, {"st": 106, "ed": 110, "text": "convolutional neural networks cnn"}, {"st": 136, "ed": 138, "text": "local features"}, {"st": 167, "ed": 169, "text": "fully connected"}, {"st": 183, "ed": 185, "text": "training procedure"}]
[{"st": 14, "ed": 16, "text": "visual words"}, {"st": 18, "ed": 20, "text": "image reconstruction"}, {"st": 46, "ed": 48, "text": "de facto"}, {"st": 51, "ed": 53, "text": "image recognition"}, {"st": 56, "ed": 58, "text": "image reconstruction"}, {"st": 74, "ed": 76, "text": "spatial information"}, {"st": 93, "ed": 95, "text": "evaluation function"}]
[{"st": 8, "ed": 10, "text": "classification problems"}, {"st": 28, "ed": 30, "text": "random walks"}, {"st": 38, "ed": 40, "text": "message passing"}, {"st": 42, "ed": 44, "text": "belief propagation"}, {"st": 45, "ed": 47, "text": "mean field"}, {"st": 53, "ed": 56, "text": "guaranteed to converge"}, {"st": 68, "ed": 70, "text": "fixed point"}, {"st": 77, "ed": 79, "text": "fixed points"}, {"st": 101, "ed": 103, "text": "theoretical results"}, {"st": 108, "ed": 110, "text": "message passing"}, {"st": 112, "ed": 114, "text": "value iteration"}, {"st": 116, "ed": 118, "text": "infinite horizon"}, {"st": 132, "ed": 134, "text": "numerical experiments"}, {"st": 135, "ed": 137, "text": "image restoration"}, {"st": 138, "ed": 140, "text": "depth estimation"}]
[{"st": 36, "ed": 38, "text": "deep learning"}, {"st": 39, "ed": 42, "text": "achieved great success"}, {"st": 55, "ed": 57, "text": "deep learning"}, {"st": 62, "ed": 64, "text": "feature extraction"}]
[{"st": 20, "ed": 24, "text": "recurrent neural network architecture"}, {"st": 31, "ed": 33, "text": "neural network"}, {"st": 48, "ed": 50, "text": "3d shapes"}, {"st": 91, "ed": 93, "text": "previous works"}, {"st": 102, "ed": 104, "text": "object class"}, {"st": 110, "ed": 112, "text": "extensive experimental"}, {"st": 112, "ed": 114, "text": "analysis shows"}]
[{"st": 52, "ed": 54, "text": "multi view"}, {"st": 55, "ed": 57, "text": "empirical results"}, {"st": 96, "ed": 98, "text": "multi view"}, {"st": 113, "ed": 115, "text": "network architectures"}, {"st": 122, "ed": 124, "text": "multi view"}, {"st": 128, "ed": 130, "text": "multi resolution"}, {"st": 150, "ed": 152, "text": "multi view"}, {"st": 155, "ed": 157, "text": "extensive experiments"}, {"st": 175, "ed": 177, "text": "object classification"}]
[{"st": 5, "ed": 7, "text": "depth perception"}, {"st": 84, "ed": 86, "text": "rgb d"}, {"st": 93, "ed": 95, "text": "significantly improves"}, {"st": 97, "ed": 99, "text": "depth perception"}]
[{"st": 1, "ed": 3, "text": "image annotation"}, {"st": 11, "ed": 13, "text": "large scale"}, {"st": 17, "ed": 19, "text": "existing methods"}, {"st": 44, "ed": 46, "text": "retrieval performance"}, {"st": 82, "ed": 84, "text": "recent progress"}, {"st": 85, "ed": 87, "text": "machine translation"}, {"st": 88, "ed": 90, "text": "image captioning"}, {"st": 101, "ed": 103, "text": "image annotation"}, {"st": 131, "ed": 133, "text": "image annotation"}, {"st": 141, "ed": 143, "text": "existing methods"}, {"st": 148, "ed": 150, "text": "evaluation measures"}, {"st": 180, "ed": 182, "text": "training phase"}]
[{"st": 43, "ed": 45, "text": "labeled data"}, {"st": 63, "ed": 65, "text": "deep learning"}, {"st": 72, "ed": 75, "text": "restricted boltzmann machine"}, {"st": 85, "ed": 87, "text": "deep learning"}, {"st": 91, "ed": 93, "text": "time series"}, {"st": 150, "ed": 155, "text": "simulated and real world data"}]
[{"st": 4, "ed": 6, "text": "generative model"}, {"st": 17, "ed": 19, "text": "multiple levels"}, {"st": 58, "ed": 60, "text": "gaussian process"}, {"st": 71, "ed": 73, "text": "joint inference"}, {"st": 92, "ed": 94, "text": "expressive power"}, {"st": 99, "ed": 101, "text": "real world"}]
[{"st": 64, "ed": 67, "text": "end to end"}, {"st": 81, "ed": 83, "text": "lip gloss"}, {"st": 84, "ed": 86, "text": "eye shadow"}, {"st": 130, "ed": 133, "text": "qualitative and quantitative"}]
[{"st": 73, "ed": 76, "text": "conditional random fields"}]
[{"st": 0, "ed": 2, "text": "change detection"}, {"st": 42, "ed": 44, "text": "change detection"}, {"st": 55, "ed": 57, "text": "semantic change"}, {"st": 81, "ed": 83, "text": "change detection"}, {"st": 116, "ed": 120, "text": "convolutional neural networks cnns"}, {"st": 124, "ed": 126, "text": "multi scale"}, {"st": 126, "ed": 128, "text": "feature representation"}, {"st": 141, "ed": 143, "text": "change detection"}, {"st": 161, "ed": 163, "text": "multi scale"}]
[{"st": 2, "ed": 4, "text": "feature selection"}, {"st": 18, "ed": 20, "text": "training set"}, {"st": 41, "ed": 43, "text": "feature selection"}, {"st": 49, "ed": 51, "text": "training data"}, {"st": 54, "ed": 56, "text": "classification results"}, {"st": 61, "ed": 63, "text": "image datasets"}, {"st": 66, "ed": 68, "text": "feature selection"}, {"st": 71, "ed": 74, "text": "naive bayes classifier"}]
[{"st": 0, "ed": 2, "text": "traditional methods"}, {"st": 3, "ed": 8, "text": "computer vision and machine learning"}, {"st": 19, "ed": 21, "text": "handwritten digits"}, {"st": 25, "ed": 27, "text": "biologically plausible"}, {"st": 27, "ed": 30, "text": "deep artificial neural"}, {"st": 30, "ed": 32, "text": "network architectures"}, {"st": 36, "ed": 38, "text": "receptive fields"}, {"st": 46, "ed": 48, "text": "network depth"}, {"st": 72, "ed": 74, "text": "deep neural"}, {"st": 87, "ed": 89, "text": "graphics cards"}, {"st": 112, "ed": 114, "text": "traffic sign"}, {"st": 137, "ed": 139, "text": "image classification"}]
[{"st": 12, "ed": 15, "text": "optical character recognition"}, {"st": 98, "ed": 100, "text": "success rate"}]
[{"st": 14, "ed": 16, "text": "visual attention"}, {"st": 52, "ed": 54, "text": "feature based"}, {"st": 69, "ed": 71, "text": "image segmentation"}, {"st": 116, "ed": 118, "text": "object recognition"}, {"st": 118, "ed": 120, "text": "scene understanding"}, {"st": 130, "ed": 132, "text": "quality control"}, {"st": 149, "ed": 151, "text": "final results"}, {"st": 182, "ed": 184, "text": "feature based"}, {"st": 243, "ed": 247, "text": "mean squared error mse"}, {"st": 253, "ed": 257, "text": "signal to noise ratio"}]
[{"st": 26, "ed": 28, "text": "domain knowledge"}, {"st": 44, "ed": 46, "text": "image segmentation"}, {"st": 59, "ed": 61, "text": "soft computing"}, {"st": 76, "ed": 79, "text": "artificial neural network"}, {"st": 85, "ed": 87, "text": "genetic algorithm"}, {"st": 92, "ed": 94, "text": "based methods"}, {"st": 188, "ed": 192, "text": "mean squared error mse"}, {"st": 197, "ed": 201, "text": "signal to noise ratio"}]
[{"st": 111, "ed": 113, "text": "gabor filter"}, {"st": 126, "ed": 128, "text": "existing techniques"}]
[{"st": 0, "ed": 3, "text": "detection and segmentation"}, {"st": 4, "ed": 6, "text": "brain tumor"}, {"st": 41, "ed": 44, "text": "artificial neural network"}, {"st": 44, "ed": 46, "text": "fuzzy inference"}, {"st": 48, "ed": 50, "text": "image classification"}, {"st": 61, "ed": 63, "text": "k nn"}, {"st": 63, "ed": 66, "text": "k nearest neighbor"}, {"st": 75, "ed": 77, "text": "fuzzy logic"}, {"st": 80, "ed": 82, "text": "feature set"}, {"st": 100, "ed": 102, "text": "promising results"}, {"st": 116, "ed": 118, "text": "k nn"}]
[{"st": 46, "ed": 48, "text": "pose estimation"}, {"st": 49, "ed": 51, "text": "mobile robot"}, {"st": 61, "ed": 63, "text": "noisy data"}]
[{"st": 8, "ed": 10, "text": "moving objects"}, {"st": 47, "ed": 50, "text": "task of classifying"}, {"st": 60, "ed": 62, "text": "moving objects"}, {"st": 101, "ed": 104, "text": "mixture of gaussians"}, {"st": 141, "ed": 143, "text": "moving objects"}, {"st": 179, "ed": 181, "text": "significant improvements"}]
[{"st": 0, "ed": 3, "text": "deep neural networks"}, {"st": 6, "ed": 8, "text": "image classification"}, {"st": 20, "ed": 22, "text": "sliding window"}, {"st": 25, "ed": 27, "text": "computational complexity"}, {"st": 40, "ed": 42, "text": "dynamic programming"}, {"st": 47, "ed": 50, "text": "orders of magnitude"}, {"st": 52, "ed": 54, "text": "max pooling"}]
[{"st": 5, "ed": 7, "text": "scene recognition"}, {"st": 59, "ed": 61, "text": "natural language"}]
[{"st": 12, "ed": 14, "text": "object detection"}, {"st": 34, "ed": 36, "text": "object detection"}, {"st": 64, "ed": 66, "text": "proposed approach"}, {"st": 90, "ed": 92, "text": "saliency map"}, {"st": 108, "ed": 110, "text": "benchmark dataset"}, {"st": 159, "ed": 161, "text": "f measure"}, {"st": 173, "ed": 175, "text": "image editing"}]
[{"st": 12, "ed": 14, "text": "machine learning"}, {"st": 20, "ed": 22, "text": "poorly understood"}, {"st": 36, "ed": 38, "text": "bayes optimal"}, {"st": 47, "ed": 49, "text": "context dependent"}, {"st": 53, "ed": 55, "text": "previously proposed"}, {"st": 72, "ed": 74, "text": "look ahead"}, {"st": 104, "ed": 106, "text": "visual search"}, {"st": 130, "ed": 132, "text": "geometric properties"}, {"st": 163, "ed": 165, "text": "problem involving"}, {"st": 165, "ed": 167, "text": "peripheral vision"}]
[{"st": 4, "ed": 7, "text": "received increasing attention"}, {"st": 24, "ed": 26, "text": "existing methods"}, {"st": 32, "ed": 34, "text": "spatial information"}, {"st": 46, "ed": 49, "text": "nonnegative matrix factorization"}, {"st": 133, "ed": 135, "text": "compact space"}, {"st": 154, "ed": 156, "text": "noise levels"}, {"st": 159, "ed": 161, "text": "method outperforms"}]
[{"st": 5, "ed": 7, "text": "edge detection"}, {"st": 10, "ed": 12, "text": "gabor filter"}, {"st": 16, "ed": 18, "text": "input image"}, {"st": 34, "ed": 36, "text": "edge detection"}, {"st": 67, "ed": 69, "text": "edge detection"}]
[{"st": 91, "ed": 93, "text": "based approach"}, {"st": 105, "ed": 107, "text": "based approach"}, {"st": 137, "ed": 139, "text": "p model"}, {"st": 165, "ed": 167, "text": "euclidean distance"}]
[{"st": 13, "ed": 15, "text": "natural language"}, {"st": 18, "ed": 20, "text": "visual content"}, {"st": 32, "ed": 35, "text": "descriptive video service"}, {"st": 42, "ed": 44, "text": "digital media"}]
[{"st": 0, "ed": 4, "text": "robust principal component analysis"}, {"st": 6, "ed": 8, "text": "rank minimization"}, {"st": 10, "ed": 12, "text": "powerful tool"}, {"st": 15, "ed": 17, "text": "low rank"}, {"st": 28, "ed": 30, "text": "low level"}, {"st": 30, "ed": 32, "text": "vision problems"}, {"st": 39, "ed": 41, "text": "underlying structure"}, {"st": 45, "ed": 47, "text": "low rank"}, {"st": 61, "ed": 63, "text": "rank minimization"}, {"st": 67, "ed": 69, "text": "objective function"}, {"st": 111, "ed": 113, "text": "nuclear norm"}, {"st": 121, "ed": 123, "text": "singular values"}, {"st": 148, "ed": 150, "text": "success rate"}, {"st": 152, "ed": 154, "text": "rank minimization"}, {"st": 180, "ed": 182, "text": "low level"}, {"st": 182, "ed": 184, "text": "vision problems"}, {"st": 186, "ed": 188, "text": "dynamic range"}, {"st": 190, "ed": 192, "text": "edge detection"}, {"st": 209, "ed": 211, "text": "nuclear norm"}, {"st": 211, "ed": 213, "text": "rank minimization"}]
[{"st": 9, "ed": 11, "text": "classification problems"}, {"st": 13, "ed": 15, "text": "output space"}, {"st": 118, "ed": 120, "text": "training data"}, {"st": 138, "ed": 140, "text": "structured prediction"}, {"st": 142, "ed": 144, "text": "tensor based"}, {"st": 144, "ed": 146, "text": "approach outperforms"}, {"st": 153, "ed": 155, "text": "relative improvement"}, {"st": 160, "ed": 162, "text": "top 5"}, {"st": 186, "ed": 188, "text": "top 5"}]
[{"st": 0, "ed": 2, "text": "attention based"}, {"st": 3, "ed": 5, "text": "encoder decoder"}, {"st": 8, "ed": 10, "text": "widely adopted"}, {"st": 16, "ed": 18, "text": "visual attention"}, {"st": 33, "ed": 35, "text": "visual information"}, {"st": 41, "ed": 43, "text": "visual words"}, {"st": 88, "ed": 90, "text": "attention model"}, {"st": 146, "ed": 148, "text": "image captioning"}]
[{"st": 24, "ed": 26, "text": "iterative optimization"}, {"st": 82, "ed": 85, "text": "convolutional neural network"}, {"st": 119, "ed": 121, "text": "style transfer"}, {"st": 140, "ed": 142, "text": "multiple scales"}, {"st": 150, "ed": 152, "text": "large scale"}]
[{"st": 11, "ed": 13, "text": "convex hull"}, {"st": 63, "ed": 65, "text": "visual perception"}, {"st": 119, "ed": 121, "text": "special cases"}]
[{"st": 0, "ed": 2, "text": "image segmentation"}, {"st": 8, "ed": 10, "text": "machine learning"}, {"st": 60, "ed": 62, "text": "community detection"}, {"st": 68, "ed": 70, "text": "traditional approaches"}, {"st": 75, "ed": 77, "text": "spectral graph"}, {"st": 88, "ed": 90, "text": "graph based"}, {"st": 99, "ed": 101, "text": "based method"}]
[{"st": 42, "ed": 45, "text": "takes advantage of"}, {"st": 48, "ed": 52, "text": "deep convolutional neural networks"}, {"st": 54, "ed": 56, "text": "multi scale"}, {"st": 100, "ed": 102, "text": "ms coco"}, {"st": 157, "ed": 159, "text": "https github.com"}]
[{"st": 0, "ed": 2, "text": "supervised learning"}, {"st": 9, "ed": 11, "text": "unsupervised learning"}, {"st": 16, "ed": 18, "text": "training data"}, {"st": 24, "ed": 26, "text": "visual perception"}, {"st": 31, "ed": 33, "text": "object classes"}, {"st": 62, "ed": 64, "text": "bounding boxes"}, {"st": 106, "ed": 108, "text": "ground truth"}, {"st": 127, "ed": 129, "text": "virtual worlds"}, {"st": 150, "ed": 152, "text": "real world"}, {"st": 157, "ed": 159, "text": "domain adaptation"}, {"st": 182, "ed": 184, "text": "real world"}, {"st": 203, "ed": 205, "text": "virtual world"}, {"st": 224, "ed": 226, "text": "real data"}, {"st": 230, "ed": 232, "text": "object appearance"}]
[{"st": 16, "ed": 18, "text": "recognition task"}, {"st": 43, "ed": 45, "text": "visual tracking"}]
[{"st": 26, "ed": 28, "text": "proposed approach"}, {"st": 43, "ed": 45, "text": "proposed approach"}, {"st": 70, "ed": 72, "text": "nearest neighbor"}, {"st": 83, "ed": 85, "text": "proposed method"}, {"st": 149, "ed": 151, "text": "proposed approach"}]
[{"st": 0, "ed": 2, "text": "object recognition"}, {"st": 7, "ed": 9, "text": "image processing"}, {"st": 19, "ed": 21, "text": "object recognition"}, {"st": 28, "ed": 32, "text": "scale invariant feature transform"}, {"st": 130, "ed": 132, "text": "similarity measure"}]
[{"st": 61, "ed": 63, "text": "object recognition"}, {"st": 69, "ed": 71, "text": "reinforcement learning"}, {"st": 77, "ed": 79, "text": "user agent"}, {"st": 99, "ed": 101, "text": "reinforcement learning"}]
[{"st": 30, "ed": 32, "text": "computer vision"}, {"st": 33, "ed": 35, "text": "object detection"}, {"st": 39, "ed": 41, "text": "ground truth"}, {"st": 134, "ed": 136, "text": "object detectors"}, {"st": 171, "ed": 173, "text": "object detectors"}, {"st": 185, "ed": 187, "text": "highly dependent"}, {"st": 298, "ed": 300, "text": "confidence bounds"}]
[{"st": 6, "ed": 8, "text": "learning framework"}, {"st": 36, "ed": 38, "text": "object detectors"}, {"st": 68, "ed": 70, "text": "learning algorithm"}, {"st": 121, "ed": 123, "text": "unlabelled data"}, {"st": 133, "ed": 135, "text": "performance improvement"}]
[{"st": 4, "ed": 6, "text": "building block"}, {"st": 8, "ed": 10, "text": "image processing"}, {"st": 74, "ed": 76, "text": "underlying structure"}, {"st": 93, "ed": 95, "text": "underlying structure"}, {"st": 111, "ed": 113, "text": "fast approximate"}, {"st": 113, "ed": 115, "text": "learning algorithm"}]
[{"st": 3, "ed": 5, "text": "message passing"}, {"st": 11, "ed": 13, "text": "map inference"}, {"st": 24, "ed": 26, "text": "objective functions"}, {"st": 43, "ed": 45, "text": "computational complexity"}, {"st": 48, "ed": 50, "text": "based methods"}, {"st": 62, "ed": 64, "text": "without sacrificing"}, {"st": 145, "ed": 147, "text": "message passing"}, {"st": 156, "ed": 158, "text": "significant improvement"}, {"st": 171, "ed": 173, "text": "higher quality"}]
[{"st": 19, "ed": 21, "text": "vending machines"}, {"st": 50, "ed": 52, "text": "transaction processing"}, {"st": 74, "ed": 76, "text": "image processing"}, {"st": 90, "ed": 92, "text": "image processing"}]
[{"st": 15, "ed": 17, "text": "grocery store"}, {"st": 54, "ed": 57, "text": "artificial neural network"}, {"st": 114, "ed": 116, "text": "extracted features"}, {"st": 126, "ed": 128, "text": "recognition rate"}]
[{"st": 30, "ed": 32, "text": "soft computing"}, {"st": 38, "ed": 40, "text": "rough set"}]
[{"st": 24, "ed": 26, "text": "prior knowledge"}, {"st": 27, "ed": 29, "text": "training set"}, {"st": 38, "ed": 40, "text": "clustering technique"}, {"st": 43, "ed": 45, "text": "image segmentation"}, {"st": 55, "ed": 57, "text": "component analysis"}, {"st": 67, "ed": 69, "text": "proposed method"}]
[{"st": 4, "ed": 6, "text": "clustering algorithm"}, {"st": 26, "ed": 28, "text": "fuzzy clustering"}, {"st": 44, "ed": 46, "text": "feature selection"}, {"st": 63, "ed": 65, "text": "decision making"}, {"st": 65, "ed": 67, "text": "application domains"}, {"st": 170, "ed": 172, "text": "error rate"}, {"st": 174, "ed": 176, "text": "performance loss"}, {"st": 186, "ed": 188, "text": "proposed method"}, {"st": 199, "ed": 201, "text": "error rate"}]
[{"st": 37, "ed": 39, "text": "recognition performance"}, {"st": 42, "ed": 44, "text": "object recognition"}, {"st": 84, "ed": 86, "text": "object recognition"}, {"st": 114, "ed": 116, "text": "visual object"}, {"st": 120, "ed": 122, "text": "object recognition"}, {"st": 143, "ed": 145, "text": "object recognition"}, {"st": 153, "ed": 155, "text": "recognition accuracy"}, {"st": 170, "ed": 172, "text": "visual object"}]
[{"st": 11, "ed": 13, "text": "active research"}, {"st": 25, "ed": 27, "text": "real world"}, {"st": 43, "ed": 45, "text": "real world"}, {"st": 61, "ed": 63, "text": "fuzzy set"}, {"st": 69, "ed": 71, "text": "great success"}, {"st": 83, "ed": 85, "text": "fuzzy set"}, {"st": 92, "ed": 94, "text": "fuzzy set"}, {"st": 145, "ed": 147, "text": "low level"}, {"st": 148, "ed": 150, "text": "mid level"}]
[{"st": 0, "ed": 2, "text": "joint attention"}, {"st": 98, "ed": 100, "text": "computer vision"}, {"st": 119, "ed": 121, "text": "computer vision"}, {"st": 185, "ed": 187, "text": "joint attention"}]
[{"st": 36, "ed": 38, "text": "semi supervised"}, {"st": 73, "ed": 76, "text": "convolutional neural network"}, {"st": 93, "ed": 95, "text": "object appearance"}, {"st": 114, "ed": 116, "text": "false positive"}]
[{"st": 9, "ed": 11, "text": "experiment results"}, {"st": 12, "ed": 14, "text": "fine grained"}]
[{"st": 6, "ed": 10, "text": "convolutional neural network cnn"}, {"st": 19, "ed": 22, "text": "convolutional neural networks"}, {"st": 42, "ed": 44, "text": "sliding window"}, {"st": 60, "ed": 62, "text": "fully convolutional"}, {"st": 98, "ed": 101, "text": "input and output"}, {"st": 102, "ed": 104, "text": "pre processing"}, {"st": 153, "ed": 155, "text": "u net"}, {"st": 168, "ed": 170, "text": "sliding window"}, {"st": 186, "ed": 188, "text": "neural networks"}, {"st": 232, "ed": 234, "text": "loss functions"}, {"st": 237, "ed": 239, "text": "softmax loss"}, {"st": 257, "ed": 260, "text": "open source software"}, {"st": 262, "ed": 264, "text": "working title"}]
[{"st": 2, "ed": 4, "text": "computational neuroscience"}, {"st": 5, "ed": 9, "text": "functional magnetic resonance imaging"}, {"st": 72, "ed": 74, "text": "neural net"}, {"st": 131, "ed": 133, "text": "fuzzy inference"}, {"st": 150, "ed": 152, "text": "feature sets"}, {"st": 179, "ed": 181, "text": "neural network"}]
[{"st": 25, "ed": 27, "text": "computer science"}, {"st": 74, "ed": 76, "text": "human perception"}]
[{"st": 2, "ed": 4, "text": "synthetic data"}, {"st": 19, "ed": 21, "text": "synthetic data"}, {"st": 31, "ed": 35, "text": "synthetic and real data"}, {"st": 64, "ed": 66, "text": "synthetic data"}, {"st": 81, "ed": 83, "text": "synthetic data"}, {"st": 88, "ed": 90, "text": "recognition task"}, {"st": 97, "ed": 99, "text": "real images"}, {"st": 108, "ed": 110, "text": "preliminary experiments"}]
[{"st": 0, "ed": 3, "text": "deep neural network"}, {"st": 7, "ed": 9, "text": "excellent results"}, {"st": 15, "ed": 17, "text": "artificial intelligence"}, {"st": 18, "ed": 20, "text": "visual recognition"}, {"st": 27, "ed": 29, "text": "hand designed"}, {"st": 33, "ed": 35, "text": "deep networks"}, {"st": 47, "ed": 49, "text": "non linearities"}, {"st": 52, "ed": 54, "text": "receptive fields"}, {"st": 64, "ed": 66, "text": "training procedure"}, {"st": 70, "ed": 72, "text": "open problem"}, {"st": 89, "ed": 91, "text": "higher order"}, {"st": 103, "ed": 105, "text": "positive definite"}, {"st": 114, "ed": 117, "text": "end to end"}, {"st": 150, "ed": 153, "text": "theory and practice"}, {"st": 175, "ed": 179, "text": "problems in machine learning"}, {"st": 201, "ed": 203, "text": "deep networks"}, {"st": 212, "ed": 216, "text": "trained end to end"}]
[{"st": 10, "ed": 12, "text": "brain tumor"}, {"st": 22, "ed": 24, "text": "brain tumor"}, {"st": 27, "ed": 29, "text": "machine learning"}, {"st": 38, "ed": 40, "text": "machine learning"}, {"st": 91, "ed": 93, "text": "semi automatic"}, {"st": 97, "ed": 99, "text": "brain tumor"}, {"st": 130, "ed": 132, "text": "significantly improve"}, {"st": 136, "ed": 138, "text": "classification methods"}, {"st": 168, "ed": 170, "text": "hyper parameters"}]
[{"st": 2, "ed": 4, "text": "event recognition"}, {"st": 6, "ed": 8, "text": "challenging task"}, {"st": 79, "ed": 83, "text": "recurrent neural network rnn"}, {"st": 92, "ed": 94, "text": "attention weights"}, {"st": 112, "ed": 114, "text": "event detection"}, {"st": 173, "ed": 175, "text": "attention mechanism"}]
[{"st": 22, "ed": 26, "text": "head and neck cancer"}, {"st": 37, "ed": 39, "text": "pre defined"}, {"st": 39, "ed": 41, "text": "hand crafted"}, {"st": 42, "ed": 44, "text": "feature sets"}, {"st": 76, "ed": 78, "text": "medical imaging"}, {"st": 123, "ed": 125, "text": "ct scans"}, {"st": 130, "ed": 132, "text": "lidc idri"}, {"st": 133, "ed": 135, "text": "preliminary results"}, {"st": 136, "ed": 138, "text": "significant improvement"}]
[{"st": 12, "ed": 14, "text": "computer vision"}, {"st": 53, "ed": 55, "text": "image regions"}, {"st": 79, "ed": 82, "text": "markov decision process"}, {"st": 85, "ed": 87, "text": "imitation learning"}, {"st": 114, "ed": 116, "text": "context information"}]
[{"st": 0, "ed": 2, "text": "instance segmentation"}, {"st": 19, "ed": 21, "text": "instance segmentation"}, {"st": 45, "ed": 47, "text": "instance segmentation"}, {"st": 51, "ed": 54, "text": "end to end"}, {"st": 68, "ed": 71, "text": "recurrent neural network"}, {"st": 88, "ed": 90, "text": "spatial memory"}, {"st": 113, "ed": 115, "text": "loss function"}, {"st": 122, "ed": 124, "text": "instance segmentation"}, {"st": 134, "ed": 136, "text": "method outperforms"}, {"st": 136, "ed": 138, "text": "recent approaches"}]
[{"st": 6, "ed": 8, "text": "turing test"}, {"st": 72, "ed": 74, "text": "natural language"}, {"st": 106, "ed": 108, "text": "joint inference"}, {"st": 118, "ed": 120, "text": "spatial temporal"}, {"st": 134, "ed": 136, "text": "recently proposed"}, {"st": 136, "ed": 139, "text": "visual question answering"}, {"st": 191, "ed": 193, "text": "benchmark dataset"}]
[{"st": 56, "ed": 58, "text": "motion capture"}, {"st": 127, "ed": 129, "text": "analysis shows"}]
[{"st": 4, "ed": 6, "text": "recent progress"}, {"st": 11, "ed": 13, "text": "computer vision"}, {"st": 15, "ed": 17, "text": "physical therapy"}, {"st": 58, "ed": 60, "text": "motion capture"}, {"st": 83, "ed": 85, "text": "noisy data"}, {"st": 97, "ed": 99, "text": "kalman filter"}, {"st": 107, "ed": 109, "text": "proposed algorithm"}]
[{"st": 4, "ed": 6, "text": "becoming increasingly"}, {"st": 8, "ed": 10, "text": "relevant information"}, {"st": 22, "ed": 24, "text": "time consuming"}, {"st": 41, "ed": 43, "text": "semantically meaningful"}, {"st": 57, "ed": 60, "text": "convolutional neural networks"}, {"st": 80, "ed": 82, "text": "temporal coherence"}, {"st": 109, "ed": 111, "text": "event recognition"}, {"st": 128, "ed": 130, "text": "proposed approach"}]
[{"st": 10, "ed": 12, "text": "key challenge"}, {"st": 47, "ed": 49, "text": "medical doctors"}, {"st": 118, "ed": 121, "text": "future research directions"}]
[{"st": 6, "ed": 8, "text": "graphical model"}, {"st": 39, "ed": 41, "text": "message passing"}, {"st": 45, "ed": 47, "text": "message passing"}, {"st": 56, "ed": 58, "text": "convergence guarantees"}, {"st": 89, "ed": 91, "text": "inference problems"}]
[{"st": 57, "ed": 59, "text": "event recognition"}, {"st": 108, "ed": 110, "text": "real world"}]
[{"st": 19, "ed": 21, "text": "input data"}, {"st": 25, "ed": 27, "text": "previous works"}, {"st": 28, "ed": 31, "text": "propose and evaluate"}, {"st": 75, "ed": 77, "text": "motion features"}, {"st": 109, "ed": 111, "text": "proposed approach"}, {"st": 144, "ed": 146, "text": "approach achieves"}]
[{"st": 17, "ed": 19, "text": "visual reasoning"}, {"st": 25, "ed": 27, "text": "human perception"}, {"st": 76, "ed": 79, "text": "visual and textual"}, {"st": 114, "ed": 116, "text": "method outperforms"}]
[{"st": 7, "ed": 9, "text": "image classification"}, {"st": 11, "ed": 13, "text": "perform poorly"}, {"st": 18, "ed": 20, "text": "image description"}]
[{"st": 0, "ed": 2, "text": "recent research"}, {"st": 3, "ed": 6, "text": "deep neural networks"}, {"st": 140, "ed": 142, "text": "https github.com"}]
[{"st": 36, "ed": 40, "text": "convolutional neural networks cnn"}, {"st": 46, "ed": 48, "text": "low level"}, {"st": 48, "ed": 50, "text": "motion features"}, {"st": 51, "ed": 53, "text": "optical flow"}, {"st": 79, "ed": 81, "text": "spatio temporal"}, {"st": 83, "ed": 85, "text": "optical flow"}, {"st": 86, "ed": 88, "text": "input data"}, {"st": 104, "ed": 106, "text": "image resolution"}, {"st": 111, "ed": 113, "text": "previously reported"}]
[{"st": 19, "ed": 23, "text": "convolutional neural networks cnns"}, {"st": 27, "ed": 29, "text": "computationally efficient"}, {"st": 63, "ed": 65, "text": "representation learning"}, {"st": 78, "ed": 80, "text": "representation learning"}, {"st": 112, "ed": 114, "text": "decision trees"}, {"st": 126, "ed": 128, "text": "weight matrices"}, {"st": 142, "ed": 144, "text": "image classification"}, {"st": 147, "ed": 150, "text": "cifar and imagenet"}]
[{"st": 0, "ed": 3, "text": "building information modeling"}, {"st": 83, "ed": 85, "text": "relevant information"}]
[{"st": 19, "ed": 21, "text": "face image"}, {"st": 30, "ed": 32, "text": "face images"}, {"st": 41, "ed": 43, "text": "feature representation"}, {"st": 54, "ed": 56, "text": "feature embedding"}, {"st": 59, "ed": 64, "text": "deep convolutional neural network cnn"}, {"st": 67, "ed": 69, "text": "face image"}, {"st": 85, "ed": 87, "text": "feature vectors"}, {"st": 94, "ed": 96, "text": "convex hull"}, {"st": 102, "ed": 104, "text": "attention mechanism"}, {"st": 118, "ed": 120, "text": "standard classification"}, {"st": 133, "ed": 135, "text": "automatically learns"}, {"st": 139, "ed": 141, "text": "face images"}, {"st": 143, "ed": 145, "text": "low quality"}, {"st": 164, "ed": 166, "text": "face recognition"}, {"st": 170, "ed": 172, "text": "consistently outperforms"}]
[{"st": 10, "ed": 13, "text": "convolutional neural network"}, {"st": 15, "ed": 17, "text": "challenging task"}, {"st": 45, "ed": 47, "text": "computational burden"}, {"st": 56, "ed": 59, "text": "efficient and effective"}, {"st": 60, "ed": 62, "text": "training scheme"}, {"st": 68, "ed": 70, "text": "image patches"}, {"st": 82, "ed": 84, "text": "class imbalance"}, {"st": 108, "ed": 110, "text": "contextual information"}, {"st": 119, "ed": 121, "text": "input images"}, {"st": 122, "ed": 124, "text": "multiple scales"}, {"st": 126, "ed": 128, "text": "post processing"}, {"st": 138, "ed": 140, "text": "fully connected"}, {"st": 140, "ed": 143, "text": "conditional random field"}, {"st": 155, "ed": 157, "text": "challenging tasks"}, {"st": 161, "ed": 163, "text": "multi channel"}, {"st": 164, "ed": 166, "text": "patient data"}, {"st": 167, "ed": 170, "text": "traumatic brain injuries"}, {"st": 203, "ed": 205, "text": "computationally efficient"}, {"st": 218, "ed": 220, "text": "source code"}]
[{"st": 4, "ed": 6, "text": "face detection"}, {"st": 6, "ed": 8, "text": "facial landmark"}, {"st": 10, "ed": 12, "text": "face recognition"}, {"st": 19, "ed": 22, "text": "effective and efficient"}, {"st": 53, "ed": 55, "text": "face detection"}, {"st": 128, "ed": 130, "text": "recently introduced"}, {"st": 158, "ed": 160, "text": "face detection"}, {"st": 162, "ed": 164, "text": "facial landmark"}, {"st": 172, "ed": 174, "text": "facial landmark"}, {"st": 186, "ed": 188, "text": "face detection"}, {"st": 192, "ed": 194, "text": "facial landmark"}]
[{"st": 26, "ed": 28, "text": "natural images"}, {"st": 30, "ed": 32, "text": "extensively studied"}, {"st": 33, "ed": 35, "text": "computer vision"}, {"st": 55, "ed": 57, "text": "challenging task"}, {"st": 120, "ed": 122, "text": "based method"}, {"st": 132, "ed": 134, "text": "attention model"}, {"st": 175, "ed": 177, "text": "question answering"}]
[{"st": 5, "ed": 7, "text": "image generation"}, {"st": 30, "ed": 32, "text": "generate realistic"}, {"st": 42, "ed": 45, "text": "generative adversarial nets"}, {"st": 55, "ed": 57, "text": "generated image"}, {"st": 68, "ed": 70, "text": "challenging task"}, {"st": 78, "ed": 80, "text": "input image"}]
[{"st": 50, "ed": 52, "text": "hand tuned"}, {"st": 65, "ed": 67, "text": "image data"}, {"st": 77, "ed": 79, "text": "recent successes"}, {"st": 81, "ed": 83, "text": "computer vision"}, {"st": 83, "ed": 85, "text": "related tasks"}, {"st": 91, "ed": 95, "text": "convolutional neural networks cnns"}, {"st": 120, "ed": 122, "text": "image data"}, {"st": 141, "ed": 143, "text": "weakly labeled"}, {"st": 185, "ed": 187, "text": "large data"}]
[{"st": 1, "ed": 3, "text": "recent years"}, {"st": 3, "ed": 5, "text": "deep architectures"}, {"st": 9, "ed": 11, "text": "transfer learning"}, {"st": 45, "ed": 47, "text": "feature vectors"}, {"st": 49, "ed": 51, "text": "deep models"}, {"st": 72, "ed": 76, "text": "deep convolutional neural network"}, {"st": 94, "ed": 96, "text": "deep features"}]
[{"st": 91, "ed": 93, "text": "facial expression"}, {"st": 137, "ed": 140, "text": "support vector machines"}, {"st": 148, "ed": 150, "text": "promising results"}]
[{"st": 145, "ed": 147, "text": "independent variable"}]
[{"st": 57, "ed": 59, "text": "message passing"}, {"st": 76, "ed": 78, "text": "belief propagation"}, {"st": 85, "ed": 87, "text": "belief propagation"}, {"st": 94, "ed": 96, "text": "contextual information"}, {"st": 186, "ed": 188, "text": "inference algorithms"}, {"st": 193, "ed": 195, "text": "local information"}]
[{"st": 9, "ed": 11, "text": "deep learning"}, {"st": 60, "ed": 62, "text": "deep learning"}, {"st": 65, "ed": 68, "text": "2d and 3d"}, {"st": 68, "ed": 70, "text": "convolutional filters"}, {"st": 97, "ed": 99, "text": "electron microscope"}]
[{"st": 6, "ed": 8, "text": "probabilistic model"}, {"st": 26, "ed": 28, "text": "posterior distribution"}, {"st": 46, "ed": 48, "text": "true distribution"}, {"st": 101, "ed": 103, "text": "probabilistic models"}]
[{"st": 9, "ed": 11, "text": "dimensional space"}, {"st": 34, "ed": 36, "text": "class specific"}, {"st": 82, "ed": 84, "text": "likelihood function"}]
[{"st": 6, "ed": 8, "text": "abstract concepts"}, {"st": 23, "ed": 26, "text": "convolutional neural networks"}, {"st": 37, "ed": 39, "text": "object categories"}, {"st": 95, "ed": 97, "text": "previously unseen"}]
[{"st": 1, "ed": 3, "text": "event detection"}, {"st": 4, "ed": 6, "text": "content analysis"}, {"st": 21, "ed": 23, "text": "time consuming"}, {"st": 35, "ed": 37, "text": "event detection"}, {"st": 41, "ed": 43, "text": "multi label"}, {"st": 45, "ed": 47, "text": "classification task"}, {"st": 50, "ed": 52, "text": "weakly labeled"}, {"st": 59, "ed": 61, "text": "practical applications"}, {"st": 73, "ed": 76, "text": "deep neural network"}, {"st": 81, "ed": 84, "text": "multi label classification"}, {"st": 96, "ed": 98, "text": "frame level"}, {"st": 118, "ed": 120, "text": "multi label"}, {"st": 158, "ed": 160, "text": "high level"}, {"st": 183, "ed": 185, "text": "generalization capability"}, {"st": 196, "ed": 200, "text": "gaussian mixture model gmm"}, {"st": 201, "ed": 204, "text": "support vector machine"}, {"st": 209, "ed": 211, "text": "dnn based"}, {"st": 218, "ed": 220, "text": "temporal information"}, {"st": 236, "ed": 238, "text": "relative improvement"}, {"st": 243, "ed": 245, "text": "based method"}]
[{"st": 1, "ed": 3, "text": "machine learning"}, {"st": 5, "ed": 7, "text": "predictive modeling"}, {"st": 8, "ed": 10, "text": "pattern recognition"}, {"st": 44, "ed": 46, "text": "deep learning"}, {"st": 49, "ed": 51, "text": "machine learning"}, {"st": 58, "ed": 60, "text": "high level"}, {"st": 66, "ed": 69, "text": "convolutional neural network"}, {"st": 140, "ed": 144, "text": "convolutional neural network cnn"}, {"st": 182, "ed": 184, "text": "scale invariant"}, {"st": 184, "ed": 186, "text": "features extracted"}, {"st": 190, "ed": 192, "text": "deep learning"}]
[{"st": 4, "ed": 6, "text": "recent progress"}, {"st": 19, "ed": 21, "text": "motion capture"}, {"st": 71, "ed": 73, "text": "active research"}, {"st": 97, "ed": 99, "text": "virtual reality"}]
[{"st": 7, "ed": 9, "text": "visual scene"}, {"st": 16, "ed": 18, "text": "important issue"}, {"st": 19, "ed": 21, "text": "computer vision"}, {"st": 30, "ed": 32, "text": "bayesian inference"}, {"st": 68, "ed": 70, "text": "distributed memory"}, {"st": 96, "ed": 98, "text": "probability distributions"}]
[{"st": 12, "ed": 14, "text": "machine learning"}, {"st": 68, "ed": 71, "text": "convolutional neural networks"}, {"st": 101, "ed": 103, "text": "visual recognition"}]
[{"st": 0, "ed": 3, "text": "linear discriminant analysis"}, {"st": 9, "ed": 11, "text": "dimensionality reduction"}, {"st": 13, "ed": 15, "text": "computer vision"}, {"st": 118, "ed": 121, "text": "linear discriminant analysis"}, {"st": 121, "ed": 123, "text": "multi class"}, {"st": 127, "ed": 129, "text": "image classification"}, {"st": 159, "ed": 161, "text": "multi label"}, {"st": 172, "ed": 174, "text": "multi label"}, {"st": 192, "ed": 194, "text": "multi label"}, {"st": 198, "ed": 200, "text": "classification accuracy"}]
[{"st": 19, "ed": 22, "text": "extreme learning machines"}, {"st": 25, "ed": 27, "text": "proposed framework"}, {"st": 34, "ed": 36, "text": "contextual information"}, {"st": 41, "ed": 43, "text": "low level"}, {"st": 43, "ed": 45, "text": "visual cues"}]
[{"st": 2, "ed": 4, "text": "computer vision"}, {"st": 5, "ed": 7, "text": "image analysis"}, {"st": 14, "ed": 16, "text": "similarity measure"}, {"st": 63, "ed": 65, "text": "invariant features"}, {"st": 90, "ed": 92, "text": "image registration"}, {"st": 100, "ed": 102, "text": "machine vision"}, {"st": 103, "ed": 105, "text": "deep learning"}, {"st": 108, "ed": 112, "text": "convolutional neural networks cnn"}, {"st": 116, "ed": 118, "text": "scale invariant"}, {"st": 141, "ed": 143, "text": "object recognition"}, {"st": 147, "ed": 149, "text": "post processing"}, {"st": 158, "ed": 160, "text": "cross correlation"}, {"st": 166, "ed": 168, "text": "decision making"}, {"st": 171, "ed": 173, "text": "significant improvement"}, {"st": 178, "ed": 180, "text": "object detection"}]
[{"st": 8, "ed": 10, "text": "class specific"}, {"st": 25, "ed": 27, "text": "recently proposed"}, {"st": 27, "ed": 29, "text": "dictionary learning"}, {"st": 50, "ed": 52, "text": "simultaneously learn"}, {"st": 60, "ed": 62, "text": "class specific"}, {"st": 70, "ed": 72, "text": "dictionary learning"}, {"st": 83, "ed": 85, "text": "class specific"}, {"st": 93, "ed": 95, "text": "low rank"}, {"st": 141, "ed": 144, "text": "fast and accurate"}, {"st": 199, "ed": 201, "text": "image datasets"}]
[{"st": 46, "ed": 48, "text": "higher levels"}, {"st": 76, "ed": 79, "text": "taking advantage of"}, {"st": 85, "ed": 87, "text": "significantly improves"}, {"st": 88, "ed": 90, "text": "detection performance"}]
[{"st": 0, "ed": 2, "text": "sentiment analysis"}, {"st": 9, "ed": 11, "text": "social media"}, {"st": 19, "ed": 21, "text": "social media"}, {"st": 22, "ed": 24, "text": "sentiment analysis"}, {"st": 26, "ed": 28, "text": "increasing attention"}, {"st": 45, "ed": 47, "text": "image content"}, {"st": 73, "ed": 75, "text": "fine grained"}, {"st": 115, "ed": 117, "text": "sentiment classification"}, {"st": 168, "ed": 170, "text": "visual sentiment"}]
[{"st": 4, "ed": 6, "text": "object detection"}, {"st": 15, "ed": 18, "text": "deep neural networks"}, {"st": 25, "ed": 27, "text": "annotated data"}, {"st": 47, "ed": 49, "text": "bounding box"}, {"st": 68, "ed": 70, "text": "parameter sharing"}, {"st": 76, "ed": 78, "text": "trainable parameters"}, {"st": 88, "ed": 90, "text": "recurrent layers"}, {"st": 92, "ed": 94, "text": "contextual information"}, {"st": 117, "ed": 119, "text": "training data"}, {"st": 129, "ed": 131, "text": "natural images"}, {"st": 133, "ed": 135, "text": "pascal voc"}]
[{"st": 31, "ed": 33, "text": "activity recognition"}, {"st": 70, "ed": 72, "text": "automatic evaluation"}, {"st": 105, "ed": 107, "text": "based approach"}, {"st": 134, "ed": 136, "text": "approach achieves"}, {"st": 137, "ed": 139, "text": "promising results"}, {"st": 145, "ed": 147, "text": "strong baseline"}, {"st": 153, "ed": 155, "text": "entire dataset"}]
[{"st": 0, "ed": 4, "text": "generative adversarial networks gans"}, {"st": 10, "ed": 12, "text": "complex data"}, {"st": 58, "ed": 60, "text": "latent space"}, {"st": 72, "ed": 74, "text": "real images"}, {"st": 105, "ed": 107, "text": "real images"}]
[{"st": 4, "ed": 7, "text": "deep network architecture"}, {"st": 11, "ed": 13, "text": "image denoising"}, {"st": 34, "ed": 36, "text": "variational methods"}, {"st": 42, "ed": 44, "text": "self similarity"}, {"st": 55, "ed": 57, "text": "deep networks"}, {"st": 91, "ed": 93, "text": "local models"}, {"st": 136, "ed": 138, "text": "deep network"}, {"st": 150, "ed": 152, "text": "local models"}, {"st": 179, "ed": 181, "text": "deep learning"}]
[{"st": 0, "ed": 2, "text": "mean field"}, {"st": 15, "ed": 17, "text": "computer vision"}, {"st": 19, "ed": 21, "text": "efficiently solve"}, {"st": 35, "ed": 37, "text": "posterior probability"}, {"st": 42, "ed": 44, "text": "marginal probabilities"}, {"st": 63, "ed": 65, "text": "mean field"}, {"st": 103, "ed": 106, "text": "conditional random field"}, {"st": 117, "ed": 119, "text": "statistical physics"}, {"st": 137, "ed": 139, "text": "previous works"}, {"st": 163, "ed": 165, "text": "mean field"}, {"st": 172, "ed": 174, "text": "real world"}]
[{"st": 17, "ed": 19, "text": "computer vision"}, {"st": 44, "ed": 46, "text": "higher level"}, {"st": 46, "ed": 48, "text": "image understanding"}, {"st": 62, "ed": 64, "text": "key contribution"}, {"st": 69, "ed": 71, "text": "large scale"}, {"st": 83, "ed": 85, "text": "visual question"}, {"st": 118, "ed": 120, "text": "deep learning"}]
[{"st": 1, "ed": 3, "text": "local models"}, {"st": 12, "ed": 14, "text": "facial landmark"}, {"st": 55, "ed": 57, "text": "facial hair"}, {"st": 79, "ed": 81, "text": "neural architectures"}, {"st": 87, "ed": 90, "text": "end to end"}, {"st": 125, "ed": 127, "text": "facial landmark"}, {"st": 130, "ed": 132, "text": "large margin"}]
[{"st": 0, "ed": 2, "text": "autonomous driving"}, {"st": 38, "ed": 40, "text": "main idea"}, {"st": 80, "ed": 82, "text": "video game"}, {"st": 82, "ed": 84, "text": "road rash"}, {"st": 99, "ed": 101, "text": "road rash"}]
[{"st": 58, "ed": 62, "text": "generative adversarial networks gans"}, {"st": 83, "ed": 85, "text": "reinforcement learning"}]
[{"st": 8, "ed": 10, "text": "context aware"}, {"st": 19, "ed": 21, "text": "visual concepts"}, {"st": 26, "ed": 28, "text": "training data"}, {"st": 46, "ed": 48, "text": "siamese cat"}, {"st": 57, "ed": 59, "text": "siamese cat"}, {"st": 79, "ed": 81, "text": "joint inference"}, {"st": 94, "ed": 96, "text": "closely related"}, {"st": 115, "ed": 117, "text": "fine grained"}, {"st": 122, "ed": 124, "text": "closely related"}, {"st": 135, "ed": 137, "text": "image captioning"}, {"st": 157, "ed": 159, "text": "ground truth"}, {"st": 166, "ed": 168, "text": "image captioning"}, {"st": 171, "ed": 173, "text": "approach outperforms"}]
[{"st": 1, "ed": 3, "text": "semi automatic"}, {"st": 3, "ed": 5, "text": "open source"}, {"st": 18, "ed": 20, "text": "rule based"}]
[{"st": 2, "ed": 4, "text": "visual perception"}, {"st": 88, "ed": 90, "text": "attention mechanism"}, {"st": 152, "ed": 155, "text": "recurrent neural network"}, {"st": 158, "ed": 160, "text": "reinforcement learning"}]
[{"st": 8, "ed": 10, "text": "fully automated"}, {"st": 24, "ed": 28, "text": "deep convolutional neural network"}, {"st": 48, "ed": 50, "text": "deep network"}]
[{"st": 8, "ed": 10, "text": "fully automated"}, {"st": 24, "ed": 28, "text": "deep convolutional neural networks"}, {"st": 50, "ed": 52, "text": "deep networks"}, {"st": 55, "ed": 57, "text": "labeled datasets"}, {"st": 59, "ed": 61, "text": "semi supervised"}]
[{"st": 1, "ed": 3, "text": "article presents"}, {"st": 14, "ed": 17, "text": "deep neural network"}, {"st": 31, "ed": 33, "text": "input image"}, {"st": 34, "ed": 36, "text": "provide evidence"}, {"st": 47, "ed": 49, "text": "previous methods"}, {"st": 56, "ed": 58, "text": "decision making"}, {"st": 62, "ed": 64, "text": "neural network"}, {"st": 84, "ed": 86, "text": "application areas"}, {"st": 96, "ed": 98, "text": "natural images"}]
[{"st": 0, "ed": 2, "text": "automatic segmentation"}, {"st": 21, "ed": 23, "text": "computer aided"}, {"st": 45, "ed": 47, "text": "fully convolutional"}, {"st": 47, "ed": 49, "text": "neural networks"}, {"st": 55, "ed": 57, "text": "large scale"}, {"st": 155, "ed": 157, "text": "experimentally demonstrate"}, {"st": 161, "ed": 163, "text": "proposed method"}]
[{"st": 7, "ed": 9, "text": "hot topic"}, {"st": 12, "ed": 14, "text": "previous research"}, {"st": 62, "ed": 66, "text": "conditional generative adversarial networks"}, {"st": 71, "ed": 73, "text": "real world"}, {"st": 80, "ed": 82, "text": "fully convolutional"}, {"st": 84, "ed": 86, "text": "multi layer"}, {"st": 91, "ed": 93, "text": "multi layer"}, {"st": 110, "ed": 112, "text": "network architecture"}, {"st": 115, "ed": 117, "text": "highly competitive"}, {"st": 125, "ed": 127, "text": "turing test"}]
[{"st": 9, "ed": 11, "text": "labeled samples"}, {"st": 30, "ed": 32, "text": "labeled samples"}, {"st": 38, "ed": 41, "text": "unsupervised domain adaptation"}, {"st": 73, "ed": 76, "text": "source and target"}, {"st": 120, "ed": 122, "text": "unlabeled samples"}, {"st": 145, "ed": 148, "text": "unsupervised domain adaptation"}, {"st": 154, "ed": 156, "text": "unlabeled samples"}, {"st": 158, "ed": 160, "text": "neural networks"}, {"st": 206, "ed": 208, "text": "digit recognition"}, {"st": 209, "ed": 211, "text": "sentiment analysis"}, {"st": 213, "ed": 215, "text": "proposed method"}, {"st": 224, "ed": 226, "text": "digit recognition"}]
[{"st": 1, "ed": 4, "text": "image to image"}, {"st": 9, "ed": 11, "text": "joint distribution"}, {"st": 21, "ed": 23, "text": "marginal distributions"}, {"st": 30, "ed": 32, "text": "infinite set"}, {"st": 33, "ed": 35, "text": "joint distributions"}, {"st": 40, "ed": 42, "text": "marginal distributions"}, {"st": 48, "ed": 50, "text": "joint distribution"}, {"st": 52, "ed": 54, "text": "marginal distributions"}, {"st": 65, "ed": 67, "text": "latent space"}, {"st": 72, "ed": 75, "text": "image to image"}, {"st": 84, "ed": 86, "text": "proposed framework"}, {"st": 87, "ed": 89, "text": "competing approaches"}, {"st": 93, "ed": 95, "text": "image translation"}, {"st": 100, "ed": 102, "text": "image translation"}, {"st": 102, "ed": 104, "text": "tasks including"}, {"st": 106, "ed": 108, "text": "image translation"}, {"st": 109, "ed": 111, "text": "image translation"}, {"st": 112, "ed": 114, "text": "face image"}, {"st": 119, "ed": 121, "text": "proposed framework"}, {"st": 122, "ed": 124, "text": "domain adaptation"}, {"st": 141, "ed": 143, "text": "https github.com"}]
[{"st": 9, "ed": 11, "text": "remote sensing"}, {"st": 19, "ed": 21, "text": "spatial resolution"}, {"st": 62, "ed": 64, "text": "benchmark datasets"}, {"st": 82, "ed": 84, "text": "benchmark dataset"}]
[{"st": 64, "ed": 66, "text": "raw data"}, {"st": 93, "ed": 95, "text": "decision tree"}, {"st": 99, "ed": 102, "text": "k nearest neighbors"}]
[{"st": 10, "ed": 12, "text": "pedestrian detection"}, {"st": 17, "ed": 19, "text": "pedestrian detection"}, {"st": 78, "ed": 81, "text": "training and testing"}, {"st": 121, "ed": 123, "text": "large scale"}, {"st": 131, "ed": 133, "text": "synthetic data"}, {"st": 167, "ed": 171, "text": "generative adversarial networks gans"}, {"st": 177, "ed": 179, "text": "synthetic data"}, {"st": 206, "ed": 208, "text": "training data"}, {"st": 249, "ed": 251, "text": "notoriously difficult"}, {"st": 253, "ed": 255, "text": "real world"}]
[{"st": 0, "ed": 4, "text": "deep convolutional neural networks"}, {"st": 17, "ed": 19, "text": "computer vision"}, {"st": 21, "ed": 23, "text": "object recognition"}, {"st": 23, "ed": 25, "text": "object detection"}, {"st": 37, "ed": 39, "text": "labeled datasets"}, {"st": 70, "ed": 72, "text": "computer vision"}]
[{"st": 11, "ed": 13, "text": "active research"}, {"st": 56, "ed": 58, "text": "analysis shows"}, {"st": 81, "ed": 83, "text": "multi level"}, {"st": 93, "ed": 95, "text": "image content"}, {"st": 109, "ed": 111, "text": "multi level"}]
[{"st": 9, "ed": 11, "text": "challenging problem"}, {"st": 17, "ed": 19, "text": "classification models"}, {"st": 42, "ed": 44, "text": "evaluation metrics"}, {"st": 50, "ed": 52, "text": "proposed approach"}, {"st": 89, "ed": 91, "text": "wide variety"}, {"st": 101, "ed": 103, "text": "embedding space"}]
[{"st": 40, "ed": 42, "text": "higher accuracy"}, {"st": 109, "ed": 111, "text": "feature embedding"}, {"st": 154, "ed": 157, "text": "end to end"}, {"st": 200, "ed": 202, "text": "face verification"}, {"st": 203, "ed": 206, "text": "person re identification"}, {"st": 213, "ed": 215, "text": "source code"}, {"st": 216, "ed": 218, "text": "network structure"}, {"st": 222, "ed": 224, "text": "https github.com"}]
[{"st": 0, "ed": 2, "text": "image captioning"}, {"st": 4, "ed": 6, "text": "challenging problem"}, {"st": 13, "ed": 15, "text": "image content"}, {"st": 24, "ed": 26, "text": "recent advances"}, {"st": 27, "ed": 30, "text": "deep neural networks"}, {"st": 46, "ed": 48, "text": "encoder decoder"}, {"st": 66, "ed": 68, "text": "decision making"}, {"st": 75, "ed": 77, "text": "policy network"}, {"st": 86, "ed": 88, "text": "policy network"}, {"st": 147, "ed": 149, "text": "ground truth"}, {"st": 158, "ed": 160, "text": "reinforcement learning"}, {"st": 167, "ed": 169, "text": "visual semantic"}, {"st": 170, "ed": 172, "text": "extensive experiments"}, {"st": 177, "ed": 179, "text": "coco dataset"}, {"st": 182, "ed": 184, "text": "proposed framework"}]
[{"st": 0, "ed": 2, "text": "reinforcement learning"}, {"st": 14, "ed": 16, "text": "autonomous driving"}, {"st": 18, "ed": 20, "text": "reinforcement learning"}, {"st": 38, "ed": 40, "text": "virtual environment"}, {"st": 62, "ed": 64, "text": "virtual environment"}, {"st": 96, "ed": 98, "text": "reinforcement learning"}, {"st": 102, "ed": 104, "text": "real world"}, {"st": 114, "ed": 116, "text": "reinforcement learning"}, {"st": 134, "ed": 136, "text": "reinforcement learning"}, {"st": 140, "ed": 142, "text": "real world"}]
[{"st": 7, "ed": 10, "text": "optical character recognition"}, {"st": 53, "ed": 56, "text": "takes advantage of"}, {"st": 113, "ed": 115, "text": "depth estimation"}, {"st": 160, "ed": 162, "text": "recognition rate"}]
[{"st": 3, "ed": 5, "text": "general framework"}, {"st": 13, "ed": 15, "text": "latent representations"}, {"st": 23, "ed": 25, "text": "hidden units"}, {"st": 36, "ed": 38, "text": "proposed method"}, {"st": 45, "ed": 47, "text": "visual concepts"}, {"st": 52, "ed": 54, "text": "hidden units"}, {"st": 80, "ed": 82, "text": "proposed method"}, {"st": 94, "ed": 96, "text": "linear combinations"}, {"st": 106, "ed": 108, "text": "latent representations"}, {"st": 119, "ed": 121, "text": "supervised training"}, {"st": 140, "ed": 142, "text": "network depth"}, {"st": 151, "ed": 153, "text": "batch normalization"}, {"st": 164, "ed": 166, "text": "proposed method"}]
[{"st": 11, "ed": 16, "text": "computer vision and machine learning"}, {"st": 28, "ed": 30, "text": "autonomous driving"}, {"st": 36, "ed": 38, "text": "augmented reality"}, {"st": 50, "ed": 52, "text": "deep learning"}, {"st": 62, "ed": 64, "text": "computer vision"}, {"st": 76, "ed": 78, "text": "deep learning"}, {"st": 126, "ed": 128, "text": "existing methods"}, {"st": 140, "ed": 142, "text": "quantitative results"}, {"st": 190, "ed": 192, "text": "deep learning"}]
[{"st": 7, "ed": 9, "text": "object detection"}, {"st": 47, "ed": 49, "text": "object class"}, {"st": 65, "ed": 67, "text": "object recognition"}, {"st": 97, "ed": 99, "text": "vision systems"}, {"st": 127, "ed": 129, "text": "applications including"}, {"st": 179, "ed": 181, "text": "object class"}, {"st": 196, "ed": 198, "text": "proposed algorithm"}, {"st": 216, "ed": 218, "text": "object detection"}, {"st": 222, "ed": 224, "text": "challenging task"}]
[{"st": 9, "ed": 11, "text": "image understanding"}, {"st": 36, "ed": 38, "text": "visual attention"}, {"st": 63, "ed": 65, "text": "image captioning"}, {"st": 72, "ed": 74, "text": "low level"}, {"st": 128, "ed": 130, "text": "ms coco"}]
[{"st": 5, "ed": 7, "text": "deep learning"}, {"st": 12, "ed": 14, "text": "pre trained"}, {"st": 27, "ed": 29, "text": "training set"}, {"st": 61, "ed": 63, "text": "unsupervised learning"}, {"st": 111, "ed": 113, "text": "class specific"}, {"st": 113, "ed": 115, "text": "domain adversarial"}, {"st": 115, "ed": 117, "text": "learning framework"}, {"st": 119, "ed": 121, "text": "pre trained"}]
[{"st": 49, "ed": 53, "text": "convolutional neural network cnn"}, {"st": 101, "ed": 103, "text": "training samples"}, {"st": 147, "ed": 149, "text": "benchmark dataset"}, {"st": 167, "ed": 169, "text": "precision recall"}, {"st": 177, "ed": 179, "text": "proposed method"}]
[{"st": 1, "ed": 4, "text": "person re identification"}, {"st": 14, "ed": 16, "text": "feature representation"}, {"st": 34, "ed": 36, "text": "jointly learning"}, {"st": 36, "ed": 39, "text": "local and global"}, {"st": 42, "ed": 46, "text": "convolutional neural network cnn"}, {"st": 51, "ed": 54, "text": "local and global"}, {"st": 64, "ed": 66, "text": "joint learning"}, {"st": 67, "ed": 70, "text": "local and global"}, {"st": 70, "ed": 72, "text": "feature selection"}, {"st": 97, "ed": 99, "text": "jointly learning"}, {"st": 103, "ed": 106, "text": "local and global"}]
[{"st": 0, "ed": 2, "text": "subspace clustering"}, {"st": 19, "ed": 21, "text": "subspace clustering"}, {"st": 50, "ed": 52, "text": "subspace clustering"}, {"st": 92, "ed": 94, "text": "kernel space"}, {"st": 122, "ed": 124, "text": "closed form"}, {"st": 145, "ed": 148, "text": "large scale data"}, {"st": 155, "ed": 157, "text": "proposed method"}, {"st": 161, "ed": 163, "text": "subspace clustering"}, {"st": 164, "ed": 166, "text": "extensive experiments"}, {"st": 168, "ed": 170, "text": "real world"}, {"st": 179, "ed": 181, "text": "proposed method"}]
[{"st": 5, "ed": 7, "text": "newtonian mechanics"}, {"st": 33, "ed": 35, "text": "artificial intelligence"}, {"st": 44, "ed": 46, "text": "neural networks"}, {"st": 66, "ed": 68, "text": "real world"}, {"st": 85, "ed": 87, "text": "neural networks"}, {"st": 93, "ed": 95, "text": "real world"}, {"st": 100, "ed": 102, "text": "visual data"}, {"st": 112, "ed": 116, "text": "recurrent neural network architecture"}, {"st": 156, "ed": 158, "text": "competitive results"}]
[{"st": 3, "ed": 5, "text": "deep learning"}, {"st": 18, "ed": 20, "text": "computational power"}, {"st": 24, "ed": 26, "text": "large scale"}, {"st": 64, "ed": 66, "text": "dataset size"}, {"st": 102, "ed": 104, "text": "noisy labels"}, {"st": 114, "ed": 116, "text": "vision tasks"}, {"st": 142, "ed": 144, "text": "vision tasks"}, {"st": 150, "ed": 152, "text": "training data"}, {"st": 157, "ed": 159, "text": "representation learning"}, {"st": 160, "ed": 162, "text": "pre training"}, {"st": 170, "ed": 172, "text": "improve performance"}, {"st": 174, "ed": 176, "text": "vision tasks"}, {"st": 196, "ed": 198, "text": "vision tasks"}, {"st": 199, "ed": 201, "text": "image classification"}, {"st": 201, "ed": 203, "text": "object detection"}, {"st": 216, "ed": 218, "text": "vision community"}]
[{"st": 10, "ed": 12, "text": "moving objects"}, {"st": 20, "ed": 23, "text": "bag of words"}, {"st": 88, "ed": 91, "text": "bag of words"}, {"st": 105, "ed": 107, "text": "feature sets"}, {"st": 202, "ed": 204, "text": "benchmark datasets"}]
[{"st": 1, "ed": 3, "text": "computer vision"}, {"st": 5, "ed": 7, "text": "object tracking"}, {"st": 55, "ed": 57, "text": "object tracking"}, {"st": 150, "ed": 152, "text": "search space"}, {"st": 206, "ed": 208, "text": "optimal solution"}, {"st": 209, "ed": 211, "text": "bounding box"}, {"st": 222, "ed": 224, "text": "based method"}, {"st": 229, "ed": 231, "text": "visual object"}]
[{"st": 10, "ed": 14, "text": "convolutional neural network cnn"}, {"st": 65, "ed": 68, "text": "end to end"}, {"st": 78, "ed": 80, "text": "deep models"}, {"st": 88, "ed": 90, "text": "task performance"}, {"st": 101, "ed": 103, "text": "image datasets"}, {"st": 138, "ed": 141, "text": "end to end"}, {"st": 148, "ed": 150, "text": "object detection"}]
[{"st": 19, "ed": 21, "text": "fully convolutional"}, {"st": 24, "ed": 28, "text": "conditional generative adversarial networks"}, {"st": 44, "ed": 46, "text": "ct scans"}, {"st": 69, "ed": 71, "text": "initial results"}, {"st": 75, "ed": 77, "text": "detection performance"}]
[{"st": 4, "ed": 6, "text": "face recognition"}, {"st": 18, "ed": 20, "text": "face recognition"}, {"st": 23, "ed": 25, "text": "face recognition"}, {"st": 31, "ed": 33, "text": "visual quality"}, {"st": 42, "ed": 44, "text": "large scale"}, {"st": 58, "ed": 60, "text": "feature level"}, {"st": 60, "ed": 62, "text": "domain adaptation"}, {"st": 72, "ed": 74, "text": "large scale"}, {"st": 89, "ed": 91, "text": "large scale"}, {"st": 96, "ed": 98, "text": "face recognition"}, {"st": 130, "ed": 132, "text": "synthetic data"}, {"st": 137, "ed": 139, "text": "domain invariant"}, {"st": 142, "ed": 144, "text": "domain adversarial"}, {"st": 147, "ed": 149, "text": "improve performance"}, {"st": 166, "ed": 168, "text": "domain specific"}, {"st": 185, "ed": 187, "text": "feature level"}, {"st": 187, "ed": 189, "text": "domain adaptation"}, {"st": 191, "ed": 193, "text": "substantially improves"}, {"st": 194, "ed": 196, "text": "face recognition"}]
[{"st": 0, "ed": 2, "text": "cross modal"}, {"st": 11, "ed": 13, "text": "large scale"}, {"st": 43, "ed": 45, "text": "image representations"}, {"st": 53, "ed": 55, "text": "existing methods"}, {"st": 63, "ed": 65, "text": "cross modal"}, {"st": 69, "ed": 71, "text": "visual search"}, {"st": 78, "ed": 80, "text": "cross modal"}, {"st": 91, "ed": 93, "text": "deep architecture"}, {"st": 120, "ed": 122, "text": "convolutional networks"}, {"st": 124, "ed": 127, "text": "short term memory"}, {"st": 195, "ed": 197, "text": "significantly outperforms"}, {"st": 208, "ed": 210, "text": "cross modal"}]
[{"st": 2, "ed": 4, "text": "surface water"}, {"st": 5, "ed": 7, "text": "natural environment"}, {"st": 19, "ed": 21, "text": "land cover"}, {"st": 58, "ed": 60, "text": "ground truth"}, {"st": 68, "ed": 70, "text": "machine learning"}, {"st": 110, "ed": 112, "text": "supervised learning"}, {"st": 189, "ed": 191, "text": "supervised learning"}, {"st": 203, "ed": 205, "text": "valuable information"}, {"st": 207, "ed": 209, "text": "land cover"}]
[{"st": 7, "ed": 11, "text": "convolutional neural networks cnns"}, {"st": 78, "ed": 80, "text": "classification models"}]
[{"st": 1, "ed": 3, "text": "object recognition"}, {"st": 9, "ed": 11, "text": "unsupervised learning"}, {"st": 56, "ed": 60, "text": "partially observable markov decision"}, {"st": 64, "ed": 66, "text": "near optimal"}, {"st": 68, "ed": 70, "text": "training data"}, {"st": 79, "ed": 82, "text": "markov decision process"}, {"st": 94, "ed": 96, "text": "near optimal"}, {"st": 98, "ed": 100, "text": "training set"}, {"st": 107, "ed": 112, "text": "long short term memory lstm"}, {"st": 121, "ed": 123, "text": "training set"}, {"st": 151, "ed": 153, "text": "policy search"}, {"st": 157, "ed": 159, "text": "lstm network"}, {"st": 161, "ed": 163, "text": "recognition accuracy"}, {"st": 198, "ed": 200, "text": "gradient based"}, {"st": 227, "ed": 229, "text": "lstm network"}]
[{"st": 0, "ed": 4, "text": "recurrent neural networks rnns"}, {"st": 25, "ed": 27, "text": "vanishing gradients"}, {"st": 61, "ed": 63, "text": "rnn model"}, {"st": 66, "ed": 68, "text": "rnn models"}, {"st": 131, "ed": 133, "text": "source code"}]
[{"st": 4, "ed": 8, "text": "convolutional neural networks cnns"}, {"st": 33, "ed": 35, "text": "recent research"}, {"st": 67, "ed": 69, "text": "convolutional layers"}, {"st": 81, "ed": 83, "text": "non linearities"}, {"st": 87, "ed": 89, "text": "activation functions"}, {"st": 117, "ed": 119, "text": "visual cortex"}, {"st": 172, "ed": 174, "text": "convolutional layers"}]
[{"st": 53, "ed": 55, "text": "machine learning"}, {"st": 71, "ed": 73, "text": "classification accuracy"}, {"st": 73, "ed": 75, "text": "computational complexity"}, {"st": 119, "ed": 122, "text": "deep belief networks"}]
[{"st": 10, "ed": 12, "text": "decision making"}, {"st": 22, "ed": 24, "text": "risk management"}, {"st": 99, "ed": 101, "text": "deep learning"}, {"st": 114, "ed": 117, "text": "convolutional neural networks"}, {"st": 192, "ed": 194, "text": "comparable results"}, {"st": 231, "ed": 233, "text": "machine learning"}, {"st": 240, "ed": 242, "text": "extracting information"}]
[{"st": 9, "ed": 11, "text": "real life"}, {"st": 62, "ed": 64, "text": "rgb d"}, {"st": 65, "ed": 67, "text": "point cloud"}, {"st": 68, "ed": 70, "text": "automatically generated"}, {"st": 91, "ed": 93, "text": "activity recognition"}, {"st": 115, "ed": 117, "text": "activity recognition"}, {"st": 130, "ed": 132, "text": "computer vision"}, {"st": 135, "ed": 137, "text": "object detection"}, {"st": 138, "ed": 140, "text": "human skeleton"}]
[{"st": 25, "ed": 27, "text": "deep network"}, {"st": 31, "ed": 33, "text": "low dimensional"}, {"st": 42, "ed": 44, "text": "deep learning"}, {"st": 72, "ed": 74, "text": "deep learning"}, {"st": 104, "ed": 106, "text": "feature space"}, {"st": 125, "ed": 127, "text": "proposed method"}, {"st": 134, "ed": 136, "text": "image classification"}, {"st": 157, "ed": 159, "text": "proposed approach"}]
[{"st": 0, "ed": 2, "text": "cross view"}, {"st": 41, "ed": 43, "text": "cross view"}, {"st": 66, "ed": 68, "text": "vision tasks"}]
[{"st": 14, "ed": 16, "text": "visual tracking"}, {"st": 159, "ed": 161, "text": "causal graph"}, {"st": 165, "ed": 167, "text": "search algorithm"}, {"st": 168, "ed": 170, "text": "dynamic programming"}, {"st": 174, "ed": 176, "text": "proposed method"}]
[{"st": 2, "ed": 6, "text": "simultaneous localization and mapping"}, {"st": 24, "ed": 26, "text": "scene understanding"}, {"st": 78, "ed": 80, "text": "mixture model"}, {"st": 93, "ed": 96, "text": "conditional random field"}]
[{"st": 6, "ed": 9, "text": "deep neural networks"}, {"st": 40, "ed": 43, "text": "spatial and temporal"}, {"st": 54, "ed": 56, "text": "neural network"}, {"st": 65, "ed": 67, "text": "convolutional neural"}, {"st": 91, "ed": 93, "text": "previously published"}, {"st": 96, "ed": 98, "text": "handwriting recognition"}, {"st": 166, "ed": 168, "text": "open source"}]
[{"st": 3, "ed": 7, "text": "convolutional neural networks cnns"}, {"st": 24, "ed": 26, "text": "key insight"}, {"st": 29, "ed": 31, "text": "convolutional networks"}, {"st": 43, "ed": 45, "text": "efficient inference"}, {"st": 56, "ed": 58, "text": "instance segmentation"}, {"st": 94, "ed": 97, "text": "deep neural network"}, {"st": 104, "ed": 106, "text": "input image"}, {"st": 110, "ed": 113, "text": "conditional random field"}, {"st": 115, "ed": 118, "text": "end to end"}, {"st": 148, "ed": 150, "text": "instance level"}, {"st": 177, "ed": 179, "text": "extensive experiments"}]
[{"st": 76, "ed": 78, "text": "motion capture"}, {"st": 89, "ed": 92, "text": "short term memory"}, {"st": 98, "ed": 100, "text": "field crf"}, {"st": 130, "ed": 132, "text": "spatio temporal"}]
[{"st": 11, "ed": 13, "text": "real world"}, {"st": 35, "ed": 37, "text": "dimensional space"}, {"st": 44, "ed": 46, "text": "feature extraction"}, {"st": 63, "ed": 65, "text": "spatial relations"}, {"st": 71, "ed": 73, "text": "feature extraction"}, {"st": 78, "ed": 80, "text": "neural network"}, {"st": 103, "ed": 105, "text": "spatial features"}, {"st": 105, "ed": 107, "text": "significantly improves"}, {"st": 111, "ed": 113, "text": "machine learning"}]
[{"st": 51, "ed": 53, "text": "bi directional"}, {"st": 72, "ed": 74, "text": "motor coordination"}, {"st": 96, "ed": 98, "text": "training samples"}, {"st": 126, "ed": 128, "text": "cross view"}, {"st": 132, "ed": 134, "text": "generalization capability"}]
[{"st": 0, "ed": 3, "text": "convolutional neural networks"}, {"st": 12, "ed": 14, "text": "discriminative features"}, {"st": 33, "ed": 35, "text": "image quality"}, {"st": 87, "ed": 89, "text": "image classification"}, {"st": 117, "ed": 120, "text": "end to end"}, {"st": 133, "ed": 135, "text": "benchmark datasets"}, {"st": 136, "ed": 138, "text": "fine grained"}, {"st": 141, "ed": 143, "text": "texture classification"}, {"st": 158, "ed": 160, "text": "promising results"}]
[{"st": 4, "ed": 8, "text": "deep convolutional neural network"}, {"st": 18, "ed": 20, "text": "feature map"}, {"st": 22, "ed": 24, "text": "convolutional network"}, {"st": 31, "ed": 33, "text": "fully connected"}, {"st": 40, "ed": 42, "text": "saliency map"}, {"st": 57, "ed": 59, "text": "object detection"}, {"st": 77, "ed": 79, "text": "skip connections"}, {"st": 80, "ed": 82, "text": "feature maps"}, {"st": 126, "ed": 128, "text": "skip connections"}, {"st": 133, "ed": 136, "text": "deep convolutional networks"}, {"st": 158, "ed": 161, "text": "deep neural network"}]
[{"st": 20, "ed": 22, "text": "style transfer"}, {"st": 26, "ed": 30, "text": "conditional generative adversarial networks"}]
[{"st": 19, "ed": 21, "text": "spatial frequency"}, {"st": 35, "ed": 38, "text": "coarse to fine"}, {"st": 47, "ed": 49, "text": "fine grained"}, {"st": 117, "ed": 120, "text": "coarse to fine"}, {"st": 120, "ed": 122, "text": "object categorization"}]
[{"st": 10, "ed": 12, "text": "computer vision"}, {"st": 26, "ed": 28, "text": "random forests"}, {"st": 28, "ed": 30, "text": "based methods"}, {"st": 53, "ed": 55, "text": "image features"}, {"st": 88, "ed": 90, "text": "line segment"}, {"st": 117, "ed": 119, "text": "proposed approach"}, {"st": 124, "ed": 127, "text": "publicly available datasets"}]
[{"st": 14, "ed": 16, "text": "image classification"}, {"st": 27, "ed": 29, "text": "convolutional filters"}, {"st": 41, "ed": 43, "text": "feature maps"}, {"st": 56, "ed": 58, "text": "biologically inspired"}, {"st": 66, "ed": 68, "text": "doesn t"}, {"st": 79, "ed": 81, "text": "promising results"}, {"st": 82, "ed": 84, "text": "unlike existing"}, {"st": 92, "ed": 94, "text": "input image"}, {"st": 109, "ed": 111, "text": "https github.com"}]
[{"st": 4, "ed": 6, "text": "saliency prediction"}, {"st": 23, "ed": 25, "text": "natural images"}, {"st": 34, "ed": 36, "text": "natural images"}, {"st": 61, "ed": 63, "text": "saliency prediction"}, {"st": 72, "ed": 74, "text": "interface design"}, {"st": 79, "ed": 81, "text": "building blocks"}, {"st": 106, "ed": 108, "text": "multi scale"}, {"st": 108, "ed": 110, "text": "deep learning"}, {"st": 113, "ed": 115, "text": "saliency prediction"}, {"st": 123, "ed": 125, "text": "saliency prediction"}, {"st": 128, "ed": 130, "text": "natural images"}]
[{"st": 75, "ed": 78, "text": "recurrent neural network"}, {"st": 107, "ed": 109, "text": "sinus rhythm"}]
[{"st": 14, "ed": 16, "text": "visual attention"}, {"st": 30, "ed": 32, "text": "saliency map"}, {"st": 36, "ed": 38, "text": "attention mechanism"}, {"st": 57, "ed": 59, "text": "visual attention"}, {"st": 63, "ed": 65, "text": "multi label"}, {"st": 65, "ed": 67, "text": "image classification"}, {"st": 76, "ed": 78, "text": "high precision"}, {"st": 88, "ed": 90, "text": "multi label"}, {"st": 90, "ed": 92, "text": "image classification"}, {"st": 101, "ed": 103, "text": "reinforcement learning"}, {"st": 104, "ed": 106, "text": "training process"}]
[{"st": 1, "ed": 3, "text": "image registration"}, {"st": 15, "ed": 17, "text": "computationally expensive"}, {"st": 20, "ed": 22, "text": "large scale"}, {"st": 44, "ed": 46, "text": "computational resources"}, {"st": 58, "ed": 60, "text": "image registration"}, {"st": 71, "ed": 73, "text": "image analysis"}, {"st": 110, "ed": 113, "text": "orders of magnitude"}, {"st": 124, "ed": 126, "text": "large scale"}, {"st": 130, "ed": 133, "text": "graphics processing unit"}, {"st": 142, "ed": 144, "text": "magnetic resonance"}]
[{"st": 4, "ed": 6, "text": "semi supervised"}, {"st": 28, "ed": 30, "text": "proposed framework"}, {"st": 33, "ed": 35, "text": "pre trained"}, {"st": 37, "ed": 39, "text": "significantly reduce"}, {"st": 42, "ed": 44, "text": "labeled data"}, {"st": 50, "ed": 52, "text": "labeled data"}, {"st": 84, "ed": 86, "text": "input image"}, {"st": 88, "ed": 90, "text": "representation space"}, {"st": 138, "ed": 140, "text": "pre trained"}, {"st": 155, "ed": 157, "text": "3d shapes"}, {"st": 164, "ed": 166, "text": "model based"}, {"st": 172, "ed": 174, "text": "low dimensional"}, {"st": 191, "ed": 193, "text": "training data"}, {"st": 211, "ed": 213, "text": "proposed framework"}, {"st": 220, "ed": 222, "text": "point cloud"}, {"st": 228, "ed": 230, "text": "3d shapes"}, {"st": 238, "ed": 240, "text": "benchmark datasets"}]
[{"st": 19, "ed": 23, "text": "generative adversarial network gan"}, {"st": 27, "ed": 29, "text": "encoder decoder"}, {"st": 30, "ed": 33, "text": "deep neural network"}, {"st": 48, "ed": 50, "text": "wasserstein gan"}]
[{"st": 3, "ed": 5, "text": "point set"}, {"st": 63, "ed": 65, "text": "prior knowledge"}, {"st": 75, "ed": 77, "text": "point set"}, {"st": 81, "ed": 84, "text": "gaussian mixture model"}, {"st": 115, "ed": 117, "text": "proposed method"}, {"st": 122, "ed": 124, "text": "point set"}, {"st": 135, "ed": 137, "text": "computational cost"}, {"st": 163, "ed": 165, "text": "existing algorithms"}]
[{"st": 3, "ed": 5, "text": "deep learning"}, {"st": 6, "ed": 8, "text": "computer vision"}, {"st": 18, "ed": 20, "text": "large scale"}, {"st": 30, "ed": 33, "text": "unsupervised domain adaptation"}, {"st": 37, "ed": 39, "text": "labeled data"}, {"st": 41, "ed": 43, "text": "source domain"}, {"st": 71, "ed": 73, "text": "neural networks"}, {"st": 126, "ed": 129, "text": "hyper parameter tuning"}, {"st": 192, "ed": 194, "text": "source domain"}]
[{"st": 136, "ed": 138, "text": "experiments conducted"}]
[{"st": 0, "ed": 2, "text": "recent progress"}, {"st": 3, "ed": 5, "text": "deep learning"}, {"st": 27, "ed": 29, "text": "computer vision"}, {"st": 35, "ed": 37, "text": "face detection"}, {"st": 77, "ed": 79, "text": "detection methods"}, {"st": 81, "ed": 83, "text": "transfer learning"}, {"st": 96, "ed": 98, "text": "strong baselines"}, {"st": 170, "ed": 172, "text": "detection task"}, {"st": 175, "ed": 177, "text": "transfer learning"}]
[{"st": 2, "ed": 4, "text": "classification methods"}, {"st": 6, "ed": 8, "text": "sparse representation"}, {"st": 12, "ed": 14, "text": "linear regression"}, {"st": 31, "ed": 33, "text": "method called"}, {"st": 41, "ed": 43, "text": "image recognition"}]
[{"st": 2, "ed": 4, "text": "image registration"}, {"st": 11, "ed": 13, "text": "x ray"}, {"st": 16, "ed": 18, "text": "challenging problem"}, {"st": 21, "ed": 23, "text": "ill posed"}, {"st": 30, "ed": 32, "text": "x ray"}, {"st": 39, "ed": 41, "text": "multi agent"}, {"st": 45, "ed": 47, "text": "attention mechanism"}, {"st": 63, "ed": 66, "text": "fully convolutional network"}, {"st": 72, "ed": 75, "text": "markov decision process"}, {"st": 128, "ed": 130, "text": "x ray"}, {"st": 139, "ed": 141, "text": "x ray"}, {"st": 155, "ed": 157, "text": "attention mechanism"}, {"st": 180, "ed": 182, "text": "significantly reduce"}, {"st": 213, "ed": 215, "text": "proposed method"}, {"st": 223, "ed": 225, "text": "computed tomography"}, {"st": 229, "ed": 233, "text": "signal to noise ratio"}]
[{"st": 7, "ed": 9, "text": "cognitive science"}, {"st": 22, "ed": 24, "text": "collected data"}, {"st": 25, "ed": 27, "text": "human brain"}, {"st": 30, "ed": 32, "text": "activity patterns"}, {"st": 40, "ed": 44, "text": "functional magnetic resonance imaging"}, {"st": 48, "ed": 50, "text": "correlation analysis"}, {"st": 61, "ed": 63, "text": "functional connectivity"}, {"st": 72, "ed": 74, "text": "structural information"}, {"st": 148, "ed": 151, "text": "support vector machine"}, {"st": 192, "ed": 194, "text": "significant improvement"}]
[{"st": 0, "ed": 2, "text": "transfer learning"}, {"st": 16, "ed": 18, "text": "previous tasks"}, {"st": 41, "ed": 43, "text": "transfer learning"}, {"st": 85, "ed": 87, "text": "target dataset"}, {"st": 137, "ed": 139, "text": "fine tuning"}, {"st": 157, "ed": 159, "text": "fine tuning"}, {"st": 160, "ed": 162, "text": "feature extraction"}, {"st": 174, "ed": 176, "text": "transfer learning"}, {"st": 187, "ed": 189, "text": "machine learning"}]
[{"st": 10, "ed": 12, "text": "real life"}, {"st": 40, "ed": 42, "text": "low level"}, {"st": 147, "ed": 150, "text": "fold cross validation"}, {"st": 179, "ed": 181, "text": "fully automated"}, {"st": 181, "ed": 183, "text": "approach outperforms"}, {"st": 206, "ed": 208, "text": "user study"}]
[{"st": 2, "ed": 4, "text": "domain specific"}, {"st": 41, "ed": 43, "text": "low latency"}, {"st": 52, "ed": 54, "text": "video game"}, {"st": 119, "ed": 121, "text": "domain specific"}, {"st": 133, "ed": 135, "text": "domain specific"}, {"st": 176, "ed": 178, "text": "benchmark datasets"}]
[{"st": 3, "ed": 5, "text": "neural architecture"}, {"st": 30, "ed": 33, "text": "constructive solid geometry"}, {"st": 61, "ed": 63, "text": "search space"}, {"st": 76, "ed": 79, "text": "recurrent neural network"}, {"st": 131, "ed": 133, "text": "ground truth"}, {"st": 136, "ed": 138, "text": "policy gradient"}]
[{"st": 9, "ed": 11, "text": "large scale"}, {"st": 58, "ed": 60, "text": "proposed framework"}, {"st": 152, "ed": 154, "text": "large scale"}, {"st": 160, "ed": 162, "text": "pre train"}, {"st": 184, "ed": 186, "text": "pre trained"}, {"st": 190, "ed": 193, "text": "trained from scratch"}, {"st": 222, "ed": 224, "text": "pre train"}]
[{"st": 8, "ed": 10, "text": "large scale"}, {"st": 30, "ed": 32, "text": "temporal dynamics"}, {"st": 109, "ed": 111, "text": "large scale"}, {"st": 121, "ed": 123, "text": "baseline models"}, {"st": 129, "ed": 131, "text": "spatial temporal"}]
[{"st": 18, "ed": 20, "text": "active research"}, {"st": 40, "ed": 42, "text": "existing methods"}, {"st": 72, "ed": 75, "text": "taking advantage of"}, {"st": 85, "ed": 87, "text": "least squares"}, {"st": 122, "ed": 124, "text": "proposed method"}, {"st": 153, "ed": 155, "text": "rgb image"}, {"st": 179, "ed": 181, "text": "proposed method"}, {"st": 190, "ed": 192, "text": "visual information"}]
[{"st": 53, "ed": 55, "text": "internal representation"}, {"st": 56, "ed": 58, "text": "discriminative features"}, {"st": 83, "ed": 86, "text": "convolutional neural networks"}, {"st": 104, "ed": 106, "text": "correlation analysis"}, {"st": 153, "ed": 155, "text": "learned representations"}, {"st": 191, "ed": 193, "text": "learned representations"}]
[{"st": 30, "ed": 34, "text": "visual question answering vqa"}, {"st": 78, "ed": 81, "text": "visual question answering"}, {"st": 111, "ed": 113, "text": "open ended"}, {"st": 138, "ed": 140, "text": "deep neural"}, {"st": 174, "ed": 176, "text": "large scale"}, {"st": 194, "ed": 196, "text": "turing test"}]
[{"st": 42, "ed": 44, "text": "online optimization"}, {"st": 80, "ed": 82, "text": "extensive experiments"}, {"st": 85, "ed": 87, "text": "significantly outperforms"}]
[{"st": 6, "ed": 8, "text": "visual attention"}, {"st": 19, "ed": 21, "text": "computer vision"}, {"st": 47, "ed": 49, "text": "machine learning"}, {"st": 61, "ed": 63, "text": "saliency map"}, {"st": 68, "ed": 70, "text": "computational models"}, {"st": 84, "ed": 86, "text": "computational models"}, {"st": 103, "ed": 105, "text": "saliency prediction"}, {"st": 179, "ed": 181, "text": "sufficiently large"}]
[{"st": 14, "ed": 16, "text": "local feature"}, {"st": 70, "ed": 72, "text": "network design"}, {"st": 94, "ed": 96, "text": "point clouds"}, {"st": 128, "ed": 130, "text": "local feature"}, {"st": 131, "ed": 134, "text": "qualitative and quantitative"}]
[{"st": 4, "ed": 7, "text": "markov random fields"}, {"st": 14, "ed": 16, "text": "belief propagation"}, {"st": 20, "ed": 22, "text": "inference problem"}, {"st": 39, "ed": 43, "text": "markov chain monte carlo"}, {"st": 52, "ed": 54, "text": "proposal distribution"}, {"st": 57, "ed": 59, "text": "carefully designed"}, {"st": 65, "ed": 67, "text": "input data"}, {"st": 78, "ed": 80, "text": "proposal distribution"}, {"st": 84, "ed": 86, "text": "sampling based"}, {"st": 89, "ed": 91, "text": "proposed approach"}, {"st": 97, "ed": 99, "text": "image denoising"}]
[{"st": 16, "ed": 18, "text": "recent studies"}, {"st": 20, "ed": 23, "text": "convolutional neural networks"}, {"st": 26, "ed": 28, "text": "spatial features"}, {"st": 56, "ed": 58, "text": "unlabeled data"}, {"st": 73, "ed": 77, "text": "generative adversarial networks gans"}, {"st": 78, "ed": 81, "text": "probabilistic graphical models"}, {"st": 96, "ed": 98, "text": "land cover"}, {"st": 111, "ed": 113, "text": "unlabeled data"}, {"st": 116, "ed": 119, "text": "conditional random field"}, {"st": 123, "ed": 125, "text": "classification results"}, {"st": 139, "ed": 141, "text": "proposed framework"}, {"st": 143, "ed": 145, "text": "classification accuracy"}]
[{"st": 120, "ed": 122, "text": "machine learning"}, {"st": 136, "ed": 138, "text": "main results"}, {"st": 194, "ed": 196, "text": "temporal data"}, {"st": 216, "ed": 218, "text": "labeled training"}, {"st": 225, "ed": 227, "text": "based approaches"}]
[{"st": 5, "ed": 8, "text": "deep neural networks"}, {"st": 46, "ed": 48, "text": "existing works"}, {"st": 50, "ed": 52, "text": "pre trained"}, {"st": 52, "ed": 54, "text": "neural network"}, {"st": 56, "ed": 58, "text": "hidden neurons"}, {"st": 59, "ed": 61, "text": "pre trained"}, {"st": 89, "ed": 91, "text": "closed form"}, {"st": 106, "ed": 108, "text": "neural networks"}, {"st": 125, "ed": 127, "text": "linear classifiers"}, {"st": 130, "ed": 132, "text": "linear classifier"}, {"st": 160, "ed": 162, "text": "extensive experiments"}, {"st": 164, "ed": 168, "text": "synthetic and real world"}]
[{"st": 0, "ed": 3, "text": "deep neural networks"}, {"st": 11, "ed": 13, "text": "machine learning"}, {"st": 27, "ed": 29, "text": "neural networks"}, {"st": 48, "ed": 50, "text": "input image"}, {"st": 76, "ed": 78, "text": "non trivial"}, {"st": 83, "ed": 85, "text": "gradient based"}, {"st": 90, "ed": 92, "text": "fine tune"}, {"st": 95, "ed": 97, "text": "adversarial examples"}, {"st": 113, "ed": 116, "text": "accuracy and robustness"}, {"st": 133, "ed": 135, "text": "proposed method"}, {"st": 147, "ed": 149, "text": "imagenet dataset"}, {"st": 151, "ed": 153, "text": "box attacks"}, {"st": 164, "ed": 166, "text": "strong baseline"}]
[{"st": 6, "ed": 8, "text": "object detection"}, {"st": 34, "ed": 36, "text": "discriminative power"}, {"st": 39, "ed": 43, "text": "convolutional neural network cnn"}, {"st": 51, "ed": 53, "text": "natural images"}, {"st": 97, "ed": 99, "text": "object detection"}, {"st": 108, "ed": 110, "text": "real world"}, {"st": 169, "ed": 171, "text": "feature maps"}, {"st": 176, "ed": 179, "text": "convolutional neural network"}, {"st": 180, "ed": 182, "text": "improve performance"}]
[{"st": 27, "ed": 29, "text": "remote sensing"}, {"st": 31, "ed": 33, "text": "image processing"}, {"st": 105, "ed": 108, "text": "artificial neural networks"}, {"st": 137, "ed": 140, "text": "mean square error"}, {"st": 168, "ed": 170, "text": "gabor filter"}, {"st": 187, "ed": 189, "text": "baseline approaches"}, {"st": 192, "ed": 194, "text": "promising results"}]
[{"st": 20, "ed": 23, "text": "fully convolutional network"}, {"st": 26, "ed": 28, "text": "conditional generative"}, {"st": 48, "ed": 50, "text": "false positive"}, {"st": 89, "ed": 91, "text": "ct scans"}, {"st": 115, "ed": 117, "text": "quantitative evaluation"}, {"st": 131, "ed": 133, "text": "false positive"}, {"st": 155, "ed": 157, "text": "false positive"}]
[{"st": 0, "ed": 2, "text": "facial expression"}, {"st": 42, "ed": 45, "text": "facial expression recognition"}, {"st": 56, "ed": 59, "text": "simple and efficient"}, {"st": 110, "ed": 112, "text": "proposed algorithm"}]
[{"st": 28, "ed": 30, "text": "great importance"}, {"st": 42, "ed": 45, "text": "traditional chinese medicine"}, {"st": 65, "ed": 67, "text": "non invasive"}, {"st": 72, "ed": 75, "text": "efficient and accurate"}, {"st": 180, "ed": 182, "text": "proposed method"}, {"st": 183, "ed": 185, "text": "conduct experiments"}, {"st": 193, "ed": 197, "text": "deep convolutional neural network"}, {"st": 200, "ed": 202, "text": "digital image"}, {"st": 214, "ed": 216, "text": "proposed method"}, {"st": 220, "ed": 222, "text": "base classifier"}, {"st": 234, "ed": 236, "text": "proposed method"}, {"st": 238, "ed": 240, "text": "classification accuracy"}, {"st": 259, "ed": 261, "text": "proposed method"}, {"st": 265, "ed": 267, "text": "classification accuracy"}]
[{"st": 3, "ed": 5, "text": "multi instance"}, {"st": 9, "ed": 12, "text": "weakly supervised learning"}, {"st": 15, "ed": 17, "text": "training set"}, {"st": 23, "ed": 25, "text": "feature vectors"}, {"st": 39, "ed": 41, "text": "multi instance"}, {"st": 49, "ed": 51, "text": "instance labels"}, {"st": 69, "ed": 71, "text": "multi instance"}, {"st": 73, "ed": 75, "text": "random fields"}, {"st": 83, "ed": 85, "text": "instance labels"}, {"st": 88, "ed": 90, "text": "latent variables"}, {"st": 109, "ed": 111, "text": "instance labels"}, {"st": 126, "ed": 128, "text": "partially observed"}, {"st": 135, "ed": 137, "text": "instance labels"}, {"st": 147, "ed": 149, "text": "weakly supervised"}, {"st": 150, "ed": 152, "text": "behavior analysis"}, {"st": 152, "ed": 154, "text": "facial action"}, {"st": 165, "ed": 167, "text": "proposed framework"}]
[{"st": 48, "ed": 50, "text": "ensemble approach"}, {"st": 65, "ed": 69, "text": "convolutional neural network cnn"}, {"st": 74, "ed": 76, "text": "image sequences"}, {"st": 78, "ed": 80, "text": "spatio temporal"}, {"st": 109, "ed": 111, "text": "ensemble approach"}, {"st": 134, "ed": 137, "text": "real world data"}]
[{"st": 1, "ed": 3, "text": "x ray"}, {"st": 37, "ed": 39, "text": "large scale"}, {"st": 51, "ed": 53, "text": "x ray"}, {"st": 76, "ed": 79, "text": "training and test"}, {"st": 89, "ed": 91, "text": "spatial information"}, {"st": 117, "ed": 119, "text": "image data"}, {"st": 120, "ed": 122, "text": "spatial information"}, {"st": 149, "ed": 151, "text": "x ray"}, {"st": 162, "ed": 165, "text": "training and test"}, {"st": 189, "ed": 191, "text": "future research"}]
[{"st": 1, "ed": 3, "text": "traffic sign"}, {"st": 22, "ed": 24, "text": "object detection"}, {"st": 43, "ed": 45, "text": "object detection"}, {"st": 56, "ed": 58, "text": "human brain"}, {"st": 64, "ed": 66, "text": "attention mechanism"}, {"st": 79, "ed": 81, "text": "neural network"}, {"st": 88, "ed": 90, "text": "fine grained"}, {"st": 97, "ed": 99, "text": "domain specific"}, {"st": 121, "ed": 123, "text": "gaussian distribution"}, {"st": 136, "ed": 138, "text": "result shows"}]
[{"st": 8, "ed": 10, "text": "visual tasks"}, {"st": 106, "ed": 108, "text": "benchmark dataset"}, {"st": 127, "ed": 130, "text": "deep neural network"}, {"st": 136, "ed": 138, "text": "frame prediction"}, {"st": 167, "ed": 169, "text": "frame prediction"}]
[{"st": 38, "ed": 40, "text": "effective temperature"}, {"st": 145, "ed": 147, "text": "evolutionary algorithm"}, {"st": 151, "ed": 153, "text": "evolutionary biology"}]
[{"st": 19, "ed": 22, "text": "high energy physics"}]
[{"st": 48, "ed": 50, "text": "proposed method"}, {"st": 60, "ed": 62, "text": "gibbs sampling"}, {"st": 74, "ed": 76, "text": "local search"}, {"st": 93, "ed": 95, "text": "gibbs sampling"}, {"st": 151, "ed": 153, "text": "global optimization"}]
[{"st": 0, "ed": 2, "text": "neural network"}, {"st": 25, "ed": 28, "text": "principal component analysis"}, {"st": 44, "ed": 46, "text": "cost function"}, {"st": 58, "ed": 60, "text": "biologically plausible"}, {"st": 81, "ed": 83, "text": "biologically plausible"}, {"st": 85, "ed": 87, "text": "subspace learning"}, {"st": 88, "ed": 90, "text": "streaming data"}, {"st": 115, "ed": 117, "text": "multidimensional scaling"}, {"st": 118, "ed": 120, "text": "cost function"}, {"st": 129, "ed": 131, "text": "biologically plausible"}, {"st": 135, "ed": 137, "text": "local learning"}, {"st": 152, "ed": 154, "text": "input data"}]
[{"st": 32, "ed": 35, "text": "nonnegative matrix factorization"}, {"st": 38, "ed": 40, "text": "similarity matrix"}, {"st": 49, "ed": 51, "text": "cost function"}, {"st": 54, "ed": 56, "text": "online algorithm"}, {"st": 62, "ed": 64, "text": "biologically plausible"}, {"st": 66, "ed": 68, "text": "local learning"}, {"st": 118, "ed": 120, "text": "learning rate"}, {"st": 139, "ed": 141, "text": "large scale"}, {"st": 145, "ed": 147, "text": "biologically inspired"}]
[{"st": 10, "ed": 13, "text": "primary visual cortex"}, {"st": 31, "ed": 33, "text": "online algorithm"}, {"st": 35, "ed": 37, "text": "gabor filter"}, {"st": 37, "ed": 39, "text": "receptive fields"}, {"st": 41, "ed": 43, "text": "natural image"}, {"st": 66, "ed": 68, "text": "neural network"}, {"st": 70, "ed": 72, "text": "learning rule"}, {"st": 76, "ed": 78, "text": "weight update"}, {"st": 102, "ed": 105, "text": "sparse dictionary learning"}, {"st": 108, "ed": 110, "text": "cost function"}, {"st": 128, "ed": 130, "text": "neural network"}, {"st": 139, "ed": 141, "text": "biologically plausible"}, {"st": 141, "ed": 143, "text": "local learning"}, {"st": 147, "ed": 149, "text": "natural images"}, {"st": 152, "ed": 154, "text": "gabor filter"}, {"st": 154, "ed": 156, "text": "receptive fields"}, {"st": 171, "ed": 173, "text": "symmetric matrix"}]
[{"st": 27, "ed": 29, "text": "time series"}, {"st": 73, "ed": 77, "text": "recurrent neural networks rnns"}, {"st": 89, "ed": 91, "text": "extensively studied"}, {"st": 97, "ed": 99, "text": "dynamical systems"}, {"st": 106, "ed": 108, "text": "maximum likelihood"}, {"st": 138, "ed": 141, "text": "expectation maximization algorithm"}, {"st": 146, "ed": 148, "text": "latent state"}, {"st": 176, "ed": 179, "text": "anterior cingulate cortex"}, {"st": 185, "ed": 187, "text": "working memory"}, {"st": 207, "ed": 209, "text": "task performance"}, {"st": 248, "ed": 250, "text": "maximum likelihood"}, {"st": 265, "ed": 267, "text": "time series"}]
[{"st": 52, "ed": 54, "text": "pattern matching"}]
[{"st": 0, "ed": 2, "text": "constrained optimization"}, {"st": 9, "ed": 11, "text": "important role"}, {"st": 31, "ed": 33, "text": "objective function"}, {"st": 47, "ed": 49, "text": "optimization algorithm"}, {"st": 69, "ed": 71, "text": "remarkable success"}, {"st": 75, "ed": 77, "text": "benchmark problems"}, {"st": 114, "ed": 116, "text": "achieve high"}, {"st": 140, "ed": 142, "text": "benchmark problems"}, {"st": 147, "ed": 149, "text": "consistently outperforms"}]
[{"st": 11, "ed": 13, "text": "decision making"}, {"st": 17, "ed": 19, "text": "predictive power"}, {"st": 52, "ed": 54, "text": "genetic algorithm"}]
[{"st": 7, "ed": 9, "text": "optimization algorithm"}, {"st": 16, "ed": 18, "text": "local minimum"}, {"st": 21, "ed": 23, "text": "scales linearly"}, {"st": 43, "ed": 45, "text": "local minimum"}, {"st": 66, "ed": 68, "text": "optimization problems"}, {"st": 71, "ed": 73, "text": "neural network"}, {"st": 76, "ed": 78, "text": "convex objectives"}]
[{"st": 11, "ed": 14, "text": "recurrent neural network"}, {"st": 26, "ed": 29, "text": "intensive care unit"}, {"st": 64, "ed": 66, "text": "rnn model"}, {"st": 88, "ed": 90, "text": "icu mortality"}, {"st": 97, "ed": 99, "text": "icu mortality"}, {"st": 101, "ed": 103, "text": "significant improvements"}, {"st": 112, "ed": 114, "text": "machine learning"}]
[{"st": 7, "ed": 9, "text": "probabilistic inference"}]
[{"st": 9, "ed": 12, "text": "artificial neural networks"}, {"st": 53, "ed": 55, "text": "low power"}]
[{"st": 16, "ed": 18, "text": "multi scale"}, {"st": 25, "ed": 27, "text": "caenorhabditis elegans"}, {"st": 66, "ed": 68, "text": "complex dynamics"}]
[{"st": 2, "ed": 4, "text": "genetic programming"}, {"st": 38, "ed": 40, "text": "probabilistic model"}, {"st": 41, "ed": 43, "text": "genetic programming"}, {"st": 81, "ed": 84, "text": "context free grammar"}, {"st": 107, "ed": 109, "text": "linear representation"}, {"st": 116, "ed": 118, "text": "probabilistic method"}, {"st": 127, "ed": 129, "text": "benchmark problems"}]
[{"st": 2, "ed": 4, "text": "neural networks"}, {"st": 23, "ed": 25, "text": "neural networks"}, {"st": 36, "ed": 38, "text": "neural network"}, {"st": 48, "ed": 50, "text": "neural networks"}, {"st": 61, "ed": 63, "text": "biological neural"}, {"st": 145, "ed": 147, "text": "speech recognition"}, {"st": 155, "ed": 157, "text": "temporal dynamics"}]
[{"st": 55, "ed": 57, "text": "temporal dynamics"}, {"st": 64, "ed": 66, "text": "non invasive"}, {"st": 77, "ed": 80, "text": "partial differential equation"}, {"st": 84, "ed": 87, "text": "ordinary differential equation"}, {"st": 133, "ed": 136, "text": "spatial and temporal"}]
[{"st": 2, "ed": 5, "text": "artificial neural networks"}, {"st": 6, "ed": 9, "text": "spiking neural networks"}, {"st": 24, "ed": 26, "text": "spatio temporal"}, {"st": 28, "ed": 30, "text": "pre training"}, {"st": 41, "ed": 43, "text": "supervised training"}, {"st": 107, "ed": 110, "text": "layer by layer"}, {"st": 115, "ed": 117, "text": "timing dependent"}, {"st": 122, "ed": 124, "text": "training phase"}, {"st": 137, "ed": 139, "text": "spatio temporal"}, {"st": 154, "ed": 156, "text": "multi layered"}, {"st": 174, "ed": 176, "text": "mnist dataset"}, {"st": 181, "ed": 183, "text": "object detection"}, {"st": 204, "ed": 206, "text": "spatio temporal"}]
[{"st": 3, "ed": 5, "text": "neural network"}, {"st": 57, "ed": 59, "text": "activation functions"}, {"st": 83, "ed": 85, "text": "neural network"}, {"st": 214, "ed": 216, "text": "neural network"}, {"st": 249, "ed": 251, "text": "neural networks"}]
[{"st": 2, "ed": 5, "text": "learning to rank"}, {"st": 32, "ed": 35, "text": "learning to rank"}, {"st": 99, "ed": 101, "text": "proposed method"}, {"st": 113, "ed": 115, "text": "loss function"}, {"st": 117, "ed": 119, "text": "strong baselines"}, {"st": 130, "ed": 132, "text": "significantly outperforms"}]
[{"st": 11, "ed": 13, "text": "neural network"}, {"st": 32, "ed": 35, "text": "spiking neural networks"}, {"st": 47, "ed": 49, "text": "learning algorithm"}, {"st": 67, "ed": 69, "text": "lateral connections"}, {"st": 131, "ed": 133, "text": "pair wise"}]
[{"st": 38, "ed": 41, "text": "short term memory"}, {"st": 46, "ed": 49, "text": "recurrent neural network"}, {"st": 77, "ed": 79, "text": "empirical evaluation"}, {"st": 81, "ed": 83, "text": "image classification"}, {"st": 93, "ed": 95, "text": "similar performance"}, {"st": 129, "ed": 131, "text": "recurrent networks"}, {"st": 134, "ed": 136, "text": "machine learning"}]
[{"st": 6, "ed": 8, "text": "cost function"}, {"st": 13, "ed": 15, "text": "salient features"}, {"st": 18, "ed": 20, "text": "taylor expansion"}, {"st": 32, "ed": 34, "text": "unified framework"}, {"st": 38, "ed": 40, "text": "deep learning"}, {"st": 56, "ed": 58, "text": "statistical model"}, {"st": 79, "ed": 81, "text": "cost function"}, {"st": 83, "ed": 85, "text": "quadratic form"}, {"st": 97, "ed": 99, "text": "free particle"}, {"st": 113, "ed": 116, "text": "taking advantage of"}, {"st": 126, "ed": 128, "text": "free energy"}]
[{"st": 9, "ed": 12, "text": "convolutional neural networks"}, {"st": 14, "ed": 16, "text": "generative model"}, {"st": 29, "ed": 31, "text": "quantitative finance"}]
[{"st": 8, "ed": 10, "text": "spike timing"}, {"st": 24, "ed": 26, "text": "computational models"}, {"st": 68, "ed": 70, "text": "spike timing"}, {"st": 136, "ed": 138, "text": "linear classification"}]
[{"st": 10, "ed": 12, "text": "computer vision"}, {"st": 34, "ed": 36, "text": "graph matching"}, {"st": 114, "ed": 116, "text": "graph matching"}, {"st": 119, "ed": 121, "text": "discrete optimization"}, {"st": 121, "ed": 124, "text": "problem and propose"}, {"st": 130, "ed": 132, "text": "continuous relaxation"}, {"st": 142, "ed": 144, "text": "existing methods"}, {"st": 147, "ed": 149, "text": "computer vision"}]
[{"st": 21, "ed": 23, "text": "l1 regularized"}, {"st": 23, "ed": 25, "text": "linear regression"}, {"st": 40, "ed": 42, "text": "sparsity inducing"}, {"st": 50, "ed": 52, "text": "feature level"}, {"st": 59, "ed": 61, "text": "group lasso"}, {"st": 113, "ed": 115, "text": "group level"}, {"st": 123, "ed": 125, "text": "group level"}, {"st": 166, "ed": 168, "text": "optimization procedure"}, {"st": 173, "ed": 175, "text": "global optimum"}, {"st": 197, "ed": 199, "text": "theoretical results"}, {"st": 200, "ed": 202, "text": "recovery guarantees"}]
[{"st": 0, "ed": 2, "text": "scene understanding"}, {"st": 8, "ed": 10, "text": "computer vision"}, {"st": 32, "ed": 34, "text": "computer vision"}, {"st": 48, "ed": 50, "text": "vision problems"}, {"st": 84, "ed": 86, "text": "approach called"}, {"st": 190, "ed": 192, "text": "benchmark datasets"}]
[{"st": 19, "ed": 22, "text": "computer aided diagnosis"}, {"st": 40, "ed": 42, "text": "computed tomography"}, {"st": 185, "ed": 187, "text": "visual words"}, {"st": 203, "ed": 205, "text": "classification tasks"}, {"st": 210, "ed": 214, "text": "magnetic resonance imaging mri"}]
[{"st": 57, "ed": 59, "text": "low level"}, {"st": 63, "ed": 65, "text": "optical flow"}, {"st": 124, "ed": 126, "text": "proposed approach"}]
[{"st": 6, "ed": 8, "text": "technique called"}, {"st": 25, "ed": 27, "text": "decision rule"}, {"st": 41, "ed": 43, "text": "objective function"}, {"st": 45, "ed": 47, "text": "penalty term"}, {"st": 50, "ed": 52, "text": "decision rule"}, {"st": 71, "ed": 73, "text": "support vectors"}]
[{"st": 96, "ed": 98, "text": "motion segmentation"}, {"st": 108, "ed": 110, "text": "extensive experimental"}, {"st": 117, "ed": 120, "text": "accuracy and speed"}, {"st": 138, "ed": 140, "text": "mnist handwritten"}]
[{"st": 13, "ed": 15, "text": "hidden markov"}, {"st": 21, "ed": 23, "text": "inverse problems"}, {"st": 28, "ed": 30, "text": "compressed sensing"}, {"st": 71, "ed": 73, "text": "iterative reconstruction"}, {"st": 84, "ed": 86, "text": "group sparsity"}, {"st": 112, "ed": 114, "text": "compressed sensing"}, {"st": 118, "ed": 120, "text": "computationally efficient"}]
[{"st": 28, "ed": 30, "text": "multi stage"}, {"st": 68, "ed": 70, "text": "decision rules"}, {"st": 80, "ed": 83, "text": "empirical risk minimization"}, {"st": 87, "ed": 89, "text": "multi stage"}, {"st": 171, "ed": 173, "text": "iterative algorithm"}]
[{"st": 6, "ed": 9, "text": "synthetic aperture radar"}, {"st": 19, "ed": 21, "text": "b spline"}, {"st": 44, "ed": 46, "text": "b spline"}, {"st": 55, "ed": 57, "text": "proposed algorithm"}]
[{"st": 37, "ed": 39, "text": "optimization framework"}, {"st": 56, "ed": 58, "text": "natural images"}, {"st": 69, "ed": 71, "text": "noise level"}, {"st": 83, "ed": 85, "text": "least squares"}, {"st": 109, "ed": 111, "text": "numerical experiments"}, {"st": 146, "ed": 149, "text": "0 p 1"}, {"st": 176, "ed": 178, "text": "signal processing"}, {"st": 189, "ed": 191, "text": "image denoising"}, {"st": 204, "ed": 206, "text": "image denoising"}, {"st": 220, "ed": 222, "text": "linear convergence"}]
[{"st": 0, "ed": 2, "text": "sparse representations"}, {"st": 16, "ed": 18, "text": "machine learning"}, {"st": 23, "ed": 25, "text": "training data"}, {"st": 45, "ed": 48, "text": "dictionary learning algorithms"}, {"st": 77, "ed": 79, "text": "sparse representations"}, {"st": 80, "ed": 82, "text": "large scale"}, {"st": 88, "ed": 90, "text": "learning algorithm"}, {"st": 101, "ed": 103, "text": "subspace clustering"}, {"st": 140, "ed": 142, "text": "ensemble approach"}, {"st": 162, "ed": 164, "text": "low complexity"}, {"st": 175, "ed": 177, "text": "proposed algorithm"}, {"st": 192, "ed": 194, "text": "subspace learning"}]
[{"st": 38, "ed": 40, "text": "gaussian noise"}, {"st": 87, "ed": 89, "text": "image representation"}, {"st": 93, "ed": 96, "text": "gaussian mixture model"}, {"st": 110, "ed": 112, "text": "sparse representation"}, {"st": 115, "ed": 117, "text": "image patches"}]
[{"st": 9, "ed": 13, "text": "robust principal component analysis"}, {"st": 15, "ed": 17, "text": "low dimensional"}, {"st": 33, "ed": 35, "text": "low dimensional"}, {"st": 64, "ed": 66, "text": "robust subspace"}, {"st": 68, "ed": 70, "text": "conventional approaches"}, {"st": 82, "ed": 84, "text": "computational complexity"}, {"st": 116, "ed": 118, "text": "sample complexity"}, {"st": 137, "ed": 139, "text": "numerical simulations"}]
[{"st": 20, "ed": 22, "text": "computer vision"}, {"st": 22, "ed": 24, "text": "signal processing"}, {"st": 61, "ed": 63, "text": "feature learning"}, {"st": 78, "ed": 80, "text": "generation process"}, {"st": 92, "ed": 94, "text": "hand crafted"}, {"st": 109, "ed": 113, "text": "deep convolutional neural networks"}, {"st": 128, "ed": 130, "text": "feature representation"}]
[{"st": 43, "ed": 45, "text": "search space"}, {"st": 99, "ed": 101, "text": "objective function"}, {"st": 110, "ed": 112, "text": "search space"}, {"st": 170, "ed": 172, "text": "normal distribution"}, {"st": 179, "ed": 181, "text": "acquisition function"}]
[{"st": 0, "ed": 3, "text": "cryo electron microscopy"}, {"st": 15, "ed": 17, "text": "structural information"}, {"st": 107, "ed": 109, "text": "machine learning"}, {"st": 194, "ed": 196, "text": "entire dataset"}, {"st": 210, "ed": 212, "text": "large datasets"}, {"st": 218, "ed": 221, "text": "orders of magnitude"}]
[{"st": 73, "ed": 75, "text": "invariant feature"}, {"st": 77, "ed": 79, "text": "machine learning"}]
[{"st": 28, "ed": 30, "text": "noisy data"}, {"st": 36, "ed": 38, "text": "training samples"}, {"st": 40, "ed": 42, "text": "intra class"}, {"st": 56, "ed": 58, "text": "local models"}, {"st": 129, "ed": 131, "text": "based methods"}, {"st": 134, "ed": 136, "text": "convex hull"}, {"st": 156, "ed": 158, "text": "maximum margin"}, {"st": 226, "ed": 228, "text": "proposed method"}]
[{"st": 0, "ed": 3, "text": "person re identification"}, {"st": 38, "ed": 40, "text": "based methods"}, {"st": 53, "ed": 56, "text": "image and video"}, {"st": 68, "ed": 70, "text": "recent approaches"}, {"st": 113, "ed": 115, "text": "recently introduced"}, {"st": 176, "ed": 179, "text": "person re identification"}, {"st": 183, "ed": 185, "text": "proposed approach"}, {"st": 197, "ed": 199, "text": "least squares"}]
[{"st": 0, "ed": 2, "text": "recent advances"}, {"st": 8, "ed": 10, "text": "positive definite"}, {"st": 28, "ed": 31, "text": "taking into account"}, {"st": 48, "ed": 52, "text": "reproducing kernel hilbert spaces"}, {"st": 66, "ed": 68, "text": "learning algorithms"}, {"st": 92, "ed": 94, "text": "non trivial"}, {"st": 126, "ed": 128, "text": "learning algorithms"}, {"st": 146, "ed": 148, "text": "random projection"}, {"st": 153, "ed": 155, "text": "random projection"}, {"st": 170, "ed": 172, "text": "face recognition"}, {"st": 172, "ed": 175, "text": "person re identification"}, {"st": 176, "ed": 178, "text": "texture classification"}, {"st": 181, "ed": 183, "text": "proposed approach"}, {"st": 185, "ed": 187, "text": "recent methods"}, {"st": 190, "ed": 192, "text": "sparse coding"}]
[{"st": 10, "ed": 12, "text": "image processing"}, {"st": 17, "ed": 19, "text": "sparse representation"}, {"st": 45, "ed": 47, "text": "based methods"}, {"st": 50, "ed": 52, "text": "total variation"}, {"st": 77, "ed": 79, "text": "optical flow"}]
[{"st": 5, "ed": 7, "text": "extreme weather"}, {"st": 9, "ed": 11, "text": "large scale"}, {"st": 18, "ed": 20, "text": "risk management"}, {"st": 38, "ed": 40, "text": "fully supervised"}, {"st": 40, "ed": 44, "text": "convolutional neural networks cnns"}, {"st": 54, "ed": 56, "text": "extreme weather"}, {"st": 61, "ed": 63, "text": "labeled data"}, {"st": 80, "ed": 82, "text": "tropical cyclones"}, {"st": 90, "ed": 92, "text": "labeled data"}, {"st": 125, "ed": 127, "text": "machine learning"}, {"st": 136, "ed": 138, "text": "semi supervised"}, {"st": 138, "ed": 140, "text": "bounding box"}, {"st": 154, "ed": 156, "text": "temporal information"}, {"st": 157, "ed": 159, "text": "unlabeled data"}, {"st": 164, "ed": 166, "text": "extreme weather"}, {"st": 191, "ed": 193, "text": "machine learning"}, {"st": 222, "ed": 226, "text": "available at https github.com"}]
[{"st": 1, "ed": 4, "text": "convolutional neural networks"}, {"st": 10, "ed": 12, "text": "object recognition"}, {"st": 27, "ed": 29, "text": "cnn based"}, {"st": 62, "ed": 64, "text": "efficient learning"}, {"st": 89, "ed": 91, "text": "local features"}, {"st": 93, "ed": 95, "text": "decision making"}, {"st": 143, "ed": 145, "text": "decision support"}]
[{"st": 31, "ed": 34, "text": "computer aided diagnosis"}, {"st": 49, "ed": 51, "text": "ct images"}, {"st": 76, "ed": 81, "text": "deep convolutional neural network cnn"}, {"st": 130, "ed": 132, "text": "fine tuned"}, {"st": 160, "ed": 162, "text": "proposed approach"}, {"st": 183, "ed": 185, "text": "transfer learning"}, {"st": 190, "ed": 192, "text": "image analysis"}]
[{"st": 3, "ed": 5, "text": "convolutional layers"}, {"st": 10, "ed": 12, "text": "neural networks"}, {"st": 98, "ed": 100, "text": "neural network"}, {"st": 104, "ed": 106, "text": "convolutional layers"}, {"st": 134, "ed": 136, "text": "convolutional layers"}]
[{"st": 0, "ed": 2, "text": "topic models"}, {"st": 95, "ed": 98, "text": "latent dirichlet allocation"}, {"st": 104, "ed": 106, "text": "parameter estimation"}, {"st": 148, "ed": 150, "text": "topic modeling"}]
[{"st": 3, "ed": 5, "text": "gaussian distributions"}, {"st": 9, "ed": 11, "text": "k means"}, {"st": 25, "ed": 27, "text": "computer vision"}, {"st": 44, "ed": 46, "text": "gaussian distribution"}, {"st": 58, "ed": 60, "text": "weighted sum"}, {"st": 113, "ed": 115, "text": "mixture distribution"}, {"st": 147, "ed": 149, "text": "method outperforms"}, {"st": 150, "ed": 152, "text": "gaussian mixture"}]
[{"st": 0, "ed": 2, "text": "sparse coding"}, {"st": 18, "ed": 20, "text": "sparse coding"}, {"st": 35, "ed": 37, "text": "sparse coding"}, {"st": 38, "ed": 40, "text": "cross domain"}, {"st": 48, "ed": 50, "text": "source domain"}, {"st": 61, "ed": 65, "text": "maximum mean discrepancy mmd"}, {"st": 69, "ed": 71, "text": "cross domain"}, {"st": 74, "ed": 76, "text": "sparse codes"}, {"st": 80, "ed": 82, "text": "sparse codes"}, {"st": 84, "ed": 86, "text": "class labels"}, {"st": 99, "ed": 101, "text": "experiment results"}, {"st": 104, "ed": 106, "text": "cross domain"}, {"st": 106, "ed": 108, "text": "sparse coding"}, {"st": 111, "ed": 113, "text": "challenging tasks"}, {"st": 113, "ed": 115, "text": "image classification"}, {"st": 118, "ed": 120, "text": "oil painting"}, {"st": 131, "ed": 133, "text": "proposed method"}, {"st": 135, "ed": 137, "text": "cross domain"}]
[{"st": 0, "ed": 4, "text": "deep convolutional neural networks"}, {"st": 5, "ed": 7, "text": "recently achieved"}, {"st": 16, "ed": 18, "text": "image recognition"}, {"st": 22, "ed": 24, "text": "large scale"}, {"st": 24, "ed": 26, "text": "visual recognition"}, {"st": 45, "ed": 47, "text": "bounding box"}, {"st": 53, "ed": 55, "text": "object category"}, {"st": 99, "ed": 101, "text": "neural network"}, {"st": 111, "ed": 113, "text": "bounding boxes"}, {"st": 162, "ed": 164, "text": "recognition performance"}, {"st": 184, "ed": 186, "text": "neural network"}]
[{"st": 2, "ed": 5, "text": "zero shot learning"}, {"st": 60, "ed": 62, "text": "zero shot"}, {"st": 79, "ed": 81, "text": "random forest"}, {"st": 84, "ed": 86, "text": "zero shot"}, {"st": 110, "ed": 112, "text": "discriminative models"}, {"st": 123, "ed": 125, "text": "few shot"}, {"st": 145, "ed": 147, "text": "training examples"}]
[{"st": 3, "ed": 5, "text": "learning framework"}, {"st": 13, "ed": 15, "text": "computer vision"}, {"st": 21, "ed": 23, "text": "computer vision"}, {"st": 53, "ed": 55, "text": "additional information"}, {"st": 57, "ed": 59, "text": "image data"}, {"st": 69, "ed": 71, "text": "computer vision"}, {"st": 73, "ed": 75, "text": "bounding boxes"}, {"st": 97, "ed": 99, "text": "maximum margin"}]
[{"st": 0, "ed": 2, "text": "object recognition"}, {"st": 8, "ed": 11, "text": "labeled training data"}, {"st": 23, "ed": 25, "text": "object recognition"}, {"st": 27, "ed": 29, "text": "training samples"}, {"st": 48, "ed": 51, "text": "zero shot learning"}, {"st": 55, "ed": 57, "text": "topic model"}, {"st": 62, "ed": 64, "text": "proposed method"}, {"st": 77, "ed": 79, "text": "achieve comparable"}, {"st": 92, "ed": 94, "text": "cifar 100"}, {"st": 104, "ed": 106, "text": "unseen classes"}]
[{"st": 4, "ed": 6, "text": "detection method"}, {"st": 35, "ed": 37, "text": "successfully applied"}, {"st": 61, "ed": 63, "text": "existing methods"}, {"st": 116, "ed": 118, "text": "proposed approach"}, {"st": 119, "ed": 121, "text": "computational costs"}, {"st": 159, "ed": 162, "text": "qualitative and quantitative"}]
[{"st": 0, "ed": 2, "text": "image understanding"}, {"st": 9, "ed": 11, "text": "computer vision"}, {"st": 15, "ed": 17, "text": "real world"}, {"st": 20, "ed": 22, "text": "image understanding"}, {"st": 26, "ed": 29, "text": "bag of words"}, {"st": 38, "ed": 40, "text": "random forest"}, {"st": 97, "ed": 99, "text": "class labels"}, {"st": 158, "ed": 160, "text": "proposed method"}, {"st": 161, "ed": 163, "text": "image understanding"}]
[{"st": 72, "ed": 75, "text": "low level features"}, {"st": 108, "ed": 110, "text": "low level"}, {"st": 127, "ed": 129, "text": "proposed approach"}]
[{"st": 8, "ed": 12, "text": "functional magnetic resonance imaging"}, {"st": 45, "ed": 47, "text": "extremely high"}, {"st": 52, "ed": 54, "text": "training samples"}, {"st": 66, "ed": 68, "text": "stability selection"}, {"st": 80, "ed": 82, "text": "finite sample"}, {"st": 108, "ed": 110, "text": "structural information"}, {"st": 112, "ed": 114, "text": "discriminative features"}, {"st": 118, "ed": 120, "text": "false negative"}, {"st": 140, "ed": 142, "text": "stability selection"}, {"st": 152, "ed": 154, "text": "numerical experiments"}, {"st": 164, "ed": 166, "text": "false negatives"}, {"st": 172, "ed": 174, "text": "false positives"}]
[{"st": 2, "ed": 4, "text": "scene recognition"}, {"st": 8, "ed": 10, "text": "image regions"}, {"st": 148, "ed": 150, "text": "generative model"}, {"st": 171, "ed": 173, "text": "real estate"}, {"st": 220, "ed": 222, "text": "training set"}, {"st": 258, "ed": 260, "text": "input images"}]
[{"st": 13, "ed": 15, "text": "important role"}, {"st": 30, "ed": 32, "text": "multiple objects"}, {"st": 70, "ed": 72, "text": "missing data"}, {"st": 87, "ed": 89, "text": "noise free"}, {"st": 136, "ed": 138, "text": "missing data"}]
[{"st": 12, "ed": 14, "text": "object localization"}, {"st": 28, "ed": 31, "text": "discrete fourier transform"}, {"st": 82, "ed": 84, "text": "optimization criteria"}, {"st": 134, "ed": 136, "text": "significantly improved"}]
[{"st": 6, "ed": 8, "text": "finite mixture"}, {"st": 34, "ed": 36, "text": "mixture model"}, {"st": 42, "ed": 44, "text": "maximum entropy"}]
[]
[{"st": 8, "ed": 10, "text": "nearest neighbors"}, {"st": 26, "ed": 28, "text": "extremely high"}, {"st": 33, "ed": 35, "text": "visual tasks"}, {"st": 41, "ed": 43, "text": "sparse linear"}, {"st": 54, "ed": 56, "text": "proposed algorithm"}, {"st": 79, "ed": 82, "text": "o n 2"}, {"st": 84, "ed": 86, "text": "proposed method"}, {"st": 154, "ed": 156, "text": "building blocks"}, {"st": 171, "ed": 174, "text": "simple and efficient"}]
[{"st": 1, "ed": 3, "text": "magnetic resonance"}, {"st": 16, "ed": 18, "text": "white matter"}, {"st": 33, "ed": 35, "text": "machine learning"}, {"st": 36, "ed": 38, "text": "pattern recognition"}, {"st": 47, "ed": 49, "text": "representation space"}, {"st": 56, "ed": 58, "text": "learning algorithms"}, {"st": 191, "ed": 194, "text": "fast and accurate"}]
[{"st": 3, "ed": 5, "text": "theoretical analysis"}, {"st": 6, "ed": 8, "text": "empirical evaluations"}, {"st": 15, "ed": 17, "text": "computational cost"}, {"st": 57, "ed": 59, "text": "floating point"}, {"st": 71, "ed": 73, "text": "optimization algorithms"}, {"st": 93, "ed": 95, "text": "learning algorithm"}, {"st": 136, "ed": 138, "text": "low power"}]
[{"st": 6, "ed": 8, "text": "generative model"}, {"st": 9, "ed": 11, "text": "image segmentation"}, {"st": 27, "ed": 29, "text": "mid level"}, {"st": 29, "ed": 31, "text": "image representation"}, {"st": 51, "ed": 53, "text": "image segmentation"}, {"st": 149, "ed": 151, "text": "quantitative results"}]
[{"st": 17, "ed": 19, "text": "open research"}, {"st": 54, "ed": 56, "text": "challenging task"}, {"st": 99, "ed": 101, "text": "deep learning"}, {"st": 104, "ed": 106, "text": "feature representations"}, {"st": 135, "ed": 138, "text": "directions for future"}]
[{"st": 74, "ed": 77, "text": "dynamic time warping"}]
[{"st": 30, "ed": 32, "text": "mean shift"}, {"st": 69, "ed": 71, "text": "mean shift"}, {"st": 99, "ed": 101, "text": "weighted average"}, {"st": 106, "ed": 108, "text": "mean shift"}]
[{"st": 0, "ed": 2, "text": "object tracking"}, {"st": 13, "ed": 15, "text": "remote sensing"}, {"st": 17, "ed": 19, "text": "computer vision"}, {"st": 30, "ed": 32, "text": "computer vision"}, {"st": 52, "ed": 54, "text": "variational bayesian"}, {"st": 79, "ed": 81, "text": "variational bayesian"}, {"st": 98, "ed": 100, "text": "expectation maximization"}, {"st": 103, "ed": 105, "text": "closed form"}, {"st": 108, "ed": 110, "text": "posterior distributions"}, {"st": 112, "ed": 114, "text": "latent variables"}, {"st": 172, "ed": 174, "text": "competitive results"}, {"st": 182, "ed": 184, "text": "object tracking"}]
[{"st": 9, "ed": 12, "text": "zero shot learning"}, {"st": 16, "ed": 19, "text": "source and target"}, {"st": 30, "ed": 32, "text": "accurately predict"}, {"st": 33, "ed": 35, "text": "class label"}, {"st": 40, "ed": 42, "text": "instance based"}, {"st": 44, "ed": 46, "text": "source domain"}, {"st": 46, "ed": 48, "text": "side information"}, {"st": 124, "ed": 126, "text": "max margin"}, {"st": 148, "ed": 150, "text": "significant improvement"}, {"st": 156, "ed": 158, "text": "benchmark datasets"}, {"st": 159, "ed": 161, "text": "zero shot"}]
[{"st": 63, "ed": 65, "text": "random variables"}, {"st": 74, "ed": 76, "text": "cross view"}, {"st": 141, "ed": 143, "text": "visual recognition"}, {"st": 148, "ed": 151, "text": "person re identification"}, {"st": 156, "ed": 158, "text": "benchmark datasets"}, {"st": 161, "ed": 163, "text": "significantly outperform"}]
[{"st": 0, "ed": 2, "text": "topic models"}, {"st": 77, "ed": 80, "text": "latent dirichlet allocation"}, {"st": 85, "ed": 87, "text": "parameter estimation"}, {"st": 92, "ed": 94, "text": "natural image"}, {"st": 98, "ed": 100, "text": "image dataset"}, {"st": 115, "ed": 117, "text": "existing methods"}]
[{"st": 82, "ed": 84, "text": "processing steps"}, {"st": 107, "ed": 109, "text": "random projections"}, {"st": 189, "ed": 191, "text": "clustering schemes"}, {"st": 217, "ed": 219, "text": "proposed approach"}, {"st": 225, "ed": 227, "text": "large scale"}, {"st": 233, "ed": 235, "text": "computational efficiency"}]
[{"st": 8, "ed": 10, "text": "sparse representations"}, {"st": 23, "ed": 25, "text": "predictive model"}, {"st": 97, "ed": 99, "text": "compressive sensing"}]
[{"st": 0, "ed": 2, "text": "transfer learning"}, {"st": 7, "ed": 9, "text": "machine learning"}, {"st": 20, "ed": 22, "text": "training data"}, {"st": 34, "ed": 37, "text": "deep neural networks"}, {"st": 42, "ed": 44, "text": "training data"}, {"st": 74, "ed": 76, "text": "training data"}, {"st": 86, "ed": 88, "text": "transfer learning"}, {"st": 101, "ed": 103, "text": "transfer learning"}]
[{"st": 2, "ed": 4, "text": "image denoising"}, {"st": 45, "ed": 47, "text": "image denoising"}, {"st": 75, "ed": 77, "text": "gaussian distribution"}, {"st": 90, "ed": 92, "text": "image denoising"}, {"st": 151, "ed": 153, "text": "variational inference"}, {"st": 178, "ed": 180, "text": "previous approaches"}, {"st": 192, "ed": 194, "text": "proposed algorithm"}, {"st": 198, "ed": 200, "text": "image denoising"}]
[{"st": 3, "ed": 5, "text": "malignant melanoma"}, {"st": 57, "ed": 59, "text": "fully automatic"}, {"st": 127, "ed": 129, "text": "image processing"}, {"st": 130, "ed": 132, "text": "machine learning"}, {"st": 141, "ed": 143, "text": "malignant melanoma"}, {"st": 244, "ed": 246, "text": "machine learning"}]
[{"st": 8, "ed": 10, "text": "dimensionality reduction"}, {"st": 25, "ed": 28, "text": "principal component analysis"}, {"st": 42, "ed": 44, "text": "multivariate regression"}, {"st": 74, "ed": 76, "text": "dimensionality reduction"}, {"st": 109, "ed": 111, "text": "recently proposed"}, {"st": 113, "ed": 116, "text": "principal components analysis"}, {"st": 135, "ed": 137, "text": "remote sensing"}, {"st": 163, "ed": 165, "text": "image patches"}, {"st": 179, "ed": 181, "text": "expressive power"}, {"st": 196, "ed": 198, "text": "land cover"}, {"st": 208, "ed": 210, "text": "recently proposed"}, {"st": 214, "ed": 216, "text": "neural networks"}]
[{"st": 4, "ed": 6, "text": "image denoising"}, {"st": 62, "ed": 64, "text": "natural image"}, {"st": 70, "ed": 73, "text": "support vector regression"}, {"st": 125, "ed": 127, "text": "mutual information"}, {"st": 138, "ed": 141, "text": "kullback leibler divergence"}, {"st": 193, "ed": 195, "text": "noise levels"}, {"st": 202, "ed": 204, "text": "proposed method"}, {"st": 254, "ed": 256, "text": "machine learning"}]
[{"st": 119, "ed": 121, "text": "existing methods"}, {"st": 154, "ed": 157, "text": "periodic boundary conditions"}, {"st": 181, "ed": 187, "text": "alternating direction method of multipliers admm"}, {"st": 219, "ed": 221, "text": "primal dual"}]
[{"st": 17, "ed": 19, "text": "important role"}, {"st": 21, "ed": 23, "text": "image analysis"}, {"st": 66, "ed": 68, "text": "contextual information"}, {"st": 77, "ed": 79, "text": "prior knowledge"}, {"st": 147, "ed": 150, "text": "gaussian mixture model"}, {"st": 169, "ed": 171, "text": "mixture components"}, {"st": 241, "ed": 244, "text": "accuracy and robustness"}]
[{"st": 3, "ed": 5, "text": "real world"}, {"st": 16, "ed": 18, "text": "image classification"}, {"st": 32, "ed": 34, "text": "training data"}, {"st": 50, "ed": 52, "text": "discriminative models"}, {"st": 54, "ed": 56, "text": "image classification"}, {"st": 62, "ed": 65, "text": "probabilistic graphical models"}, {"st": 66, "ed": 68, "text": "sparse signal"}, {"st": 69, "ed": 72, "text": "probabilistic graphical models"}, {"st": 94, "ed": 96, "text": "probability distributions"}, {"st": 130, "ed": 132, "text": "discriminative learning"}, {"st": 183, "ed": 185, "text": "graphical model"}]
[{"st": 6, "ed": 8, "text": "image segmentation"}, {"st": 94, "ed": 96, "text": "kernel based"}, {"st": 128, "ed": 130, "text": "existing algorithms"}, {"st": 155, "ed": 158, "text": "qualitative and quantitative"}, {"st": 188, "ed": 190, "text": "magnetic resonance"}, {"st": 201, "ed": 203, "text": "proposed algorithm"}]
[{"st": 15, "ed": 17, "text": "object categories"}, {"st": 37, "ed": 39, "text": "causal discovery"}, {"st": 53, "ed": 55, "text": "causal direction"}, {"st": 58, "ed": 60, "text": "random variables"}, {"st": 70, "ed": 72, "text": "causal direction"}, {"st": 91, "ed": 93, "text": "experiments demonstrate"}]
[{"st": 0, "ed": 2, "text": "driving styles"}, {"st": 16, "ed": 18, "text": "driving styles"}, {"st": 27, "ed": 29, "text": "pattern recognition"}, {"st": 38, "ed": 40, "text": "driving styles"}, {"st": 44, "ed": 46, "text": "probability density"}, {"st": 68, "ed": 70, "text": "kernel density"}, {"st": 89, "ed": 91, "text": "driving styles"}, {"st": 97, "ed": 99, "text": "posterior probability"}, {"st": 103, "ed": 105, "text": "feature vector"}, {"st": 150, "ed": 152, "text": "feature vector"}, {"st": 152, "ed": 154, "text": "driving styles"}, {"st": 173, "ed": 175, "text": "pattern recognition"}, {"st": 185, "ed": 187, "text": "fuzzy logic"}, {"st": 188, "ed": 190, "text": "pattern recognition"}, {"st": 192, "ed": 194, "text": "experiment results"}, {"st": 199, "ed": 201, "text": "pattern recognition"}, {"st": 203, "ed": 205, "text": "driving styles"}, {"st": 207, "ed": 210, "text": "kernel density estimation"}, {"st": 217, "ed": 219, "text": "fuzzy logic"}]
[{"st": 2, "ed": 4, "text": "large scale"}, {"st": 9, "ed": 13, "text": "visual question answering vqa"}, {"st": 21, "ed": 23, "text": "answer questions"}, {"st": 62, "ed": 64, "text": "attention maps"}, {"st": 70, "ed": 72, "text": "vqa models"}, {"st": 92, "ed": 94, "text": "attention models"}]
[{"st": 1, "ed": 3, "text": "predictive performance"}, {"st": 17, "ed": 20, "text": "computer aided diagnosis"}, {"st": 30, "ed": 32, "text": "specifically designed"}, {"st": 72, "ed": 74, "text": "classification results"}]
[{"st": 3, "ed": 5, "text": "image analysis"}, {"st": 8, "ed": 10, "text": "gained increasing"}, {"st": 27, "ed": 29, "text": "image representation"}, {"st": 61, "ed": 63, "text": "spatial resolution"}, {"st": 66, "ed": 68, "text": "spatial resolution"}, {"st": 78, "ed": 80, "text": "spatial resolution"}, {"st": 84, "ed": 86, "text": "spatial resolution"}, {"st": 97, "ed": 99, "text": "context information"}, {"st": 123, "ed": 125, "text": "machine learning"}, {"st": 157, "ed": 159, "text": "classification task"}, {"st": 162, "ed": 164, "text": "proposed approach"}, {"st": 168, "ed": 170, "text": "classification accuracy"}, {"st": 171, "ed": 173, "text": "conventional approaches"}]
[{"st": 12, "ed": 14, "text": "feature extractors"}, {"st": 55, "ed": 57, "text": "previous research"}, {"st": 77, "ed": 82, "text": "deep convolutional neural networks cnns"}, {"st": 84, "ed": 86, "text": "improve performance"}, {"st": 137, "ed": 139, "text": "error rate"}, {"st": 156, "ed": 158, "text": "multiple classifiers"}, {"st": 167, "ed": 169, "text": "feature space"}, {"st": 184, "ed": 186, "text": "analysis shows"}]
[{"st": 18, "ed": 20, "text": "gaussian process"}, {"st": 21, "ed": 23, "text": "auto encoder"}, {"st": 38, "ed": 40, "text": "latent space"}, {"st": 61, "ed": 63, "text": "latent representations"}, {"st": 101, "ed": 103, "text": "benchmark datasets"}, {"st": 104, "ed": 106, "text": "machine learning"}, {"st": 125, "ed": 127, "text": "facial action"}, {"st": 129, "ed": 131, "text": "experiments demonstrate"}, {"st": 135, "ed": 137, "text": "proposed approach"}]
[{"st": 0, "ed": 2, "text": "image denoising"}, {"st": 5, "ed": 7, "text": "pre processing"}, {"st": 30, "ed": 32, "text": "conventional methods"}, {"st": 32, "ed": 34, "text": "deep learning"}, {"st": 50, "ed": 52, "text": "training sample"}, {"st": 64, "ed": 66, "text": "small sample"}, {"st": 67, "ed": 69, "text": "denoising autoencoders"}, {"st": 71, "ed": 73, "text": "convolutional layers"}, {"st": 89, "ed": 91, "text": "sample size"}]
[{"st": 4, "ed": 7, "text": "accuracy and speed"}, {"st": 9, "ed": 12, "text": "image super resolution"}, {"st": 16, "ed": 19, "text": "convolutional neural networks"}, {"st": 37, "ed": 39, "text": "at large"}, {"st": 46, "ed": 48, "text": "super resolution"}, {"st": 115, "ed": 119, "text": "generative adversarial network gan"}, {"st": 120, "ed": 123, "text": "image super resolution"}, {"st": 136, "ed": 138, "text": "photo realistic"}, {"st": 138, "ed": 140, "text": "natural images"}, {"st": 151, "ed": 153, "text": "loss function"}, {"st": 157, "ed": 159, "text": "adversarial loss"}, {"st": 164, "ed": 166, "text": "adversarial loss"}, {"st": 171, "ed": 173, "text": "natural image"}, {"st": 190, "ed": 192, "text": "photo realistic"}, {"st": 211, "ed": 213, "text": "deep residual"}, {"st": 218, "ed": 220, "text": "photo realistic"}, {"st": 237, "ed": 239, "text": "significant gains"}]
[{"st": 5, "ed": 8, "text": "deep neural networks"}, {"st": 9, "ed": 12, "text": "achieved great success"}, {"st": 17, "ed": 20, "text": "accuracy and computational"}, {"st": 30, "ed": 32, "text": "low resolution"}, {"st": 33, "ed": 35, "text": "input image"}, {"st": 56, "ed": 58, "text": "super resolution"}, {"st": 83, "ed": 87, "text": "convolutional neural network cnn"}, {"st": 111, "ed": 113, "text": "feature maps"}, {"st": 141, "ed": 143, "text": "feature maps"}, {"st": 170, "ed": 172, "text": "feature map"}, {"st": 176, "ed": 178, "text": "computational complexity"}, {"st": 186, "ed": 188, "text": "proposed approach"}, {"st": 189, "ed": 192, "text": "images and videos"}, {"st": 193, "ed": 196, "text": "publicly available datasets"}]
[{"st": 3, "ed": 5, "text": "anomaly detection"}, {"st": 23, "ed": 25, "text": "anomaly detection"}, {"st": 57, "ed": 59, "text": "training data"}, {"st": 110, "ed": 112, "text": "anomaly detection"}]
[{"st": 0, "ed": 2, "text": "computed tomography"}, {"st": 32, "ed": 34, "text": "time consuming"}, {"st": 53, "ed": 56, "text": "deep artificial neural"}, {"st": 56, "ed": 58, "text": "network architecture"}, {"st": 61, "ed": 63, "text": "fully automated"}, {"st": 100, "ed": 103, "text": "convolutional and recurrent"}, {"st": 109, "ed": 111, "text": "image representations"}, {"st": 143, "ed": 145, "text": "lidc idri"}, {"st": 151, "ed": 153, "text": "ct scans"}, {"st": 165, "ed": 167, "text": "false positives"}, {"st": 173, "ed": 175, "text": "multi channel"}, {"st": 175, "ed": 178, "text": "convolutional neural network"}, {"st": 190, "ed": 192, "text": "provide evidence"}]
[{"st": 8, "ed": 10, "text": "probabilistic models"}, {"st": 20, "ed": 23, "text": "convolutional neural networks"}, {"st": 35, "ed": 37, "text": "exponential family"}, {"st": 70, "ed": 72, "text": "generator network"}, {"st": 92, "ed": 94, "text": "latent factors"}, {"st": 99, "ed": 101, "text": "maximum likelihood"}, {"st": 118, "ed": 120, "text": "back propagation"}]
[{"st": 9, "ed": 11, "text": "deep learning"}, {"st": 17, "ed": 19, "text": "fiber optic"}, {"st": 28, "ed": 30, "text": "detection algorithms"}, {"st": 32, "ed": 34, "text": "non trivial"}, {"st": 82, "ed": 84, "text": "event detection"}, {"st": 95, "ed": 98, "text": "deep convolutional networks"}, {"st": 113, "ed": 116, "text": "real life data"}, {"st": 129, "ed": 131, "text": "detection algorithms"}]
[{"st": 18, "ed": 20, "text": "clustering algorithms"}, {"st": 33, "ed": 35, "text": "clustering process"}, {"st": 66, "ed": 68, "text": "noise level"}, {"st": 91, "ed": 93, "text": "fuzzy set"}, {"st": 114, "ed": 116, "text": "clustering process"}]
[{"st": 7, "ed": 10, "text": "a long standing"}, {"st": 26, "ed": 30, "text": "generative adversarial networks gans"}, {"st": 58, "ed": 60, "text": "image quality"}, {"st": 75, "ed": 77, "text": "class conditional"}, {"st": 77, "ed": 79, "text": "image synthesis"}, {"st": 93, "ed": 95, "text": "low resolution"}]
[{"st": 0, "ed": 4, "text": "generative adversarial networks gans"}, {"st": 10, "ed": 12, "text": "generative models"}, {"st": 35, "ed": 37, "text": "texture synthesis"}, {"st": 69, "ed": 71, "text": "texture synthesis"}, {"st": 89, "ed": 91, "text": "texture synthesis"}, {"st": 111, "ed": 113, "text": "texture synthesis"}, {"st": 114, "ed": 116, "text": "image quality"}]
[{"st": 35, "ed": 37, "text": "scale parameter"}, {"st": 48, "ed": 50, "text": "feature extraction"}]
[{"st": 33, "ed": 35, "text": "features extracted"}, {"st": 37, "ed": 39, "text": "facial recognition"}, {"st": 40, "ed": 42, "text": "unlike previous"}, {"st": 45, "ed": 47, "text": "feature vector"}, {"st": 62, "ed": 64, "text": "decoder network"}]
[{"st": 14, "ed": 16, "text": "machine vision"}, {"st": 34, "ed": 36, "text": "vision problems"}, {"st": 42, "ed": 45, "text": "trained from scratch"}, {"st": 46, "ed": 48, "text": "fine tuned"}, {"st": 81, "ed": 83, "text": "vision problems"}, {"st": 95, "ed": 97, "text": "training data"}]
[{"st": 16, "ed": 18, "text": "time consuming"}, {"st": 40, "ed": 42, "text": "patient specific"}, {"st": 103, "ed": 107, "text": "convolutional neural network cnn"}, {"st": 112, "ed": 114, "text": "decision process"}, {"st": 124, "ed": 126, "text": "proposed method"}, {"st": 134, "ed": 136, "text": "amniotic fluid"}, {"st": 148, "ed": 150, "text": "proposed method"}, {"st": 166, "ed": 168, "text": "training samples"}, {"st": 173, "ed": 175, "text": "classification results"}, {"st": 183, "ed": 185, "text": "proposed method"}, {"st": 252, "ed": 254, "text": "amniotic fluid"}]
[{"st": 3, "ed": 5, "text": "collaborative filtering"}, {"st": 33, "ed": 35, "text": "image representation"}, {"st": 42, "ed": 44, "text": "noise free"}, {"st": 79, "ed": 81, "text": "image quality"}, {"st": 91, "ed": 94, "text": "bias and variance"}, {"st": 129, "ed": 131, "text": "collaborative filtering"}]
[{"st": 87, "ed": 89, "text": "recently proposed"}, {"st": 97, "ed": 99, "text": "computationally efficient"}, {"st": 129, "ed": 131, "text": "computationally expensive"}]
[{"st": 1, "ed": 3, "text": "multiple classifiers"}, {"st": 12, "ed": 14, "text": "visible light"}, {"st": 60, "ed": 62, "text": "spectral density"}, {"st": 80, "ed": 82, "text": "machine learning"}, {"st": 87, "ed": 89, "text": "multiple classifiers"}, {"st": 95, "ed": 97, "text": "multiple classifiers"}, {"st": 125, "ed": 127, "text": "least square"}, {"st": 132, "ed": 134, "text": "least square"}, {"st": 149, "ed": 153, "text": "singular value decomposition svd"}, {"st": 161, "ed": 163, "text": "numerical stability"}, {"st": 170, "ed": 172, "text": "experiments conducted"}]
[{"st": 29, "ed": 31, "text": "time consuming"}, {"st": 108, "ed": 110, "text": "random forests"}, {"st": 145, "ed": 147, "text": "sliding window"}]
[{"st": 2, "ed": 4, "text": "deep learning"}, {"st": 8, "ed": 11, "text": "convolutional neural networks"}, {"st": 17, "ed": 19, "text": "prostate cancer"}, {"st": 31, "ed": 34, "text": "end to end"}, {"st": 61, "ed": 63, "text": "machine learning"}, {"st": 66, "ed": 68, "text": "engineered features"}, {"st": 101, "ed": 103, "text": "great potential"}, {"st": 104, "ed": 106, "text": "deep learning"}]
[{"st": 0, "ed": 4, "text": "chronic obstructive pulmonary disease"}, {"st": 10, "ed": 12, "text": "early detection"}, {"st": 24, "ed": 26, "text": "computed tomography"}, {"st": 75, "ed": 79, "text": "multiple instance learning mil"}, {"st": 86, "ed": 88, "text": "weakly labeled"}, {"st": 143, "ed": 145, "text": "previously reported"}, {"st": 155, "ed": 157, "text": "training set"}, {"st": 164, "ed": 166, "text": "significantly higher"}]
[{"st": 0, "ed": 2, "text": "supervised learning"}, {"st": 7, "ed": 9, "text": "automatic segmentation"}, {"st": 39, "ed": 41, "text": "transfer learning"}, {"st": 47, "ed": 50, "text": "training and test"}, {"st": 85, "ed": 87, "text": "similarity measures"}, {"st": 95, "ed": 97, "text": "labeled data"}, {"st": 142, "ed": 144, "text": "similarity measures"}, {"st": 146, "ed": 148, "text": "similarity measure"}, {"st": 157, "ed": 159, "text": "excellent results"}, {"st": 167, "ed": 169, "text": "white matter"}]
[{"st": 10, "ed": 14, "text": "multiple instance learning mil"}, {"st": 29, "ed": 31, "text": "fine grained"}, {"st": 33, "ed": 35, "text": "image pixels"}, {"st": 43, "ed": 46, "text": "computer aided diagnosis"}, {"st": 50, "ed": 52, "text": "image analysis"}, {"st": 63, "ed": 65, "text": "instance labels"}, {"st": 75, "ed": 77, "text": "training data"}, {"st": 119, "ed": 121, "text": "instance labels"}, {"st": 134, "ed": 136, "text": "medical image"}, {"st": 139, "ed": 141, "text": "diabetic retinopathy"}, {"st": 142, "ed": 144, "text": "computed tomography"}]
[{"st": 56, "ed": 58, "text": "common practice"}, {"st": 103, "ed": 105, "text": "constrained optimization"}, {"st": 128, "ed": 130, "text": "low rank"}, {"st": 132, "ed": 135, "text": "nonnegative matrix factorization"}, {"st": 147, "ed": 149, "text": "ell 2"}, {"st": 149, "ed": 151, "text": "ell 1"}, {"st": 171, "ed": 174, "text": "simulated and real"}]
[{"st": 3, "ed": 5, "text": "component analysis"}, {"st": 15, "ed": 17, "text": "active research"}, {"st": 24, "ed": 26, "text": "image processing"}, {"st": 26, "ed": 28, "text": "computer vision"}, {"st": 36, "ed": 38, "text": "current methods"}, {"st": 42, "ed": 44, "text": "k svd"}, {"st": 55, "ed": 57, "text": "sparse coding"}, {"st": 63, "ed": 65, "text": "component analysis"}, {"st": 69, "ed": 71, "text": "principal component"}, {"st": 76, "ed": 78, "text": "low rank"}, {"st": 88, "ed": 90, "text": "k svd"}, {"st": 144, "ed": 147, "text": "sparse dictionary learning"}, {"st": 155, "ed": 157, "text": "component analysis"}, {"st": 167, "ed": 169, "text": "low rank"}, {"st": 175, "ed": 177, "text": "significantly smaller"}, {"st": 181, "ed": 183, "text": "efficient learning"}, {"st": 198, "ed": 200, "text": "proposed approach"}, {"st": 203, "ed": 205, "text": "real world"}, {"st": 210, "ed": 212, "text": "image denoising"}]
[{"st": 12, "ed": 14, "text": "ground truth"}, {"st": 22, "ed": 24, "text": "computer vision"}, {"st": 35, "ed": 37, "text": "ground truth"}, {"st": 43, "ed": 45, "text": "rgb d"}, {"st": 54, "ed": 56, "text": "non trivial"}, {"st": 100, "ed": 102, "text": "ground truth"}, {"st": 154, "ed": 156, "text": "pre trained"}, {"st": 156, "ed": 158, "text": "deep learning"}, {"st": 173, "ed": 175, "text": "scene understanding"}]
[{"st": 31, "ed": 33, "text": "optic nerve"}, {"st": 37, "ed": 39, "text": "optic nerve"}, {"st": 72, "ed": 74, "text": "optic disc"}, {"st": 87, "ed": 89, "text": "computer vision"}, {"st": 97, "ed": 99, "text": "optic disc"}, {"st": 106, "ed": 108, "text": "deep learning"}, {"st": 111, "ed": 113, "text": "u net"}, {"st": 138, "ed": 140, "text": "optic disc"}, {"st": 144, "ed": 146, "text": "method achieves"}]
[{"st": 5, "ed": 7, "text": "max pooling"}, {"st": 11, "ed": 13, "text": "training data"}, {"st": 23, "ed": 26, "text": "deep neural networks"}, {"st": 31, "ed": 33, "text": "real world"}, {"st": 42, "ed": 44, "text": "object categories"}, {"st": 74, "ed": 76, "text": "classification results"}, {"st": 89, "ed": 91, "text": "cost sensitive"}, {"st": 105, "ed": 107, "text": "intra class"}, {"st": 111, "ed": 113, "text": "theoretical justification"}, {"st": 130, "ed": 133, "text": "pascal voc 2012"}, {"st": 138, "ed": 140, "text": "improved results"}]
[{"st": 1, "ed": 3, "text": "computer vision"}, {"st": 21, "ed": 23, "text": "outlier detection"}, {"st": 26, "ed": 28, "text": "robust statistics"}, {"st": 40, "ed": 42, "text": "low rank"}, {"st": 60, "ed": 62, "text": "low dimensional"}, {"st": 68, "ed": 70, "text": "outlier detection"}, {"st": 75, "ed": 77, "text": "sparse representation"}, {"st": 78, "ed": 80, "text": "random walks"}, {"st": 94, "ed": 96, "text": "sparse linear"}, {"st": 104, "ed": 106, "text": "affinity matrix"}, {"st": 155, "ed": 157, "text": "theoretical analysis"}, {"st": 186, "ed": 188, "text": "low rank"}, {"st": 188, "ed": 190, "text": "outlier detection"}]
[{"st": 1, "ed": 3, "text": "image regions"}, {"st": 29, "ed": 31, "text": "recurrent neural"}, {"st": 47, "ed": 49, "text": "natural language"}, {"st": 63, "ed": 65, "text": "neural network"}, {"st": 80, "ed": 82, "text": "training objective"}, {"st": 98, "ed": 100, "text": "image regions"}, {"st": 104, "ed": 107, "text": "positive and negative"}, {"st": 118, "ed": 120, "text": "significantly outperforms"}, {"st": 146, "ed": 148, "text": "natural language"}]
[{"st": 1, "ed": 3, "text": "convolutional networks"}, {"st": 13, "ed": 16, "text": "stochastic gradient descent"}, {"st": 59, "ed": 62, "text": "mixture of experts"}, {"st": 71, "ed": 73, "text": "large scale"}, {"st": 77, "ed": 80, "text": "mixture of experts"}, {"st": 113, "ed": 115, "text": "weakly supervised"}, {"st": 126, "ed": 129, "text": "each data point"}, {"st": 174, "ed": 176, "text": "feature embedding"}]
[{"st": 9, "ed": 11, "text": "compressive sensing"}, {"st": 25, "ed": 27, "text": "prior knowledge"}, {"st": 44, "ed": 46, "text": "compressive sensing"}, {"st": 63, "ed": 65, "text": "prior knowledge"}, {"st": 74, "ed": 76, "text": "compressive sensing"}, {"st": 79, "ed": 81, "text": "object detection"}, {"st": 95, "ed": 97, "text": "real datasets"}, {"st": 117, "ed": 119, "text": "greedy algorithm"}]
[{"st": 5, "ed": 7, "text": "object categories"}, {"st": 10, "ed": 12, "text": "open problem"}, {"st": 31, "ed": 33, "text": "object categories"}, {"st": 58, "ed": 61, "text": "deep neural network"}, {"st": 81, "ed": 83, "text": "object instances"}, {"st": 119, "ed": 121, "text": "manually annotated"}, {"st": 124, "ed": 126, "text": "benchmark datasets"}]
[{"st": 0, "ed": 2, "text": "image denoising"}, {"st": 7, "ed": 9, "text": "noise levels"}, {"st": 14, "ed": 16, "text": "low dose"}, {"st": 16, "ed": 18, "text": "computed tomography"}, {"st": 20, "ed": 22, "text": "machine learning"}, {"st": 27, "ed": 29, "text": "great potential"}, {"st": 66, "ed": 68, "text": "trained cnn"}, {"st": 87, "ed": 91, "text": "convolutional neural networks cnn"}, {"st": 108, "ed": 110, "text": "low dose"}]
[{"st": 22, "ed": 26, "text": "magnetic resonance imaging mri"}, {"st": 44, "ed": 46, "text": "image segmentation"}, {"st": 54, "ed": 56, "text": "quantitative analysis"}, {"st": 73, "ed": 75, "text": "deep learning"}, {"st": 96, "ed": 98, "text": "multi view"}, {"st": 98, "ed": 102, "text": "convolutional neural network cnn"}, {"st": 110, "ed": 112, "text": "loss function"}, {"st": 153, "ed": 156, "text": "qualitative and quantitative"}, {"st": 165, "ed": 167, "text": "proposed method"}]
[{"st": 7, "ed": 9, "text": "texture synthesis"}, {"st": 11, "ed": 14, "text": "generative adversarial networks"}, {"st": 83, "ed": 85, "text": "image generation"}, {"st": 141, "ed": 143, "text": "image data"}, {"st": 147, "ed": 149, "text": "highly scalable"}]
[{"st": 34, "ed": 36, "text": "visual representation"}, {"st": 75, "ed": 78, "text": "deep network architecture"}, {"st": 96, "ed": 98, "text": "method achieves"}, {"st": 102, "ed": 104, "text": "parameter sharing"}, {"st": 112, "ed": 114, "text": "domain specific"}]
[{"st": 60, "ed": 63, "text": "gaussian mixture model"}, {"st": 85, "ed": 87, "text": "training phase"}, {"st": 99, "ed": 101, "text": "prior distribution"}, {"st": 137, "ed": 139, "text": "learning rate"}]
[{"st": 39, "ed": 41, "text": "multivariate gaussian"}, {"st": 44, "ed": 47, "text": "principal component analysis"}]
[{"st": 2, "ed": 4, "text": "convolutional networks"}, {"st": 15, "ed": 17, "text": "prior knowledge"}, {"st": 50, "ed": 52, "text": "improve performance"}, {"st": 53, "ed": 55, "text": "cifar 10"}, {"st": 90, "ed": 92, "text": "pre defined"}, {"st": 98, "ed": 101, "text": "training and inference"}, {"st": 141, "ed": 143, "text": "contour detection"}, {"st": 145, "ed": 147, "text": "approach outperforms"}, {"st": 148, "ed": 150, "text": "competing approaches"}, {"st": 163, "ed": 165, "text": "based regularization"}]
[{"st": 10, "ed": 12, "text": "labeled data"}, {"st": 22, "ed": 24, "text": "challenging problem"}, {"st": 34, "ed": 36, "text": "deep learning"}, {"st": 37, "ed": 39, "text": "computer vision"}, {"st": 81, "ed": 85, "text": "convolutional neural network cnn"}, {"st": 135, "ed": 137, "text": "fine tuning"}]
[{"st": 7, "ed": 9, "text": "significantly improved"}, {"st": 78, "ed": 80, "text": "multi modal"}, {"st": 98, "ed": 100, "text": "learning framework"}, {"st": 134, "ed": 137, "text": "recurrent neural network"}, {"st": 168, "ed": 170, "text": "prediction tasks"}, {"st": 172, "ed": 174, "text": "accurately predict"}]
[{"st": 9, "ed": 11, "text": "optimal transport"}, {"st": 19, "ed": 21, "text": "wasserstein distance"}, {"st": 32, "ed": 34, "text": "joint distributions"}, {"st": 39, "ed": 41, "text": "marginal distributions"}, {"st": 52, "ed": 54, "text": "wasserstein distance"}, {"st": 70, "ed": 72, "text": "auto encoders"}, {"st": 74, "ed": 78, "text": "generative adversarial networks gans"}, {"st": 99, "ed": 101, "text": "generative models"}, {"st": 104, "ed": 107, "text": "qualitatively and quantitatively"}]
[{"st": 3, "ed": 5, "text": "key challenges"}, {"st": 6, "ed": 8, "text": "visual perception"}, {"st": 17, "ed": 19, "text": "object categories"}, {"st": 88, "ed": 90, "text": "neural network"}, {"st": 93, "ed": 95, "text": "image pixels"}, {"st": 125, "ed": 127, "text": "optical flow"}]
[{"st": 19, "ed": 21, "text": "ejection fraction"}, {"st": 33, "ed": 35, "text": "image segmentation"}, {"st": 39, "ed": 41, "text": "time consuming"}, {"st": 61, "ed": 63, "text": "deep learning"}, {"st": 66, "ed": 68, "text": "fully automated"}, {"st": 130, "ed": 132, "text": "correlation coefficient"}, {"st": 163, "ed": 165, "text": "is 95"}, {"st": 223, "ed": 226, "text": "deep neural network"}, {"st": 258, "ed": 260, "text": "time consuming"}]
[{"st": 11, "ed": 14, "text": "convolutional neural network"}, {"st": 33, "ed": 35, "text": "dimensionality reduction"}, {"st": 47, "ed": 49, "text": "significantly reduce"}, {"st": 79, "ed": 81, "text": "proposed approach"}]
[{"st": 22, "ed": 24, "text": "generative model"}]
[{"st": 9, "ed": 11, "text": "remote sensing"}, {"st": 58, "ed": 60, "text": "large scale"}, {"st": 74, "ed": 76, "text": "deep learning"}]
[{"st": 30, "ed": 32, "text": "convolutional networks"}, {"st": 34, "ed": 36, "text": "generalization error"}, {"st": 37, "ed": 39, "text": "generalization capability"}, {"st": 69, "ed": 71, "text": "convolutional networks"}, {"st": 95, "ed": 97, "text": "feature map"}, {"st": 99, "ed": 101, "text": "feature vectors"}, {"st": 140, "ed": 143, "text": "end to end"}, {"st": 169, "ed": 171, "text": "digit recognition"}]
[{"st": 7, "ed": 9, "text": "computer vision"}, {"st": 18, "ed": 20, "text": "deep learning"}, {"st": 112, "ed": 115, "text": "accuracy and robustness"}, {"st": 132, "ed": 134, "text": "empirical evaluation"}]
[{"st": 4, "ed": 6, "text": "video games"}, {"st": 33, "ed": 35, "text": "recent advances"}, {"st": 36, "ed": 38, "text": "deep generative"}]
[{"st": 4, "ed": 6, "text": "rgb d"}, {"st": 15, "ed": 17, "text": "computer vision"}, {"st": 48, "ed": 51, "text": "convolutional neural networks"}, {"st": 61, "ed": 63, "text": "nearest neighbors"}, {"st": 83, "ed": 85, "text": "point clouds"}, {"st": 89, "ed": 91, "text": "convolutional neural"}, {"st": 105, "ed": 107, "text": "feature engineering"}, {"st": 111, "ed": 113, "text": "computationally efficient"}]
[{"st": 0, "ed": 2, "text": "image understanding"}, {"st": 3, "ed": 6, "text": "deep convolutional network"}, {"st": 13, "ed": 15, "text": "closely related"}, {"st": 35, "ed": 38, "text": "support vector machines"}, {"st": 43, "ed": 47, "text": "deep convolutional neural network"}, {"st": 60, "ed": 62, "text": "activity recognition"}, {"st": 76, "ed": 79, "text": "deep neural networks"}, {"st": 99, "ed": 102, "text": "deep neural network"}, {"st": 115, "ed": 117, "text": "hand crafted"}]
[{"st": 0, "ed": 2, "text": "non invasive"}, {"st": 11, "ed": 13, "text": "image analysis"}, {"st": 32, "ed": 34, "text": "ejection fraction"}, {"st": 69, "ed": 71, "text": "image segmentation"}, {"st": 87, "ed": 89, "text": "proposed method"}, {"st": 94, "ed": 99, "text": "deep convolutional neural networks cnn"}, {"st": 108, "ed": 110, "text": "complementary information"}, {"st": 155, "ed": 157, "text": "left ventricle"}, {"st": 161, "ed": 163, "text": "left ventricle"}, {"st": 170, "ed": 172, "text": "ascending aorta"}, {"st": 175, "ed": 177, "text": "pulmonary artery"}, {"st": 183, "ed": 185, "text": "proposed method"}, {"st": 187, "ed": 190, "text": "fold cross validation"}, {"st": 220, "ed": 222, "text": "mr images"}]
[{"st": 43, "ed": 45, "text": "correctly classified"}, {"st": 60, "ed": 62, "text": "computer vision"}]
[{"st": 4, "ed": 6, "text": "big data"}, {"st": 38, "ed": 40, "text": "multi view"}, {"st": 41, "ed": 43, "text": "multi modal"}, {"st": 45, "ed": 47, "text": "missing data"}, {"st": 54, "ed": 57, "text": "multi view data"}, {"st": 78, "ed": 80, "text": "matrix completion"}, {"st": 117, "ed": 119, "text": "sample size"}, {"st": 122, "ed": 124, "text": "statistical power"}, {"st": 140, "ed": 144, "text": "generative adversarial networks gans"}, {"st": 179, "ed": 181, "text": "multi modal"}, {"st": 181, "ed": 183, "text": "denoising autoencoder"}, {"st": 226, "ed": 228, "text": "empirical results"}, {"st": 229, "ed": 231, "text": "benchmark datasets"}]
[{"st": 1, "ed": 4, "text": "deep residual networks"}, {"st": 6, "ed": 8, "text": "successfully applied"}, {"st": 10, "ed": 12, "text": "computer vision"}, {"st": 13, "ed": 15, "text": "natural language"}, {"st": 34, "ed": 37, "text": "deep residual networks"}, {"st": 38, "ed": 41, "text": "ordinary differential equations"}, {"st": 53, "ed": 56, "text": "theoretical and empirical"}, {"st": 63, "ed": 65, "text": "theoretical framework"}, {"st": 70, "ed": 73, "text": "deep neural networks"}, {"st": 77, "ed": 79, "text": "neural network"}, {"st": 93, "ed": 95, "text": "efficient implementation"}, {"st": 127, "ed": 129, "text": "theoretical analyses"}, {"st": 142, "ed": 144, "text": "strong baselines"}, {"st": 145, "ed": 149, "text": "cifar 10 cifar 100"}]
[{"st": 53, "ed": 55, "text": "significantly reduce"}, {"st": 64, "ed": 66, "text": "significantly improve"}, {"st": 108, "ed": 110, "text": "log likelihood"}, {"st": 119, "ed": 121, "text": "performance improvement"}, {"st": 175, "ed": 177, "text": "error rates"}, {"st": 184, "ed": 188, "text": "cifar 10 cifar 100"}, {"st": 198, "ed": 200, "text": "error rate"}, {"st": 221, "ed": 223, "text": "error rates"}]
[{"st": 5, "ed": 7, "text": "instance segmentation"}, {"st": 58, "ed": 60, "text": "small scale"}, {"st": 80, "ed": 82, "text": "approach achieves"}, {"st": 88, "ed": 90, "text": "instance segmentation"}, {"st": 106, "ed": 108, "text": "proposed algorithm"}]
[{"st": 13, "ed": 17, "text": "magnetic resonance imaging mri"}, {"st": 36, "ed": 38, "text": "field strength"}, {"st": 51, "ed": 55, "text": "signal to noise ratio"}, {"st": 94, "ed": 96, "text": "neural network"}, {"st": 128, "ed": 131, "text": "simulated and real"}, {"st": 138, "ed": 140, "text": "invariant representation"}, {"st": 154, "ed": 156, "text": "linear classifier"}, {"st": 162, "ed": 165, "text": "convolutional neural network"}, {"st": 172, "ed": 174, "text": "training data"}]
[{"st": 5, "ed": 7, "text": "contrastive divergence"}, {"st": 31, "ed": 33, "text": "probabilistic model"}, {"st": 43, "ed": 46, "text": "convolutional neural network"}, {"st": 66, "ed": 68, "text": "learning algorithm"}, {"st": 91, "ed": 93, "text": "x 1"}, {"st": 179, "ed": 181, "text": "contrastive divergence"}]
[{"st": 7, "ed": 9, "text": "robust pca"}, {"st": 10, "ed": 12, "text": "total variation"}, {"st": 25, "ed": 27, "text": "proposed algorithm"}, {"st": 52, "ed": 54, "text": "low rank"}, {"st": 88, "ed": 90, "text": "unlike existing"}, {"st": 92, "ed": 94, "text": "proposed algorithm"}, {"st": 97, "ed": 99, "text": "low rank"}, {"st": 104, "ed": 107, "text": "field of view"}, {"st": 117, "ed": 119, "text": "low rank"}, {"st": 122, "ed": 124, "text": "robust pca"}, {"st": 132, "ed": 134, "text": "low rank"}, {"st": 156, "ed": 159, "text": "corrupted by noise"}]
[{"st": 0, "ed": 2, "text": "face recognition"}, {"st": 9, "ed": 11, "text": "large scale"}, {"st": 15, "ed": 17, "text": "social media"}, {"st": 26, "ed": 28, "text": "face recognition"}, {"st": 41, "ed": 44, "text": "deep learning based"}, {"st": 52, "ed": 54, "text": "face recognition"}, {"st": 76, "ed": 78, "text": "face representation"}, {"st": 93, "ed": 95, "text": "face representation"}, {"st": 105, "ed": 107, "text": "face representation"}, {"st": 132, "ed": 134, "text": "face recognition"}, {"st": 140, "ed": 142, "text": "estimation problem"}, {"st": 151, "ed": 153, "text": "gaussian noise"}, {"st": 196, "ed": 198, "text": "dnn based"}, {"st": 198, "ed": 200, "text": "face representation"}, {"st": 250, "ed": 252, "text": "face representation"}, {"st": 283, "ed": 285, "text": "empirical performance"}]
[{"st": 25, "ed": 27, "text": "recent works"}, {"st": 36, "ed": 38, "text": "lighting conditions"}, {"st": 90, "ed": 92, "text": "image dataset"}, {"st": 127, "ed": 129, "text": "extensive simulations"}]
[{"st": 34, "ed": 36, "text": "based approach"}, {"st": 66, "ed": 68, "text": "underlying structure"}, {"st": 96, "ed": 98, "text": "existing approaches"}, {"st": 117, "ed": 119, "text": "synthetic data"}]
[{"st": 0, "ed": 4, "text": "functional magnetic resonance imaging"}, {"st": 7, "ed": 9, "text": "multi step"}, {"st": 20, "ed": 22, "text": "crucial step"}, {"st": 49, "ed": 51, "text": "deep learning"}, {"st": 60, "ed": 63, "text": "end to end"}]
[{"st": 52, "ed": 54, "text": "machine learning"}, {"st": 70, "ed": 72, "text": "spectral analysis"}, {"st": 87, "ed": 90, "text": "deep convolutional network"}, {"st": 93, "ed": 95, "text": "visual recognition"}, {"st": 101, "ed": 103, "text": "transfer learning"}]
[{"st": 3, "ed": 5, "text": "deep learning"}, {"st": 12, "ed": 14, "text": "computed tomography"}, {"st": 28, "ed": 30, "text": "ct scan"}, {"st": 84, "ed": 86, "text": "proposed approach"}, {"st": 109, "ed": 112, "text": "recurrent neural network"}, {"st": 118, "ed": 120, "text": "real world"}, {"st": 142, "ed": 144, "text": "prediction accuracy"}, {"st": 154, "ed": 156, "text": "achieves higher"}]
[{"st": 139, "ed": 141, "text": "explicitly model"}, {"st": 153, "ed": 156, "text": "synthetic and real"}, {"st": 156, "ed": 158, "text": "benchmark datasets"}]
[{"st": 8, "ed": 10, "text": "deep learning"}, {"st": 24, "ed": 26, "text": "computed tomography"}, {"st": 31, "ed": 34, "text": "local and global"}, {"st": 71, "ed": 73, "text": "nearest neighbors"}, {"st": 90, "ed": 92, "text": "joint representation"}, {"st": 94, "ed": 96, "text": "nearest neighbors"}]
[{"st": 0, "ed": 3, "text": "deep residual networks"}, {"st": 12, "ed": 14, "text": "computer vision"}, {"st": 16, "ed": 18, "text": "natural language"}, {"st": 55, "ed": 57, "text": "dynamical systems"}, {"st": 64, "ed": 66, "text": "dynamical systems"}, {"st": 66, "ed": 69, "text": "point of view"}, {"st": 97, "ed": 99, "text": "proposed method"}, {"st": 107, "ed": 109, "text": "image classification"}]
[{"st": 0, "ed": 4, "text": "nonnegative matrix factorization nmf"}, {"st": 6, "ed": 8, "text": "powerful tool"}, {"st": 15, "ed": 17, "text": "big data"}, {"st": 37, "ed": 39, "text": "least squares"}, {"st": 53, "ed": 55, "text": "input data"}, {"st": 67, "ed": 69, "text": "big data"}, {"st": 73, "ed": 75, "text": "near optimal"}, {"st": 96, "ed": 98, "text": "proposed algorithm"}, {"st": 101, "ed": 105, "text": "synthetic and real world"}]
[{"st": 2, "ed": 4, "text": "real world"}, {"st": 8, "ed": 10, "text": "multiple modalities"}, {"st": 25, "ed": 27, "text": "latent space"}, {"st": 34, "ed": 36, "text": "cross view"}, {"st": 36, "ed": 38, "text": "learning algorithms"}, {"st": 93, "ed": 95, "text": "latent space"}, {"st": 99, "ed": 101, "text": "multi modal"}, {"st": 108, "ed": 110, "text": "cross domain"}, {"st": 110, "ed": 112, "text": "representation learning"}, {"st": 121, "ed": 123, "text": "representation learning"}, {"st": 123, "ed": 125, "text": "auto encoders"}, {"st": 130, "ed": 132, "text": "multi modal"}, {"st": 141, "ed": 143, "text": "fully supervised"}, {"st": 143, "ed": 145, "text": "semi supervised"}, {"st": 150, "ed": 152, "text": "multi modal"}, {"st": 155, "ed": 157, "text": "promising results"}, {"st": 158, "ed": 160, "text": "image captioning"}]
[{"st": 21, "ed": 23, "text": "highly accurate"}, {"st": 23, "ed": 25, "text": "prediction model"}, {"st": 35, "ed": 37, "text": "manually annotated"}, {"st": 48, "ed": 50, "text": "annotated data"}, {"st": 117, "ed": 119, "text": "significantly outperforms"}]
[{"st": 4, "ed": 6, "text": "deep learning"}, {"st": 70, "ed": 72, "text": "clinical practice"}]
[{"st": 0, "ed": 2, "text": "hash codes"}, {"st": 21, "ed": 23, "text": "random forest"}, {"st": 29, "ed": 33, "text": "convolutional neural networks cnn"}, {"st": 35, "ed": 37, "text": "random forests"}, {"st": 38, "ed": 40, "text": "near optimal"}, {"st": 80, "ed": 82, "text": "random forests"}, {"st": 96, "ed": 98, "text": "random forests"}, {"st": 126, "ed": 128, "text": "class classification"}, {"st": 165, "ed": 167, "text": "low rank"}, {"st": 174, "ed": 176, "text": "weak learners"}, {"st": 182, "ed": 184, "text": "intra class"}, {"st": 187, "ed": 189, "text": "inter class"}, {"st": 216, "ed": 218, "text": "near optimal"}, {"st": 224, "ed": 226, "text": "proposed approach"}, {"st": 226, "ed": 228, "text": "significantly outperforms"}, {"st": 235, "ed": 237, "text": "image retrieval"}, {"st": 239, "ed": 241, "text": "large scale"}, {"st": 254, "ed": 256, "text": "image classification"}]
[{"st": 1, "ed": 3, "text": "deep learning"}, {"st": 111, "ed": 113, "text": "deep generative"}, {"st": 164, "ed": 166, "text": "ill posed"}, {"st": 186, "ed": 189, "text": "quantitative and qualitative"}, {"st": 196, "ed": 198, "text": "approach outperforms"}]
[{"st": 85, "ed": 87, "text": "previously learned"}]
[{"st": 0, "ed": 2, "text": "computed tomography"}, {"st": 10, "ed": 12, "text": "wide variety"}, {"st": 78, "ed": 80, "text": "existing techniques"}, {"st": 102, "ed": 104, "text": "real world"}, {"st": 121, "ed": 124, "text": "convolutional neural networks"}, {"st": 144, "ed": 146, "text": "x ray"}, {"st": 172, "ed": 174, "text": "iterative reconstruction"}, {"st": 237, "ed": 239, "text": "ground truth"}]
[{"st": 31, "ed": 33, "text": "generative adversarial"}, {"st": 37, "ed": 40, "text": "encoder decoder architecture"}, {"st": 49, "ed": 52, "text": "encoder decoder architecture"}, {"st": 62, "ed": 64, "text": "latent representation"}, {"st": 66, "ed": 68, "text": "face image"}, {"st": 75, "ed": 77, "text": "existing methods"}, {"st": 82, "ed": 84, "text": "latent representation"}, {"st": 98, "ed": 100, "text": "face image"}, {"st": 106, "ed": 108, "text": "latent representation"}, {"st": 122, "ed": 124, "text": "generated images"}, {"st": 146, "ed": 148, "text": "latent representation"}, {"st": 156, "ed": 158, "text": "generated image"}, {"st": 185, "ed": 187, "text": "generated image"}, {"st": 215, "ed": 217, "text": "method outperforms"}]
[{"st": 0, "ed": 3, "text": "deep convolutional networks"}, {"st": 9, "ed": 11, "text": "image generation"}, {"st": 15, "ed": 17, "text": "excellent performance"}, {"st": 21, "ed": 24, "text": "ability to learn"}, {"st": 47, "ed": 49, "text": "generator network"}, {"st": 57, "ed": 59, "text": "low level"}, {"st": 76, "ed": 78, "text": "neural network"}, {"st": 86, "ed": 88, "text": "excellent results"}, {"st": 90, "ed": 92, "text": "inverse problems"}, {"st": 95, "ed": 97, "text": "super resolution"}, {"st": 108, "ed": 110, "text": "deep neural"}, {"st": 134, "ed": 136, "text": "inductive bias"}, {"st": 139, "ed": 141, "text": "generator network"}, {"st": 153, "ed": 155, "text": "image restoration"}, {"st": 157, "ed": 159, "text": "based methods"}, {"st": 160, "ed": 163, "text": "deep convolutional networks"}]
[{"st": 22, "ed": 24, "text": "domain shift"}, {"st": 26, "ed": 28, "text": "domain adversarial"}, {"st": 38, "ed": 40, "text": "metric learning"}, {"st": 49, "ed": 51, "text": "source data"}, {"st": 55, "ed": 57, "text": "labeled data"}, {"st": 78, "ed": 80, "text": "labeled examples"}, {"st": 87, "ed": 89, "text": "fine tuning"}, {"st": 101, "ed": 103, "text": "transfer learning"}, {"st": 106, "ed": 108, "text": "object recognition"}]
[{"st": 63, "ed": 65, "text": "generative adversarial"}, {"st": 67, "ed": 69, "text": "texture synthesis"}, {"st": 86, "ed": 88, "text": "fully convolutional"}]
[{"st": 5, "ed": 7, "text": "ill posed"}, {"st": 7, "ed": 9, "text": "computer vision"}, {"st": 16, "ed": 18, "text": "main challenges"}, {"st": 37, "ed": 39, "text": "future frames"}, {"st": 57, "ed": 59, "text": "gated recurrent"}, {"st": 65, "ed": 68, "text": "input and output"}, {"st": 76, "ed": 78, "text": "auto encoders"}, {"st": 82, "ed": 85, "text": "encoder and decoder"}, {"st": 119, "ed": 121, "text": "computational cost"}, {"st": 148, "ed": 150, "text": "trained model"}, {"st": 179, "ed": 181, "text": "prediction results"}, {"st": 187, "ed": 189, "text": "competitive results"}, {"st": 200, "ed": 202, "text": "computational cost"}]
[{"st": 2, "ed": 4, "text": "supervised classification"}, {"st": 5, "ed": 7, "text": "labeled data"}, {"st": 22, "ed": 24, "text": "dimensionality reduction"}, {"st": 38, "ed": 40, "text": "latent variables"}, {"st": 48, "ed": 50, "text": "unified framework"}, {"st": 54, "ed": 56, "text": "low level"}, {"st": 66, "ed": 68, "text": "latent variables"}, {"st": 80, "ed": 82, "text": "latent variable"}, {"st": 85, "ed": 88, "text": "hierarchical bayesian model"}, {"st": 95, "ed": 97, "text": "low level"}, {"st": 101, "ed": 103, "text": "latent variables"}]
[{"st": 5, "ed": 7, "text": "computer aided"}, {"st": 23, "ed": 25, "text": "multi stage"}, {"st": 49, "ed": 51, "text": "deep learning"}, {"st": 59, "ed": 61, "text": "deep learning"}, {"st": 70, "ed": 72, "text": "multi stage"}, {"st": 73, "ed": 75, "text": "deep learning"}, {"st": 97, "ed": 99, "text": "final prediction"}]
[{"st": 0, "ed": 2, "text": "machine learning"}, {"st": 4, "ed": 6, "text": "great potential"}, {"st": 25, "ed": 27, "text": "manually annotated"}, {"st": 79, "ed": 81, "text": "nodule detection"}, {"st": 89, "ed": 92, "text": "convolutional neural networks"}, {"st": 98, "ed": 100, "text": "network architectures"}, {"st": 117, "ed": 119, "text": "bounding boxes"}, {"st": 133, "ed": 135, "text": "visual attention"}, {"st": 152, "ed": 154, "text": "convolutional layers"}, {"st": 164, "ed": 166, "text": "ground truth"}, {"st": 191, "ed": 193, "text": "attention model"}, {"st": 217, "ed": 219, "text": "reward function"}, {"st": 238, "ed": 240, "text": "empirical results"}]
[{"st": 1, "ed": 3, "text": "image generation"}, {"st": 14, "ed": 16, "text": "great success"}, {"st": 17, "ed": 21, "text": "generative adversarial networks gans"}, {"st": 29, "ed": 31, "text": "real valued"}, {"st": 31, "ed": 33, "text": "image generation"}, {"st": 53, "ed": 55, "text": "real world"}, {"st": 66, "ed": 68, "text": "image generation"}, {"st": 74, "ed": 76, "text": "image generation"}, {"st": 80, "ed": 82, "text": "image generation"}, {"st": 109, "ed": 111, "text": "wasserstein gans"}, {"st": 124, "ed": 126, "text": "image generation"}, {"st": 130, "ed": 132, "text": "benchmark datasets"}, {"st": 134, "ed": 136, "text": "cifar 10"}, {"st": 154, "ed": 156, "text": "experimentally demonstrate"}]
[{"st": 6, "ed": 8, "text": "support vector"}, {"st": 76, "ed": 78, "text": "3d shapes"}, {"st": 84, "ed": 86, "text": "3d shapes"}, {"st": 99, "ed": 101, "text": "3d shapes"}]
[{"st": 7, "ed": 9, "text": "robust pca"}, {"st": 10, "ed": 12, "text": "total variation"}, {"st": 25, "ed": 27, "text": "proposed algorithm"}, {"st": 52, "ed": 54, "text": "low rank"}, {"st": 83, "ed": 85, "text": "unlike existing"}, {"st": 87, "ed": 89, "text": "proposed algorithm"}, {"st": 92, "ed": 94, "text": "low rank"}, {"st": 99, "ed": 102, "text": "field of view"}, {"st": 112, "ed": 114, "text": "low rank"}, {"st": 117, "ed": 119, "text": "robust pca"}, {"st": 127, "ed": 129, "text": "low rank"}, {"st": 151, "ed": 154, "text": "corrupted by noise"}]
[{"st": 9, "ed": 11, "text": "object class"}, {"st": 108, "ed": 110, "text": "downstream tasks"}, {"st": 148, "ed": 150, "text": "multilayer perceptrons"}, {"st": 153, "ed": 155, "text": "deep networks"}, {"st": 174, "ed": 176, "text": "google earth"}, {"st": 179, "ed": 181, "text": "competitive results"}, {"st": 182, "ed": 186, "text": "mnist and cifar 10"}]
[{"st": 22, "ed": 24, "text": "automatic segmentation"}, {"st": 44, "ed": 46, "text": "fully automated"}, {"st": 70, "ed": 72, "text": "proposed method"}, {"st": 81, "ed": 84, "text": "convolutional neural networks"}, {"st": 86, "ed": 89, "text": "spatial and temporal"}]
[{"st": 30, "ed": 32, "text": "distance metric"}, {"st": 55, "ed": 59, "text": "deep convolutional neural networks"}, {"st": 65, "ed": 67, "text": "low dimensional"}, {"st": 79, "ed": 81, "text": "loss functions"}, {"st": 97, "ed": 99, "text": "large scale"}, {"st": 118, "ed": 120, "text": "natural language"}, {"st": 137, "ed": 139, "text": "image retrieval"}]
[{"st": 4, "ed": 6, "text": "left ventricle"}, {"st": 21, "ed": 23, "text": "magnetic resonance"}, {"st": 41, "ed": 43, "text": "ground truth"}, {"st": 63, "ed": 66, "text": "convolutional neural networks"}, {"st": 67, "ed": 69, "text": "auto encoders"}]
[{"st": 0, "ed": 2, "text": "image denoising"}, {"st": 5, "ed": 7, "text": "challenging task"}, {"st": 11, "ed": 13, "text": "computer vision"}, {"st": 23, "ed": 25, "text": "encoder decoder"}, {"st": 53, "ed": 56, "text": "convolutional neural network"}, {"st": 62, "ed": 65, "text": "short term memory"}, {"st": 107, "ed": 109, "text": "mnist handwritten"}, {"st": 166, "ed": 168, "text": "encoder decoder"}]
[{"st": 12, "ed": 14, "text": "deep representations"}, {"st": 70, "ed": 72, "text": "saliency prediction"}, {"st": 102, "ed": 104, "text": "real world"}, {"st": 110, "ed": 112, "text": "crucial step"}]
[{"st": 4, "ed": 6, "text": "large networks"}, {"st": 19, "ed": 21, "text": "computer vision"}, {"st": 42, "ed": 44, "text": "training data"}, {"st": 52, "ed": 54, "text": "training data"}, {"st": 60, "ed": 62, "text": "visual recognition"}, {"st": 73, "ed": 75, "text": "computer vision"}, {"st": 80, "ed": 82, "text": "optical flow"}, {"st": 107, "ed": 109, "text": "generated data"}, {"st": 114, "ed": 116, "text": "deep networks"}, {"st": 137, "ed": 139, "text": "generalization properties"}]
[{"st": 39, "ed": 41, "text": "real world"}]
[{"st": 0, "ed": 2, "text": "scene recognition"}, {"st": 7, "ed": 9, "text": "extensively studied"}, {"st": 18, "ed": 22, "text": "convolutional neural networks cnn"}, {"st": 29, "ed": 31, "text": "rgb d"}, {"st": 41, "ed": 43, "text": "large datasets"}, {"st": 50, "ed": 52, "text": "fine tuning"}, {"st": 55, "ed": 57, "text": "rgb d"}, {"st": 99, "ed": 101, "text": "weakly supervised"}, {"st": 107, "ed": 109, "text": "fine tuning"}, {"st": 126, "ed": 128, "text": "without resorting"}, {"st": 154, "ed": 156, "text": "rgb d"}, {"st": 156, "ed": 158, "text": "scene recognition"}, {"st": 169, "ed": 171, "text": "common space"}, {"st": 183, "ed": 186, "text": "end to end"}, {"st": 199, "ed": 201, "text": "rgb d"}, {"st": 207, "ed": 209, "text": "rgb d"}]
[{"st": 0, "ed": 3, "text": "deep neural networks"}, {"st": 13, "ed": 15, "text": "supervised classification"}, {"st": 19, "ed": 21, "text": "supervised classification"}, {"st": 25, "ed": 27, "text": "large datasets"}, {"st": 32, "ed": 34, "text": "real world"}, {"st": 59, "ed": 61, "text": "similarity metric"}, {"st": 75, "ed": 77, "text": "image recognition"}, {"st": 82, "ed": 84, "text": "proposed method"}, {"st": 89, "ed": 91, "text": "existing methods"}, {"st": 97, "ed": 99, "text": "source code"}, {"st": 104, "ed": 108, "text": "available at https github.com"}]
[{"st": 24, "ed": 26, "text": "extracting information"}, {"st": 41, "ed": 43, "text": "higher level"}, {"st": 149, "ed": 151, "text": "face detection"}, {"st": 154, "ed": 156, "text": "false positives"}, {"st": 164, "ed": 166, "text": "facial landmark"}, {"st": 190, "ed": 192, "text": "face detection"}, {"st": 204, "ed": 207, "text": "quantitatively and qualitatively"}]
[{"st": 22, "ed": 24, "text": "feature maps"}, {"st": 90, "ed": 92, "text": "cifar 10"}, {"st": 116, "ed": 118, "text": "cifar 10"}]
[{"st": 0, "ed": 3, "text": "sparse subspace clustering"}, {"st": 33, "ed": 35, "text": "large data"}, {"st": 44, "ed": 47, "text": "each data point"}, {"st": 81, "ed": 83, "text": "hierarchical clustering"}, {"st": 96, "ed": 99, "text": "each data point"}, {"st": 155, "ed": 158, "text": "k means clustering"}, {"st": 170, "ed": 172, "text": "spectral clustering"}, {"st": 178, "ed": 182, "text": "synthetic and real world"}, {"st": 186, "ed": 188, "text": "proposed method"}, {"st": 193, "ed": 196, "text": "scale to large"}, {"st": 213, "ed": 215, "text": "noisy data"}]
[{"st": 1, "ed": 3, "text": "deep learning"}, {"st": 10, "ed": 14, "text": "mean squared error mse"}, {"st": 22, "ed": 24, "text": "ground truth"}, {"st": 39, "ed": 41, "text": "training data"}, {"st": 48, "ed": 50, "text": "application areas"}, {"st": 65, "ed": 67, "text": "ground truth"}, {"st": 77, "ed": 79, "text": "based method"}, {"st": 81, "ed": 83, "text": "deep learning"}, {"st": 86, "ed": 88, "text": "ground truth"}, {"st": 94, "ed": 96, "text": "based method"}, {"st": 99, "ed": 101, "text": "input data"}, {"st": 120, "ed": 122, "text": "deep learning"}, {"st": 124, "ed": 126, "text": "ground truth"}]
[{"st": 0, "ed": 2, "text": "low rank"}, {"st": 14, "ed": 16, "text": "image processing"}, {"st": 24, "ed": 27, "text": "low rank tensor"}, {"st": 27, "ed": 29, "text": "factor analysis"}, {"st": 37, "ed": 39, "text": "low rank"}, {"st": 58, "ed": 60, "text": "compressive sensing"}, {"st": 63, "ed": 65, "text": "deep convolutional"}, {"st": 78, "ed": 80, "text": "iterative algorithm"}, {"st": 83, "ed": 86, "text": "low rank tensor"}, {"st": 108, "ed": 110, "text": "proposed approach"}]
[{"st": 65, "ed": 67, "text": "evaluation function"}]
[{"st": 23, "ed": 25, "text": "game tree"}, {"st": 42, "ed": 44, "text": "evaluation function"}]
[{"st": 9, "ed": 11, "text": "automatically identify"}, {"st": 71, "ed": 73, "text": "mutual information"}]
[{"st": 108, "ed": 110, "text": "square root"}, {"st": 120, "ed": 122, "text": "prediction problems"}]
[]
[{"st": 12, "ed": 14, "text": "based methods"}, {"st": 133, "ed": 135, "text": "reinforcement learning"}]
[{"st": 0, "ed": 2, "text": "mutual information"}, {"st": 6, "ed": 8, "text": "artificial intelligence"}, {"st": 49, "ed": 51, "text": "mutual information"}, {"st": 98, "ed": 100, "text": "incremental learning"}, {"st": 122, "ed": 124, "text": "mutual information"}, {"st": 128, "ed": 130, "text": "real data"}]
[{"st": 4, "ed": 6, "text": "inductive logic"}, {"st": 18, "ed": 20, "text": "learning algorithms"}, {"st": 43, "ed": 45, "text": "learning algorithm"}, {"st": 61, "ed": 63, "text": "inductive logic"}, {"st": 88, "ed": 90, "text": "set theoretic"}, {"st": 136, "ed": 138, "text": "inductive logic"}]
[{"st": 6, "ed": 9, "text": "support vector machine"}, {"st": 15, "ed": 17, "text": "input space"}, {"st": 46, "ed": 48, "text": "input space"}, {"st": 53, "ed": 55, "text": "feature space"}, {"st": 114, "ed": 116, "text": "quadratic programming"}, {"st": 126, "ed": 128, "text": "local optimum"}, {"st": 128, "ed": 131, "text": "under mild conditions"}, {"st": 152, "ed": 154, "text": "quadratic programming"}]
[{"st": 11, "ed": 13, "text": "unsupervised learning"}, {"st": 55, "ed": 57, "text": "local geometry"}, {"st": 141, "ed": 143, "text": "higher dimensional"}, {"st": 150, "ed": 152, "text": "face images"}]
[{"st": 7, "ed": 9, "text": "kalman filter"}, {"st": 33, "ed": 35, "text": "optimal control"}, {"st": 38, "ed": 40, "text": "off line"}, {"st": 59, "ed": 61, "text": "kalman filter"}, {"st": 68, "ed": 70, "text": "optimal control"}, {"st": 79, "ed": 81, "text": "learning rule"}]
[{"st": 7, "ed": 9, "text": "unsupervised learning"}, {"st": 42, "ed": 44, "text": "pattern recognition"}]
[{"st": 0, "ed": 2, "text": "reinforcement learning"}, {"st": 11, "ed": 13, "text": "positive results"}, {"st": 37, "ed": 39, "text": "linear function"}, {"st": 60, "ed": 62, "text": "control problem"}, {"st": 70, "ed": 72, "text": "gaussian noise"}]
[{"st": 1, "ed": 3, "text": "human brain"}, {"st": 66, "ed": 68, "text": "learning systems"}, {"st": 106, "ed": 108, "text": "neural networks"}]
[{"st": 1, "ed": 3, "text": "evolutionary algorithms"}, {"st": 23, "ed": 26, "text": "rate of convergence"}, {"st": 37, "ed": 39, "text": "selection pressure"}, {"st": 52, "ed": 54, "text": "local optimum"}, {"st": 72, "ed": 74, "text": "conceptually simple"}]
[{"st": 5, "ed": 9, "text": "prediction with expert advice"}, {"st": 10, "ed": 12, "text": "learning rate"}, {"st": 69, "ed": 71, "text": "loss bounds"}, {"st": 73, "ed": 75, "text": "learning rate"}, {"st": 95, "ed": 97, "text": "loss bounds"}]
[{"st": 63, "ed": 65, "text": "reinforcement learning"}, {"st": 101, "ed": 103, "text": "non trivial"}, {"st": 158, "ed": 160, "text": "extensively evaluate"}, {"st": 191, "ed": 193, "text": "convergence speed"}]
[{"st": 7, "ed": 9, "text": "recent research"}, {"st": 43, "ed": 45, "text": "time series"}, {"st": 51, "ed": 53, "text": "internal representation"}, {"st": 75, "ed": 77, "text": "l1 regularization"}, {"st": 95, "ed": 97, "text": "maximum likelihood"}, {"st": 102, "ed": 104, "text": "l1 regularization"}, {"st": 105, "ed": 107, "text": "heavy tailed"}]
[{"st": 13, "ed": 15, "text": "random variables"}, {"st": 21, "ed": 23, "text": "pac bayesian"}, {"st": 36, "ed": 38, "text": "pac bayesian"}]
[{"st": 27, "ed": 29, "text": "countably infinite"}, {"st": 46, "ed": 48, "text": "loss bounds"}, {"st": 79, "ed": 81, "text": "significantly outperform"}]
[{"st": 12, "ed": 14, "text": "critical applications"}, {"st": 43, "ed": 45, "text": "decision tree"}, {"st": 46, "ed": 48, "text": "classification models"}, {"st": 55, "ed": 57, "text": "additional information"}, {"st": 67, "ed": 71, "text": "markov chain monte carlo"}, {"st": 145, "ed": 147, "text": "prior information"}, {"st": 190, "ed": 192, "text": "synthetic data"}, {"st": 208, "ed": 210, "text": "posterior probabilities"}]
[{"st": 12, "ed": 14, "text": "decision tree"}, {"st": 40, "ed": 42, "text": "machine learning"}, {"st": 44, "ed": 46, "text": "quantitative evaluation"}, {"st": 58, "ed": 60, "text": "posterior distribution"}]
[{"st": 5, "ed": 9, "text": "prediction with expert advice"}, {"st": 10, "ed": 12, "text": "learning rate"}, {"st": 70, "ed": 72, "text": "loss bounds"}, {"st": 74, "ed": 76, "text": "learning rate"}, {"st": 96, "ed": 98, "text": "loss bounds"}]
[{"st": 4, "ed": 6, "text": "least squares"}, {"st": 6, "ed": 9, "text": "support vector machines"}, {"st": 15, "ed": 17, "text": "additive models"}, {"st": 25, "ed": 27, "text": "primal dual"}, {"st": 115, "ed": 117, "text": "additive models"}]
[]
[{"st": 5, "ed": 8, "text": "sequential decision making"}, {"st": 44, "ed": 46, "text": "decision rules"}, {"st": 72, "ed": 74, "text": "finite dimensional"}, {"st": 86, "ed": 88, "text": "decision rules"}, {"st": 107, "ed": 109, "text": "loss functions"}, {"st": 115, "ed": 117, "text": "log loss"}, {"st": 139, "ed": 141, "text": "decision rule"}, {"st": 165, "ed": 167, "text": "recent results"}, {"st": 184, "ed": 186, "text": "well calibrated"}, {"st": 198, "ed": 200, "text": "expected loss"}]
[{"st": 5, "ed": 7, "text": "learning dynamics"}, {"st": 10, "ed": 12, "text": "learning algorithms"}, {"st": 46, "ed": 48, "text": "matrix games"}]
[{"st": 61, "ed": 63, "text": "prediction task"}, {"st": 99, "ed": 101, "text": "small sample"}, {"st": 104, "ed": 106, "text": "prediction tasks"}]
[{"st": 4, "ed": 6, "text": "nucleic acid"}, {"st": 18, "ed": 22, "text": "regulation of gene expression"}, {"st": 34, "ed": 36, "text": "machine learning"}, {"st": 40, "ed": 42, "text": "amino acids"}, {"st": 54, "ed": 56, "text": "nucleic acids"}, {"st": 143, "ed": 145, "text": "structural information"}, {"st": 198, "ed": 200, "text": "structural information"}]
[{"st": 5, "ed": 7, "text": "reinforcement learning"}, {"st": 19, "ed": 21, "text": "past observations"}, {"st": 55, "ed": 57, "text": "sufficient conditions"}, {"st": 96, "ed": 98, "text": "reinforcement learning"}, {"st": 103, "ed": 106, "text": "markov decision processes"}]
[{"st": 5, "ed": 8, "text": "exploration and exploitation"}, {"st": 9, "ed": 11, "text": "reinforcement learning"}, {"st": 25, "ed": 29, "text": "multi armed bandit problem"}, {"st": 32, "ed": 34, "text": "infinite horizon"}, {"st": 41, "ed": 43, "text": "finite horizon"}]
[{"st": 4, "ed": 6, "text": "artificial intelligence"}, {"st": 53, "ed": 55, "text": "human intelligence"}]
[{"st": 34, "ed": 36, "text": "multi resolution"}, {"st": 89, "ed": 91, "text": "soft computing"}]
[{"st": 10, "ed": 12, "text": "board game"}, {"st": 18, "ed": 20, "text": "reinforcement learning"}]
[{"st": 0, "ed": 2, "text": "sequential decision"}, {"st": 17, "ed": 19, "text": "prior probability"}, {"st": 46, "ed": 48, "text": "parameter free"}]
[{"st": 9, "ed": 11, "text": "training set"}, {"st": 38, "ed": 40, "text": "np hard"}, {"st": 78, "ed": 80, "text": "total order"}, {"st": 99, "ed": 101, "text": "convex function"}]
[{"st": 10, "ed": 12, "text": "belief propagation"}, {"st": 25, "ed": 27, "text": "belief propagation"}, {"st": 41, "ed": 43, "text": "belief propagation"}, {"st": 56, "ed": 58, "text": "expectation propagation"}]
[{"st": 5, "ed": 8, "text": "support vector machines"}, {"st": 12, "ed": 14, "text": "working set"}, {"st": 30, "ed": 32, "text": "working set"}, {"st": 39, "ed": 41, "text": "working set"}, {"st": 56, "ed": 58, "text": "working set"}, {"st": 68, "ed": 70, "text": "experiments demonstrate"}, {"st": 72, "ed": 74, "text": "proposed method"}]
[{"st": 21, "ed": 24, "text": "undirected graphical model"}, {"st": 32, "ed": 34, "text": "maximum likelihood"}, {"st": 38, "ed": 41, "text": "l 1 norm"}, {"st": 92, "ed": 95, "text": "block coordinate descent"}, {"st": 101, "ed": 104, "text": "l 1 norm"}, {"st": 115, "ed": 117, "text": "method yields"}, {"st": 139, "ed": 142, "text": "log partition function"}, {"st": 159, "ed": 161, "text": "maximum likelihood"}, {"st": 171, "ed": 173, "text": "synthetic data"}]
[{"st": 16, "ed": 18, "text": "linear combination"}, {"st": 20, "ed": 22, "text": "input variables"}, {"st": 37, "ed": 40, "text": "principal component analysis"}, {"st": 48, "ed": 50, "text": "machine learning"}, {"st": 64, "ed": 66, "text": "greedy algorithm"}, {"st": 103, "ed": 105, "text": "sufficient conditions"}, {"st": 106, "ed": 108, "text": "global optimality"}, {"st": 128, "ed": 130, "text": "sparse recovery"}, {"st": 143, "ed": 145, "text": "globally optimal"}]
[{"st": 58, "ed": 61, "text": "point of view"}, {"st": 65, "ed": 67, "text": "social network"}, {"st": 107, "ed": 109, "text": "social network"}, {"st": 118, "ed": 120, "text": "structural properties"}, {"st": 125, "ed": 127, "text": "cluster analysis"}, {"st": 130, "ed": 132, "text": "social network"}]
[{"st": 0, "ed": 3, "text": "support vector machines"}, {"st": 8, "ed": 10, "text": "supervised classification"}, {"st": 13, "ed": 15, "text": "land cover"}, {"st": 22, "ed": 24, "text": "statistical learning"}, {"st": 61, "ed": 63, "text": "classification tasks"}, {"st": 65, "ed": 67, "text": "remote sensing"}, {"st": 101, "ed": 103, "text": "land cover"}, {"st": 127, "ed": 129, "text": "classification accuracy"}]
[{"st": 9, "ed": 11, "text": "value iteration"}, {"st": 14, "ed": 16, "text": "approximate solution"}, {"st": 18, "ed": 21, "text": "markov decision processes"}, {"st": 26, "ed": 28, "text": "value iteration"}, {"st": 37, "ed": 39, "text": "least squares"}, {"st": 68, "ed": 70, "text": "exponentially large"}, {"st": 107, "ed": 109, "text": "approximate solution"}]
[{"st": 5, "ed": 8, "text": "support vector machine"}, {"st": 20, "ed": 22, "text": "loss function"}, {"st": 26, "ed": 28, "text": "support vectors"}, {"st": 31, "ed": 33, "text": "kernel matrix"}, {"st": 45, "ed": 47, "text": "kernel learning"}, {"st": 50, "ed": 52, "text": "kernel matrices"}, {"st": 56, "ed": 58, "text": "noisy observations"}, {"st": 75, "ed": 77, "text": "solved efficiently"}, {"st": 84, "ed": 86, "text": "cutting plane"}]
[{"st": 6, "ed": 8, "text": "semi supervised"}, {"st": 8, "ed": 10, "text": "dimensionality reduction"}, {"st": 17, "ed": 20, "text": "supervised and unsupervised"}, {"st": 37, "ed": 40, "text": "labeled and unlabeled"}, {"st": 77, "ed": 79, "text": "semi supervised"}, {"st": 80, "ed": 82, "text": "framework called"}]
[{"st": 11, "ed": 13, "text": "mahalanobis distance"}, {"st": 28, "ed": 30, "text": "component analysis"}, {"st": 30, "ed": 32, "text": "large margin"}, {"st": 32, "ed": 34, "text": "nearest neighbors"}, {"st": 57, "ed": 59, "text": "framework called"}, {"st": 136, "ed": 138, "text": "mahalanobis distance"}, {"st": 140, "ed": 142, "text": "unlike previous"}, {"st": 172, "ed": 174, "text": "numerical results"}, {"st": 176, "ed": 178, "text": "real world"}]
[{"st": 41, "ed": 44, "text": "a reproducing kernel"}, {"st": 115, "ed": 117, "text": "banach space"}, {"st": 128, "ed": 130, "text": "problems including"}, {"st": 143, "ed": 145, "text": "excellent performance"}]
[{"st": 5, "ed": 7, "text": "unsupervised learning"}, {"st": 26, "ed": 30, "text": "problems in machine learning"}, {"st": 32, "ed": 35, "text": "semi supervised learning"}, {"st": 78, "ed": 81, "text": "guaranteed to converge"}, {"st": 96, "ed": 98, "text": "learning algorithm"}, {"st": 104, "ed": 106, "text": "learning rates"}, {"st": 111, "ed": 114, "text": "data generating distribution"}]
[{"st": 9, "ed": 11, "text": "starting point"}, {"st": 28, "ed": 30, "text": "prior knowledge"}, {"st": 49, "ed": 51, "text": "computationally efficient"}]
[{"st": 3, "ed": 7, "text": "prediction with expert advice"}]
[{"st": 13, "ed": 15, "text": "probability distributions"}]
[{"st": 4, "ed": 6, "text": "method called"}, {"st": 15, "ed": 18, "text": "online learning algorithms"}, {"st": 92, "ed": 94, "text": "online learning"}]
[{"st": 0, "ed": 2, "text": "multi instance"}, {"st": 8, "ed": 10, "text": "training set"}, {"st": 19, "ed": 21, "text": "previous studies"}, {"st": 72, "ed": 75, "text": "simple yet effective"}, {"st": 75, "ed": 77, "text": "multi instance"}, {"st": 120, "ed": 122, "text": "proposed method"}]
[{"st": 22, "ed": 25, "text": "multiple kernel learning"}, {"st": 38, "ed": 40, "text": "classification performance"}, {"st": 46, "ed": 48, "text": "cutting plane"}, {"st": 52, "ed": 54, "text": "kernel learning"}]
[{"st": 0, "ed": 3, "text": "hidden markov models"}, {"st": 44, "ed": 46, "text": "local optima"}, {"st": 77, "ed": 79, "text": "sample complexity"}, {"st": 124, "ed": 126, "text": "natural language"}, {"st": 147, "ed": 150, "text": "singular value decomposition"}]
[{"st": 22, "ed": 25, "text": "statistical relational learning"}, {"st": 32, "ed": 34, "text": "statistical models"}, {"st": 71, "ed": 73, "text": "computer science"}, {"st": 110, "ed": 112, "text": "instance level"}, {"st": 138, "ed": 141, "text": "orders of magnitude"}, {"st": 160, "ed": 162, "text": "bayes net"}, {"st": 176, "ed": 178, "text": "computationally feasible"}, {"st": 203, "ed": 205, "text": "bayes net"}, {"st": 209, "ed": 211, "text": "bayes net"}]
[{"st": 17, "ed": 19, "text": "partially ordered"}, {"st": 57, "ed": 59, "text": "efficient algorithms"}]
[{"st": 9, "ed": 11, "text": "class classification"}, {"st": 54, "ed": 56, "text": "square root"}]
[{"st": 7, "ed": 9, "text": "domain adaptation"}, {"st": 63, "ed": 65, "text": "rademacher complexity"}, {"st": 84, "ed": 86, "text": "generalization bounds"}, {"st": 87, "ed": 89, "text": "domain adaptation"}, {"st": 113, "ed": 116, "text": "support vector machines"}, {"st": 117, "ed": 120, "text": "kernel ridge regression"}, {"st": 139, "ed": 141, "text": "loss functions"}, {"st": 153, "ed": 155, "text": "preliminary experiments"}]
[{"st": 2, "ed": 4, "text": "time series"}, {"st": 14, "ed": 17, "text": "blind source separation"}, {"st": 23, "ed": 25, "text": "time series"}, {"st": 41, "ed": 43, "text": "time series"}, {"st": 48, "ed": 50, "text": "density function"}, {"st": 105, "ed": 107, "text": "higher order"}]
[{"st": 10, "ed": 12, "text": "ovarian cancer"}, {"st": 62, "ed": 64, "text": "classification accuracy"}, {"st": 79, "ed": 81, "text": "linear combination"}, {"st": 161, "ed": 163, "text": "proposed algorithm"}]
[{"st": 29, "ed": 31, "text": "evaluation function"}, {"st": 43, "ed": 45, "text": "chess engine"}, {"st": 47, "ed": 49, "text": "encouraging results"}, {"st": 104, "ed": 107, "text": "directions for future"}]
[{"st": 23, "ed": 25, "text": "exponential families"}, {"st": 26, "ed": 28, "text": "sufficient statistics"}, {"st": 51, "ed": 53, "text": "exact inference"}, {"st": 68, "ed": 70, "text": "max margin"}, {"st": 83, "ed": 85, "text": "sample sizes"}, {"st": 99, "ed": 101, "text": "computer vision"}, {"st": 106, "ed": 108, "text": "standard benchmark"}, {"st": 127, "ed": 129, "text": "max margin"}, {"st": 138, "ed": 140, "text": "max margin"}]
[{"st": 10, "ed": 12, "text": "reinforcement learning"}, {"st": 14, "ed": 17, "text": "markov decision processes"}, {"st": 72, "ed": 74, "text": "fixed point"}, {"st": 76, "ed": 78, "text": "value iteration"}, {"st": 89, "ed": 91, "text": "near optimal"}]
[{"st": 0, "ed": 2, "text": "artificial intelligence"}, {"st": 19, "ed": 21, "text": "machine learning"}, {"st": 27, "ed": 29, "text": "artificial intelligence"}, {"st": 44, "ed": 46, "text": "machine learning"}, {"st": 54, "ed": 56, "text": "human intelligence"}]
[{"st": 46, "ed": 48, "text": "motion capture"}]
[{"st": 16, "ed": 18, "text": "extensive experiments"}]
[{"st": 0, "ed": 2, "text": "ensemble methods"}, {"st": 9, "ed": 11, "text": "predictive accuracy"}, {"st": 17, "ed": 19, "text": "machine learning"}, {"st": 43, "ed": 45, "text": "ensemble methods"}, {"st": 77, "ed": 79, "text": "improved accuracy"}, {"st": 86, "ed": 88, "text": "linear regression"}, {"st": 103, "ed": 105, "text": "linear functions"}, {"st": 143, "ed": 145, "text": "collaborative filtering"}]
[{"st": 0, "ed": 2, "text": "machine learning"}, {"st": 17, "ed": 19, "text": "raw data"}, {"st": 82, "ed": 84, "text": "machine learning"}]
[{"st": 8, "ed": 10, "text": "artificial intelligence"}, {"st": 81, "ed": 84, "text": "kullback leibler kl"}, {"st": 192, "ed": 194, "text": "adaptive control"}]
[{"st": 4, "ed": 6, "text": "artificial intelligence"}, {"st": 18, "ed": 20, "text": "partially observable"}, {"st": 68, "ed": 70, "text": "near optimal"}, {"st": 90, "ed": 92, "text": "predictive state"}, {"st": 108, "ed": 110, "text": "vision based"}, {"st": 110, "ed": 112, "text": "mobile robot"}, {"st": 118, "ed": 120, "text": "point based"}, {"st": 149, "ed": 151, "text": "accurate prediction"}]
[{"st": 6, "ed": 8, "text": "discriminative learning"}, {"st": 11, "ed": 13, "text": "structured data"}, {"st": 28, "ed": 30, "text": "existing algorithms"}, {"st": 48, "ed": 50, "text": "partially ordered"}, {"st": 70, "ed": 72, "text": "learning algorithms"}, {"st": 103, "ed": 105, "text": "ridge regression"}, {"st": 133, "ed": 135, "text": "structured prediction"}, {"st": 145, "ed": 147, "text": "learning problems"}, {"st": 152, "ed": 155, "text": "multi label classification"}, {"st": 157, "ed": 159, "text": "hierarchical classification"}]
[{"st": 12, "ed": 14, "text": "reinforcement learning"}, {"st": 15, "ed": 17, "text": "near optimal"}, {"st": 33, "ed": 35, "text": "optimal solution"}, {"st": 66, "ed": 69, "text": "branch and bound"}]
[{"st": 4, "ed": 6, "text": "adaptive control"}, {"st": 38, "ed": 40, "text": "input output"}]
[{"st": 41, "ed": 43, "text": "content based"}, {"st": 61, "ed": 63, "text": "content based"}, {"st": 87, "ed": 89, "text": "proposed method"}]
[{"st": 39, "ed": 41, "text": "regret bounds"}, {"st": 53, "ed": 56, "text": "online convex optimization"}, {"st": 72, "ed": 74, "text": "large scale"}, {"st": 74, "ed": 76, "text": "machine learning"}]
[{"st": 19, "ed": 21, "text": "machine learning"}, {"st": 22, "ed": 24, "text": "multi label"}, {"st": 28, "ed": 30, "text": "proposed approach"}, {"st": 34, "ed": 36, "text": "decision tree"}, {"st": 48, "ed": 51, "text": "coronary heart disease"}, {"st": 54, "ed": 56, "text": "preliminary results"}]
[{"st": 5, "ed": 7, "text": "reinforcement learning"}]
[{"st": 7, "ed": 9, "text": "primal dual"}, {"st": 9, "ed": 11, "text": "message passing"}, {"st": 14, "ed": 16, "text": "large scale"}, {"st": 16, "ed": 18, "text": "structured prediction"}]
[{"st": 0, "ed": 2, "text": "online learning"}, {"st": 4, "ed": 6, "text": "increasingly popular"}, {"st": 14, "ed": 16, "text": "online learning"}, {"st": 32, "ed": 34, "text": "online learning"}, {"st": 67, "ed": 69, "text": "regret bound"}, {"st": 70, "ed": 72, "text": "strongly convex"}, {"st": 132, "ed": 134, "text": "raw data"}]
[{"st": 5, "ed": 7, "text": "multi class"}, {"st": 33, "ed": 35, "text": "feature vector"}, {"st": 40, "ed": 42, "text": "feature construction"}, {"st": 48, "ed": 50, "text": "learning algorithms"}, {"st": 79, "ed": 81, "text": "feature selection"}, {"st": 93, "ed": 95, "text": "local search"}, {"st": 105, "ed": 107, "text": "proposed method"}, {"st": 110, "ed": 112, "text": "real world"}, {"st": 124, "ed": 127, "text": "hidden markov models"}, {"st": 130, "ed": 133, "text": "conditional random fields"}]
[{"st": 7, "ed": 9, "text": "search algorithm"}, {"st": 12, "ed": 14, "text": "recent theoretical"}, {"st": 19, "ed": 21, "text": "gaussian processes"}, {"st": 35, "ed": 37, "text": "reward function"}, {"st": 55, "ed": 57, "text": "confidence intervals"}, {"st": 68, "ed": 70, "text": "upper confidence"}, {"st": 74, "ed": 76, "text": "efficient implementation"}, {"st": 82, "ed": 84, "text": "regret bounds"}, {"st": 94, "ed": 96, "text": "kernel matrix"}, {"st": 109, "ed": 111, "text": "feature space"}, {"st": 128, "ed": 130, "text": "square root"}, {"st": 132, "ed": 135, "text": "number of iterations"}, {"st": 138, "ed": 141, "text": "a logarithmic factor"}, {"st": 148, "ed": 150, "text": "gaussian kernel"}, {"st": 174, "ed": 177, "text": "markov decision processes"}, {"st": 192, "ed": 194, "text": "regret bounds"}]
[{"st": 6, "ed": 9, "text": "value function approximation"}, {"st": 12, "ed": 14, "text": "temporal difference"}, {"st": 14, "ed": 16, "text": "reinforcement learning"}, {"st": 20, "ed": 22, "text": "practical applications"}, {"st": 22, "ed": 24, "text": "reinforcement learning"}, {"st": 82, "ed": 84, "text": "feature set"}, {"st": 106, "ed": 108, "text": "reinforcement learning"}, {"st": 135, "ed": 137, "text": "predictive state"}, {"st": 137, "ed": 139, "text": "temporal difference"}, {"st": 145, "ed": 147, "text": "predictive state"}]
[{"st": 1, "ed": 3, "text": "practical applications"}, {"st": 3, "ed": 5, "text": "machine learning"}, {"st": 14, "ed": 16, "text": "domain specific"}, {"st": 38, "ed": 40, "text": "performance measures"}, {"st": 54, "ed": 56, "text": "specific performance"}, {"st": 68, "ed": 70, "text": "approach called"}, {"st": 90, "ed": 92, "text": "specific performance"}, {"st": 156, "ed": 158, "text": "performance measures"}, {"st": 161, "ed": 163, "text": "performance measure"}, {"st": 166, "ed": 168, "text": "contingency table"}, {"st": 175, "ed": 177, "text": "empirical studies"}, {"st": 185, "ed": 187, "text": "computational efficiency"}]
[{"st": 25, "ed": 27, "text": "text categorization"}, {"st": 41, "ed": 43, "text": "text categorization"}, {"st": 97, "ed": 99, "text": "based approach"}]
[{"st": 49, "ed": 52, "text": "optical character recognition"}, {"st": 69, "ed": 71, "text": "labeled training"}, {"st": 77, "ed": 79, "text": "fine tuning"}, {"st": 130, "ed": 132, "text": "probabilistic inference"}, {"st": 153, "ed": 155, "text": "machine learning"}]
[{"st": 0, "ed": 2, "text": "reinforcement learning"}, {"st": 9, "ed": 11, "text": "partially observed"}, {"st": 16, "ed": 18, "text": "learning agent"}, {"st": 33, "ed": 35, "text": "markov property"}, {"st": 44, "ed": 46, "text": "policy optimization"}, {"st": 62, "ed": 64, "text": "markov property"}, {"st": 78, "ed": 80, "text": "finite state"}, {"st": 96, "ed": 98, "text": "pac man"}, {"st": 105, "ed": 108, "text": "point of view"}]
[{"st": 6, "ed": 8, "text": "machine translation"}, {"st": 17, "ed": 19, "text": "neural network"}, {"st": 21, "ed": 23, "text": "n grams"}, {"st": 28, "ed": 30, "text": "d dimensional"}, {"st": 33, "ed": 35, "text": "n grams"}, {"st": 50, "ed": 52, "text": "n grams"}, {"st": 53, "ed": 55, "text": "n grams"}, {"st": 83, "ed": 85, "text": "natural language"}, {"st": 103, "ed": 105, "text": "neural network"}, {"st": 119, "ed": 121, "text": "efficient implementation"}, {"st": 136, "ed": 139, "text": "neural language models"}]
[{"st": 0, "ed": 2, "text": "feature selection"}, {"st": 5, "ed": 7, "text": "pre processing"}, {"st": 10, "ed": 12, "text": "pattern classification"}, {"st": 14, "ed": 16, "text": "feature selection"}, {"st": 32, "ed": 34, "text": "classification accuracy"}, {"st": 42, "ed": 44, "text": "performance metric"}, {"st": 62, "ed": 64, "text": "multi class"}, {"st": 90, "ed": 92, "text": "classification accuracy"}, {"st": 107, "ed": 109, "text": "feature selection"}, {"st": 127, "ed": 129, "text": "binary classification"}, {"st": 136, "ed": 138, "text": "feature selection"}, {"st": 143, "ed": 145, "text": "feature selection"}, {"st": 149, "ed": 152, "text": "multi class classification"}, {"st": 164, "ed": 166, "text": "specifically designed"}, {"st": 176, "ed": 178, "text": "extensive empirical"}, {"st": 187, "ed": 189, "text": "feature selection"}]
[{"st": 140, "ed": 142, "text": "greedy algorithms"}, {"st": 178, "ed": 180, "text": "domain specific"}]
[{"st": 0, "ed": 3, "text": "loopy belief propagation"}, {"st": 4, "ed": 6, "text": "approximate inference"}, {"st": 23, "ed": 25, "text": "learning algorithms"}, {"st": 48, "ed": 50, "text": "belief propagation"}, {"st": 63, "ed": 65, "text": "probability distributions"}, {"st": 72, "ed": 74, "text": "belief propagation"}, {"st": 97, "ed": 100, "text": "bethe free energy"}, {"st": 102, "ed": 104, "text": "positive definite"}, {"st": 109, "ed": 111, "text": "learning algorithms"}, {"st": 112, "ed": 114, "text": "belief propagation"}, {"st": 145, "ed": 147, "text": "belief propagation"}]
[{"st": 2, "ed": 4, "text": "dialogue policy"}, {"st": 17, "ed": 19, "text": "reinforcement learning"}, {"st": 24, "ed": 26, "text": "dialogue policy"}, {"st": 33, "ed": 35, "text": "reinforcement learning"}, {"st": 50, "ed": 52, "text": "empirical evaluation"}, {"st": 83, "ed": 85, "text": "reinforcement learning"}]
[{"st": 18, "ed": 21, "text": "ability to learn"}, {"st": 55, "ed": 57, "text": "reinforcement learning"}, {"st": 67, "ed": 69, "text": "reinforcement learning"}, {"st": 101, "ed": 103, "text": "learning agent"}, {"st": 137, "ed": 139, "text": "improved performance"}]
[{"st": 2, "ed": 4, "text": "least squares"}, {"st": 30, "ed": 32, "text": "fast convergence"}, {"st": 50, "ed": 52, "text": "reinforcement learning"}, {"st": 56, "ed": 58, "text": "reinforcement learning"}, {"st": 111, "ed": 113, "text": "multi step"}, {"st": 113, "ed": 116, "text": "temporal difference td"}, {"st": 116, "ed": 118, "text": "learning algorithm"}]
[{"st": 15, "ed": 17, "text": "temporal logic"}, {"st": 65, "ed": 67, "text": "complexity bounds"}]
[{"st": 11, "ed": 14, "text": "case based reasoning"}, {"st": 20, "ed": 22, "text": "commonsense reasoning"}, {"st": 96, "ed": 98, "text": "conventional approaches"}, {"st": 112, "ed": 114, "text": "real world"}]
[{"st": 5, "ed": 8, "text": "problem of classifying"}, {"st": 15, "ed": 17, "text": "nearest neighbor"}, {"st": 29, "ed": 31, "text": "nearest neighbor"}, {"st": 49, "ed": 51, "text": "nearest neighbor"}, {"st": 57, "ed": 59, "text": "nearest neighbor"}, {"st": 61, "ed": 63, "text": "nearest neighbor"}, {"st": 111, "ed": 113, "text": "nearest neighbor"}, {"st": 113, "ed": 115, "text": "decision rule"}, {"st": 122, "ed": 125, "text": "effective and efficient"}, {"st": 129, "ed": 131, "text": "nearest neighbor"}, {"st": 152, "ed": 154, "text": "nearest neighbor"}, {"st": 167, "ed": 170, "text": "effective and efficient"}]
[{"st": 15, "ed": 17, "text": "markov network"}, {"st": 24, "ed": 26, "text": "increasingly important"}, {"st": 27, "ed": 29, "text": "machine learning"}, {"st": 44, "ed": 47, "text": "probabilistic graphical models"}, {"st": 53, "ed": 55, "text": "probability distributions"}, {"st": 93, "ed": 95, "text": "exponential growth"}, {"st": 131, "ed": 134, "text": "networks from data"}, {"st": 144, "ed": 146, "text": "sufficiently large"}, {"st": 186, "ed": 188, "text": "open problems"}]
[{"st": 7, "ed": 9, "text": "automated reasoning"}, {"st": 32, "ed": 34, "text": "machine learning"}, {"st": 35, "ed": 37, "text": "large corpora"}, {"st": 91, "ed": 93, "text": "machine learning"}, {"st": 99, "ed": 101, "text": "kernel methods"}, {"st": 153, "ed": 155, "text": "automated reasoning"}]
[{"st": 40, "ed": 42, "text": "sparsity inducing"}, {"st": 42, "ed": 44, "text": "empirical risk"}, {"st": 51, "ed": 53, "text": "l 0"}, {"st": 56, "ed": 58, "text": "classification problem"}, {"st": 62, "ed": 64, "text": "sequential decision"}, {"st": 83, "ed": 85, "text": "multi class"}, {"st": 101, "ed": 103, "text": "linear classifier"}, {"st": 120, "ed": 122, "text": "linear models"}, {"st": 134, "ed": 136, "text": "multi class"}, {"st": 151, "ed": 153, "text": "improved performance"}]
[{"st": 1, "ed": 3, "text": "reinforcement learning"}, {"st": 22, "ed": 25, "text": "simple and effective"}, {"st": 38, "ed": 40, "text": "training set"}, {"st": 53, "ed": 55, "text": "theoretical properties"}, {"st": 75, "ed": 78, "text": "source and target"}]
[{"st": 11, "ed": 13, "text": "machine learning"}]
[{"st": 62, "ed": 64, "text": "learning algorithm"}]
[{"st": 7, "ed": 9, "text": "generative models"}, {"st": 15, "ed": 17, "text": "probabilistic programs"}, {"st": 39, "ed": 42, "text": "algebraic data type"}, {"st": 74, "ed": 76, "text": "posterior probability"}, {"st": 84, "ed": 86, "text": "previous approaches"}]
[{"st": 4, "ed": 7, "text": "support vector machines"}, {"st": 13, "ed": 15, "text": "large scale"}, {"st": 16, "ed": 18, "text": "online learning"}, {"st": 28, "ed": 30, "text": "strongly convex"}, {"st": 46, "ed": 48, "text": "low dimensional"}, {"st": 82, "ed": 84, "text": "prediction accuracy"}, {"st": 111, "ed": 113, "text": "kernel approximation"}]
[{"st": 4, "ed": 6, "text": "online learning"}, {"st": 20, "ed": 22, "text": "web search"}, {"st": 23, "ed": 25, "text": "recommender systems"}, {"st": 90, "ed": 92, "text": "learning algorithm"}, {"st": 94, "ed": 96, "text": "regret bounds"}, {"st": 98, "ed": 100, "text": "online learning"}, {"st": 107, "ed": 109, "text": "web search"}, {"st": 119, "ed": 121, "text": "interactive learning"}]
[{"st": 22, "ed": 25, "text": "real time strategy"}, {"st": 46, "ed": 48, "text": "noisy observations"}, {"st": 59, "ed": 61, "text": "machine learning"}]
[{"st": 150, "ed": 152, "text": "co occurrences"}]
[{"st": 1, "ed": 3, "text": "classification algorithm"}, {"st": 21, "ed": 23, "text": "proposed algorithm"}, {"st": 58, "ed": 60, "text": "k means"}, {"st": 65, "ed": 68, "text": "real life data"}, {"st": 88, "ed": 90, "text": "classification methods"}]
[{"st": 12, "ed": 14, "text": "recently introduced"}, {"st": 100, "ed": 102, "text": "control theory"}, {"st": 158, "ed": 160, "text": "small scale"}, {"st": 166, "ed": 168, "text": "transition probabilities"}, {"st": 203, "ed": 205, "text": "continuous state"}, {"st": 209, "ed": 211, "text": "monte carlo"}, {"st": 226, "ed": 228, "text": "gaussian processes"}, {"st": 238, "ed": 241, "text": "continuous control tasks"}]
[{"st": 8, "ed": 10, "text": "reinforcement learning"}, {"st": 19, "ed": 21, "text": "specifically designed"}, {"st": 29, "ed": 31, "text": "sample complexity"}, {"st": 41, "ed": 44, "text": "exploration and exploitation"}, {"st": 61, "ed": 63, "text": "sample efficient"}, {"st": 70, "ed": 72, "text": "theoretical analysis"}, {"st": 77, "ed": 79, "text": "generalization capabilities"}, {"st": 111, "ed": 113, "text": "gaussian processes"}, {"st": 140, "ed": 142, "text": "highly accurate"}, {"st": 173, "ed": 175, "text": "uncertainty principle"}]
[{"st": 0, "ed": 2, "text": "feature selection"}, {"st": 3, "ed": 5, "text": "reinforcement learning"}, {"st": 8, "ed": 10, "text": "basis functions"}, {"st": 26, "ed": 28, "text": "main challenges"}, {"st": 32, "ed": 34, "text": "real world"}, {"st": 39, "ed": 41, "text": "gaussian process"}, {"st": 46, "ed": 48, "text": "policy evaluation"}, {"st": 50, "ed": 52, "text": "feature selection"}, {"st": 53, "ed": 55, "text": "marginal likelihood"}, {"st": 75, "ed": 77, "text": "policy evaluation"}, {"st": 84, "ed": 86, "text": "learning task"}]
[{"st": 16, "ed": 18, "text": "empirical results"}, {"st": 20, "ed": 22, "text": "consistently outperform"}, {"st": 53, "ed": 55, "text": "base kernels"}, {"st": 75, "ed": 77, "text": "similarity measure"}, {"st": 89, "ed": 92, "text": "theoretical and empirical"}, {"st": 107, "ed": 109, "text": "efficient algorithms"}, {"st": 151, "ed": 153, "text": "theoretical results"}, {"st": 162, "ed": 164, "text": "kernel matrices"}, {"st": 189, "ed": 191, "text": "generalization bounds"}]
[{"st": 10, "ed": 12, "text": "deep architectures"}, {"st": 28, "ed": 30, "text": "human brain"}, {"st": 37, "ed": 39, "text": "local minima"}, {"st": 51, "ed": 53, "text": "higher level"}, {"st": 61, "ed": 63, "text": "highly nonlinear"}, {"st": 87, "ed": 89, "text": "deep architectures"}, {"st": 91, "ed": 93, "text": "human brain"}, {"st": 158, "ed": 160, "text": "internal representations"}, {"st": 216, "ed": 219, "text": "deep artificial neural"}]
[{"st": 3, "ed": 6, "text": "multi armed bandit"}, {"st": 129, "ed": 131, "text": "asymptotically optimal"}, {"st": 142, "ed": 144, "text": "constant factor"}]
[{"st": 1, "ed": 3, "text": "real world"}, {"st": 54, "ed": 56, "text": "problem specific"}, {"st": 81, "ed": 83, "text": "problem specific"}, {"st": 162, "ed": 164, "text": "space exploration"}]
[{"st": 7, "ed": 9, "text": "learning agents"}, {"st": 24, "ed": 26, "text": "efficiently learn"}, {"st": 43, "ed": 45, "text": "learning agents"}]
[{"st": 10, "ed": 12, "text": "board game"}, {"st": 57, "ed": 59, "text": "statistically significant"}, {"st": 87, "ed": 89, "text": "applications including"}]
[{"st": 0, "ed": 2, "text": "feature selection"}, {"st": 8, "ed": 10, "text": "machine learning"}, {"st": 14, "ed": 16, "text": "real world"}, {"st": 59, "ed": 61, "text": "feature selection"}, {"st": 80, "ed": 82, "text": "constraint satisfaction"}, {"st": 102, "ed": 104, "text": "medium sized"}, {"st": 113, "ed": 115, "text": "large datasets"}, {"st": 131, "ed": 133, "text": "optimal solution"}, {"st": 141, "ed": 143, "text": "feature selection"}, {"st": 149, "ed": 151, "text": "decision theoretic"}]
[{"st": 1, "ed": 3, "text": "reinforcement learning"}, {"st": 41, "ed": 43, "text": "reinforcement learning"}]
[{"st": 3, "ed": 5, "text": "closed form"}, {"st": 10, "ed": 12, "text": "marginal likelihood"}, {"st": 16, "ed": 18, "text": "naive bayesian"}, {"st": 22, "ed": 24, "text": "hidden states"}, {"st": 50, "ed": 52, "text": "statistical models"}, {"st": 67, "ed": 69, "text": "exponential families"}]
[{"st": 1, "ed": 3, "text": "variational approximation"}, {"st": 9, "ed": 11, "text": "approximate inference"}, {"st": 13, "ed": 15, "text": "posterior distributions"}, {"st": 47, "ed": 49, "text": "variational approximations"}, {"st": 87, "ed": 89, "text": "chain graphs"}]
[{"st": 9, "ed": 11, "text": "posterior probability"}, {"st": 124, "ed": 126, "text": "learning algorithms"}]
[{"st": 13, "ed": 15, "text": "expectation propagation"}, {"st": 26, "ed": 28, "text": "kalman filter"}, {"st": 29, "ed": 32, "text": "loopy belief propagation"}, {"st": 35, "ed": 37, "text": "belief propagation"}, {"st": 59, "ed": 62, "text": "loopy belief propagation"}, {"st": 75, "ed": 77, "text": "belief networks"}, {"st": 84, "ed": 86, "text": "expectation propagation"}, {"st": 118, "ed": 121, "text": "discrete and continuous"}, {"st": 122, "ed": 124, "text": "expectation propagation"}, {"st": 126, "ed": 128, "text": "belief propagation"}, {"st": 145, "ed": 148, "text": "gaussian mixture models"}, {"st": 149, "ed": 151, "text": "expectation propagation"}, {"st": 159, "ed": 161, "text": "computational cost"}, {"st": 164, "ed": 166, "text": "variational bayes"}, {"st": 169, "ed": 171, "text": "expectation propagation"}]
[{"st": 57, "ed": 59, "text": "importance sampling"}, {"st": 74, "ed": 76, "text": "low variance"}, {"st": 88, "ed": 90, "text": "pair wise"}, {"st": 107, "ed": 109, "text": "greedy search"}]
[{"st": 4, "ed": 6, "text": "inference algorithm"}, {"st": 12, "ed": 15, "text": "loopy belief propagation"}, {"st": 17, "ed": 19, "text": "fixed point"}, {"st": 42, "ed": 44, "text": "marginal probabilities"}, {"st": 55, "ed": 57, "text": "marginal probabilities"}, {"st": 78, "ed": 81, "text": "bethe free energy"}, {"st": 90, "ed": 93, "text": "guaranteed to converge"}, {"st": 102, "ed": 105, "text": "bethe free energy"}, {"st": 110, "ed": 112, "text": "free energy"}, {"st": 125, "ed": 127, "text": "belief propagation"}, {"st": 142, "ed": 144, "text": "belief propagation"}]
[{"st": 92, "ed": 94, "text": "reinforcement learning"}, {"st": 108, "ed": 110, "text": "empirically demonstrate"}, {"st": 134, "ed": 136, "text": "performance improvement"}]
[{"st": 12, "ed": 14, "text": "markov networks"}, {"st": 122, "ed": 124, "text": "local search"}, {"st": 133, "ed": 135, "text": "extensive empirical"}, {"st": 137, "ed": 141, "text": "synthetic and real data"}, {"st": 176, "ed": 178, "text": "real world"}, {"st": 185, "ed": 187, "text": "evolutionary algorithms"}, {"st": 189, "ed": 191, "text": "structure learning"}]
[{"st": 0, "ed": 2, "text": "decision theory"}, {"st": 34, "ed": 36, "text": "random variable"}, {"st": 38, "ed": 40, "text": "density function"}, {"st": 58, "ed": 60, "text": "density function"}, {"st": 73, "ed": 75, "text": "bayesian learning"}, {"st": 86, "ed": 89, "text": "mixture of gaussians"}, {"st": 91, "ed": 93, "text": "mixture components"}, {"st": 131, "ed": 133, "text": "search procedure"}, {"st": 161, "ed": 163, "text": "significantly smaller"}]
[{"st": 2, "ed": 4, "text": "belief networks"}, {"st": 5, "ed": 7, "text": "gibbs sampling"}, {"st": 50, "ed": 52, "text": "gibbs sampling"}, {"st": 85, "ed": 87, "text": "large networks"}, {"st": 129, "ed": 131, "text": "gibbs sampling"}]
[{"st": 16, "ed": 19, "text": "markov decision process"}, {"st": 22, "ed": 26, "text": "partially observable markov decision"}, {"st": 68, "ed": 70, "text": "policy search"}, {"st": 98, "ed": 100, "text": "policy search"}, {"st": 126, "ed": 128, "text": "theoretical results"}, {"st": 139, "ed": 141, "text": "sample complexity"}, {"st": 172, "ed": 174, "text": "empirical results"}, {"st": 186, "ed": 188, "text": "continuous state"}, {"st": 188, "ed": 190, "text": "continuous action"}, {"st": 190, "ed": 192, "text": "problem involving"}]
[{"st": 0, "ed": 3, "text": "dynamic bayesian networks"}, {"st": 56, "ed": 58, "text": "dynamic systems"}, {"st": 65, "ed": 67, "text": "partially observed"}, {"st": 78, "ed": 80, "text": "expectation maximization"}, {"st": 84, "ed": 86, "text": "computational cost"}, {"st": 106, "ed": 108, "text": "sufficient statistics"}, {"st": 123, "ed": 125, "text": "hidden variables"}, {"st": 139, "ed": 141, "text": "dynamic systems"}, {"st": 166, "ed": 168, "text": "hidden variables"}, {"st": 173, "ed": 175, "text": "empirical results"}, {"st": 186, "ed": 188, "text": "complex systems"}, {"st": 190, "ed": 192, "text": "computationally tractable"}]
[{"st": 7, "ed": 9, "text": "maximum likelihood"}, {"st": 17, "ed": 19, "text": "performance guarantee"}, {"st": 60, "ed": 62, "text": "np hard"}]
[{"st": 0, "ed": 2, "text": "reinforcement learning"}, {"st": 107, "ed": 109, "text": "probability distributions"}]
[{"st": 5, "ed": 8, "text": "loopy belief propagation"}, {"st": 21, "ed": 23, "text": "error correcting"}, {"st": 45, "ed": 48, "text": "loopy belief propagation"}, {"st": 65, "ed": 68, "text": "error correcting code"}, {"st": 76, "ed": 78, "text": "approximate inference"}, {"st": 98, "ed": 100, "text": "network architectures"}, {"st": 102, "ed": 104, "text": "real world"}]
[]
[{"st": 5, "ed": 7, "text": "compact representation"}, {"st": 79, "ed": 81, "text": "empirical results"}]
[{"st": 21, "ed": 24, "text": "continuous and discrete"}, {"st": 42, "ed": 45, "text": "taking into account"}]
[{"st": 19, "ed": 21, "text": "learning rate"}, {"st": 43, "ed": 45, "text": "update rule"}, {"st": 79, "ed": 81, "text": "training set"}, {"st": 91, "ed": 93, "text": "weight vector"}, {"st": 95, "ed": 97, "text": "linear combination"}]
[{"st": 3, "ed": 5, "text": "infinite horizon"}, {"st": 6, "ed": 8, "text": "optimal control"}, {"st": 17, "ed": 19, "text": "policy search"}, {"st": 24, "ed": 26, "text": "optimal policy"}, {"st": 30, "ed": 32, "text": "policy iteration"}, {"st": 55, "ed": 57, "text": "performance bounds"}, {"st": 59, "ed": 61, "text": "policy iteration"}, {"st": 76, "ed": 78, "text": "policy iteration"}, {"st": 123, "ed": 125, "text": "frac 1"}, {"st": 135, "ed": 137, "text": "non stationary"}, {"st": 138, "ed": 140, "text": "policy iteration"}, {"st": 151, "ed": 153, "text": "policy search"}, {"st": 154, "ed": 156, "text": "dynamic programming"}, {"st": 163, "ed": 165, "text": "infinite horizon"}, {"st": 173, "ed": 175, "text": "non stationary"}, {"st": 205, "ed": 207, "text": "performance guarantee"}]
[{"st": 0, "ed": 3, "text": "markov random fields"}, {"st": 5, "ed": 7, "text": "compact representation"}, {"st": 8, "ed": 10, "text": "joint probability"}, {"st": 42, "ed": 44, "text": "important issue"}, {"st": 64, "ed": 66, "text": "context specific"}, {"st": 109, "ed": 112, "text": "markov random field"}, {"st": 132, "ed": 134, "text": "main contribution"}, {"st": 135, "ed": 137, "text": "context specific"}]
[{"st": 49, "ed": 51, "text": "performance guarantees"}, {"st": 85, "ed": 87, "text": "reduction techniques"}, {"st": 97, "ed": 99, "text": "real valued"}, {"st": 117, "ed": 119, "text": "training set"}, {"st": 138, "ed": 140, "text": "experimental evaluation"}]
[{"st": 0, "ed": 2, "text": "hierarchical classification"}, {"st": 4, "ed": 7, "text": "problem of classifying"}, {"st": 14, "ed": 16, "text": "important issue"}, {"st": 17, "ed": 19, "text": "hierarchical classification"}, {"st": 24, "ed": 26, "text": "classification algorithms"}, {"st": 37, "ed": 39, "text": "evaluation measures"}, {"st": 43, "ed": 45, "text": "hierarchical classification"}, {"st": 59, "ed": 61, "text": "hierarchical classification"}, {"st": 105, "ed": 107, "text": "large datasets"}, {"st": 114, "ed": 116, "text": "empirical results"}, {"st": 121, "ed": 123, "text": "existing approaches"}]
[{"st": 37, "ed": 39, "text": "adaptive control"}, {"st": 74, "ed": 76, "text": "optimal solution"}, {"st": 146, "ed": 149, "text": "under mild assumptions"}]
[{"st": 5, "ed": 8, "text": "temporal difference learning"}, {"st": 16, "ed": 18, "text": "variational principle"}, {"st": 41, "ed": 44, "text": "temporal difference learning"}, {"st": 76, "ed": 78, "text": "learning rate"}, {"st": 90, "ed": 92, "text": "learning rule"}, {"st": 117, "ed": 119, "text": "temporal difference"}, {"st": 149, "ed": 151, "text": "learning rate"}]
[{"st": 2, "ed": 4, "text": "based approaches"}, {"st": 10, "ed": 12, "text": "reinforcement learning"}, {"st": 28, "ed": 30, "text": "convergence properties"}, {"st": 41, "ed": 43, "text": "temporal difference"}, {"st": 63, "ed": 65, "text": "linear function"}, {"st": 79, "ed": 81, "text": "temporal difference"}, {"st": 136, "ed": 138, "text": "temporal difference"}, {"st": 156, "ed": 158, "text": "proposed approach"}, {"st": 192, "ed": 194, "text": "temporal difference"}, {"st": 213, "ed": 215, "text": "reinforcement learning"}]
[{"st": 0, "ed": 2, "text": "ensemble learning"}, {"st": 5, "ed": 7, "text": "generalization ability"}, {"st": 23, "ed": 25, "text": "base learners"}, {"st": 35, "ed": 37, "text": "unlabeled data"}, {"st": 41, "ed": 43, "text": "ensemble learning"}, {"st": 54, "ed": 56, "text": "semi supervised"}, {"st": 56, "ed": 58, "text": "ensemble method"}, {"st": 62, "ed": 64, "text": "unlike existing"}, {"st": 64, "ed": 66, "text": "semi supervised"}, {"st": 66, "ed": 68, "text": "ensemble methods"}, {"st": 69, "ed": 71, "text": "error prone"}, {"st": 76, "ed": 78, "text": "unlabeled data"}, {"st": 81, "ed": 83, "text": "labeled data"}, {"st": 92, "ed": 94, "text": "base learners"}, {"st": 95, "ed": 97, "text": "labeled data"}, {"st": 112, "ed": 114, "text": "unlabeled data"}, {"st": 115, "ed": 117, "text": "ensemble learning"}, {"st": 119, "ed": 121, "text": "highly competitive"}, {"st": 124, "ed": 126, "text": "semi supervised"}]
[{"st": 1, "ed": 3, "text": "article presents"}, {"st": 19, "ed": 21, "text": "graphical model"}, {"st": 41, "ed": 43, "text": "inference algorithms"}, {"st": 45, "ed": 47, "text": "inference algorithms"}, {"st": 48, "ed": 50, "text": "efficiently compute"}, {"st": 106, "ed": 109, "text": "entities and relations"}, {"st": 142, "ed": 144, "text": "starting point"}]
[{"st": 6, "ed": 8, "text": "quadratic programming"}, {"st": 11, "ed": 13, "text": "semi supervised"}, {"st": 13, "ed": 16, "text": "support vector machine"}, {"st": 25, "ed": 27, "text": "efficiently solved"}, {"st": 53, "ed": 56, "text": "semi supervised learning"}, {"st": 66, "ed": 69, "text": "semi supervised learning"}, {"st": 87, "ed": 89, "text": "submodular optimization"}, {"st": 134, "ed": 136, "text": "significant improvement"}]
[{"st": 0, "ed": 3, "text": "artificial general intelligence"}, {"st": 46, "ed": 48, "text": "asymptotically optimal"}]
[{"st": 1, "ed": 3, "text": "junction tree"}, {"st": 13, "ed": 15, "text": "probability distributions"}, {"st": 23, "ed": 25, "text": "junction tree"}, {"st": 32, "ed": 34, "text": "junction tree"}, {"st": 50, "ed": 52, "text": "belief propagation"}, {"st": 68, "ed": 70, "text": "belief propagation"}, {"st": 198, "ed": 200, "text": "significantly faster"}]
[{"st": 0, "ed": 2, "text": "recent research"}, {"st": 12, "ed": 14, "text": "significantly outperformed"}, {"st": 33, "ed": 35, "text": "supervised learning"}, {"st": 38, "ed": 40, "text": "features extracted"}, {"st": 66, "ed": 68, "text": "constraint satisfaction"}, {"st": 68, "ed": 70, "text": "optimization problem"}, {"st": 84, "ed": 86, "text": "empirical results"}]
[{"st": 45, "ed": 47, "text": "machine learning"}, {"st": 48, "ed": 51, "text": "decision support systems"}, {"st": 56, "ed": 59, "text": "advantages and disadvantages"}, {"st": 61, "ed": 63, "text": "previous methods"}, {"st": 125, "ed": 127, "text": "majority voting"}, {"st": 131, "ed": 133, "text": "post processing"}, {"st": 144, "ed": 146, "text": "decision trees"}, {"st": 162, "ed": 164, "text": "computational complexity"}]
[{"st": 16, "ed": 19, "text": "markov decision process"}, {"st": 29, "ed": 31, "text": "dynamical systems"}, {"st": 33, "ed": 35, "text": "low dimensional"}, {"st": 37, "ed": 39, "text": "latent factors"}]
[{"st": 46, "ed": 48, "text": "rule based"}, {"st": 54, "ed": 56, "text": "cross entropy"}, {"st": 59, "ed": 61, "text": "global optimization"}, {"st": 70, "ed": 72, "text": "hand crafted"}, {"st": 111, "ed": 113, "text": "low complexity"}, {"st": 115, "ed": 117, "text": "low complexity"}, {"st": 133, "ed": 135, "text": "theoretical framework"}]
[{"st": 3, "ed": 6, "text": "support vector machines"}, {"st": 17, "ed": 19, "text": "robust optimization"}, {"st": 26, "ed": 28, "text": "robust optimization"}, {"st": 77, "ed": 79, "text": "robust optimization"}]
[{"st": 43, "ed": 45, "text": "fully supervised"}, {"st": 45, "ed": 47, "text": "binary classification"}, {"st": 50, "ed": 52, "text": "partial information"}, {"st": 73, "ed": 75, "text": "k 1"}, {"st": 108, "ed": 111, "text": "training and test"}]
[{"st": 25, "ed": 27, "text": "learning task"}, {"st": 44, "ed": 46, "text": "multiple datasets"}, {"st": 92, "ed": 94, "text": "learning task"}, {"st": 118, "ed": 120, "text": "algorithmic framework"}, {"st": 125, "ed": 127, "text": "kernel methods"}]
[{"st": 5, "ed": 7, "text": "decision theoretic"}, {"st": 7, "ed": 9, "text": "online learning"}, {"st": 13, "ed": 15, "text": "practical applications"}, {"st": 37, "ed": 39, "text": "learning rate"}, {"st": 45, "ed": 47, "text": "online learning"}, {"st": 48, "ed": 50, "text": "practical applications"}, {"st": 84, "ed": 86, "text": "parameter free"}]
[{"st": 1, "ed": 3, "text": "reinforcement learning"}]
[{"st": 7, "ed": 9, "text": "conditional probability"}, {"st": 47, "ed": 49, "text": "regret bound"}, {"st": 65, "ed": 67, "text": "online algorithm"}]
[{"st": 22, "ed": 24, "text": "underlying structure"}, {"st": 44, "ed": 46, "text": "special case"}, {"st": 74, "ed": 76, "text": "feature selection"}, {"st": 80, "ed": 82, "text": "ell 0"}, {"st": 82, "ed": 84, "text": "regularization methods"}, {"st": 89, "ed": 92, "text": "minimum description length"}, {"st": 102, "ed": 104, "text": "feature selection"}, {"st": 109, "ed": 111, "text": "approach called"}, {"st": 140, "ed": 143, "text": "synthetic and real"}, {"st": 150, "ed": 152, "text": "prediction error"}, {"st": 210, "ed": 212, "text": "synthetic data"}]
[{"st": 7, "ed": 9, "text": "classification methods"}, {"st": 20, "ed": 22, "text": "distance metric"}, {"st": 38, "ed": 40, "text": "widely applied"}, {"st": 43, "ed": 45, "text": "relevant information"}, {"st": 72, "ed": 75, "text": "scale to large"}, {"st": 85, "ed": 87, "text": "feature mapping"}, {"st": 91, "ed": 94, "text": "deep neural network"}, {"st": 96, "ed": 99, "text": "restricted boltzmann machines"}, {"st": 105, "ed": 107, "text": "large margin"}, {"st": 132, "ed": 134, "text": "handwritten digit"}, {"st": 144, "ed": 146, "text": "large margin"}, {"st": 156, "ed": 158, "text": "deep autoencoder"}]
[{"st": 5, "ed": 8, "text": "hidden markov model"}, {"st": 23, "ed": 25, "text": "dynamical systems"}, {"st": 32, "ed": 34, "text": "predictive distributions"}, {"st": 45, "ed": 47, "text": "latent state"}, {"st": 53, "ed": 55, "text": "transition matrix"}, {"st": 67, "ed": 69, "text": "dimensional subspace"}, {"st": 76, "ed": 78, "text": "predictive distributions"}, {"st": 82, "ed": 84, "text": "latent state"}, {"st": 106, "ed": 108, "text": "computationally efficient"}, {"st": 125, "ed": 127, "text": "recently proposed"}, {"st": 128, "ed": 130, "text": "learning algorithm"}, {"st": 158, "ed": 160, "text": "local optima"}, {"st": 164, "ed": 166, "text": "performance guarantees"}, {"st": 184, "ed": 186, "text": "kernel density"}, {"st": 215, "ed": 217, "text": "synthetic data"}, {"st": 235, "ed": 237, "text": "compare favorably"}]
[{"st": 6, "ed": 8, "text": "kernel machines"}, {"st": 29, "ed": 31, "text": "multiple layers"}, {"st": 35, "ed": 37, "text": "kernel learning"}, {"st": 54, "ed": 56, "text": "linear combinations"}, {"st": 73, "ed": 77, "text": "reproducing kernel hilbert spaces"}, {"st": 80, "ed": 82, "text": "input output"}, {"st": 97, "ed": 99, "text": "kernel machines"}, {"st": 102, "ed": 104, "text": "kernel function"}, {"st": 114, "ed": 117, "text": "multiple kernel learning"}, {"st": 124, "ed": 126, "text": "machine learning"}, {"st": 130, "ed": 133, "text": "multiple kernel learning"}, {"st": 141, "ed": 143, "text": "kernel machines"}, {"st": 155, "ed": 158, "text": "simple and effective"}, {"st": 158, "ed": 161, "text": "multiple kernel learning"}, {"st": 161, "ed": 163, "text": "method called"}, {"st": 165, "ed": 167, "text": "least squares"}, {"st": 177, "ed": 179, "text": "learning problems"}, {"st": 183, "ed": 185, "text": "open source"}, {"st": 196, "ed": 198, "text": "user interface"}]
[{"st": 11, "ed": 13, "text": "class classification"}, {"st": 72, "ed": 74, "text": "false positive"}, {"st": 75, "ed": 77, "text": "false alarm"}, {"st": 81, "ed": 83, "text": "false negative"}, {"st": 94, "ed": 97, "text": "zero sum game"}, {"st": 132, "ed": 134, "text": "class classification"}]
[{"st": 26, "ed": 28, "text": "control problem"}, {"st": 31, "ed": 33, "text": "continuous state"}, {"st": 57, "ed": 60, "text": "a sufficient condition"}, {"st": 109, "ed": 111, "text": "policy gradient"}, {"st": 123, "ed": 125, "text": "weight update"}, {"st": 130, "ed": 132, "text": "weight update"}, {"st": 143, "ed": 145, "text": "reinforcement learning"}, {"st": 150, "ed": 152, "text": "control problems"}, {"st": 160, "ed": 162, "text": "smooth function"}]
[{"st": 28, "ed": 30, "text": "feature selection"}, {"st": 95, "ed": 97, "text": "extensive experimental"}]
[{"st": 7, "ed": 9, "text": "natural language"}, {"st": 63, "ed": 65, "text": "constant factor"}, {"st": 72, "ed": 74, "text": "ensemble method"}, {"st": 95, "ed": 97, "text": "theoretical justification"}, {"st": 123, "ed": 125, "text": "learning algorithms"}]
[{"st": 1, "ed": 4, "text": "undirected graphical models"}, {"st": 33, "ed": 35, "text": "machine learning"}, {"st": 56, "ed": 58, "text": "main contribution"}, {"st": 63, "ed": 65, "text": "proposed method"}, {"st": 105, "ed": 107, "text": "graphical model"}, {"st": 123, "ed": 125, "text": "conditional independence"}, {"st": 168, "ed": 170, "text": "higher order"}]
[{"st": 2, "ed": 4, "text": "finite horizon"}, {"st": 4, "ed": 7, "text": "markov decision processes"}, {"st": 8, "ed": 10, "text": "performance measures"}, {"st": 53, "ed": 55, "text": "np hard"}, {"st": 60, "ed": 62, "text": "np hard"}]
[{"st": 6, "ed": 8, "text": "asymptotically optimal"}, {"st": 8, "ed": 10, "text": "reinforcement learning"}, {"st": 22, "ed": 24, "text": "asymptotically optimal"}]
[{"st": 12, "ed": 14, "text": "meta learning"}, {"st": 30, "ed": 32, "text": "learning algorithm"}, {"st": 40, "ed": 42, "text": "data mining"}, {"st": 95, "ed": 97, "text": "data mining"}, {"st": 104, "ed": 106, "text": "meta learning"}, {"st": 109, "ed": 112, "text": "classification and regression"}, {"st": 158, "ed": 161, "text": "cold start problem"}]
[{"st": 0, "ed": 3, "text": "multi view learning"}, {"st": 4, "ed": 6, "text": "typically assume"}, {"st": 47, "ed": 49, "text": "multi view"}, {"st": 65, "ed": 67, "text": "pairwise constraints"}, {"st": 78, "ed": 80, "text": "similarity measure"}, {"st": 136, "ed": 138, "text": "learning process"}, {"st": 141, "ed": 143, "text": "approach produces"}, {"st": 158, "ed": 160, "text": "significantly improves"}, {"st": 160, "ed": 162, "text": "clustering performance"}, {"st": 171, "ed": 173, "text": "multi view"}, {"st": 209, "ed": 211, "text": "clustering performance"}, {"st": 215, "ed": 217, "text": "multi view"}]
[{"st": 36, "ed": 38, "text": "prior knowledge"}, {"st": 57, "ed": 59, "text": "multiple choice"}, {"st": 101, "ed": 104, "text": "probabilistic graphical model"}, {"st": 110, "ed": 112, "text": "decision theoretic"}, {"st": 131, "ed": 133, "text": "em algorithm"}, {"st": 134, "ed": 136, "text": "jointly learn"}, {"st": 147, "ed": 149, "text": "multiple tasks"}, {"st": 155, "ed": 158, "text": "amazon mechanical turk"}, {"st": 195, "ed": 197, "text": "em algorithm"}, {"st": 198, "ed": 200, "text": "majority voting"}]
[{"st": 29, "ed": 31, "text": "collected data"}, {"st": 171, "ed": 173, "text": "learning algorithm"}]
[{"st": 65, "ed": 68, "text": "recurrent neural network"}, {"st": 75, "ed": 77, "text": "computational problem"}, {"st": 124, "ed": 126, "text": "problem solving"}, {"st": 145, "ed": 147, "text": "open ended"}]
[{"st": 8, "ed": 10, "text": "multi objective"}, {"st": 10, "ed": 12, "text": "reinforcement learning"}, {"st": 25, "ed": 27, "text": "artificial intelligence"}, {"st": 43, "ed": 45, "text": "multi objective"}, {"st": 54, "ed": 56, "text": "based approach"}, {"st": 71, "ed": 73, "text": "policy gradient"}, {"st": 73, "ed": 75, "text": "multi objective"}, {"st": 113, "ed": 115, "text": "gradient based"}, {"st": 130, "ed": 132, "text": "parameter space"}, {"st": 164, "ed": 166, "text": "non trivial"}, {"st": 185, "ed": 187, "text": "proposed approach"}, {"st": 188, "ed": 190, "text": "empirically evaluated"}]
[{"st": 5, "ed": 7, "text": "pairwise constraint"}, {"st": 13, "ed": 15, "text": "constraint propagation"}, {"st": 21, "ed": 24, "text": "semi supervised learning"}, {"st": 37, "ed": 40, "text": "k nearest neighbor"}, {"st": 54, "ed": 56, "text": "pairwise constraints"}, {"st": 66, "ed": 68, "text": "pairwise constraints"}, {"st": 78, "ed": 80, "text": "pairwise constraints"}, {"st": 86, "ed": 88, "text": "similarity matrix"}, {"st": 96, "ed": 98, "text": "constraint propagation"}, {"st": 100, "ed": 102, "text": "source data"}, {"st": 110, "ed": 112, "text": "constraint propagation"}, {"st": 114, "ed": 116, "text": "source data"}, {"st": 118, "ed": 120, "text": "pairwise constraint"}, {"st": 134, "ed": 136, "text": "constraint propagation"}, {"st": 141, "ed": 143, "text": "cross modal"}]
[{"st": 8, "ed": 10, "text": "linear regression"}, {"st": 29, "ed": 31, "text": "complexity bounds"}, {"st": 52, "ed": 54, "text": "reinforcement learning"}, {"st": 59, "ed": 61, "text": "linear regression"}, {"st": 67, "ed": 69, "text": "reward function"}, {"st": 83, "ed": 85, "text": "object oriented"}, {"st": 105, "ed": 107, "text": "linear regression"}, {"st": 117, "ed": 119, "text": "models including"}, {"st": 126, "ed": 128, "text": "reward functions"}]
[{"st": 3, "ed": 6, "text": "loopy belief propagation"}, {"st": 17, "ed": 19, "text": "recent works"}, {"st": 33, "ed": 35, "text": "local optima"}, {"st": 41, "ed": 43, "text": "free energy"}, {"st": 47, "ed": 49, "text": "inference procedures"}, {"st": 97, "ed": 99, "text": "compare favorably"}, {"st": 105, "ed": 107, "text": "free energy"}]
[{"st": 0, "ed": 2, "text": "message passing"}, {"st": 9, "ed": 11, "text": "approximate inference"}, {"st": 57, "ed": 59, "text": "message passing"}, {"st": 70, "ed": 72, "text": "message passing"}, {"st": 139, "ed": 141, "text": "sum product"}]
[{"st": 9, "ed": 11, "text": "real world"}, {"st": 126, "ed": 128, "text": "training set"}, {"st": 134, "ed": 136, "text": "logistic regression"}, {"st": 153, "ed": 155, "text": "real world"}, {"st": 155, "ed": 157, "text": "collaborative filtering"}, {"st": 161, "ed": 163, "text": "proposed approach"}]
[{"st": 29, "ed": 32, "text": "conditional random field"}, {"st": 38, "ed": 41, "text": "conditional random fields"}, {"st": 81, "ed": 83, "text": "generative models"}, {"st": 84, "ed": 86, "text": "training data"}, {"st": 100, "ed": 102, "text": "generative models"}, {"st": 109, "ed": 112, "text": "positive and negative"}]
[{"st": 1, "ed": 3, "text": "belief propagation"}, {"st": 12, "ed": 14, "text": "approximate inference"}, {"st": 72, "ed": 74, "text": "recently introduced"}, {"st": 89, "ed": 91, "text": "free energy"}, {"st": 96, "ed": 98, "text": "fixed points"}, {"st": 120, "ed": 122, "text": "computational complexity"}, {"st": 153, "ed": 155, "text": "improved accuracy"}]
[{"st": 1, "ed": 3, "text": "dynamical systems"}, {"st": 20, "ed": 22, "text": "predictive state"}, {"st": 26, "ed": 28, "text": "recently introduced"}, {"st": 37, "ed": 39, "text": "key idea"}, {"st": 43, "ed": 45, "text": "closely related"}, {"st": 91, "ed": 93, "text": "hidden state"}, {"st": 153, "ed": 156, "text": "directions for future"}]
[{"st": 8, "ed": 10, "text": "monte carlo"}, {"st": 16, "ed": 19, "text": "markov decision processes"}, {"st": 28, "ed": 32, "text": "multi armed bandit problem"}, {"st": 77, "ed": 79, "text": "simple regret"}, {"st": 91, "ed": 94, "text": "multi armed bandits"}, {"st": 100, "ed": 102, "text": "simple regret"}, {"st": 153, "ed": 155, "text": "non trivial"}, {"st": 172, "ed": 174, "text": "sampling scheme"}, {"st": 184, "ed": 186, "text": "empirical evaluation"}]
[{"st": 8, "ed": 10, "text": "monte carlo"}, {"st": 16, "ed": 19, "text": "markov decision processes"}, {"st": 28, "ed": 32, "text": "multi armed bandit problem"}, {"st": 87, "ed": 89, "text": "empirical evaluation"}]
[{"st": 1, "ed": 3, "text": "probabilistic logic"}, {"st": 8, "ed": 10, "text": "increasing attention"}, {"st": 48, "ed": 50, "text": "structure learning"}, {"st": 51, "ed": 53, "text": "probabilistic logic"}, {"st": 74, "ed": 76, "text": "greedy search"}, {"st": 83, "ed": 85, "text": "log likelihood"}, {"st": 95, "ed": 97, "text": "log likelihood"}, {"st": 99, "ed": 101, "text": "expectation maximization"}, {"st": 110, "ed": 112, "text": "real world"}, {"st": 139, "ed": 141, "text": "achieves higher"}, {"st": 144, "ed": 146, "text": "precision recall"}]
[{"st": 48, "ed": 50, "text": "based methods"}, {"st": 120, "ed": 122, "text": "time series"}, {"st": 169, "ed": 171, "text": "real life"}, {"st": 171, "ed": 173, "text": "time series"}]
[{"st": 139, "ed": 141, "text": "fully automated"}]
[{"st": 0, "ed": 2, "text": "differentially private"}, {"st": 2, "ed": 4, "text": "collaborative filtering"}, {"st": 6, "ed": 8, "text": "challenging task"}, {"st": 23, "ed": 25, "text": "differentially private"}, {"st": 34, "ed": 36, "text": "differential privacy"}, {"st": 37, "ed": 39, "text": "bayesian posterior"}, {"st": 58, "ed": 60, "text": "systems design"}, {"st": 64, "ed": 66, "text": "power law"}]
[{"st": 0, "ed": 2, "text": "least squares"}, {"st": 3, "ed": 6, "text": "support vector machine"}, {"st": 25, "ed": 27, "text": "least squares"}, {"st": 56, "ed": 58, "text": "classification tasks"}, {"st": 67, "ed": 69, "text": "real world"}, {"st": 73, "ed": 75, "text": "real world"}, {"st": 75, "ed": 77, "text": "classification problems"}, {"st": 93, "ed": 95, "text": "real world"}, {"st": 159, "ed": 162, "text": "each data point"}, {"st": 196, "ed": 198, "text": "real world"}]
[{"st": 69, "ed": 72, "text": "theoretical and empirical"}]
[{"st": 14, "ed": 16, "text": "reinforcement learning"}, {"st": 18, "ed": 20, "text": "historical data"}, {"st": 36, "ed": 38, "text": "historical data"}, {"st": 65, "ed": 68, "text": "orders of magnitude"}, {"st": 69, "ed": 72, "text": "mean squared error"}, {"st": 73, "ed": 75, "text": "existing methods"}, {"st": 114, "ed": 116, "text": "importance sampling"}]
[{"st": 7, "ed": 10, "text": "markov decision processes"}, {"st": 70, "ed": 73, "text": "discrete and continuous"}, {"st": 84, "ed": 86, "text": "theoretical analyses"}, {"st": 94, "ed": 96, "text": "error bounds"}]
[{"st": 28, "ed": 30, "text": "recently proposed"}, {"st": 30, "ed": 33, "text": "recurrent neural network"}, {"st": 66, "ed": 68, "text": "based methods"}, {"st": 110, "ed": 112, "text": "non trivial"}, {"st": 122, "ed": 124, "text": "improved performance"}, {"st": 129, "ed": 131, "text": "rnn based"}, {"st": 148, "ed": 150, "text": "rnn based"}]
[{"st": 45, "ed": 47, "text": "network architecture"}, {"st": 58, "ed": 60, "text": "neural networks"}, {"st": 78, "ed": 80, "text": "neural network"}, {"st": 94, "ed": 96, "text": "neural networks"}, {"st": 120, "ed": 122, "text": "reverse engineering"}, {"st": 122, "ed": 124, "text": "truth table"}, {"st": 130, "ed": 132, "text": "real data"}]
[{"st": 10, "ed": 12, "text": "text documents"}, {"st": 15, "ed": 17, "text": "data stream"}, {"st": 50, "ed": 52, "text": "low dimensional"}, {"st": 52, "ed": 54, "text": "feature space"}, {"st": 76, "ed": 78, "text": "multi label"}, {"st": 125, "ed": 127, "text": "real world"}, {"st": 127, "ed": 129, "text": "text data"}, {"st": 150, "ed": 152, "text": "representation space"}, {"st": 155, "ed": 157, "text": "predictive performance"}, {"st": 178, "ed": 180, "text": "tf idf"}, {"st": 195, "ed": 197, "text": "significantly fewer"}, {"st": 202, "ed": 204, "text": "method achieves"}, {"st": 208, "ed": 210, "text": "f measure"}]
[{"st": 10, "ed": 12, "text": "large scale"}, {"st": 30, "ed": 32, "text": "large scale"}, {"st": 36, "ed": 38, "text": "convex problems"}, {"st": 43, "ed": 46, "text": "deep neural network"}, {"st": 50, "ed": 53, "text": "convolutional neural network"}, {"st": 91, "ed": 93, "text": "sparse models"}, {"st": 119, "ed": 121, "text": "optimal solution"}, {"st": 161, "ed": 163, "text": "convergence speed"}, {"st": 172, "ed": 174, "text": "learning problems"}]
[{"st": 8, "ed": 10, "text": "puzzle game"}, {"st": 36, "ed": 38, "text": "discrete state"}, {"st": 38, "ed": 41, "text": "markov decision processes"}, {"st": 69, "ed": 72, "text": "temporal difference learning"}, {"st": 85, "ed": 87, "text": "significantly improved"}, {"st": 88, "ed": 90, "text": "temporal coherence"}, {"st": 91, "ed": 93, "text": "multi stage"}, {"st": 126, "ed": 128, "text": "learning process"}, {"st": 172, "ed": 174, "text": "discrete state"}]
[{"st": 51, "ed": 53, "text": "neural network"}, {"st": 83, "ed": 85, "text": "neural network"}, {"st": 176, "ed": 178, "text": "random projections"}, {"st": 193, "ed": 195, "text": "neural networks"}, {"st": 196, "ed": 198, "text": "related methods"}]
[{"st": 9, "ed": 12, "text": "cold start problem"}, {"st": 17, "ed": 19, "text": "contextual bandit"}, {"st": 21, "ed": 23, "text": "contextual bandit"}, {"st": 82, "ed": 84, "text": "approach achieves"}, {"st": 86, "ed": 88, "text": "regret bound"}, {"st": 102, "ed": 104, "text": "real world"}]
[{"st": 31, "ed": 33, "text": "procedural knowledge"}, {"st": 58, "ed": 60, "text": "task oriented"}]
[{"st": 3, "ed": 5, "text": "lifelong learning"}, {"st": 13, "ed": 15, "text": "transfer knowledge"}, {"st": 24, "ed": 26, "text": "previously learned"}, {"st": 42, "ed": 44, "text": "video game"}, {"st": 51, "ed": 53, "text": "lifelong learning"}, {"st": 72, "ed": 75, "text": "deep reinforcement learning"}, {"st": 117, "ed": 119, "text": "lifelong learning"}, {"st": 141, "ed": 143, "text": "sample complexity"}, {"st": 147, "ed": 150, "text": "deep q network"}]
[{"st": 26, "ed": 29, "text": "training and testing"}, {"st": 31, "ed": 33, "text": "embedding models"}, {"st": 73, "ed": 75, "text": "embedding models"}, {"st": 135, "ed": 137, "text": "large margin"}]
[{"st": 62, "ed": 64, "text": "training data"}, {"st": 72, "ed": 74, "text": "linear regression"}, {"st": 91, "ed": 93, "text": "originally designed"}, {"st": 145, "ed": 147, "text": "real world"}]
[{"st": 7, "ed": 9, "text": "multi instance"}, {"st": 9, "ed": 12, "text": "multi label learning"}, {"st": 72, "ed": 74, "text": "problems involving"}]
[{"st": 109, "ed": 111, "text": "batch setting"}, {"st": 122, "ed": 125, "text": "number of iterations"}, {"st": 146, "ed": 148, "text": "gaussian process"}, {"st": 178, "ed": 180, "text": "provide theoretical"}, {"st": 198, "ed": 200, "text": "method achieves"}]
[{"st": 0, "ed": 4, "text": "partially observable markov decision"}, {"st": 13, "ed": 15, "text": "real world"}, {"st": 15, "ed": 17, "text": "decision making"}, {"st": 37, "ed": 40, "text": "markov decision process"}, {"st": 80, "ed": 82, "text": "decision making"}]
[{"st": 5, "ed": 8, "text": "markov decision processes"}, {"st": 61, "ed": 63, "text": "simple regret"}, {"st": 69, "ed": 71, "text": "performance loss"}, {"st": 106, "ed": 108, "text": "simple regret"}, {"st": 115, "ed": 117, "text": "monte carlo"}, {"st": 118, "ed": 120, "text": "search algorithm"}, {"st": 127, "ed": 129, "text": "simple regret"}, {"st": 141, "ed": 143, "text": "standard state"}, {"st": 144, "ed": 146, "text": "sampling scheme"}, {"st": 161, "ed": 163, "text": "empirical evaluation"}, {"st": 169, "ed": 171, "text": "superior performance"}]
[{"st": 21, "ed": 23, "text": "extensively studied"}, {"st": 86, "ed": 90, "text": "prediction with expert advice"}, {"st": 141, "ed": 143, "text": "prior knowledge"}, {"st": 179, "ed": 181, "text": "training set"}]
[{"st": 5, "ed": 7, "text": "weight vectors"}, {"st": 11, "ed": 13, "text": "recently shown"}, {"st": 162, "ed": 164, "text": "faster convergence"}]
[{"st": 151, "ed": 153, "text": "optimization problem"}, {"st": 158, "ed": 160, "text": "soft margin"}, {"st": 183, "ed": 185, "text": "synthetic data"}, {"st": 188, "ed": 190, "text": "social network"}, {"st": 195, "ed": 197, "text": "proposed method"}, {"st": 213, "ed": 215, "text": "random noise"}]
[{"st": 8, "ed": 10, "text": "artificial intelligence"}, {"st": 15, "ed": 17, "text": "reinforcement learning"}, {"st": 41, "ed": 43, "text": "wide variety"}, {"st": 85, "ed": 87, "text": "mobile robot"}, {"st": 136, "ed": 138, "text": "off policy"}, {"st": 150, "ed": 152, "text": "recently introduced"}, {"st": 164, "ed": 166, "text": "simultaneously learn"}, {"st": 210, "ed": 212, "text": "off policy"}, {"st": 227, "ed": 229, "text": "off policy"}, {"st": 258, "ed": 260, "text": "off policy"}]
[{"st": 99, "ed": 101, "text": "key idea"}, {"st": 115, "ed": 117, "text": "dynamic programming"}]
[{"st": 8, "ed": 10, "text": "challenging problem"}, {"st": 25, "ed": 27, "text": "empirical comparison"}, {"st": 48, "ed": 51, "text": "national football league"}, {"st": 82, "ed": 84, "text": "prediction accuracy"}]
[{"st": 34, "ed": 36, "text": "et al"}, {"st": 45, "ed": 47, "text": "compact representation"}, {"st": 81, "ed": 83, "text": "real life"}]
[{"st": 0, "ed": 2, "text": "recent research"}, {"st": 10, "ed": 12, "text": "linear programming"}, {"st": 25, "ed": 27, "text": "significantly outperformed"}, {"st": 39, "ed": 41, "text": "empirical evaluation"}, {"st": 48, "ed": 50, "text": "constraint satisfaction"}, {"st": 63, "ed": 65, "text": "machine learning"}, {"st": 86, "ed": 88, "text": "evaluation metrics"}]
[{"st": 4, "ed": 6, "text": "time series"}, {"st": 22, "ed": 24, "text": "biomedical engineering"}, {"st": 27, "ed": 29, "text": "important applications"}, {"st": 35, "ed": 38, "text": "simple yet effective"}, {"st": 38, "ed": 41, "text": "bag of words"}, {"st": 48, "ed": 51, "text": "local and global"}, {"st": 58, "ed": 60, "text": "time series"}, {"st": 66, "ed": 69, "text": "bag of words"}, {"st": 76, "ed": 78, "text": "proposed method"}, {"st": 80, "ed": 82, "text": "time series"}, {"st": 92, "ed": 94, "text": "time series"}, {"st": 98, "ed": 100, "text": "time series"}, {"st": 134, "ed": 137, "text": "bag of words"}, {"st": 144, "ed": 146, "text": "structural information"}, {"st": 148, "ed": 151, "text": "local and global"}, {"st": 151, "ed": 153, "text": "structural information"}, {"st": 160, "ed": 163, "text": "bag of words"}, {"st": 182, "ed": 184, "text": "proposed method"}, {"st": 192, "ed": 195, "text": "bag of words"}]
[{"st": 17, "ed": 19, "text": "existing algorithms"}, {"st": 24, "ed": 26, "text": "look ahead"}, {"st": 29, "ed": 31, "text": "multi link"}, {"st": 31, "ed": 33, "text": "look ahead"}, {"st": 37, "ed": 39, "text": "computational complexity"}, {"st": 41, "ed": 43, "text": "learning algorithm"}, {"st": 74, "ed": 76, "text": "learning task"}, {"st": 102, "ed": 104, "text": "large datasets"}]
[{"st": 62, "ed": 64, "text": "network structure"}, {"st": 131, "ed": 133, "text": "scoring functions"}, {"st": 142, "ed": 144, "text": "empirical study"}]
[{"st": 102, "ed": 104, "text": "learning task"}]
[{"st": 8, "ed": 10, "text": "domain model"}, {"st": 50, "ed": 52, "text": "multi link"}]
[{"st": 58, "ed": 60, "text": "approximate posterior"}]
[{"st": 9, "ed": 11, "text": "belief networks"}]
[{"st": 22, "ed": 24, "text": "belief networks"}]
[{"st": 48, "ed": 50, "text": "large scale"}, {"st": 71, "ed": 73, "text": "scales linearly"}, {"st": 156, "ed": 158, "text": "inference algorithms"}]
[{"st": 4, "ed": 6, "text": "large scale"}, {"st": 18, "ed": 20, "text": "complex network"}, {"st": 43, "ed": 45, "text": "pattern formation"}, {"st": 60, "ed": 62, "text": "low level"}, {"st": 68, "ed": 70, "text": "low level"}, {"st": 77, "ed": 79, "text": "classification techniques"}, {"st": 82, "ed": 84, "text": "classification task"}, {"st": 155, "ed": 157, "text": "pattern formation"}]
[{"st": 1, "ed": 3, "text": "a weighting"}, {"st": 9, "ed": 12, "text": "stochastic gradient descent"}, {"st": 18, "ed": 20, "text": "convergence rate"}, {"st": 25, "ed": 27, "text": "strongly convex"}]
[{"st": 25, "ed": 27, "text": "data mining"}, {"st": 85, "ed": 87, "text": "multilayer perceptron"}, {"st": 103, "ed": 105, "text": "lda based"}]
[{"st": 1, "ed": 3, "text": "accurate prediction"}, {"st": 4, "ed": 6, "text": "crude oil"}, {"st": 38, "ed": 42, "text": "feed forward neural network"}, {"st": 48, "ed": 50, "text": "based method"}, {"st": 60, "ed": 62, "text": "crude oil"}, {"st": 66, "ed": 68, "text": "multi step"}, {"st": 84, "ed": 86, "text": "multiple output"}, {"st": 102, "ed": 104, "text": "multi step"}, {"st": 108, "ed": 110, "text": "crude oil"}, {"st": 119, "ed": 121, "text": "west texas"}, {"st": 122, "ed": 124, "text": "crude oil"}, {"st": 157, "ed": 159, "text": "prediction accuracy"}, {"st": 186, "ed": 188, "text": "prediction accuracy"}]
[{"st": 102, "ed": 104, "text": "learning algorithms"}, {"st": 135, "ed": 137, "text": "classification problem"}, {"st": 159, "ed": 161, "text": "character recognition"}]
[{"st": 57, "ed": 59, "text": "functional form"}, {"st": 87, "ed": 91, "text": "number of model parameters"}, {"st": 95, "ed": 97, "text": "representational power"}, {"st": 114, "ed": 116, "text": "maximum likelihood"}, {"st": 118, "ed": 120, "text": "variational bayesian"}, {"st": 124, "ed": 126, "text": "expectation maximization"}, {"st": 205, "ed": 207, "text": "sample based"}, {"st": 209, "ed": 211, "text": "sample based"}]
[{"st": 10, "ed": 12, "text": "reinforcement learning"}, {"st": 22, "ed": 24, "text": "decision making"}, {"st": 29, "ed": 31, "text": "recent years"}, {"st": 38, "ed": 40, "text": "reinforcement learning"}, {"st": 42, "ed": 45, "text": "decision support systems"}, {"st": 52, "ed": 54, "text": "conventional methods"}, {"st": 55, "ed": 57, "text": "reinforcement learning"}, {"st": 65, "ed": 68, "text": "sequential decision making"}, {"st": 77, "ed": 80, "text": "decision support systems"}, {"st": 141, "ed": 143, "text": "decision making"}, {"st": 149, "ed": 151, "text": "near optimal"}, {"st": 178, "ed": 182, "text": "synthetic and real world"}]
[{"st": 1, "ed": 3, "text": "artificial intelligence"}, {"st": 67, "ed": 69, "text": "f measure"}, {"st": 118, "ed": 120, "text": "empirical results"}, {"st": 132, "ed": 134, "text": "clustering problem"}, {"st": 140, "ed": 143, "text": "precision and recall"}]
[{"st": 0, "ed": 4, "text": "bayesian network structure learning"}, {"st": 6, "ed": 8, "text": "notoriously difficult"}, {"st": 30, "ed": 32, "text": "worst case"}, {"st": 34, "ed": 36, "text": "exact bayesian"}, {"st": 36, "ed": 38, "text": "network structure"}, {"st": 53, "ed": 55, "text": "undirected graph"}, {"st": 85, "ed": 87, "text": "score based"}, {"st": 87, "ed": 91, "text": "bayesian network structure learning"}, {"st": 108, "ed": 110, "text": "exact bayesian"}, {"st": 110, "ed": 112, "text": "network structure"}, {"st": 155, "ed": 157, "text": "exact bayesian"}, {"st": 157, "ed": 159, "text": "network structure"}, {"st": 170, "ed": 172, "text": "positive results"}, {"st": 206, "ed": 208, "text": "exact bayesian"}, {"st": 208, "ed": 210, "text": "network structure"}, {"st": 212, "ed": 214, "text": "np hard"}, {"st": 235, "ed": 237, "text": "globally optimal"}, {"st": 260, "ed": 262, "text": "local search"}]
[{"st": 15, "ed": 17, "text": "reinforcement learning"}, {"st": 58, "ed": 61, "text": "trial and error"}, {"st": 141, "ed": 143, "text": "continuous state"}, {"st": 161, "ed": 163, "text": "proposed method"}, {"st": 165, "ed": 167, "text": "complex tasks"}]
[{"st": 12, "ed": 14, "text": "active learning"}, {"st": 67, "ed": 69, "text": "sufficient conditions"}, {"st": 104, "ed": 106, "text": "efficient implementation"}, {"st": 126, "ed": 128, "text": "practical applications"}]
[{"st": 0, "ed": 2, "text": "event recognition"}, {"st": 29, "ed": 31, "text": "error prone"}, {"st": 45, "ed": 48, "text": "inductive logic programming"}, {"st": 51, "ed": 53, "text": "machine learning"}, {"st": 75, "ed": 77, "text": "event recognition"}, {"st": 80, "ed": 82, "text": "challenging task"}, {"st": 111, "ed": 113, "text": "temporal data"}, {"st": 175, "ed": 177, "text": "real life"}, {"st": 177, "ed": 179, "text": "temporal data"}, {"st": 209, "ed": 211, "text": "proposed algorithm"}, {"st": 239, "ed": 241, "text": "empirical evaluation"}, {"st": 245, "ed": 249, "text": "real and synthetic data"}, {"st": 250, "ed": 252, "text": "activity recognition"}]
[{"st": 5, "ed": 9, "text": "multi armed bandit problem"}, {"st": 26, "ed": 28, "text": "empirical study"}, {"st": 32, "ed": 35, "text": "multi armed bandit"}, {"st": 50, "ed": 52, "text": "epsilon greedy"}, {"st": 56, "ed": 58, "text": "theoretically sound"}, {"st": 151, "ed": 153, "text": "bandit algorithms"}, {"st": 159, "ed": 161, "text": "clinical trials"}, {"st": 167, "ed": 169, "text": "practical problems"}, {"st": 172, "ed": 175, "text": "multi armed bandits"}, {"st": 175, "ed": 177, "text": "bandit algorithms"}, {"st": 200, "ed": 202, "text": "clinical trial"}, {"st": 206, "ed": 208, "text": "bandit algorithms"}, {"st": 268, "ed": 270, "text": "bandit algorithms"}]
[{"st": 0, "ed": 2, "text": "recent years"}, {"st": 9, "ed": 11, "text": "probabilistic logic"}, {"st": 14, "ed": 17, "text": "statistical relational learning"}, {"st": 24, "ed": 26, "text": "structure learning"}, {"st": 34, "ed": 37, "text": "inductive logic programming"}, {"st": 39, "ed": 41, "text": "statistical learning"}, {"st": 61, "ed": 63, "text": "machine learning"}, {"st": 83, "ed": 85, "text": "current methods"}, {"st": 87, "ed": 89, "text": "joint probability"}, {"st": 93, "ed": 95, "text": "graphical model"}, {"st": 117, "ed": 119, "text": "inductive logic"}, {"st": 128, "ed": 130, "text": "feature based"}, {"st": 150, "ed": 152, "text": "benchmark datasets"}]
[{"st": 4, "ed": 6, "text": "data stream"}]
[{"st": 3, "ed": 5, "text": "random variables"}, {"st": 9, "ed": 11, "text": "joint distribution"}, {"st": 27, "ed": 29, "text": "probabilistic models"}, {"st": 31, "ed": 33, "text": "building blocks"}, {"st": 76, "ed": 78, "text": "extensive experiments"}, {"st": 91, "ed": 93, "text": "probabilistic models"}]
[{"st": 12, "ed": 14, "text": "monte carlo"}, {"st": 18, "ed": 22, "text": "mean squared error mse"}, {"st": 34, "ed": 38, "text": "multi armed bandit problem"}, {"st": 86, "ed": 88, "text": "monte carlo"}, {"st": 94, "ed": 96, "text": "previous approaches"}]
[{"st": 0, "ed": 2, "text": "recent advances"}, {"st": 4, "ed": 6, "text": "temporal difference"}, {"st": 10, "ed": 12, "text": "off policy"}, {"st": 20, "ed": 22, "text": "convergence guarantees"}]
[{"st": 16, "ed": 18, "text": "simulated annealing"}]
[{"st": 2, "ed": 4, "text": "point set"}, {"st": 123, "ed": 125, "text": "empirical study"}]
[{"st": 0, "ed": 2, "text": "active learning"}, {"st": 11, "ed": 13, "text": "supervised classification"}, {"st": 27, "ed": 29, "text": "active learning"}, {"st": 34, "ed": 36, "text": "decision boundary"}, {"st": 61, "ed": 63, "text": "active learning"}, {"st": 73, "ed": 75, "text": "statistically significant"}]
[{"st": 13, "ed": 16, "text": "self organizing map"}, {"st": 19, "ed": 22, "text": "support vector machines"}, {"st": 30, "ed": 32, "text": "raw data"}, {"st": 37, "ed": 40, "text": "statistical machine learning"}, {"st": 56, "ed": 58, "text": "clustering algorithm"}, {"st": 62, "ed": 64, "text": "input space"}, {"st": 98, "ed": 100, "text": "fuzzy inference"}, {"st": 105, "ed": 107, "text": "proposed approach"}]
[{"st": 0, "ed": 3, "text": "support vector machines"}, {"st": 16, "ed": 18, "text": "learning algorithms"}, {"st": 23, "ed": 26, "text": "support vector machines"}, {"st": 40, "ed": 43, "text": "support vector machines"}, {"st": 64, "ed": 67, "text": "support vector machine"}]
[{"st": 7, "ed": 9, "text": "machine learning"}, {"st": 12, "ed": 14, "text": "prediction models"}]
[{"st": 0, "ed": 2, "text": "submodular function"}, {"st": 6, "ed": 8, "text": "optimization problem"}, {"st": 14, "ed": 16, "text": "machine learning"}, {"st": 41, "ed": 43, "text": "large scale"}, {"st": 48, "ed": 50, "text": "convex optimization"}, {"st": 71, "ed": 73, "text": "coordinate descent"}, {"st": 79, "ed": 81, "text": "linear convergence"}, {"st": 98, "ed": 100, "text": "dimensional vector"}, {"st": 105, "ed": 107, "text": "significantly fewer"}]
[{"st": 1, "ed": 4, "text": "multi label classification"}, {"st": 30, "ed": 32, "text": "feature space"}, {"st": 49, "ed": 51, "text": "input space"}, {"st": 63, "ed": 65, "text": "multi label"}, {"st": 76, "ed": 78, "text": "input data"}, {"st": 89, "ed": 92, "text": "taking advantage of"}, {"st": 104, "ed": 106, "text": "feature space"}, {"st": 126, "ed": 128, "text": "deep learning"}, {"st": 136, "ed": 138, "text": "deep network"}, {"st": 141, "ed": 143, "text": "empirical evaluation"}]
[{"st": 76, "ed": 78, "text": "predictive model"}, {"st": 80, "ed": 82, "text": "collaborative filtering"}, {"st": 166, "ed": 168, "text": "structural properties"}, {"st": 188, "ed": 190, "text": "dynamic programming"}, {"st": 226, "ed": 228, "text": "frac 1"}, {"st": 240, "ed": 242, "text": "extensive simulations"}]
[{"st": 28, "ed": 30, "text": "as 90"}, {"st": 77, "ed": 79, "text": "machine learning"}, {"st": 89, "ed": 91, "text": "classification accuracy"}, {"st": 103, "ed": 105, "text": "machine learning"}, {"st": 106, "ed": 108, "text": "nonparametric models"}, {"st": 112, "ed": 115, "text": "high computational cost"}, {"st": 124, "ed": 126, "text": "learning task"}, {"st": 133, "ed": 135, "text": "hidden nodes"}, {"st": 164, "ed": 166, "text": "large scale"}, {"st": 175, "ed": 177, "text": "computational requirements"}, {"st": 201, "ed": 203, "text": "computational cost"}, {"st": 208, "ed": 210, "text": "classification accuracy"}]
[{"st": 6, "ed": 9, "text": "multi armed bandit"}, {"st": 13, "ed": 15, "text": "thompson sampling"}, {"st": 33, "ed": 35, "text": "regret bounds"}]
[{"st": 12, "ed": 14, "text": "gradient based"}, {"st": 17, "ed": 20, "text": "conditional random fields"}, {"st": 35, "ed": 37, "text": "based optimization"}, {"st": 79, "ed": 81, "text": "real world"}, {"st": 94, "ed": 96, "text": "online learning"}, {"st": 113, "ed": 115, "text": "efficient online"}, {"st": 127, "ed": 129, "text": "based optimization"}, {"st": 130, "ed": 133, "text": "easy to implement"}, {"st": 140, "ed": 142, "text": "theoretical guarantees"}, {"st": 162, "ed": 164, "text": "training speed"}, {"st": 184, "ed": 186, "text": "highly competitive"}, {"st": 197, "ed": 199, "text": "https github.com"}]
[{"st": 9, "ed": 11, "text": "human intelligence"}, {"st": 33, "ed": 35, "text": "complex systems"}, {"st": 92, "ed": 94, "text": "rigid body"}, {"st": 151, "ed": 153, "text": "physics engine"}, {"st": 166, "ed": 168, "text": "wide variety"}, {"st": 170, "ed": 172, "text": "real world"}]
[{"st": 4, "ed": 6, "text": "physics engine"}, {"st": 41, "ed": 43, "text": "neural network"}, {"st": 45, "ed": 47, "text": "compositional structure"}, {"st": 56, "ed": 58, "text": "physics engine"}, {"st": 73, "ed": 75, "text": "neural network"}, {"st": 80, "ed": 83, "text": "stochastic gradient descent"}, {"st": 103, "ed": 105, "text": "rigid body"}]
[{"st": 3, "ed": 6, "text": "k nearest neighbor"}, {"st": 8, "ed": 10, "text": "classification methods"}, {"st": 20, "ed": 22, "text": "pattern recognition"}, {"st": 57, "ed": 59, "text": "supervised classification"}, {"st": 85, "ed": 87, "text": "based method"}]
[{"st": 11, "ed": 13, "text": "continuous state"}, {"st": 13, "ed": 15, "text": "continuous action"}, {"st": 17, "ed": 20, "text": "markov decision processes"}, {"st": 37, "ed": 40, "text": "low computational cost"}, {"st": 150, "ed": 152, "text": "multi resolution"}, {"st": 171, "ed": 173, "text": "reinforcement learning"}, {"st": 174, "ed": 176, "text": "inverted pendulum"}, {"st": 231, "ed": 233, "text": "optimal solutions"}, {"st": 236, "ed": 238, "text": "epsilon greedy"}]
[{"st": 44, "ed": 46, "text": "previously proposed"}, {"st": 50, "ed": 52, "text": "temporal data"}, {"st": 72, "ed": 74, "text": "pattern mining"}, {"st": 81, "ed": 83, "text": "proposed method"}, {"st": 85, "ed": 87, "text": "pattern mining"}, {"st": 128, "ed": 130, "text": "proposed approach"}]
[{"st": 52, "ed": 54, "text": "transfer knowledge"}]
[{"st": 6, "ed": 8, "text": "neural networks"}, {"st": 8, "ed": 10, "text": "representation learning"}, {"st": 20, "ed": 22, "text": "representation learning"}, {"st": 34, "ed": 36, "text": "relevant information"}, {"st": 43, "ed": 46, "text": "factors of variation"}, {"st": 72, "ed": 74, "text": "applications including"}, {"st": 82, "ed": 84, "text": "novelty detection"}, {"st": 95, "ed": 97, "text": "learning algorithm"}, {"st": 108, "ed": 111, "text": "unsupervised and supervised"}, {"st": 114, "ed": 116, "text": "inductive biases"}, {"st": 165, "ed": 167, "text": "inductive biases"}, {"st": 174, "ed": 176, "text": "additional information"}, {"st": 228, "ed": 230, "text": "wide variety"}, {"st": 258, "ed": 261, "text": "supervised and unsupervised"}]
[{"st": 1, "ed": 4, "text": "national basketball association"}, {"st": 20, "ed": 22, "text": "performance metrics"}, {"st": 33, "ed": 35, "text": "performance metrics"}, {"st": 71, "ed": 73, "text": "existing methods"}, {"st": 138, "ed": 141, "text": "gaussian mixture models"}]
[{"st": 16, "ed": 18, "text": "reinforcement learning"}, {"st": 89, "ed": 91, "text": "error bound"}, {"st": 122, "ed": 124, "text": "performance guarantees"}]
[{"st": 1, "ed": 3, "text": "pattern classification"}, {"st": 30, "ed": 33, "text": "support vector machines"}, {"st": 49, "ed": 52, "text": "curse of dimensionality"}, {"st": 71, "ed": 73, "text": "learning algorithms"}, {"st": 79, "ed": 81, "text": "optimization problems"}, {"st": 92, "ed": 94, "text": "prevent overfitting"}, {"st": 101, "ed": 104, "text": "large training sets"}]
[{"st": 1, "ed": 4, "text": "statistical machine learning"}, {"st": 49, "ed": 51, "text": "world model"}, {"st": 236, "ed": 238, "text": "detection task"}]
[]
[{"st": 22, "ed": 24, "text": "ad hoc"}, {"st": 56, "ed": 58, "text": "description logic"}, {"st": 102, "ed": 105, "text": "inductive logic programming"}, {"st": 156, "ed": 159, "text": "inductive logic programming"}, {"st": 172, "ed": 175, "text": "theory and practice"}, {"st": 176, "ed": 178, "text": "logic programming"}]
[{"st": 12, "ed": 14, "text": "cost function"}, {"st": 25, "ed": 27, "text": "cost function"}, {"st": 28, "ed": 30, "text": "prediction market"}, {"st": 44, "ed": 46, "text": "expert advice"}, {"st": 77, "ed": 79, "text": "o sqrt"}, {"st": 80, "ed": 82, "text": "regret bound"}, {"st": 97, "ed": 99, "text": "cost functions"}, {"st": 109, "ed": 111, "text": "learning algorithms"}, {"st": 116, "ed": 118, "text": "cost function"}, {"st": 195, "ed": 197, "text": "accurate estimates"}]
[{"st": 16, "ed": 18, "text": "contextual bandit"}, {"st": 66, "ed": 68, "text": "learning process"}, {"st": 98, "ed": 100, "text": "historical data"}, {"st": 118, "ed": 121, "text": "real world data"}]
[{"st": 80, "ed": 82, "text": "causal discovery"}]
[{"st": 5, "ed": 8, "text": "conditional random fields"}, {"st": 13, "ed": 15, "text": "large scale"}, {"st": 47, "ed": 49, "text": "undirected models"}, {"st": 53, "ed": 55, "text": "co occurrence"}, {"st": 105, "ed": 107, "text": "competitive results"}]
[{"st": 1, "ed": 3, "text": "learning algorithm"}, {"st": 5, "ed": 7, "text": "primary school"}, {"st": 43, "ed": 45, "text": "incremental learning"}, {"st": 67, "ed": 69, "text": "machine learning"}, {"st": 85, "ed": 87, "text": "feature space"}, {"st": 90, "ed": 92, "text": "generalization ability"}]
[{"st": 2, "ed": 4, "text": "computer science"}, {"st": 48, "ed": 50, "text": "infinite set"}, {"st": 62, "ed": 64, "text": "algorithmic framework"}, {"st": 95, "ed": 97, "text": "previously learned"}, {"st": 115, "ed": 117, "text": "previously learned"}, {"st": 134, "ed": 136, "text": "previously learned"}, {"st": 153, "ed": 155, "text": "space complexity"}, {"st": 177, "ed": 179, "text": "computational costs"}, {"st": 236, "ed": 238, "text": "problem solving"}]
[{"st": 12, "ed": 14, "text": "learning algorithm"}, {"st": 64, "ed": 66, "text": "machine learning"}, {"st": 103, "ed": 105, "text": "learning algorithms"}, {"st": 141, "ed": 143, "text": "hamming distance"}, {"st": 149, "ed": 151, "text": "training data"}, {"st": 218, "ed": 220, "text": "decision trees"}]
[{"st": 2, "ed": 4, "text": "feature selection"}, {"st": 40, "ed": 42, "text": "feature selection"}, {"st": 49, "ed": 51, "text": "data model"}, {"st": 52, "ed": 54, "text": "normal distribution"}, {"st": 91, "ed": 93, "text": "feature selection"}]
[{"st": 116, "ed": 118, "text": "o sqrt"}, {"st": 121, "ed": 123, "text": "regret bound"}, {"st": 141, "ed": 143, "text": "o sqrt"}, {"st": 183, "ed": 185, "text": "non trivial"}, {"st": 188, "ed": 190, "text": "o sqrt"}, {"st": 248, "ed": 250, "text": "near optimal"}]
[{"st": 85, "ed": 87, "text": "scale free"}, {"st": 132, "ed": 134, "text": "scale free"}, {"st": 140, "ed": 142, "text": "relevant information"}, {"st": 164, "ed": 166, "text": "time series"}]
[{"st": 2, "ed": 4, "text": "infinite horizon"}, {"st": 7, "ed": 10, "text": "markov decision processes"}, {"st": 25, "ed": 27, "text": "policy iteration"}, {"st": 67, "ed": 69, "text": "policy iteration"}, {"st": 71, "ed": 73, "text": "non stationary"}, {"st": 89, "ed": 91, "text": "significant improvement"}, {"st": 109, "ed": 111, "text": "near optimal"}, {"st": 111, "ed": 113, "text": "non stationary"}, {"st": 121, "ed": 123, "text": "near optimal"}]
[]
[{"st": 11, "ed": 14, "text": "a posteriori map"}, {"st": 18, "ed": 20, "text": "random field"}, {"st": 29, "ed": 31, "text": "discrete optimization"}, {"st": 37, "ed": 39, "text": "multi dimensional"}, {"st": 74, "ed": 76, "text": "higher order"}, {"st": 76, "ed": 78, "text": "penalty functions"}, {"st": 107, "ed": 109, "text": "discrete optimization"}, {"st": 115, "ed": 117, "text": "shortest path"}, {"st": 124, "ed": 126, "text": "image segmentation"}, {"st": 143, "ed": 145, "text": "ground truth"}, {"st": 150, "ed": 152, "text": "continuous relaxation"}]
[{"st": 1, "ed": 3, "text": "computational models"}, {"st": 11, "ed": 13, "text": "source domain"}, {"st": 36, "ed": 39, "text": "long term memory"}]
[{"st": 2, "ed": 4, "text": "structured data"}, {"st": 20, "ed": 22, "text": "computational complexity"}, {"st": 24, "ed": 26, "text": "machine learning"}, {"st": 28, "ed": 30, "text": "real world"}, {"st": 40, "ed": 43, "text": "statistical relational learning"}, {"st": 95, "ed": 97, "text": "feature construction"}, {"st": 152, "ed": 154, "text": "real world"}]
[{"st": 9, "ed": 12, "text": "markov decision processes"}, {"st": 14, "ed": 16, "text": "optimal policies"}, {"st": 30, "ed": 32, "text": "transfer learning"}, {"st": 42, "ed": 44, "text": "previous tasks"}, {"st": 87, "ed": 89, "text": "optimal policy"}, {"st": 159, "ed": 161, "text": "cost function"}]
[{"st": 6, "ed": 8, "text": "expert systems"}, {"st": 10, "ed": 12, "text": "decision support"}, {"st": 33, "ed": 36, "text": "taking into account"}, {"st": 99, "ed": 102, "text": "case based reasoning"}]
[{"st": 11, "ed": 13, "text": "machine learning"}, {"st": 32, "ed": 34, "text": "highly successful"}, {"st": 143, "ed": 145, "text": "experimental evaluation"}]
[{"st": 1, "ed": 3, "text": "class classification"}, {"st": 8, "ed": 10, "text": "classification models"}, {"st": 58, "ed": 60, "text": "novelty detection"}, {"st": 93, "ed": 95, "text": "training data"}, {"st": 99, "ed": 101, "text": "application domains"}, {"st": 143, "ed": 145, "text": "open research"}]
[{"st": 45, "ed": 47, "text": "low power"}, {"st": 118, "ed": 121, "text": "artificial intelligence ai"}, {"st": 121, "ed": 123, "text": "based approach"}, {"st": 127, "ed": 129, "text": "rough set"}, {"st": 135, "ed": 137, "text": "rough set"}, {"st": 147, "ed": 149, "text": "rough set"}, {"st": 166, "ed": 168, "text": "rough set"}, {"st": 200, "ed": 202, "text": "rough set"}]
[{"st": 30, "ed": 32, "text": "main idea"}, {"st": 155, "ed": 157, "text": "strategy game"}, {"st": 164, "ed": 166, "text": "proposed approach"}, {"st": 227, "ed": 229, "text": "binary classification"}, {"st": 232, "ed": 234, "text": "supervised learning"}, {"st": 267, "ed": 269, "text": "learned models"}]
[{"st": 1, "ed": 3, "text": "sample based"}, {"st": 9, "ed": 11, "text": "great promise"}, {"st": 42, "ed": 44, "text": "grows exponentially"}, {"st": 61, "ed": 63, "text": "sample based"}, {"st": 90, "ed": 92, "text": "reinforcement learning"}]
[{"st": 3, "ed": 5, "text": "reinforcement learning"}, {"st": 32, "ed": 34, "text": "reinforcement learning"}, {"st": 60, "ed": 62, "text": "finite state"}, {"st": 67, "ed": 69, "text": "efficiently solved"}, {"st": 94, "ed": 96, "text": "optimal policies"}]
[{"st": 2, "ed": 4, "text": "big data"}, {"st": 16, "ed": 19, "text": "probabilistic graphical models"}, {"st": 38, "ed": 40, "text": "big data"}, {"st": 54, "ed": 56, "text": "random variables"}, {"st": 58, "ed": 60, "text": "random variable"}, {"st": 85, "ed": 88, "text": "hundreds of thousands"}, {"st": 109, "ed": 111, "text": "large data"}, {"st": 119, "ed": 121, "text": "probability distributions"}, {"st": 131, "ed": 133, "text": "random variable"}, {"st": 134, "ed": 137, "text": "hundreds of thousands"}, {"st": 152, "ed": 154, "text": "random variables"}, {"st": 190, "ed": 192, "text": "random variables"}, {"st": 206, "ed": 209, "text": "probabilistic graphical model"}, {"st": 241, "ed": 244, "text": "massive amounts of"}, {"st": 247, "ed": 249, "text": "successfully applied"}, {"st": 270, "ed": 272, "text": "latent semantic"}]
[{"st": 12, "ed": 14, "text": "machine learning"}, {"st": 19, "ed": 21, "text": "fixed point"}, {"st": 25, "ed": 27, "text": "computable function"}, {"st": 59, "ed": 61, "text": "deep learning"}]
[{"st": 1, "ed": 3, "text": "reinforcement learning"}, {"st": 37, "ed": 39, "text": "feature representation"}]
[{"st": 13, "ed": 15, "text": "low dimensional"}, {"st": 118, "ed": 120, "text": "highly effective"}, {"st": 120, "ed": 122, "text": "dimensionality reduction"}, {"st": 125, "ed": 127, "text": "reinforcement learning"}]
[{"st": 3, "ed": 5, "text": "data mining"}, {"st": 18, "ed": 20, "text": "domains including"}, {"st": 20, "ed": 22, "text": "collaborative filtering"}, {"st": 22, "ed": 24, "text": "link prediction"}, {"st": 29, "ed": 31, "text": "machine learning"}, {"st": 43, "ed": 45, "text": "successfully applied"}, {"st": 59, "ed": 61, "text": "feature based"}, {"st": 61, "ed": 63, "text": "matrix factorization"}, {"st": 70, "ed": 72, "text": "matrix factorization"}, {"st": 85, "ed": 87, "text": "real world"}, {"st": 120, "ed": 122, "text": "feature based"}, {"st": 128, "ed": 130, "text": "coordinate descent"}, {"st": 177, "ed": 179, "text": "proposed method"}, {"st": 190, "ed": 192, "text": "significantly improved"}]
[{"st": 0, "ed": 2, "text": "reinforcement learning"}, {"st": 66, "ed": 68, "text": "learning algorithms"}, {"st": 70, "ed": 72, "text": "challenging problem"}]
[{"st": 13, "ed": 15, "text": "failure rate"}, {"st": 29, "ed": 31, "text": "failure rate"}, {"st": 41, "ed": 43, "text": "failure rate"}, {"st": 52, "ed": 54, "text": "additional information"}, {"st": 122, "ed": 124, "text": "empirical results"}]
[{"st": 4, "ed": 6, "text": "classification models"}, {"st": 87, "ed": 89, "text": "decision tree"}]
[{"st": 36, "ed": 38, "text": "closed form"}, {"st": 38, "ed": 40, "text": "parameter estimates"}, {"st": 54, "ed": 56, "text": "parameter estimates"}, {"st": 57, "ed": 59, "text": "missing data"}, {"st": 73, "ed": 76, "text": "orders of magnitude"}, {"st": 95, "ed": 98, "text": "orders of magnitude"}]
[{"st": 19, "ed": 21, "text": "probabilistic framework"}, {"st": 23, "ed": 25, "text": "computational intelligence"}]
[{"st": 9, "ed": 12, "text": "learning and inference"}, {"st": 14, "ed": 16, "text": "probabilistic models"}, {"st": 45, "ed": 47, "text": "exact inference"}]
[{"st": 6, "ed": 10, "text": "activities of daily living"}, {"st": 36, "ed": 38, "text": "classification algorithms"}, {"st": 44, "ed": 46, "text": "training data"}, {"st": 71, "ed": 73, "text": "training data"}, {"st": 87, "ed": 90, "text": "hidden markov model"}, {"st": 113, "ed": 115, "text": "cross validation"}, {"st": 148, "ed": 150, "text": "activity recognition"}, {"st": 182, "ed": 184, "text": "log likelihood"}, {"st": 189, "ed": 191, "text": "ill posed"}, {"st": 198, "ed": 200, "text": "supervised classification"}, {"st": 201, "ed": 203, "text": "perform poorly"}]
[{"st": 0, "ed": 2, "text": "machine learning"}, {"st": 20, "ed": 23, "text": "classification and clustering"}, {"st": 37, "ed": 40, "text": "modern machine learning"}]
[{"st": 39, "ed": 41, "text": "mixture distribution"}, {"st": 44, "ed": 46, "text": "multi level"}, {"st": 74, "ed": 76, "text": "mixture components"}, {"st": 93, "ed": 96, "text": "hierarchical bayesian model"}, {"st": 99, "ed": 101, "text": "existing models"}, {"st": 115, "ed": 118, "text": "takes into account"}, {"st": 134, "ed": 136, "text": "mixture components"}, {"st": 155, "ed": 157, "text": "gibbs sampling"}, {"st": 158, "ed": 160, "text": "inference algorithm"}, {"st": 170, "ed": 172, "text": "outperforms existing"}]
[{"st": 11, "ed": 14, "text": "markov decision processes"}, {"st": 22, "ed": 24, "text": "cumulative reward"}, {"st": 70, "ed": 72, "text": "cumulative reward"}, {"st": 78, "ed": 80, "text": "reinforcement learning"}, {"st": 124, "ed": 126, "text": "total number"}, {"st": 127, "ed": 129, "text": "cumulative reward"}]
[{"st": 18, "ed": 20, "text": "data stream"}, {"st": 21, "ed": 23, "text": "previous research"}, {"st": 28, "ed": 30, "text": "decision trees"}, {"st": 36, "ed": 39, "text": "discrete fourier transform"}, {"st": 84, "ed": 86, "text": "ensemble approach"}, {"st": 91, "ed": 93, "text": "empirical results"}, {"st": 94, "ed": 97, "text": "real world data"}, {"st": 98, "ed": 100, "text": "synthetic data"}, {"st": 108, "ed": 110, "text": "ensemble approach"}, {"st": 118, "ed": 120, "text": "classification accuracy"}]
[{"st": 7, "ed": 9, "text": "neural networks"}, {"st": 25, "ed": 28, "text": "deep neural networks"}, {"st": 37, "ed": 40, "text": "deep belief networks"}, {"st": 42, "ed": 45, "text": "restricted boltzmann machines"}, {"st": 94, "ed": 96, "text": "hand written"}, {"st": 122, "ed": 124, "text": "error rate"}]
[{"st": 63, "ed": 65, "text": "large data"}, {"st": 71, "ed": 73, "text": "equivalence class"}]
[{"st": 39, "ed": 41, "text": "increasing attention"}, {"st": 148, "ed": 150, "text": "wide variety"}, {"st": 190, "ed": 192, "text": "performance improvements"}]
[{"st": 6, "ed": 8, "text": "related tasks"}, {"st": 13, "ed": 15, "text": "reinforcement learning"}, {"st": 20, "ed": 22, "text": "empirical evidence"}, {"st": 49, "ed": 52, "text": "markov decision processes"}, {"st": 59, "ed": 61, "text": "finite set"}, {"st": 83, "ed": 85, "text": "lifelong learning"}, {"st": 110, "ed": 112, "text": "sample complexity"}, {"st": 143, "ed": 145, "text": "problems including"}, {"st": 146, "ed": 148, "text": "recently introduced"}]
[{"st": 6, "ed": 8, "text": "clustering algorithm"}, {"st": 28, "ed": 30, "text": "clustering methods"}, {"st": 32, "ed": 34, "text": "k means"}, {"st": 52, "ed": 54, "text": "clustering techniques"}, {"st": 121, "ed": 124, "text": "number of clusters"}, {"st": 135, "ed": 137, "text": "data stream"}, {"st": 149, "ed": 152, "text": "provide theoretical guarantees"}]
[{"st": 17, "ed": 19, "text": "transfer learning"}, {"st": 20, "ed": 22, "text": "reinforcement learning"}, {"st": 24, "ed": 26, "text": "special case"}, {"st": 77, "ed": 79, "text": "linear function"}, {"st": 96, "ed": 98, "text": "theoretical results"}]
[{"st": 3, "ed": 6, "text": "temporal difference learning"}, {"st": 25, "ed": 27, "text": "recent works"}, {"st": 53, "ed": 55, "text": "off policy"}, {"st": 57, "ed": 59, "text": "linear function"}]
[{"st": 49, "ed": 51, "text": "asymptotically optimal"}, {"st": 51, "ed": 53, "text": "reinforcement learning"}]
[{"st": 4, "ed": 6, "text": "spectral graph"}, {"st": 10, "ed": 12, "text": "weakly supervised"}, {"st": 36, "ed": 38, "text": "training set"}, {"st": 88, "ed": 90, "text": "similarity graph"}, {"st": 97, "ed": 99, "text": "similarity graph"}, {"st": 114, "ed": 116, "text": "weakly supervised"}, {"st": 126, "ed": 128, "text": "benchmark datasets"}, {"st": 137, "ed": 139, "text": "proposed approach"}, {"st": 140, "ed": 142, "text": "weakly supervised"}, {"st": 148, "ed": 150, "text": "classification performance"}, {"st": 154, "ed": 156, "text": "similarity graph"}, {"st": 174, "ed": 176, "text": "similarity graph"}, {"st": 184, "ed": 186, "text": "weakly supervised"}]
[{"st": 16, "ed": 18, "text": "a level"}, {"st": 86, "ed": 88, "text": "mathematical model"}, {"st": 103, "ed": 105, "text": "error rate"}, {"st": 126, "ed": 128, "text": "low quality"}]
[{"st": 7, "ed": 9, "text": "causal model"}, {"st": 14, "ed": 16, "text": "causal relationships"}, {"st": 29, "ed": 31, "text": "central role"}, {"st": 135, "ed": 137, "text": "conditional independence"}]
[{"st": 24, "ed": 26, "text": "question answering"}, {"st": 27, "ed": 29, "text": "latent variable"}, {"st": 36, "ed": 38, "text": "statistical modeling"}, {"st": 42, "ed": 44, "text": "promising results"}, {"st": 48, "ed": 50, "text": "knowledge graph"}, {"st": 99, "ed": 101, "text": "statistical modeling"}, {"st": 102, "ed": 104, "text": "latent variable"}, {"st": 109, "ed": 111, "text": "prior knowledge"}, {"st": 122, "ed": 124, "text": "latent variable"}, {"st": 130, "ed": 132, "text": "prior knowledge"}, {"st": 135, "ed": 137, "text": "significantly improves"}, {"st": 143, "ed": 145, "text": "link prediction"}]
[{"st": 5, "ed": 7, "text": "prediction market"}, {"st": 30, "ed": 32, "text": "machine learning"}, {"st": 41, "ed": 43, "text": "prediction market"}, {"st": 67, "ed": 69, "text": "decision making"}]
[{"st": 4, "ed": 7, "text": "value function approximation"}, {"st": 26, "ed": 28, "text": "low rank"}, {"st": 29, "ed": 31, "text": "sparse matrix"}, {"st": 46, "ed": 48, "text": "low rank"}, {"st": 56, "ed": 60, "text": "robust principal component analysis"}, {"st": 66, "ed": 68, "text": "robust pca"}, {"st": 75, "ed": 77, "text": "principal component"}, {"st": 78, "ed": 80, "text": "convex optimization"}, {"st": 92, "ed": 94, "text": "method yields"}]
[{"st": 9, "ed": 12, "text": "markov decision processes"}, {"st": 57, "ed": 59, "text": "local optimum"}, {"st": 64, "ed": 66, "text": "policy search"}]
[{"st": 21, "ed": 23, "text": "large scale"}, {"st": 54, "ed": 56, "text": "proposed approach"}, {"st": 126, "ed": 128, "text": "low resolution"}, {"st": 143, "ed": 145, "text": "low resolution"}, {"st": 157, "ed": 159, "text": "real data"}, {"st": 173, "ed": 175, "text": "approach yields"}]
[{"st": 9, "ed": 11, "text": "great importance"}, {"st": 13, "ed": 15, "text": "machine learning"}, {"st": 34, "ed": 36, "text": "feature selection"}, {"st": 144, "ed": 146, "text": "problem specific"}, {"st": 169, "ed": 171, "text": "previously proposed"}]
[{"st": 78, "ed": 80, "text": "real world"}]
[{"st": 41, "ed": 43, "text": "negative transfer"}, {"st": 78, "ed": 80, "text": "multiple source"}, {"st": 101, "ed": 103, "text": "deep architecture"}, {"st": 125, "ed": 127, "text": "empirical evaluations"}, {"st": 129, "ed": 131, "text": "learning algorithms"}, {"st": 145, "ed": 147, "text": "negative transfer"}, {"st": 151, "ed": 153, "text": "multiple source"}]
[{"st": 0, "ed": 2, "text": "constraint programming"}, {"st": 8, "ed": 10, "text": "real world"}, {"st": 10, "ed": 12, "text": "optimisation problems"}, {"st": 17, "ed": 19, "text": "resource allocation"}, {"st": 35, "ed": 37, "text": "constraint programming"}, {"st": 59, "ed": 61, "text": "constraint programming"}, {"st": 83, "ed": 85, "text": "constraint programming"}, {"st": 94, "ed": 96, "text": "data mining"}, {"st": 97, "ed": 99, "text": "machine learning"}, {"st": 104, "ed": 106, "text": "constraint programming"}]
[{"st": 8, "ed": 10, "text": "time series"}, {"st": 35, "ed": 37, "text": "streaming data"}, {"st": 39, "ed": 41, "text": "difficult task"}, {"st": 91, "ed": 93, "text": "open source"}, {"st": 98, "ed": 100, "text": "anomaly detection"}, {"st": 117, "ed": 119, "text": "false alarms"}, {"st": 121, "ed": 123, "text": "real world"}, {"st": 123, "ed": 125, "text": "time series"}, {"st": 157, "ed": 159, "text": "benchmark dataset"}, {"st": 161, "ed": 163, "text": "real world"}, {"st": 163, "ed": 165, "text": "time series"}, {"st": 177, "ed": 179, "text": "open source"}, {"st": 191, "ed": 193, "text": "open source"}, {"st": 196, "ed": 199, "text": "the research community"}]
[]
[{"st": 2, "ed": 4, "text": "open question"}, {"st": 5, "ed": 8, "text": "algorithmic information theory"}, {"st": 13, "ed": 16, "text": "universal turing machine"}, {"st": 42, "ed": 44, "text": "intelligent agent"}]
[{"st": 0, "ed": 2, "text": "latent variable"}, {"st": 44, "ed": 46, "text": "large scale"}, {"st": 70, "ed": 73, "text": "orders of magnitude"}, {"st": 75, "ed": 77, "text": "previous works"}, {"st": 119, "ed": 121, "text": "sufficient statistics"}, {"st": 136, "ed": 138, "text": "generative models"}, {"st": 138, "ed": 140, "text": "statistical modeling"}, {"st": 142, "ed": 145, "text": "latent dirichlet allocation"}, {"st": 152, "ed": 155, "text": "hierarchical dirichlet process"}, {"st": 239, "ed": 242, "text": "orders of magnitude"}, {"st": 245, "ed": 247, "text": "previously published"}]
[{"st": 81, "ed": 83, "text": "method called"}, {"st": 87, "ed": 89, "text": "key insight"}, {"st": 117, "ed": 119, "text": "latent variables"}, {"st": 223, "ed": 225, "text": "message passing"}, {"st": 229, "ed": 231, "text": "approximate posterior"}, {"st": 259, "ed": 261, "text": "true label"}, {"st": 266, "ed": 268, "text": "real world"}]
[{"st": 1, "ed": 3, "text": "machine learning"}, {"st": 16, "ed": 18, "text": "learned models"}, {"st": 25, "ed": 28, "text": "efficient and effective"}, {"st": 36, "ed": 38, "text": "non stationary"}, {"st": 109, "ed": 111, "text": "quantitative analysis"}, {"st": 149, "ed": 151, "text": "concept drift"}]
[{"st": 47, "ed": 49, "text": "theoretical guarantees"}, {"st": 61, "ed": 63, "text": "labelled data"}, {"st": 81, "ed": 83, "text": "unlike previous"}, {"st": 110, "ed": 112, "text": "invariant features"}]
[{"st": 32, "ed": 34, "text": "leading edge"}, {"st": 38, "ed": 40, "text": "evaluation function"}, {"st": 47, "ed": 49, "text": "recent works"}, {"st": 69, "ed": 71, "text": "pattern matching"}, {"st": 75, "ed": 79, "text": "deep convolutional neural network"}, {"st": 90, "ed": 92, "text": "monte carlo"}, {"st": 96, "ed": 98, "text": "open source"}, {"st": 132, "ed": 134, "text": "substantially improves"}, {"st": 138, "ed": 140, "text": "pattern matching"}, {"st": 143, "ed": 145, "text": "based approaches"}, {"st": 171, "ed": 173, "text": "substantial improvement"}]
[{"st": 46, "ed": 49, "text": "input and output"}, {"st": 58, "ed": 60, "text": "neural network"}, {"st": 73, "ed": 75, "text": "training instances"}]
[{"st": 11, "ed": 14, "text": "conjunctive normal form"}, {"st": 20, "ed": 23, "text": "disjunctive normal form"}, {"st": 44, "ed": 46, "text": "classification accuracy"}, {"st": 63, "ed": 65, "text": "objective function"}, {"st": 70, "ed": 72, "text": "total number"}, {"st": 76, "ed": 78, "text": "total number"}, {"st": 87, "ed": 89, "text": "previously proposed"}, {"st": 89, "ed": 91, "text": "linear programming"}, {"st": 107, "ed": 109, "text": "classification error"}, {"st": 111, "ed": 113, "text": "hamming distance"}, {"st": 133, "ed": 136, "text": "block coordinate descent"}, {"st": 137, "ed": 139, "text": "alternating minimization"}, {"st": 172, "ed": 174, "text": "hamming distance"}, {"st": 190, "ed": 192, "text": "proposed approach"}, {"st": 199, "ed": 201, "text": "optimal solutions"}]
[{"st": 0, "ed": 2, "text": "artificially intelligent"}, {"st": 29, "ed": 32, "text": "deep reinforcement learning"}, {"st": 46, "ed": 48, "text": "previous studies"}, {"st": 56, "ed": 58, "text": "supervised learning"}, {"st": 60, "ed": 62, "text": "reinforcement learning"}, {"st": 71, "ed": 73, "text": "linear function"}, {"st": 89, "ed": 91, "text": "board game"}, {"st": 125, "ed": 127, "text": "significantly outperformed"}, {"st": 131, "ed": 133, "text": "rule based"}, {"st": 184, "ed": 186, "text": "dialogue systems"}]
[{"st": 2, "ed": 4, "text": "computational efficiency"}, {"st": 5, "ed": 7, "text": "sample efficiency"}, {"st": 14, "ed": 17, "text": "temporal difference td"}, {"st": 17, "ed": 19, "text": "learning algorithms"}, {"st": 35, "ed": 37, "text": "least squares"}, {"st": 37, "ed": 39, "text": "temporal difference"}, {"st": 42, "ed": 44, "text": "sample efficient"}, {"st": 61, "ed": 63, "text": "low rank"}, {"st": 105, "ed": 107, "text": "sample complexity"}, {"st": 120, "ed": 122, "text": "low rank"}, {"st": 125, "ed": 127, "text": "bias variance"}, {"st": 142, "ed": 144, "text": "computational complexity"}, {"st": 145, "ed": 147, "text": "sample efficiency"}, {"st": 148, "ed": 150, "text": "policy evaluation"}]
[]
[{"st": 3, "ed": 5, "text": "reinforcement learning"}, {"st": 24, "ed": 26, "text": "learning algorithm"}]
[{"st": 1, "ed": 3, "text": "machine learning"}, {"st": 13, "ed": 15, "text": "machine translation"}, {"st": 18, "ed": 20, "text": "training set"}, {"st": 82, "ed": 84, "text": "theoretical results"}, {"st": 107, "ed": 109, "text": "empirical performance"}, {"st": 194, "ed": 196, "text": "individual words"}, {"st": 250, "ed": 252, "text": "training sample"}]
[{"st": 59, "ed": 61, "text": "domain knowledge"}, {"st": 84, "ed": 86, "text": "graphical model"}, {"st": 121, "ed": 123, "text": "proposed method"}, {"st": 124, "ed": 127, "text": "image and text"}]
[{"st": 1, "ed": 6, "text": "graph based semi supervised learning"}, {"st": 10, "ed": 12, "text": "widely applied"}, {"st": 16, "ed": 18, "text": "massive data"}, {"st": 45, "ed": 47, "text": "recent works"}, {"st": 49, "ed": 51, "text": "based methods"}, {"st": 105, "ed": 107, "text": "space complexity"}, {"st": 126, "ed": 128, "text": "large data"}, {"st": 131, "ed": 133, "text": "real world"}, {"st": 138, "ed": 140, "text": "method achieves"}, {"st": 150, "ed": 152, "text": "significant reduction"}, {"st": 163, "ed": 165, "text": "natural language"}, {"st": 180, "ed": 182, "text": "deep learning"}]
[{"st": 1, "ed": 4, "text": "deep neural nets"}, {"st": 8, "ed": 10, "text": "reinforcement learning"}, {"st": 23, "ed": 25, "text": "real world"}, {"st": 47, "ed": 49, "text": "learning process"}, {"st": 51, "ed": 55, "text": "deep q network dqn"}, {"st": 75, "ed": 77, "text": "significantly reduce"}, {"st": 89, "ed": 91, "text": "learning rate"}, {"st": 110, "ed": 112, "text": "neural networks"}, {"st": 118, "ed": 121, "text": "approximate dynamic programming"}, {"st": 131, "ed": 133, "text": "local optimum"}, {"st": 135, "ed": 137, "text": "learning process"}, {"st": 143, "ed": 145, "text": "exploration exploitation"}]
[{"st": 0, "ed": 2, "text": "real data"}, {"st": 7, "ed": 10, "text": "discrete and continuous"}, {"st": 13, "ed": 17, "text": "bayesian network structure learning"}, {"st": 18, "ed": 20, "text": "inference algorithms"}, {"st": 22, "ed": 24, "text": "random variables"}, {"st": 26, "ed": 28, "text": "continuous variables"}, {"st": 59, "ed": 61, "text": "continuous variables"}, {"st": 81, "ed": 83, "text": "proposed method"}, {"st": 99, "ed": 101, "text": "existing methods"}, {"st": 103, "ed": 105, "text": "structure learning"}, {"st": 109, "ed": 111, "text": "continuous variables"}, {"st": 112, "ed": 114, "text": "simultaneously learn"}, {"st": 114, "ed": 116, "text": "bayesian network"}]
[{"st": 13, "ed": 15, "text": "convolutional networks"}, {"st": 26, "ed": 28, "text": "monte carlo"}, {"st": 31, "ed": 33, "text": "thompson sampling"}, {"st": 35, "ed": 37, "text": "convolutional network"}, {"st": 51, "ed": 53, "text": "open source"}, {"st": 57, "ed": 59, "text": "competitive results"}]
[{"st": 1, "ed": 3, "text": "temporal difference"}, {"st": 24, "ed": 27, "text": "low computational cost"}, {"st": 45, "ed": 47, "text": "true online"}, {"st": 50, "ed": 52, "text": "true online"}, {"st": 89, "ed": 91, "text": "true online"}, {"st": 96, "ed": 98, "text": "theoretical properties"}, {"st": 118, "ed": 120, "text": "extensive empirical"}, {"st": 127, "ed": 129, "text": "true online"}, {"st": 143, "ed": 145, "text": "real world"}, {"st": 158, "ed": 160, "text": "linear function"}, {"st": 173, "ed": 175, "text": "true online"}, {"st": 186, "ed": 188, "text": "learning speed"}, {"st": 190, "ed": 192, "text": "true online"}, {"st": 220, "ed": 222, "text": "true online"}, {"st": 225, "ed": 227, "text": "empirical results"}, {"st": 237, "ed": 239, "text": "true online"}, {"st": 248, "ed": 250, "text": "true online"}, {"st": 250, "ed": 252, "text": "temporal difference"}]
[{"st": 0, "ed": 2, "text": "off policy"}, {"st": 26, "ed": 28, "text": "off policy"}, {"st": 28, "ed": 30, "text": "learning algorithms"}, {"st": 59, "ed": 61, "text": "control problems"}, {"st": 91, "ed": 93, "text": "learning algorithms"}, {"st": 102, "ed": 104, "text": "policy gradient"}, {"st": 121, "ed": 123, "text": "empirical evidence"}, {"st": 126, "ed": 128, "text": "compare favorably"}]
[{"st": 0, "ed": 2, "text": "knowledge graph"}, {"st": 14, "ed": 17, "text": "entities and relations"}, {"st": 22, "ed": 24, "text": "existing methods"}, {"st": 28, "ed": 30, "text": "knowledge graph"}, {"st": 45, "ed": 47, "text": "ill posed"}, {"st": 97, "ed": 99, "text": "extensive experiments"}, {"st": 105, "ed": 107, "text": "substantial improvements"}, {"st": 118, "ed": 120, "text": "prediction task"}]
[{"st": 74, "ed": 76, "text": "continuous space"}, {"st": 82, "ed": 84, "text": "empirical results"}, {"st": 101, "ed": 103, "text": "sufficient conditions"}, {"st": 133, "ed": 135, "text": "learning algorithm"}, {"st": 148, "ed": 150, "text": "empirical study"}]
[{"st": 0, "ed": 2, "text": "feature based"}, {"st": 11, "ed": 13, "text": "machine learning"}, {"st": 37, "ed": 39, "text": "representation space"}, {"st": 97, "ed": 99, "text": "feature sets"}, {"st": 161, "ed": 163, "text": "feature sets"}, {"st": 179, "ed": 182, "text": "statistical hypothesis testing"}, {"st": 213, "ed": 215, "text": "feature sets"}]
[{"st": 0, "ed": 2, "text": "multi relational"}, {"st": 15, "ed": 17, "text": "existing methods"}, {"st": 21, "ed": 23, "text": "per iteration"}, {"st": 41, "ed": 43, "text": "trace norm"}, {"st": 72, "ed": 74, "text": "higher order"}, {"st": 82, "ed": 84, "text": "equivalence relation"}, {"st": 97, "ed": 99, "text": "rank tensor"}, {"st": 110, "ed": 112, "text": "trace norm"}, {"st": 120, "ed": 122, "text": "lagrange multiplier"}, {"st": 130, "ed": 132, "text": "extensive experiments"}, {"st": 134, "ed": 137, "text": "real and synthetic"}]
[{"st": 10, "ed": 14, "text": "sum product networks spns"}, {"st": 43, "ed": 45, "text": "mixture model"}, {"st": 49, "ed": 51, "text": "objective function"}, {"st": 57, "ed": 59, "text": "maximum likelihood"}, {"st": 66, "ed": 68, "text": "optimization problem"}, {"st": 78, "ed": 80, "text": "parameter learning"}, {"st": 115, "ed": 117, "text": "unified framework"}, {"st": 133, "ed": 136, "text": "expectation maximization em"}]
[{"st": 35, "ed": 37, "text": "computer vision"}, {"st": 45, "ed": 47, "text": "epsilon greedy"}, {"st": 50, "ed": 52, "text": "linear function"}, {"st": 59, "ed": 61, "text": "least squares"}, {"st": 61, "ed": 63, "text": "value iteration"}, {"st": 72, "ed": 74, "text": "posterior distribution"}, {"st": 82, "ed": 84, "text": "efficient exploration"}, {"st": 85, "ed": 87, "text": "increasingly important"}, {"st": 91, "ed": 93, "text": "faster learning"}]
[{"st": 9, "ed": 11, "text": "systems analysis"}, {"st": 12, "ed": 14, "text": "ensemble methods"}, {"st": 16, "ed": 18, "text": "ensemble methods"}, {"st": 23, "ed": 25, "text": "decision rules"}, {"st": 26, "ed": 28, "text": "feature space"}, {"st": 46, "ed": 48, "text": "decision trees"}, {"st": 68, "ed": 70, "text": "random forests"}, {"st": 100, "ed": 102, "text": "air conditioning"}, {"st": 104, "ed": 106, "text": "electric vehicles"}, {"st": 114, "ed": 116, "text": "classification methods"}, {"st": 142, "ed": 144, "text": "steady state"}]
[{"st": 6, "ed": 10, "text": "sum product networks spns"}, {"st": 18, "ed": 20, "text": "latent variables"}, {"st": 36, "ed": 38, "text": "em algorithm"}, {"st": 146, "ed": 148, "text": "em algorithm"}, {"st": 189, "ed": 191, "text": "theoretical results"}, {"st": 196, "ed": 198, "text": "synthetic data"}, {"st": 200, "ed": 202, "text": "real world"}]
[{"st": 6, "ed": 8, "text": "anomaly detection"}, {"st": 10, "ed": 12, "text": "large datasets"}, {"st": 16, "ed": 18, "text": "method named"}, {"st": 23, "ed": 25, "text": "kernel based"}, {"st": 28, "ed": 30, "text": "efficiently compute"}, {"st": 48, "ed": 50, "text": "inner product"}, {"st": 51, "ed": 54, "text": "a reproducing kernel"}, {"st": 87, "ed": 89, "text": "incremental learning"}, {"st": 118, "ed": 120, "text": "concept drift"}, {"st": 127, "ed": 129, "text": "real datasets"}, {"st": 143, "ed": 145, "text": "anomaly detection"}]
[{"st": 54, "ed": 56, "text": "empirical results"}, {"st": 58, "ed": 60, "text": "multi agent"}, {"st": 60, "ed": 62, "text": "learning problems"}, {"st": 91, "ed": 94, "text": "deep reinforcement learning"}]
[{"st": 9, "ed": 11, "text": "partially ordered"}, {"st": 97, "ed": 100, "text": "takes advantage of"}, {"st": 114, "ed": 117, "text": "provide theoretical guarantees"}]
[{"st": 4, "ed": 7, "text": "markov decision processes"}, {"st": 62, "ed": 64, "text": "main contribution"}, {"st": 94, "ed": 96, "text": "experiments demonstrate"}, {"st": 100, "ed": 102, "text": "near optimal"}]
[{"st": 0, "ed": 2, "text": "domain adaptation"}, {"st": 7, "ed": 9, "text": "training data"}, {"st": 39, "ed": 42, "text": "unsupervised domain adaptation"}, {"st": 57, "ed": 59, "text": "noisy labels"}, {"st": 66, "ed": 69, "text": "source and target"}, {"st": 124, "ed": 127, "text": "simulated and real"}]
[{"st": 30, "ed": 32, "text": "expert knowledge"}, {"st": 35, "ed": 37, "text": "existing works"}, {"st": 41, "ed": 43, "text": "supervised learning"}, {"st": 51, "ed": 53, "text": "probabilistic models"}, {"st": 61, "ed": 64, "text": "bag of words"}, {"st": 164, "ed": 166, "text": "evaluation shows"}]
[{"st": 97, "ed": 100, "text": "probabilistic graphical models"}, {"st": 126, "ed": 128, "text": "likelihood based"}]
[{"st": 36, "ed": 38, "text": "accurate prediction"}, {"st": 43, "ed": 45, "text": "machine learning"}, {"st": 107, "ed": 110, "text": "real world data"}, {"st": 117, "ed": 119, "text": "fuzzy logic"}, {"st": 156, "ed": 158, "text": "smart meter"}]
[{"st": 25, "ed": 27, "text": "relational database"}, {"st": 54, "ed": 56, "text": "data mining"}, {"st": 83, "ed": 85, "text": "relational databases"}, {"st": 112, "ed": 114, "text": "proposed method"}, {"st": 126, "ed": 128, "text": "randomly generated"}, {"st": 145, "ed": 147, "text": "machine learning"}, {"st": 170, "ed": 172, "text": "database management"}]
[{"st": 5, "ed": 7, "text": "multi task"}, {"st": 7, "ed": 9, "text": "reinforcement learning"}, {"st": 62, "ed": 64, "text": "explicitly model"}, {"st": 84, "ed": 86, "text": "jointly learn"}, {"st": 93, "ed": 95, "text": "value iteration"}, {"st": 96, "ed": 98, "text": "policy iteration"}, {"st": 102, "ed": 104, "text": "shared representation"}, {"st": 110, "ed": 112, "text": "multi task"}, {"st": 170, "ed": 172, "text": "shared representation"}, {"st": 178, "ed": 180, "text": "multi task"}]
[{"st": 7, "ed": 9, "text": "differentiable function"}, {"st": 41, "ed": 43, "text": "reinforcement learning"}]
[{"st": 1, "ed": 3, "text": "learning algorithms"}, {"st": 40, "ed": 42, "text": "learning framework"}, {"st": 60, "ed": 62, "text": "multi task"}, {"st": 119, "ed": 121, "text": "previously learned"}]
[{"st": 5, "ed": 7, "text": "ensemble learning"}, {"st": 28, "ed": 30, "text": "transaction costs"}, {"st": 40, "ed": 42, "text": "regret bound"}, {"st": 54, "ed": 56, "text": "numerical examples"}, {"st": 64, "ed": 66, "text": "significant improvement"}]
[{"st": 5, "ed": 7, "text": "neural networks"}, {"st": 22, "ed": 25, "text": "video game console"}, {"st": 32, "ed": 34, "text": "neural networks"}, {"st": 79, "ed": 81, "text": "comparable results"}, {"st": 118, "ed": 120, "text": "improved performance"}]
[{"st": 37, "ed": 39, "text": "causal models"}, {"st": 68, "ed": 70, "text": "causal structure"}, {"st": 73, "ed": 75, "text": "real data"}, {"st": 89, "ed": 91, "text": "causal models"}, {"st": 92, "ed": 94, "text": "observational data"}, {"st": 157, "ed": 159, "text": "causal structure"}, {"st": 164, "ed": 166, "text": "causal models"}, {"st": 189, "ed": 191, "text": "domain experts"}, {"st": 199, "ed": 201, "text": "data mining"}]
[{"st": 3, "ed": 5, "text": "pre processing"}, {"st": 7, "ed": 9, "text": "feature extraction"}, {"st": 18, "ed": 20, "text": "data mining"}, {"st": 81, "ed": 83, "text": "specific knowledge"}, {"st": 115, "ed": 117, "text": "classification problems"}, {"st": 170, "ed": 172, "text": "data mining"}, {"st": 183, "ed": 185, "text": "case studies"}, {"st": 193, "ed": 195, "text": "data mining"}]
[{"st": 25, "ed": 27, "text": "generative models"}, {"st": 49, "ed": 51, "text": "discriminative models"}, {"st": 65, "ed": 67, "text": "latent variable"}, {"st": 81, "ed": 83, "text": "latent state"}, {"st": 90, "ed": 92, "text": "computation graph"}, {"st": 95, "ed": 98, "text": "simple and effective"}, {"st": 133, "ed": 135, "text": "nonlinear function"}, {"st": 150, "ed": 153, "text": "recurrent neural network"}, {"st": 164, "ed": 166, "text": "network architecture"}, {"st": 183, "ed": 185, "text": "raw image"}, {"st": 199, "ed": 201, "text": "significant improvement"}]
[{"st": 4, "ed": 6, "text": "active learning"}, {"st": 86, "ed": 88, "text": "computationally efficient"}, {"st": 88, "ed": 90, "text": "active learning"}, {"st": 93, "ed": 96, "text": "strong theoretical guarantees"}, {"st": 104, "ed": 106, "text": "directly optimizing"}, {"st": 107, "ed": 109, "text": "prediction error"}, {"st": 125, "ed": 128, "text": "takes into account"}, {"st": 154, "ed": 156, "text": "near optimal"}, {"st": 160, "ed": 162, "text": "empirical performance"}, {"st": 166, "ed": 168, "text": "problem instances"}, {"st": 190, "ed": 192, "text": "learning task"}]
[{"st": 0, "ed": 2, "text": "artificial intelligence"}, {"st": 16, "ed": 18, "text": "reinforcement learning"}, {"st": 23, "ed": 25, "text": "reward functions"}, {"st": 46, "ed": 48, "text": "reward signal"}, {"st": 51, "ed": 53, "text": "reward signal"}]
[{"st": 82, "ed": 84, "text": "improved performance"}]
[{"st": 70, "ed": 72, "text": "recent works"}, {"st": 89, "ed": 91, "text": "low level"}]
[{"st": 1, "ed": 3, "text": "imitation learning"}, {"st": 4, "ed": 6, "text": "agent learns"}, {"st": 15, "ed": 17, "text": "cost function"}, {"st": 22, "ed": 24, "text": "imitation learning"}, {"st": 33, "ed": 35, "text": "reinforcement learning"}, {"st": 93, "ed": 95, "text": "cost function"}, {"st": 106, "ed": 108, "text": "policy gradients"}]
[{"st": 0, "ed": 2, "text": "document classification"}, {"st": 35, "ed": 37, "text": "machine learning"}, {"st": 55, "ed": 57, "text": "machine learning"}, {"st": 58, "ed": 60, "text": "semi supervised"}, {"st": 106, "ed": 108, "text": "machine learning"}, {"st": 144, "ed": 146, "text": "semi supervised"}, {"st": 214, "ed": 216, "text": "preliminary results"}, {"st": 222, "ed": 224, "text": "real world"}]
[{"st": 6, "ed": 8, "text": "reinforcement learning"}, {"st": 15, "ed": 17, "text": "benchmark problems"}]
[{"st": 53, "ed": 55, "text": "text classification"}, {"st": 61, "ed": 63, "text": "final solution"}]
[{"st": 26, "ed": 28, "text": "cost function"}, {"st": 29, "ed": 32, "text": "inverse reinforcement learning"}, {"st": 38, "ed": 40, "text": "cost function"}, {"st": 70, "ed": 72, "text": "reinforcement learning"}, {"st": 89, "ed": 91, "text": "imitation learning"}, {"st": 92, "ed": 95, "text": "generative adversarial networks"}, {"st": 102, "ed": 104, "text": "imitation learning"}, {"st": 107, "ed": 110, "text": "significant performance gains"}]
[{"st": 4, "ed": 8, "text": "deep recurrent neural network"}, {"st": 17, "ed": 20, "text": "end to end"}, {"st": 28, "ed": 30, "text": "reinforcement learning"}, {"st": 55, "ed": 57, "text": "internal representation"}, {"st": 120, "ed": 122, "text": "experimentally demonstrate"}, {"st": 129, "ed": 131, "text": "atari games"}, {"st": 171, "ed": 173, "text": "prediction task"}, {"st": 177, "ed": 179, "text": "n grams"}]
[{"st": 3, "ed": 5, "text": "machine learning"}, {"st": 6, "ed": 9, "text": "artificial intelligence ai"}, {"st": 11, "ed": 13, "text": "increasing attention"}, {"st": 36, "ed": 38, "text": "machine learning"}, {"st": 52, "ed": 54, "text": "real world"}, {"st": 80, "ed": 82, "text": "objective function"}, {"st": 90, "ed": 92, "text": "objective function"}, {"st": 106, "ed": 108, "text": "learning process"}]
[{"st": 17, "ed": 19, "text": "real world"}, {"st": 39, "ed": 41, "text": "side information"}, {"st": 162, "ed": 164, "text": "proposed method"}]
[{"st": 13, "ed": 15, "text": "developing countries"}, {"st": 123, "ed": 125, "text": "time series"}, {"st": 151, "ed": 153, "text": "big data"}, {"st": 172, "ed": 174, "text": "machine learning"}, {"st": 180, "ed": 182, "text": "big data"}, {"st": 188, "ed": 190, "text": "logistic regression"}, {"st": 190, "ed": 193, "text": "k nearest neighbors"}, {"st": 194, "ed": 197, "text": "support vector machine"}, {"st": 209, "ed": 211, "text": "time series"}, {"st": 217, "ed": 219, "text": "big data"}]
[{"st": 3, "ed": 6, "text": "naive bayes classifier"}, {"st": 10, "ed": 13, "text": "probabilistic graphical model"}, {"st": 30, "ed": 32, "text": "naive bayes"}, {"st": 44, "ed": 46, "text": "learning process"}, {"st": 63, "ed": 65, "text": "predictive performance"}, {"st": 70, "ed": 73, "text": "naive bayes classifier"}, {"st": 79, "ed": 81, "text": "class distributions"}, {"st": 87, "ed": 89, "text": "gene ontology"}]
[{"st": 1, "ed": 3, "text": "driving styles"}, {"st": 20, "ed": 22, "text": "real world"}, {"st": 37, "ed": 39, "text": "autonomous driving"}, {"st": 47, "ed": 49, "text": "traditional methods"}, {"st": 52, "ed": 54, "text": "handcrafted features"}, {"st": 56, "ed": 58, "text": "machine learning"}, {"st": 71, "ed": 73, "text": "deep learning"}, {"st": 85, "ed": 87, "text": "deep learning"}, {"st": 89, "ed": 91, "text": "behavior analysis"}, {"st": 96, "ed": 98, "text": "proposed approach"}]
[{"st": 71, "ed": 73, "text": "likelihood function"}]
[{"st": 3, "ed": 5, "text": "event recognition"}, {"st": 29, "ed": 31, "text": "temporal logic"}, {"st": 39, "ed": 41, "text": "event recognition"}, {"st": 48, "ed": 50, "text": "machine learning"}, {"st": 51, "ed": 54, "text": "inductive logic programming"}, {"st": 62, "ed": 64, "text": "online learning"}, {"st": 74, "ed": 76, "text": "learning strategy"}, {"st": 103, "ed": 105, "text": "learning process"}, {"st": 117, "ed": 120, "text": "inductive logic programming"}, {"st": 132, "ed": 134, "text": "activity recognition"}, {"st": 154, "ed": 156, "text": "speed ups"}, {"st": 162, "ed": 164, "text": "hand crafted"}, {"st": 179, "ed": 181, "text": "noise free"}]
[{"st": 0, "ed": 2, "text": "active learning"}, {"st": 5, "ed": 7, "text": "machine learning"}, {"st": 15, "ed": 17, "text": "active learning"}, {"st": 49, "ed": 51, "text": "active learning"}, {"st": 62, "ed": 64, "text": "active learning"}, {"st": 73, "ed": 75, "text": "active learning"}, {"st": 91, "ed": 93, "text": "active learning"}, {"st": 102, "ed": 105, "text": "upper confidence bound"}, {"st": 108, "ed": 110, "text": "contextual bandit"}, {"st": 131, "ed": 133, "text": "empirical studies"}]
[{"st": 45, "ed": 48, "text": "short term memory"}, {"st": 49, "ed": 51, "text": "encoder decoder"}, {"st": 67, "ed": 69, "text": "time series"}, {"st": 77, "ed": 79, "text": "time series"}, {"st": 108, "ed": 110, "text": "turbofan engine"}, {"st": 111, "ed": 113, "text": "milling machine"}, {"st": 120, "ed": 122, "text": "real world"}]
[{"st": 21, "ed": 23, "text": "knowledge graph"}, {"st": 27, "ed": 30, "text": "entities and relations"}, {"st": 36, "ed": 38, "text": "related methods"}, {"st": 45, "ed": 47, "text": "knowledge graph"}, {"st": 55, "ed": 57, "text": "embedding methods"}, {"st": 68, "ed": 70, "text": "question answering"}, {"st": 84, "ed": 86, "text": "knowledge graph"}, {"st": 94, "ed": 96, "text": "generative process"}, {"st": 138, "ed": 140, "text": "extensive experiments"}]
[{"st": 5, "ed": 7, "text": "becoming increasingly"}, {"st": 30, "ed": 32, "text": "collected data"}, {"st": 47, "ed": 49, "text": "real estate"}, {"st": 113, "ed": 116, "text": "real world data"}]
[{"st": 5, "ed": 9, "text": "real time strategy game"}, {"st": 14, "ed": 16, "text": "reinforcement learning"}, {"st": 29, "ed": 31, "text": "low level"}, {"st": 40, "ed": 42, "text": "reinforcement learning"}, {"st": 42, "ed": 45, "text": "point of view"}, {"st": 63, "ed": 65, "text": "feature representation"}, {"st": 81, "ed": 84, "text": "deep neural network"}, {"st": 100, "ed": 102, "text": "reinforcement learning"}, {"st": 134, "ed": 136, "text": "epsilon greedy"}, {"st": 146, "ed": 148, "text": "non trivial"}]
[{"st": 11, "ed": 13, "text": "reinforcement learning"}, {"st": 62, "ed": 65, "text": "multi armed bandits"}]
[{"st": 6, "ed": 8, "text": "generative models"}, {"st": 8, "ed": 11, "text": "generative adversarial nets"}, {"st": 15, "ed": 17, "text": "discriminative model"}, {"st": 23, "ed": 25, "text": "generative model"}, {"st": 31, "ed": 33, "text": "real valued"}, {"st": 59, "ed": 61, "text": "generative model"}, {"st": 71, "ed": 73, "text": "discriminative model"}, {"st": 79, "ed": 81, "text": "discriminative model"}, {"st": 95, "ed": 97, "text": "non trivial"}, {"st": 121, "ed": 123, "text": "framework called"}, {"st": 137, "ed": 139, "text": "reinforcement learning"}, {"st": 154, "ed": 156, "text": "reward signal"}, {"st": 172, "ed": 174, "text": "intermediate state"}, {"st": 177, "ed": 179, "text": "monte carlo"}, {"st": 180, "ed": 182, "text": "extensive experiments"}, {"st": 183, "ed": 185, "text": "synthetic data"}, {"st": 186, "ed": 188, "text": "real world"}, {"st": 190, "ed": 192, "text": "significant improvements"}]
[{"st": 0, "ed": 3, "text": "deep reinforcement learning"}, {"st": 8, "ed": 11, "text": "deep neural networks"}, {"st": 18, "ed": 21, "text": "trial and error"}, {"st": 34, "ed": 36, "text": "video games"}, {"st": 55, "ed": 57, "text": "deep learning"}, {"st": 63, "ed": 65, "text": "large datasets"}, {"st": 104, "ed": 106, "text": "transfer learning"}, {"st": 136, "ed": 139, "text": "end to end"}, {"st": 139, "ed": 141, "text": "reinforcement learning"}, {"st": 150, "ed": 152, "text": "front end"}, {"st": 162, "ed": 165, "text": "proof of concept"}]
[{"st": 2, "ed": 5, "text": "deep reinforcement learning"}, {"st": 13, "ed": 15, "text": "atari games"}, {"st": 20, "ed": 22, "text": "raw pixels"}, {"st": 56, "ed": 59, "text": "first person shooter"}, {"st": 62, "ed": 64, "text": "partially observable"}, {"st": 66, "ed": 69, "text": "deep reinforcement learning"}, {"st": 72, "ed": 74, "text": "visual input"}, {"st": 106, "ed": 108, "text": "simultaneously learn"}, {"st": 115, "ed": 117, "text": "learning objective"}, {"st": 124, "ed": 126, "text": "training speed"}]
[{"st": 108, "ed": 110, "text": "network topology"}, {"st": 111, "ed": 113, "text": "l1 norm"}, {"st": 139, "ed": 141, "text": "real world"}]
[{"st": 7, "ed": 9, "text": "great importance"}, {"st": 18, "ed": 20, "text": "challenging task"}, {"st": 39, "ed": 41, "text": "deep learning"}, {"st": 41, "ed": 43, "text": "based approach"}, {"st": 68, "ed": 71, "text": "end to end"}, {"st": 92, "ed": 94, "text": "neural networks"}, {"st": 142, "ed": 144, "text": "neural networks"}]
[{"st": 17, "ed": 19, "text": "training set"}, {"st": 41, "ed": 44, "text": "classification and regression"}, {"st": 100, "ed": 102, "text": "evolutionary algorithms"}, {"st": 109, "ed": 111, "text": "feature space"}, {"st": 135, "ed": 137, "text": "training data"}]
[{"st": 6, "ed": 9, "text": "temporal difference td"}, {"st": 22, "ed": 24, "text": "learning algorithms"}, {"st": 27, "ed": 29, "text": "linear function"}, {"st": 31, "ed": 33, "text": "off policy"}, {"st": 89, "ed": 91, "text": "regularization techniques"}, {"st": 93, "ed": 95, "text": "ell 1"}, {"st": 113, "ed": 115, "text": "ell 1"}, {"st": 121, "ed": 123, "text": "ell 1"}, {"st": 136, "ed": 138, "text": "convergence properties"}]
[{"st": 2, "ed": 4, "text": "reinforcement learning"}, {"st": 31, "ed": 33, "text": "complex environments"}, {"st": 35, "ed": 37, "text": "current methods"}, {"st": 93, "ed": 95, "text": "achieve high"}, {"st": 114, "ed": 116, "text": "starting point"}]
[{"st": 2, "ed": 4, "text": "machine learning"}, {"st": 24, "ed": 26, "text": "gain insight"}, {"st": 35, "ed": 37, "text": "natural sciences"}, {"st": 89, "ed": 92, "text": "end to end"}, {"st": 93, "ed": 95, "text": "feed forward"}]
[{"st": 29, "ed": 32, "text": "boosted decision trees"}]
[{"st": 0, "ed": 2, "text": "accurate prediction"}, {"st": 29, "ed": 31, "text": "based approach"}, {"st": 81, "ed": 85, "text": "national renewable energy laboratory"}]
[{"st": 12, "ed": 14, "text": "prediction problems"}, {"st": 22, "ed": 25, "text": "generalized linear models"}, {"st": 78, "ed": 80, "text": "prediction tasks"}, {"st": 136, "ed": 138, "text": "extensive experiments"}, {"st": 145, "ed": 147, "text": "competitive accuracy"}, {"st": 172, "ed": 174, "text": "case study"}, {"st": 190, "ed": 192, "text": "exponentially large"}]
[{"st": 9, "ed": 11, "text": "latent topic"}, {"st": 14, "ed": 16, "text": "real data"}, {"st": 20, "ed": 22, "text": "ad hoc"}, {"st": 33, "ed": 35, "text": "poor performance"}, {"st": 47, "ed": 49, "text": "stochastic matrix"}, {"st": 54, "ed": 56, "text": "previous methods"}, {"st": 88, "ed": 90, "text": "method achieves"}]
[{"st": 7, "ed": 9, "text": "deep learning"}, {"st": 11, "ed": 14, "text": "real time strategy"}, {"st": 18, "ed": 21, "text": "starcraft brood war"}, {"st": 31, "ed": 33, "text": "machine learning"}, {"st": 37, "ed": 39, "text": "white paper"}]
[{"st": 1, "ed": 3, "text": "reinforcement learning"}, {"st": 44, "ed": 46, "text": "reinforcement learning"}]
[{"st": 4, "ed": 6, "text": "machine learning"}, {"st": 12, "ed": 15, "text": "accuracy and efficiency"}, {"st": 23, "ed": 25, "text": "random forest"}, {"st": 26, "ed": 29, "text": "deep neural nets"}, {"st": 51, "ed": 53, "text": "prediction results"}, {"st": 61, "ed": 64, "text": "customer relationship management"}, {"st": 73, "ed": 75, "text": "accurate prediction"}, {"st": 131, "ed": 133, "text": "real applications"}, {"st": 179, "ed": 181, "text": "random forest"}, {"st": 223, "ed": 225, "text": "random forest"}, {"st": 261, "ed": 264, "text": "effectiveness and efficiency"}, {"st": 266, "ed": 268, "text": "proposed approach"}, {"st": 290, "ed": 292, "text": "machine learning"}]
[{"st": 129, "ed": 131, "text": "significantly outperforms"}]
[{"st": 2, "ed": 4, "text": "video game"}, {"st": 32, "ed": 34, "text": "recent years"}, {"st": 43, "ed": 45, "text": "reinforcement learning"}, {"st": 122, "ed": 126, "text": "super nintendo entertainment system"}, {"st": 127, "ed": 129, "text": "sega genesis"}, {"st": 141, "ed": 143, "text": "video games"}, {"st": 179, "ed": 181, "text": "higher level"}]
[{"st": 17, "ed": 21, "text": "recurrent neural network rnn"}, {"st": 37, "ed": 39, "text": "pre trained"}, {"st": 42, "ed": 44, "text": "maximum likelihood"}, {"st": 73, "ed": 75, "text": "reinforcement learning"}, {"st": 78, "ed": 80, "text": "higher quality"}, {"st": 84, "ed": 86, "text": "domain specific"}, {"st": 105, "ed": 107, "text": "off policy"}, {"st": 141, "ed": 143, "text": "proposed method"}, {"st": 145, "ed": 147, "text": "desired properties"}]
[{"st": 3, "ed": 5, "text": "probabilistic models"}, {"st": 18, "ed": 21, "text": "sum product networks"}, {"st": 25, "ed": 27, "text": "deep models"}, {"st": 38, "ed": 40, "text": "hidden layers"}, {"st": 55, "ed": 57, "text": "learning problems"}, {"st": 72, "ed": 74, "text": "constraint satisfaction"}, {"st": 102, "ed": 104, "text": "previous results"}, {"st": 117, "ed": 119, "text": "learned models"}, {"st": 138, "ed": 140, "text": "structured prediction"}]
[{"st": 0, "ed": 4, "text": "generative adversarial networks gans"}, {"st": 6, "ed": 8, "text": "recently proposed"}, {"st": 10, "ed": 12, "text": "generative models"}, {"st": 21, "ed": 23, "text": "cost function"}, {"st": 36, "ed": 38, "text": "cost functions"}, {"st": 45, "ed": 47, "text": "generative modeling"}, {"st": 56, "ed": 58, "text": "reinforcement learning"}, {"st": 62, "ed": 64, "text": "imitation learning"}, {"st": 70, "ed": 72, "text": "cost function"}, {"st": 73, "ed": 75, "text": "observed behavior"}, {"st": 78, "ed": 81, "text": "inverse reinforcement learning"}, {"st": 100, "ed": 102, "text": "generative modeling"}, {"st": 133, "ed": 135, "text": "sample based"}, {"st": 137, "ed": 139, "text": "maximum entropy"}, {"st": 163, "ed": 165, "text": "maximum entropy"}, {"st": 168, "ed": 170, "text": "special case"}]
[{"st": 16, "ed": 18, "text": "reinforcement learning"}, {"st": 24, "ed": 27, "text": "markov decision processes"}, {"st": 35, "ed": 37, "text": "based methods"}, {"st": 53, "ed": 55, "text": "deep rl"}, {"st": 55, "ed": 57, "text": "exploration strategies"}, {"st": 64, "ed": 66, "text": "continuous state"}, {"st": 96, "ed": 98, "text": "count based"}, {"st": 114, "ed": 116, "text": "deep rl"}, {"st": 121, "ed": 123, "text": "hash codes"}, {"st": 155, "ed": 157, "text": "hash functions"}, {"st": 181, "ed": 183, "text": "detailed analysis"}, {"st": 189, "ed": 191, "text": "hash function"}, {"st": 205, "ed": 207, "text": "exploration strategy"}, {"st": 216, "ed": 219, "text": "continuous control tasks"}]
[{"st": 44, "ed": 46, "text": "traditional methods"}, {"st": 67, "ed": 70, "text": "divide and conquer"}, {"st": 121, "ed": 123, "text": "proposed method"}]
[{"st": 77, "ed": 79, "text": "learning algorithm"}, {"st": 88, "ed": 90, "text": "machine learning"}, {"st": 104, "ed": 106, "text": "learning algorithms"}]
[{"st": 9, "ed": 11, "text": "reinforcement learning"}, {"st": 67, "ed": 69, "text": "neural network"}, {"st": 92, "ed": 95, "text": "quantitative and qualitative"}]
[{"st": 3, "ed": 5, "text": "higher level"}, {"st": 5, "ed": 7, "text": "associative memory"}, {"st": 11, "ed": 15, "text": "generative adversarial network gan"}, {"st": 55, "ed": 57, "text": "representation space"}, {"st": 70, "ed": 72, "text": "higher level"}, {"st": 87, "ed": 91, "text": "restricted boltzmann machines rbms"}, {"st": 93, "ed": 95, "text": "higher level"}, {"st": 95, "ed": 97, "text": "associative memory"}, {"st": 104, "ed": 106, "text": "high level"}, {"st": 111, "ed": 113, "text": "associative memory"}, {"st": 132, "ed": 134, "text": "adversarial networks"}, {"st": 136, "ed": 138, "text": "generative models"}, {"st": 140, "ed": 142, "text": "higher levels"}, {"st": 161, "ed": 163, "text": "higher level"}]
[{"st": 8, "ed": 10, "text": "reinforcement learning"}, {"st": 42, "ed": 44, "text": "mutual information"}, {"st": 59, "ed": 61, "text": "policy gradient"}, {"st": 68, "ed": 70, "text": "embedding space"}]
[{"st": 9, "ed": 12, "text": "inverse reinforcement learning"}, {"st": 73, "ed": 75, "text": "proposed framework"}, {"st": 81, "ed": 84, "text": "global and local"}]
[{"st": 48, "ed": 50, "text": "existing methods"}, {"st": 123, "ed": 125, "text": "neural networks"}, {"st": 171, "ed": 173, "text": "consistently outperforms"}]
[{"st": 7, "ed": 9, "text": "policy gradient"}, {"st": 12, "ed": 14, "text": "reinforcement learning"}, {"st": 21, "ed": 23, "text": "based methods"}, {"st": 38, "ed": 40, "text": "high dimensional"}, {"st": 49, "ed": 51, "text": "exploration strategy"}, {"st": 81, "ed": 83, "text": "exploration strategy"}, {"st": 84, "ed": 87, "text": "easy to implement"}, {"st": 116, "ed": 118, "text": "hyper parameter"}, {"st": 121, "ed": 123, "text": "significant improvements"}]
[{"st": 0, "ed": 3, "text": "generative adversarial networks"}, {"st": 12, "ed": 14, "text": "deep generative"}, {"st": 19, "ed": 21, "text": "generative adversarial"}, {"st": 26, "ed": 28, "text": "sequential data"}]
[{"st": 5, "ed": 7, "text": "nearest neighbor"}, {"st": 62, "ed": 65, "text": "orders of magnitude"}]
[{"st": 11, "ed": 13, "text": "hyper parameters"}, {"st": 14, "ed": 16, "text": "deep learning"}, {"st": 28, "ed": 30, "text": "deep learning"}]
[{"st": 0, "ed": 3, "text": "deep reinforcement learning"}, {"st": 10, "ed": 12, "text": "complex tasks"}, {"st": 13, "ed": 15, "text": "partially observable"}, {"st": 47, "ed": 49, "text": "reinforcement learning"}, {"st": 53, "ed": 56, "text": "convolutional neural networks"}, {"st": 74, "ed": 76, "text": "value iteration"}, {"st": 81, "ed": 83, "text": "partially observable"}, {"st": 101, "ed": 103, "text": "transition model"}, {"st": 104, "ed": 106, "text": "reward function"}, {"st": 159, "ed": 161, "text": "near optimal"}]
[{"st": 1, "ed": 3, "text": "reinforcement learning"}, {"st": 4, "ed": 7, "text": "with expert advice"}, {"st": 27, "ed": 29, "text": "complex environments"}, {"st": 46, "ed": 48, "text": "learning scheme"}, {"st": 85, "ed": 87, "text": "reinforcement learning"}, {"st": 89, "ed": 91, "text": "strong assumptions"}, {"st": 103, "ed": 105, "text": "existing approaches"}, {"st": 116, "ed": 118, "text": "special cases"}, {"st": 123, "ed": 125, "text": "preliminary experiments"}]
[{"st": 7, "ed": 9, "text": "reinforcement learning"}, {"st": 79, "ed": 81, "text": "theoretical guarantees"}, {"st": 95, "ed": 97, "text": "empirically demonstrate"}]
[{"st": 0, "ed": 2, "text": "deep learning"}, {"st": 24, "ed": 26, "text": "reinforcement learning"}, {"st": 29, "ed": 32, "text": "deep q networks"}, {"st": 35, "ed": 38, "text": "vulnerable to adversarial"}, {"st": 45, "ed": 47, "text": "adversarial examples"}, {"st": 71, "ed": 73, "text": "learning process"}, {"st": 85, "ed": 87, "text": "adversarial examples"}]
[{"st": 6, "ed": 8, "text": "thompson sampling"}, {"st": 10, "ed": 12, "text": "sequential decision"}, {"st": 28, "ed": 31, "text": "provide theoretical guarantees"}, {"st": 48, "ed": 50, "text": "thompson sampling"}, {"st": 62, "ed": 64, "text": "upper confidence"}, {"st": 73, "ed": 76, "text": "extensive experimental results"}, {"st": 91, "ed": 93, "text": "power law"}, {"st": 119, "ed": 121, "text": "related methods"}, {"st": 123, "ed": 125, "text": "upper confidence"}]
[{"st": 4, "ed": 6, "text": "rank aggregation"}, {"st": 95, "ed": 97, "text": "small sample"}, {"st": 123, "ed": 125, "text": "similar performance"}]
[{"st": 4, "ed": 6, "text": "machine learning"}, {"st": 20, "ed": 22, "text": "time series"}, {"st": 63, "ed": 65, "text": "representation learning"}, {"st": 74, "ed": 76, "text": "time series"}, {"st": 91, "ed": 93, "text": "pairwise similarities"}, {"st": 94, "ed": 96, "text": "time series"}, {"st": 104, "ed": 106, "text": "feature representation"}, {"st": 113, "ed": 115, "text": "learning problems"}, {"st": 126, "ed": 128, "text": "time series"}, {"st": 135, "ed": 137, "text": "partially observed"}, {"st": 137, "ed": 139, "text": "similarity matrix"}, {"st": 148, "ed": 150, "text": "time series"}, {"st": 169, "ed": 171, "text": "np hard"}, {"st": 179, "ed": 181, "text": "partially observed"}, {"st": 186, "ed": 188, "text": "learned features"}, {"st": 189, "ed": 191, "text": "conduct experiments"}, {"st": 194, "ed": 197, "text": "classification and clustering"}, {"st": 199, "ed": 202, "text": "extensive experimental results"}, {"st": 205, "ed": 207, "text": "proposed framework"}]
[{"st": 44, "ed": 46, "text": "machine learning"}, {"st": 61, "ed": 63, "text": "big data"}, {"st": 66, "ed": 69, "text": "modern machine learning"}, {"st": 112, "ed": 114, "text": "machine learning"}, {"st": 138, "ed": 140, "text": "machine learning"}, {"st": 144, "ed": 146, "text": "covariate shift"}, {"st": 199, "ed": 201, "text": "covariate shift"}]
[{"st": 10, "ed": 13, "text": "multi task learning"}, {"st": 31, "ed": 33, "text": "related methods"}, {"st": 34, "ed": 37, "text": "multi task learning"}, {"st": 52, "ed": 55, "text": "hierarchical bayesian model"}, {"st": 65, "ed": 67, "text": "inference problem"}, {"st": 70, "ed": 72, "text": "convex optimization"}, {"st": 83, "ed": 85, "text": "closed form"}, {"st": 122, "ed": 124, "text": "matching problem"}, {"st": 126, "ed": 129, "text": "complete bipartite graph"}, {"st": 152, "ed": 154, "text": "linear convergence"}, {"st": 162, "ed": 164, "text": "multiple tasks"}, {"st": 185, "ed": 187, "text": "synthetic datasets"}, {"st": 188, "ed": 190, "text": "real world"}, {"st": 196, "ed": 198, "text": "optimization method"}, {"st": 199, "ed": 202, "text": "orders of magnitude"}, {"st": 222, "ed": 224, "text": "multiple tasks"}]
[{"st": 18, "ed": 20, "text": "clustering algorithm"}, {"st": 43, "ed": 45, "text": "k means"}, {"st": 67, "ed": 69, "text": "mathbb r"}, {"st": 147, "ed": 149, "text": "real world"}, {"st": 149, "ed": 151, "text": "k means"}]
[{"st": 1, "ed": 4, "text": "modern machine learning"}, {"st": 4, "ed": 6, "text": "pattern recognition"}, {"st": 63, "ed": 65, "text": "multi step"}, {"st": 74, "ed": 76, "text": "ad hoc"}]
[{"st": 1, "ed": 3, "text": "online algorithms"}, {"st": 4, "ed": 8, "text": "sum product networks spns"}, {"st": 12, "ed": 14, "text": "posterior distribution"}, {"st": 52, "ed": 54, "text": "scales linearly"}, {"st": 61, "ed": 63, "text": "online algorithms"}, {"st": 63, "ed": 65, "text": "prohibitively expensive"}, {"st": 88, "ed": 91, "text": "directed acyclic graph"}, {"st": 99, "ed": 101, "text": "online algorithms"}, {"st": 137, "ed": 139, "text": "joint inference"}, {"st": 179, "ed": 181, "text": "dynamic programming"}]
[{"st": 20, "ed": 22, "text": "atari games"}, {"st": 23, "ed": 25, "text": "motor control"}, {"st": 26, "ed": 28, "text": "board games"}, {"st": 37, "ed": 39, "text": "deep learning"}, {"st": 80, "ed": 82, "text": "complex dynamics"}, {"st": 83, "ed": 85, "text": "partial observability"}, {"st": 106, "ed": 108, "text": "recent advances"}, {"st": 143, "ed": 145, "text": "video game"}]
[{"st": 13, "ed": 15, "text": "generating process"}, {"st": 50, "ed": 52, "text": "policy search"}, {"st": 57, "ed": 59, "text": "sample efficiency"}, {"st": 66, "ed": 68, "text": "sample complexity"}, {"st": 73, "ed": 75, "text": "uniform convergence"}]
[{"st": 9, "ed": 11, "text": "causal direction"}, {"st": 13, "ed": 15, "text": "random variables"}, {"st": 32, "ed": 34, "text": "halting problem"}, {"st": 45, "ed": 47, "text": "causal inference"}, {"st": 60, "ed": 63, "text": "minimum description length"}, {"st": 93, "ed": 95, "text": "true distribution"}, {"st": 125, "ed": 127, "text": "discrete variables"}, {"st": 138, "ed": 140, "text": "highly accurate"}, {"st": 146, "ed": 149, "text": "real world data"}]
[{"st": 9, "ed": 11, "text": "phase transitions"}, {"st": 40, "ed": 42, "text": "critical phenomena"}, {"st": 53, "ed": 55, "text": "statistical physics"}, {"st": 58, "ed": 60, "text": "critical point"}, {"st": 62, "ed": 64, "text": "feed forward"}, {"st": 64, "ed": 66, "text": "fully connected"}]
[{"st": 12, "ed": 15, "text": "states and actions"}, {"st": 30, "ed": 32, "text": "maximum entropy"}, {"st": 45, "ed": 47, "text": "optimal policy"}, {"st": 54, "ed": 56, "text": "recently proposed"}, {"st": 77, "ed": 79, "text": "proposed algorithm"}, {"st": 115, "ed": 117, "text": "approximate inference"}]
[{"st": 6, "ed": 8, "text": "machine learning"}, {"st": 10, "ed": 12, "text": "data mining"}, {"st": 15, "ed": 17, "text": "difficult task"}, {"st": 31, "ed": 33, "text": "real world"}, {"st": 35, "ed": 37, "text": "benchmark datasets"}, {"st": 64, "ed": 66, "text": "machine learning"}, {"st": 87, "ed": 90, "text": "strengths and weaknesses"}, {"st": 92, "ed": 94, "text": "machine learning"}, {"st": 104, "ed": 106, "text": "benchmark datasets"}, {"st": 123, "ed": 125, "text": "machine learning"}]
[{"st": 0, "ed": 2, "text": "representation learning"}, {"st": 12, "ed": 14, "text": "reinforcement learning"}, {"st": 26, "ed": 28, "text": "representation learning"}, {"st": 53, "ed": 55, "text": "reward functions"}, {"st": 77, "ed": 79, "text": "multiple tasks"}]
[{"st": 17, "ed": 19, "text": "artificial intelligence"}, {"st": 116, "ed": 118, "text": "mathbb r"}, {"st": 120, "ed": 122, "text": "linearly separable"}, {"st": 180, "ed": 182, "text": "machine learning"}]
[{"st": 31, "ed": 33, "text": "monte carlo"}, {"st": 115, "ed": 117, "text": "multi step"}, {"st": 121, "ed": 123, "text": "q sigma"}, {"st": 128, "ed": 130, "text": "existing algorithms"}, {"st": 180, "ed": 182, "text": "q sigma"}, {"st": 189, "ed": 191, "text": "off policy"}, {"st": 221, "ed": 223, "text": "existing algorithms"}]
[{"st": 0, "ed": 2, "text": "cluster analysis"}, {"st": 4, "ed": 6, "text": "important role"}, {"st": 7, "ed": 9, "text": "decision making"}, {"st": 18, "ed": 20, "text": "wide variety"}, {"st": 25, "ed": 27, "text": "applications including"}, {"st": 30, "ed": 32, "text": "probabilistic models"}, {"st": 45, "ed": 47, "text": "big bang"}, {"st": 47, "ed": 49, "text": "big crunch"}, {"st": 56, "ed": 58, "text": "proposed method"}, {"st": 60, "ed": 63, "text": "takes advantage of"}, {"st": 68, "ed": 70, "text": "clustering algorithms"}, {"st": 72, "ed": 74, "text": "k means"}, {"st": 95, "ed": 97, "text": "proposed algorithm"}, {"st": 122, "ed": 124, "text": "proposed method"}]
[{"st": 10, "ed": 12, "text": "large scale"}, {"st": 47, "ed": 49, "text": "large scale"}, {"st": 55, "ed": 57, "text": "small sample"}, {"st": 65, "ed": 67, "text": "causal relationships"}, {"st": 100, "ed": 102, "text": "comparative study"}, {"st": 121, "ed": 123, "text": "breast cancer"}]
[{"st": 14, "ed": 16, "text": "random variables"}, {"st": 24, "ed": 26, "text": "search space"}, {"st": 41, "ed": 43, "text": "np hard"}, {"st": 70, "ed": 72, "text": "structure learning"}, {"st": 98, "ed": 100, "text": "structural constraints"}, {"st": 124, "ed": 126, "text": "structure learning"}, {"st": 129, "ed": 131, "text": "extensive simulations"}, {"st": 142, "ed": 144, "text": "local search"}, {"st": 150, "ed": 152, "text": "main results"}, {"st": 160, "ed": 162, "text": "learning task"}, {"st": 166, "ed": 168, "text": "search space"}]
[{"st": 1, "ed": 4, "text": "deep neural networks"}, {"st": 16, "ed": 18, "text": "step sizes"}, {"st": 39, "ed": 41, "text": "deep learning"}, {"st": 48, "ed": 50, "text": "loss function"}, {"st": 88, "ed": 90, "text": "hand crafted"}, {"st": 90, "ed": 92, "text": "optimization algorithms"}, {"st": 117, "ed": 119, "text": "tasks including"}]
[{"st": 5, "ed": 7, "text": "machine learning"}, {"st": 45, "ed": 48, "text": "end to end"}, {"st": 52, "ed": 54, "text": "machine learning"}, {"st": 85, "ed": 87, "text": "proposed approach"}, {"st": 93, "ed": 95, "text": "real world"}, {"st": 95, "ed": 97, "text": "electrical grid"}, {"st": 101, "ed": 103, "text": "real world"}, {"st": 111, "ed": 113, "text": "proposed approach"}, {"st": 122, "ed": 124, "text": "policy optimization"}]
[{"st": 1, "ed": 3, "text": "policy gradients"}, {"st": 69, "ed": 71, "text": "reinforcement learning"}, {"st": 104, "ed": 106, "text": "policy gradients"}]
[{"st": 0, "ed": 2, "text": "catastrophic forgetting"}, {"st": 6, "ed": 8, "text": "neural networks"}, {"st": 28, "ed": 30, "text": "moment matching"}, {"st": 42, "ed": 44, "text": "posterior distribution"}, {"st": 46, "ed": 48, "text": "neural network"}, {"st": 62, "ed": 64, "text": "search space"}, {"st": 75, "ed": 77, "text": "transfer learning"}, {"st": 110, "ed": 113, "text": "mnist cifar 10"}]
[{"st": 1, "ed": 3, "text": "artificial intelligence"}, {"st": 16, "ed": 18, "text": "efficient learning"}, {"st": 41, "ed": 43, "text": "case study"}, {"st": 114, "ed": 116, "text": "labelled data"}, {"st": 159, "ed": 161, "text": "large scale"}, {"st": 161, "ed": 163, "text": "real world"}]
[{"st": 2, "ed": 4, "text": "autonomous systems"}, {"st": 54, "ed": 56, "text": "reinforcement learning"}, {"st": 103, "ed": 105, "text": "reinforcement learning"}]
[{"st": 11, "ed": 13, "text": "multi label"}, {"st": 14, "ed": 16, "text": "existing approaches"}, {"st": 47, "ed": 49, "text": "real world"}, {"st": 68, "ed": 70, "text": "local group"}, {"st": 118, "ed": 120, "text": "multi label"}, {"st": 134, "ed": 137, "text": "global and local"}, {"st": 151, "ed": 153, "text": "extensive experimental"}]
[{"st": 0, "ed": 4, "text": "deep reinforcement learning rl"}, {"st": 12, "ed": 14, "text": "decision making"}, {"st": 18, "ed": 20, "text": "typically require"}, {"st": 54, "ed": 56, "text": "deep rl"}, {"st": 58, "ed": 60, "text": "real world"}, {"st": 104, "ed": 106, "text": "demonstration data"}, {"st": 110, "ed": 112, "text": "learning process"}, {"st": 118, "ed": 120, "text": "demonstration data"}, {"st": 130, "ed": 132, "text": "demonstration data"}, {"st": 144, "ed": 146, "text": "temporal difference"}, {"st": 148, "ed": 150, "text": "supervised classification"}, {"st": 167, "ed": 170, "text": "deep q networks"}, {"st": 248, "ed": 250, "text": "demonstration data"}]
[{"st": 22, "ed": 25, "text": "domain specific language"}, {"st": 45, "ed": 47, "text": "regular expression"}, {"st": 79, "ed": 81, "text": "search algorithm"}, {"st": 82, "ed": 84, "text": "recently introduced"}, {"st": 84, "ed": 86, "text": "neural architectures"}, {"st": 88, "ed": 90, "text": "input output"}, {"st": 106, "ed": 108, "text": "baseline methods"}, {"st": 113, "ed": 117, "text": "synthetic and real world"}]
[{"st": 1, "ed": 3, "text": "reinforcement learning"}, {"st": 5, "ed": 7, "text": "increasingly popular"}, {"st": 39, "ed": 41, "text": "online learning"}, {"st": 64, "ed": 67, "text": "online learning algorithms"}, {"st": 103, "ed": 105, "text": "main idea"}, {"st": 116, "ed": 118, "text": "decision rule"}, {"st": 137, "ed": 139, "text": "online learning"}, {"st": 146, "ed": 148, "text": "online learning"}, {"st": 162, "ed": 164, "text": "online learning"}, {"st": 169, "ed": 171, "text": "online learning"}, {"st": 176, "ed": 178, "text": "decision rules"}, {"st": 206, "ed": 208, "text": "experiment results"}]
[{"st": 3, "ed": 5, "text": "reinforcement learning"}, {"st": 9, "ed": 11, "text": "bias variance"}, {"st": 41, "ed": 43, "text": "recurrent networks"}, {"st": 60, "ed": 62, "text": "atari games"}]
[{"st": 9, "ed": 11, "text": "distributed computing"}, {"st": 15, "ed": 17, "text": "social network"}, {"st": 100, "ed": 102, "text": "time series"}, {"st": 111, "ed": 113, "text": "kernel based"}, {"st": 118, "ed": 120, "text": "nearest neighbor"}, {"st": 120, "ed": 122, "text": "kernel regression"}, {"st": 123, "ed": 125, "text": "gaussian process"}, {"st": 157, "ed": 159, "text": "kernel based"}, {"st": 167, "ed": 169, "text": "gaussian processes"}, {"st": 192, "ed": 194, "text": "real data"}, {"st": 210, "ed": 212, "text": "kernel regression"}, {"st": 224, "ed": 226, "text": "gaussian process"}, {"st": 227, "ed": 229, "text": "significantly improves"}, {"st": 230, "ed": 232, "text": "prediction error"}]
[{"st": 1, "ed": 3, "text": "time series"}, {"st": 24, "ed": 26, "text": "feature engineering"}, {"st": 32, "ed": 35, "text": "change point detection"}, {"st": 39, "ed": 41, "text": "similarity based"}, {"st": 41, "ed": 43, "text": "classification models"}, {"st": 67, "ed": 69, "text": "missing values"}, {"st": 73, "ed": 75, "text": "dimensionality reduction"}, {"st": 120, "ed": 123, "text": "positive or negative"}, {"st": 139, "ed": 142, "text": "intensive care unit"}, {"st": 150, "ed": 153, "text": "k nearest neighbor"}, {"st": 161, "ed": 163, "text": "significantly outperform"}, {"st": 168, "ed": 170, "text": "icu mortality"}, {"st": 176, "ed": 178, "text": "time series"}, {"st": 181, "ed": 183, "text": "icu mortality"}, {"st": 189, "ed": 191, "text": "feature engineering"}, {"st": 195, "ed": 197, "text": "similarity based"}, {"st": 197, "ed": 199, "text": "time series"}]
[{"st": 8, "ed": 11, "text": "intensive care unit"}, {"st": 11, "ed": 13, "text": "icu mortality"}, {"st": 23, "ed": 25, "text": "predictive modeling"}, {"st": 32, "ed": 34, "text": "icu mortality"}, {"st": 71, "ed": 73, "text": "time series"}, {"st": 75, "ed": 77, "text": "vital signs"}, {"st": 87, "ed": 89, "text": "approximation algorithm"}, {"st": 121, "ed": 123, "text": "significantly outperforms"}, {"st": 143, "ed": 145, "text": "time series"}, {"st": 148, "ed": 150, "text": "feature selection"}]
[{"st": 0, "ed": 3, "text": "semi supervised learning"}, {"st": 5, "ed": 7, "text": "important role"}, {"st": 8, "ed": 10, "text": "large scale"}, {"st": 15, "ed": 17, "text": "unlabeled data"}, {"st": 24, "ed": 26, "text": "machine learning"}, {"st": 30, "ed": 32, "text": "machine learning"}, {"st": 62, "ed": 64, "text": "generative models"}, {"st": 84, "ed": 88, "text": "supervised and semi supervised"}, {"st": 96, "ed": 98, "text": "generative models"}, {"st": 115, "ed": 117, "text": "proposed method"}, {"st": 119, "ed": 121, "text": "image classification"}, {"st": 123, "ed": 125, "text": "pascal voc"}, {"st": 130, "ed": 132, "text": "proposed method"}, {"st": 144, "ed": 147, "text": "semi supervised learning"}]
[{"st": 15, "ed": 18, "text": "deep q network"}, {"st": 32, "ed": 35, "text": "deep reinforcement learning"}, {"st": 83, "ed": 85, "text": "pre trained"}, {"st": 91, "ed": 93, "text": "fine tuned"}, {"st": 128, "ed": 130, "text": "lifelong learning"}, {"st": 151, "ed": 153, "text": "catastrophic forgetting"}, {"st": 165, "ed": 168, "text": "long term memory"}]
[{"st": 5, "ed": 7, "text": "online learning"}, {"st": 23, "ed": 25, "text": "lifelong learning"}, {"st": 58, "ed": 60, "text": "metric learning"}, {"st": 62, "ed": 64, "text": "metric learning"}, {"st": 112, "ed": 114, "text": "task specific"}, {"st": 137, "ed": 139, "text": "optimization algorithm"}, {"st": 150, "ed": 152, "text": "task specific"}, {"st": 166, "ed": 168, "text": "multi task"}, {"st": 168, "ed": 170, "text": "metric learning"}, {"st": 171, "ed": 174, "text": "extensive experimental results"}, {"st": 175, "ed": 178, "text": "effectiveness and efficiency"}]
[{"st": 1, "ed": 3, "text": "machine learning"}, {"st": 43, "ed": 45, "text": "computational budget"}, {"st": 98, "ed": 100, "text": "predictive models"}, {"st": 121, "ed": 123, "text": "reinforcement learning"}, {"st": 132, "ed": 134, "text": "optimization procedure"}, {"st": 179, "ed": 181, "text": "off policy"}, {"st": 212, "ed": 214, "text": "decision making"}, {"st": 270, "ed": 272, "text": "computational cost"}]
[{"st": 7, "ed": 9, "text": "empirical study"}, {"st": 12, "ed": 15, "text": "temporal difference learning"}, {"st": 21, "ed": 24, "text": "temporal difference learning"}, {"st": 34, "ed": 36, "text": "off policy"}, {"st": 53, "ed": 55, "text": "convergence properties"}, {"st": 56, "ed": 58, "text": "off policy"}, {"st": 82, "ed": 84, "text": "off policy"}, {"st": 132, "ed": 134, "text": "off policy"}, {"st": 141, "ed": 144, "text": "guaranteed to converge"}]
[{"st": 5, "ed": 8, "text": "inverse reinforcement learning"}]
[{"st": 3, "ed": 5, "text": "ensemble learning"}, {"st": 15, "ed": 17, "text": "complex environments"}, {"st": 21, "ed": 24, "text": "bias and variance"}, {"st": 48, "ed": 50, "text": "ensemble learning"}, {"st": 52, "ed": 54, "text": "non stationary"}, {"st": 70, "ed": 72, "text": "base classifier"}, {"st": 78, "ed": 80, "text": "sliding window"}, {"st": 128, "ed": 131, "text": "online feature selection"}, {"st": 181, "ed": 183, "text": "generalization error"}, {"st": 189, "ed": 192, "text": "online feature selection"}, {"st": 207, "ed": 209, "text": "input features"}, {"st": 222, "ed": 224, "text": "classification decision"}]
[{"st": 0, "ed": 3, "text": "statistical relational learning"}, {"st": 6, "ed": 8, "text": "anomaly detection"}, {"st": 18, "ed": 20, "text": "online learning"}, {"st": 32, "ed": 34, "text": "learning process"}, {"st": 56, "ed": 58, "text": "common sense"}, {"st": 80, "ed": 82, "text": "learning algorithms"}, {"st": 86, "ed": 88, "text": "online learning"}, {"st": 132, "ed": 134, "text": "anomaly detection"}]
[{"st": 3, "ed": 5, "text": "sparse rewards"}, {"st": 16, "ed": 18, "text": "reinforcement learning"}, {"st": 98, "ed": 101, "text": "end to end"}, {"st": 102, "ed": 104, "text": "reinforcement learning"}, {"st": 143, "ed": 145, "text": "sparse rewards"}, {"st": 148, "ed": 150, "text": "agent learns"}, {"st": 151, "ed": 154, "text": "times faster than"}, {"st": 195, "ed": 197, "text": "comparable performance"}, {"st": 222, "ed": 225, "text": "ability to learn"}]
[{"st": 0, "ed": 3, "text": "deep reinforcement learning"}, {"st": 16, "ed": 18, "text": "decision making"}, {"st": 23, "ed": 25, "text": "decision making"}, {"st": 36, "ed": 38, "text": "compositional structure"}, {"st": 76, "ed": 78, "text": "control policies"}, {"st": 89, "ed": 91, "text": "compositional structure"}, {"st": 99, "ed": 101, "text": "learning paradigm"}, {"st": 115, "ed": 118, "text": "deep reinforcement learning"}, {"st": 177, "ed": 180, "text": "advantage actor critic"}]
[{"st": 0, "ed": 2, "text": "reinforcement learning"}, {"st": 11, "ed": 14, "text": "sequential decision making"}, {"st": 21, "ed": 24, "text": "temporal difference td"}, {"st": 58, "ed": 60, "text": "monte carlo"}, {"st": 69, "ed": 71, "text": "extensively studied"}, {"st": 116, "ed": 118, "text": "ad hoc"}, {"st": 141, "ed": 143, "text": "rl agent"}, {"st": 153, "ed": 156, "text": "end to end"}, {"st": 206, "ed": 208, "text": "empirically demonstrate"}, {"st": 214, "ed": 216, "text": "multi step"}, {"st": 237, "ed": 240, "text": "advantage actor critic"}]
[{"st": 1, "ed": 3, "text": "classification techniques"}, {"st": 17, "ed": 20, "text": "deep neural networks"}, {"st": 26, "ed": 28, "text": "significantly improve"}, {"st": 29, "ed": 31, "text": "prediction performance"}, {"st": 81, "ed": 83, "text": "empirical results"}, {"st": 84, "ed": 87, "text": "cifar and imagenet"}, {"st": 115, "ed": 117, "text": "top 5"}, {"st": 117, "ed": 119, "text": "imagenet classification"}]
[{"st": 3, "ed": 5, "text": "probabilistic framework"}, {"st": 6, "ed": 8, "text": "domain adaptation"}, {"st": 11, "ed": 14, "text": "generative and discriminative"}, {"st": 22, "ed": 25, "text": "generative and discriminative"}, {"st": 45, "ed": 48, "text": "generative and discriminative"}, {"st": 72, "ed": 74, "text": "labeled instances"}, {"st": 76, "ed": 78, "text": "source domain"}, {"st": 81, "ed": 83, "text": "unlabeled instances"}, {"st": 113, "ed": 115, "text": "unlabeled instances"}, {"st": 116, "ed": 118, "text": "marginal distributions"}, {"st": 122, "ed": 124, "text": "kernel density"}, {"st": 132, "ed": 134, "text": "empirical success"}, {"st": 137, "ed": 139, "text": "domain adaptation"}, {"st": 148, "ed": 150, "text": "neural networks"}, {"st": 176, "ed": 178, "text": "domain adaptation"}]
[{"st": 7, "ed": 9, "text": "widely studied"}, {"st": 49, "ed": 51, "text": "wide variety"}, {"st": 55, "ed": 57, "text": "hand crafted"}, {"st": 60, "ed": 62, "text": "deep learning"}, {"st": 82, "ed": 85, "text": "hand crafted features"}, {"st": 106, "ed": 109, "text": "end to end"}, {"st": 109, "ed": 111, "text": "deep learning"}, {"st": 137, "ed": 139, "text": "soft attention"}, {"st": 163, "ed": 165, "text": "proposed method"}, {"st": 170, "ed": 172, "text": "consistently outperforms"}, {"st": 172, "ed": 174, "text": "previously proposed"}, {"st": 183, "ed": 185, "text": "large scale"}]
[{"st": 68, "ed": 70, "text": "probabilistic model"}, {"st": 78, "ed": 80, "text": "main contributions"}, {"st": 100, "ed": 103, "text": "taking into account"}, {"st": 112, "ed": 114, "text": "real world"}]
[{"st": 0, "ed": 3, "text": "semi supervised learning"}, {"st": 6, "ed": 10, "text": "generative adversarial networks gans"}, {"st": 12, "ed": 14, "text": "empirical results"}, {"st": 25, "ed": 27, "text": "joint training"}, {"st": 34, "ed": 37, "text": "semi supervised classification"}, {"st": 84, "ed": 86, "text": "substantially improves"}]
[{"st": 8, "ed": 10, "text": "gradient based"}, {"st": 29, "ed": 31, "text": "optimisation problems"}, {"st": 40, "ed": 42, "text": "based approaches"}, {"st": 48, "ed": 50, "text": "gradient based"}, {"st": 69, "ed": 71, "text": "fine grained"}, {"st": 78, "ed": 80, "text": "local optima"}, {"st": 86, "ed": 88, "text": "online learning"}, {"st": 94, "ed": 97, "text": "trial and error"}, {"st": 138, "ed": 140, "text": "linear approximation"}, {"st": 202, "ed": 204, "text": "empirical results"}, {"st": 216, "ed": 218, "text": "improved performance"}, {"st": 244, "ed": 246, "text": "competing approaches"}, {"st": 264, "ed": 266, "text": "improved performance"}]
[{"st": 15, "ed": 17, "text": "deep learning"}, {"st": 32, "ed": 35, "text": "statistical relational learning"}, {"st": 41, "ed": 43, "text": "neural networks"}, {"st": 55, "ed": 57, "text": "outperform existing"}, {"st": 90, "ed": 93, "text": "standard benchmark datasets"}, {"st": 107, "ed": 110, "text": "orders of magnitude"}]
[{"st": 10, "ed": 12, "text": "network architecture"}, {"st": 74, "ed": 78, "text": "mean squared error mse"}, {"st": 154, "ed": 156, "text": "significantly improves"}]
[{"st": 29, "ed": 31, "text": "standard datasets"}, {"st": 73, "ed": 75, "text": "performance improvements"}, {"st": 86, "ed": 89, "text": "hyper parameter tuning"}, {"st": 96, "ed": 98, "text": "future research"}]
[{"st": 0, "ed": 2, "text": "human trafficking"}, {"st": 12, "ed": 14, "text": "challenging problems"}, {"st": 28, "ed": 30, "text": "textual data"}, {"st": 43, "ed": 45, "text": "human trafficking"}, {"st": 63, "ed": 65, "text": "ground truth"}, {"st": 97, "ed": 99, "text": "regularization term"}, {"st": 106, "ed": 108, "text": "feature space"}, {"st": 118, "ed": 120, "text": "proposed method"}, {"st": 121, "ed": 124, "text": "labeled and unlabeled"}, {"st": 133, "ed": 135, "text": "unlabeled data"}, {"st": 139, "ed": 141, "text": "unseen data"}, {"st": 155, "ed": 157, "text": "semi supervised"}, {"st": 158, "ed": 160, "text": "supervised approaches"}, {"st": 162, "ed": 164, "text": "labeled data"}]
[{"st": 7, "ed": 9, "text": "value iteration"}, {"st": 14, "ed": 17, "text": "end to end"}, {"st": 17, "ed": 19, "text": "neural network"}, {"st": 24, "ed": 26, "text": "value iteration"}, {"st": 103, "ed": 105, "text": "real world"}, {"st": 120, "ed": 122, "text": "larger scale"}]
[{"st": 136, "ed": 139, "text": "semi supervised learning"}]
[{"st": 8, "ed": 10, "text": "multi agent"}, {"st": 19, "ed": 21, "text": "multi agent"}, {"st": 37, "ed": 39, "text": "reinforcement learning"}, {"st": 40, "ed": 42, "text": "evolutionary algorithm"}, {"st": 80, "ed": 82, "text": "reinforcement learning"}, {"st": 84, "ed": 86, "text": "deep learning"}, {"st": 89, "ed": 91, "text": "efficiently learn"}, {"st": 98, "ed": 100, "text": "partially observable"}, {"st": 115, "ed": 118, "text": "continuous and discrete"}]
[{"st": 16, "ed": 19, "text": "multi label classification"}, {"st": 33, "ed": 35, "text": "prior knowledge"}, {"st": 74, "ed": 76, "text": "multi output"}, {"st": 77, "ed": 79, "text": "inference problems"}, {"st": 91, "ed": 93, "text": "loss function"}, {"st": 94, "ed": 96, "text": "significantly lower"}, {"st": 136, "ed": 138, "text": "real world"}, {"st": 173, "ed": 175, "text": "intrinsic dimension"}, {"st": 185, "ed": 187, "text": "optimal solutions"}, {"st": 196, "ed": 198, "text": "feature set"}]
[{"st": 11, "ed": 13, "text": "computational requirements"}, {"st": 14, "ed": 17, "text": "training and inference"}, {"st": 130, "ed": 132, "text": "reward signal"}, {"st": 143, "ed": 145, "text": "main result"}, {"st": 151, "ed": 153, "text": "imagenet classification"}, {"st": 161, "ed": 164, "text": "neural machine translation"}, {"st": 167, "ed": 169, "text": "non trivial"}, {"st": 173, "ed": 175, "text": "hand crafted"}]
[{"st": 5, "ed": 7, "text": "zero shot"}, {"st": 8, "ed": 10, "text": "generalization capabilities"}, {"st": 11, "ed": 13, "text": "reinforcement learning"}, {"st": 47, "ed": 49, "text": "previously unseen"}, {"st": 109, "ed": 111, "text": "neural architecture"}]
[{"st": 87, "ed": 89, "text": "game tree"}, {"st": 132, "ed": 134, "text": "sample complexity"}, {"st": 139, "ed": 141, "text": "previous results"}]
[{"st": 4, "ed": 6, "text": "deep learning"}, {"st": 18, "ed": 20, "text": "recurrent neural"}, {"st": 52, "ed": 54, "text": "input data"}, {"st": 65, "ed": 69, "text": "feed forward neural networks"}, {"st": 70, "ed": 72, "text": "skip connections"}, {"st": 74, "ed": 76, "text": "special cases"}, {"st": 81, "ed": 83, "text": "connectivity patterns"}, {"st": 94, "ed": 97, "text": "recurrent neural networks"}]
[{"st": 0, "ed": 2, "text": "multi agent"}, {"st": 2, "ed": 4, "text": "predictive modeling"}, {"st": 27, "ed": 29, "text": "multi agent"}, {"st": 44, "ed": 46, "text": "higher order"}, {"st": 63, "ed": 65, "text": "multi agent"}, {"st": 65, "ed": 67, "text": "predictive modeling"}, {"st": 68, "ed": 70, "text": "scales linearly"}, {"st": 82, "ed": 84, "text": "multi agent"}, {"st": 94, "ed": 96, "text": "multi agent"}, {"st": 104, "ed": 106, "text": "multi agent"}]
[{"st": 12, "ed": 14, "text": "policy gradient"}]
[{"st": 32, "ed": 34, "text": "continual learning"}, {"st": 83, "ed": 85, "text": "transfer knowledge"}, {"st": 93, "ed": 95, "text": "continual learning"}, {"st": 97, "ed": 99, "text": "episodic memory"}, {"st": 118, "ed": 121, "text": "mnist and cifar"}, {"st": 125, "ed": 127, "text": "strong performance"}]
[{"st": 0, "ed": 2, "text": "virtual reality"}, {"st": 25, "ed": 28, "text": "received much attention"}, {"st": 59, "ed": 61, "text": "prediction models"}, {"st": 66, "ed": 68, "text": "existing techniques"}, {"st": 92, "ed": 94, "text": "random forest"}, {"st": 94, "ed": 96, "text": "based method"}, {"st": 108, "ed": 110, "text": "temporal bone"}, {"st": 115, "ed": 117, "text": "proposed method"}, {"st": 121, "ed": 123, "text": "highly effective"}]
[{"st": 6, "ed": 8, "text": "variational bayes"}, {"st": 10, "ed": 12, "text": "rnn model"}, {"st": 18, "ed": 20, "text": "temporal patterns"}, {"st": 32, "ed": 34, "text": "weighted sum"}, {"st": 89, "ed": 91, "text": "regularization term"}, {"st": 129, "ed": 131, "text": "autism spectrum"}]
[{"st": 71, "ed": 73, "text": "learning curve"}, {"st": 108, "ed": 110, "text": "hand crafted"}, {"st": 126, "ed": 128, "text": "automatically generated"}, {"st": 159, "ed": 161, "text": "uniform sampling"}]
[{"st": 1, "ed": 3, "text": "rank aggregation"}, {"st": 4, "ed": 6, "text": "score based"}, {"st": 26, "ed": 28, "text": "submodular optimization"}, {"st": 29, "ed": 31, "text": "rank aggregation"}, {"st": 32, "ed": 34, "text": "score based"}, {"st": 49, "ed": 51, "text": "bregman divergence"}, {"st": 65, "ed": 67, "text": "stochastic optimization"}, {"st": 72, "ed": 74, "text": "training process"}, {"st": 75, "ed": 77, "text": "efficient algorithms"}, {"st": 90, "ed": 92, "text": "neural networks"}, {"st": 94, "ed": 96, "text": "social networks"}, {"st": 98, "ed": 101, "text": "automatic speech recognition"}]
[{"st": 8, "ed": 10, "text": "optimization strategy"}, {"st": 23, "ed": 26, "text": "support vector machine"}, {"st": 30, "ed": 32, "text": "optimization strategy"}, {"st": 38, "ed": 40, "text": "local search"}, {"st": 44, "ed": 46, "text": "hill climbing"}, {"st": 51, "ed": 53, "text": "local search"}]
[{"st": 13, "ed": 15, "text": "multi dimensional"}, {"st": 18, "ed": 21, "text": "each data point"}, {"st": 25, "ed": 27, "text": "multi dimensional"}, {"st": 39, "ed": 41, "text": "kernel method"}, {"st": 53, "ed": 55, "text": "kernel methods"}]
[{"st": 5, "ed": 9, "text": "deep reinforcement learning rl"}, {"st": 95, "ed": 97, "text": "prediction model"}, {"st": 102, "ed": 106, "text": "deep q network dqn"}, {"st": 108, "ed": 110, "text": "atari games"}]
[{"st": 0, "ed": 2, "text": "domain knowledge"}, {"st": 14, "ed": 16, "text": "convolutional layers"}, {"st": 27, "ed": 29, "text": "sample complexity"}, {"st": 44, "ed": 46, "text": "sample complexity"}, {"st": 62, "ed": 65, "text": "structure and parameters"}, {"st": 75, "ed": 77, "text": "sample complexity"}, {"st": 95, "ed": 97, "text": "gain insight"}]
[{"st": 9, "ed": 11, "text": "algorithmic framework"}, {"st": 37, "ed": 39, "text": "step size"}, {"st": 56, "ed": 58, "text": "proposed approach"}, {"st": 73, "ed": 75, "text": "convergence rate"}, {"st": 82, "ed": 84, "text": "conduct extensive"}, {"st": 84, "ed": 86, "text": "empirical studies"}, {"st": 90, "ed": 92, "text": "neural network"}, {"st": 92, "ed": 94, "text": "optimization problems"}, {"st": 95, "ed": 98, "text": "multi layer perceptron"}, {"st": 99, "ed": 101, "text": "neural networks"}, {"st": 113, "ed": 115, "text": "step size"}, {"st": 125, "ed": 127, "text": "significant speedup"}]
[{"st": 4, "ed": 7, "text": "deep neural network"}, {"st": 10, "ed": 12, "text": "reinforcement learning"}, {"st": 12, "ed": 14, "text": "based approaches"}, {"st": 15, "ed": 17, "text": "recently shown"}, {"st": 26, "ed": 28, "text": "computational resources"}, {"st": 93, "ed": 95, "text": "reinforcement learning"}, {"st": 106, "ed": 108, "text": "network depth"}, {"st": 147, "ed": 150, "text": "convolutional neural networks"}, {"st": 151, "ed": 153, "text": "skip connections"}, {"st": 157, "ed": 159, "text": "benchmark datasets"}, {"st": 159, "ed": 161, "text": "cifar 10"}, {"st": 164, "ed": 166, "text": "computational resources"}, {"st": 173, "ed": 175, "text": "highly competitive"}, {"st": 177, "ed": 179, "text": "outperform existing"}, {"st": 186, "ed": 188, "text": "cifar 10"}, {"st": 191, "ed": 193, "text": "skip connections"}, {"st": 195, "ed": 197, "text": "test error"}]
[{"st": 7, "ed": 10, "text": "first order logic"}, {"st": 25, "ed": 27, "text": "neural network"}, {"st": 44, "ed": 46, "text": "deep learning"}, {"st": 53, "ed": 55, "text": "deep learning"}, {"st": 74, "ed": 76, "text": "problems involving"}, {"st": 76, "ed": 79, "text": "hundreds of thousands"}, {"st": 84, "ed": 87, "text": "tens of thousands"}]
[{"st": 0, "ed": 2, "text": "imitation learning"}, {"st": 18, "ed": 20, "text": "generative adversarial"}, {"st": 20, "ed": 22, "text": "imitation learning"}, {"st": 54, "ed": 56, "text": "cost function"}, {"st": 67, "ed": 69, "text": "heavy tailed"}, {"st": 93, "ed": 95, "text": "catastrophic failure"}, {"st": 170, "ed": 172, "text": "imitation learning"}]
[{"st": 10, "ed": 12, "text": "generated images"}, {"st": 13, "ed": 17, "text": "generative adversarial networks gans"}, {"st": 46, "ed": 48, "text": "likelihood function"}, {"st": 63, "ed": 65, "text": "likelihood function"}, {"st": 75, "ed": 77, "text": "real images"}, {"st": 97, "ed": 99, "text": "generated images"}, {"st": 118, "ed": 120, "text": "generated images"}, {"st": 125, "ed": 127, "text": "empirical results"}, {"st": 128, "ed": 130, "text": "cifar 10"}]
[{"st": 0, "ed": 3, "text": "multi task learning"}, {"st": 6, "ed": 8, "text": "learning paradigm"}, {"st": 9, "ed": 11, "text": "machine learning"}, {"st": 22, "ed": 24, "text": "related tasks"}, {"st": 28, "ed": 30, "text": "generalization performance"}, {"st": 52, "ed": 54, "text": "feature learning"}, {"st": 55, "ed": 57, "text": "low rank"}, {"st": 59, "ed": 61, "text": "clustering approach"}, {"st": 67, "ed": 69, "text": "multi level"}, {"st": 71, "ed": 73, "text": "deep learning"}, {"st": 106, "ed": 109, "text": "semi supervised learning"}, {"st": 109, "ed": 111, "text": "active learning"}, {"st": 111, "ed": 113, "text": "reinforcement learning"}, {"st": 113, "ed": 115, "text": "multi view"}, {"st": 163, "ed": 165, "text": "real world"}, {"st": 181, "ed": 183, "text": "theoretical analyses"}, {"st": 186, "ed": 188, "text": "future directions"}]
[{"st": 4, "ed": 6, "text": "artificial intelligence"}, {"st": 18, "ed": 20, "text": "probabilistic models"}, {"st": 30, "ed": 32, "text": "probabilistic model"}, {"st": 36, "ed": 38, "text": "practical problems"}, {"st": 46, "ed": 48, "text": "limited data"}, {"st": 65, "ed": 67, "text": "probabilistic model"}, {"st": 92, "ed": 94, "text": "probability distributions"}, {"st": 101, "ed": 103, "text": "learning algorithm"}, {"st": 216, "ed": 218, "text": "special cases"}]
[{"st": 10, "ed": 12, "text": "lifelong learning"}, {"st": 34, "ed": 36, "text": "self experimentation"}, {"st": 45, "ed": 47, "text": "reinforcement learning"}, {"st": 54, "ed": 56, "text": "approach called"}, {"st": 86, "ed": 88, "text": "reinforcement learning"}, {"st": 109, "ed": 111, "text": "policy search"}, {"st": 154, "ed": 156, "text": "automatically generate"}, {"st": 166, "ed": 168, "text": "humanoid robot"}, {"st": 198, "ed": 200, "text": "increasing complexity"}, {"st": 244, "ed": 246, "text": "computational efficiency"}, {"st": 257, "ed": 259, "text": "low level"}, {"st": 262, "ed": 264, "text": "search algorithm"}]
[{"st": 1, "ed": 4, "text": "k nearest neighbor"}, {"st": 55, "ed": 57, "text": "distance measures"}, {"st": 71, "ed": 73, "text": "similarity measures"}, {"st": 88, "ed": 91, "text": "precision and recall"}, {"st": 99, "ed": 101, "text": "distance measures"}, {"st": 106, "ed": 108, "text": "real world"}, {"st": 148, "ed": 150, "text": "recently proposed"}, {"st": 180, "ed": 182, "text": "noise level"}]
[{"st": 4, "ed": 6, "text": "starcraft ii"}, {"st": 9, "ed": 11, "text": "reinforcement learning"}, {"st": 15, "ed": 17, "text": "starcraft ii"}, {"st": 26, "ed": 28, "text": "reinforcement learning"}, {"st": 44, "ed": 46, "text": "multi agent"}, {"st": 58, "ed": 60, "text": "partially observed"}, {"st": 116, "ed": 118, "text": "starcraft ii"}, {"st": 122, "ed": 124, "text": "open source"}, {"st": 152, "ed": 154, "text": "starcraft ii"}, {"st": 180, "ed": 182, "text": "neural networks"}, {"st": 201, "ed": 204, "text": "deep reinforcement learning"}, {"st": 208, "ed": 210, "text": "starcraft ii"}, {"st": 216, "ed": 218, "text": "agents learn"}, {"st": 220, "ed": 222, "text": "a level"}, {"st": 256, "ed": 259, "text": "deep reinforcement learning"}]
[{"st": 1, "ed": 3, "text": "decision tree"}, {"st": 30, "ed": 32, "text": "valuable information"}, {"st": 44, "ed": 46, "text": "multi relational"}, {"st": 46, "ed": 48, "text": "decision tree"}, {"st": 48, "ed": 50, "text": "learning algorithm"}, {"st": 96, "ed": 98, "text": "pattern mining"}, {"st": 115, "ed": 117, "text": "multi relational"}, {"st": 117, "ed": 119, "text": "machine learning"}, {"st": 250, "ed": 252, "text": "pattern mining"}]
[{"st": 1, "ed": 3, "text": "real world"}, {"st": 3, "ed": 5, "text": "reinforcement learning"}, {"st": 21, "ed": 23, "text": "partial observability"}, {"st": 31, "ed": 34, "text": "recurrent neural networks"}, {"st": 83, "ed": 85, "text": "finite state"}, {"st": 132, "ed": 134, "text": "optimal policies"}, {"st": 141, "ed": 143, "text": "sample efficient"}, {"st": 145, "ed": 148, "text": "recurrent neural network"}]
[{"st": 16, "ed": 18, "text": "computational budget"}, {"st": 27, "ed": 29, "text": "vision systems"}, {"st": 30, "ed": 32, "text": "streaming data"}, {"st": 57, "ed": 59, "text": "accurate predictions"}, {"st": 60, "ed": 63, "text": "deep neural networks"}, {"st": 68, "ed": 70, "text": "computationally expensive"}, {"st": 99, "ed": 101, "text": "weighted sum"}, {"st": 113, "ed": 115, "text": "neural networks"}, {"st": 120, "ed": 122, "text": "without sacrificing"}, {"st": 123, "ed": 125, "text": "final performance"}, {"st": 148, "ed": 150, "text": "near optimal"}, {"st": 180, "ed": 182, "text": "visual recognition"}]
[{"st": 0, "ed": 3, "text": "the past decade"}, {"st": 74, "ed": 76, "text": "recent developments"}, {"st": 113, "ed": 115, "text": "most probable"}]
[{"st": 39, "ed": 41, "text": "fine grained"}, {"st": 52, "ed": 55, "text": "gaussian processes gps"}, {"st": 107, "ed": 109, "text": "main idea"}, {"st": 117, "ed": 120, "text": "spatial and temporal"}, {"st": 124, "ed": 127, "text": "number of clusters"}, {"st": 171, "ed": 175, "text": "non negative matrix factorization"}, {"st": 192, "ed": 194, "text": "kernel functions"}, {"st": 197, "ed": 199, "text": "network topology"}, {"st": 203, "ed": 205, "text": "extensive experiments"}, {"st": 206, "ed": 208, "text": "real world"}, {"st": 227, "ed": 229, "text": "significantly improve"}, {"st": 239, "ed": 242, "text": "global and local"}]
[{"st": 25, "ed": 27, "text": "saddle points"}, {"st": 61, "ed": 63, "text": "large scale"}, {"st": 71, "ed": 73, "text": "generic framework"}, {"st": 109, "ed": 111, "text": "saddle points"}, {"st": 122, "ed": 124, "text": "empirical results"}]
[{"st": 38, "ed": 40, "text": "takes place"}, {"st": 114, "ed": 116, "text": "control problem"}, {"st": 120, "ed": 123, "text": "deep reinforcement learning"}, {"st": 145, "ed": 150, "text": "long short term memory lstm"}, {"st": 169, "ed": 173, "text": "deep q network dqn"}, {"st": 224, "ed": 226, "text": "optimal control"}, {"st": 240, "ed": 242, "text": "autonomous driving"}, {"st": 251, "ed": 253, "text": "changing lanes"}, {"st": 255, "ed": 257, "text": "traffic flow"}]
[{"st": 3, "ed": 5, "text": "knowledge graph"}, {"st": 8, "ed": 10, "text": "multi relational"}, {"st": 60, "ed": 62, "text": "multi relational"}, {"st": 62, "ed": 64, "text": "image retrieval"}, {"st": 88, "ed": 91, "text": "deep convolutional networks"}, {"st": 95, "ed": 97, "text": "knowledge graph"}, {"st": 120, "ed": 123, "text": "zero shot learning"}, {"st": 144, "ed": 146, "text": "multi relational"}, {"st": 153, "ed": 155, "text": "knowledge graph"}, {"st": 164, "ed": 166, "text": "conduct experiments"}, {"st": 171, "ed": 173, "text": "deep architectures"}]
[{"st": 14, "ed": 16, "text": "machine learning"}, {"st": 61, "ed": 63, "text": "machine learning"}]
[{"st": 11, "ed": 13, "text": "reinforcement learning"}, {"st": 28, "ed": 30, "text": "neural network"}, {"st": 81, "ed": 83, "text": "efficient implementation"}, {"st": 86, "ed": 88, "text": "policy optimization"}, {"st": 100, "ed": 102, "text": "starting point"}, {"st": 107, "ed": 109, "text": "future research"}]
[{"st": 9, "ed": 11, "text": "machine learning"}, {"st": 22, "ed": 24, "text": "standard library"}, {"st": 42, "ed": 45, "text": "context free grammar"}]
[{"st": 6, "ed": 10, "text": "recurrent neural network rnn"}, {"st": 26, "ed": 28, "text": "residual connections"}, {"st": 41, "ed": 43, "text": "hidden states"}, {"st": 59, "ed": 61, "text": "residual connections"}, {"st": 71, "ed": 73, "text": "attention mechanism"}, {"st": 89, "ed": 91, "text": "hidden states"}, {"st": 111, "ed": 113, "text": "sentiment analysis"}, {"st": 118, "ed": 120, "text": "experiments demonstrate"}, {"st": 125, "ed": 127, "text": "faster convergence"}, {"st": 140, "ed": 142, "text": "highly competitive"}]
[{"st": 12, "ed": 14, "text": "case study"}, {"st": 54, "ed": 56, "text": "chemical compounds"}, {"st": 80, "ed": 82, "text": "machine learning"}, {"st": 120, "ed": 122, "text": "machine learning"}, {"st": 157, "ed": 159, "text": "meta learning"}, {"st": 175, "ed": 177, "text": "meta learning"}, {"st": 185, "ed": 187, "text": "random forests"}, {"st": 201, "ed": 203, "text": "meta learning"}, {"st": 225, "ed": 227, "text": "meta learning"}, {"st": 238, "ed": 240, "text": "meta learning"}]
[{"st": 0, "ed": 3, "text": "deep reinforcement learning"}, {"st": 3, "ed": 5, "text": "deep rl"}, {"st": 16, "ed": 19, "text": "deep neural network"}, {"st": 38, "ed": 40, "text": "deep rl"}, {"st": 44, "ed": 46, "text": "feature representation"}, {"st": 59, "ed": 61, "text": "deep rl"}, {"st": 81, "ed": 83, "text": "deep rl"}, {"st": 84, "ed": 86, "text": "real world"}, {"st": 104, "ed": 106, "text": "deep rl"}, {"st": 120, "ed": 122, "text": "important features"}, {"st": 123, "ed": 125, "text": "pre training"}, {"st": 125, "ed": 127, "text": "deep rl"}, {"st": 129, "ed": 131, "text": "hidden layers"}, {"st": 132, "ed": 134, "text": "supervised learning"}, {"st": 142, "ed": 144, "text": "empirically evaluate"}, {"st": 147, "ed": 151, "text": "deep q network dqn"}, {"st": 153, "ed": 156, "text": "advantage actor critic"}, {"st": 173, "ed": 175, "text": "pre training"}, {"st": 180, "ed": 182, "text": "supervised learning"}, {"st": 190, "ed": 192, "text": "pre training"}, {"st": 199, "ed": 201, "text": "deep rl"}, {"st": 204, "ed": 206, "text": "pre trained"}, {"st": 209, "ed": 211, "text": "significant improvement"}, {"st": 216, "ed": 218, "text": "pre training"}]
[{"st": 11, "ed": 13, "text": "neural network"}, {"st": 36, "ed": 38, "text": "weight sharing"}, {"st": 81, "ed": 83, "text": "supervised training"}, {"st": 110, "ed": 112, "text": "significantly outperform"}]
[{"st": 0, "ed": 3, "text": "deep reinforcement learning"}, {"st": 16, "ed": 18, "text": "video games"}, {"st": 19, "ed": 21, "text": "continuous control"}, {"st": 68, "ed": 70, "text": "real world"}, {"st": 91, "ed": 93, "text": "learning framework"}, {"st": 110, "ed": 112, "text": "transfer learning"}, {"st": 124, "ed": 126, "text": "reinforcement learning"}, {"st": 145, "ed": 147, "text": "special case"}, {"st": 163, "ed": 165, "text": "empirically demonstrate"}, {"st": 167, "ed": 169, "text": "proposed framework"}, {"st": 175, "ed": 177, "text": "learning process"}, {"st": 182, "ed": 184, "text": "computational overhead"}]
[{"st": 9, "ed": 11, "text": "semi supervised"}, {"st": 25, "ed": 27, "text": "input data"}, {"st": 53, "ed": 55, "text": "feature learning"}, {"st": 62, "ed": 64, "text": "reinforcement learning"}, {"st": 79, "ed": 82, "text": "image and text"}, {"st": 86, "ed": 89, "text": "encoder and decoder"}, {"st": 90, "ed": 92, "text": "extensive experiments"}, {"st": 93, "ed": 96, "text": "image and text"}]
[{"st": 81, "ed": 83, "text": "feature selection"}, {"st": 108, "ed": 110, "text": "theoretical framework"}, {"st": 128, "ed": 130, "text": "matching pursuit"}, {"st": 174, "ed": 176, "text": "network size"}, {"st": 178, "ed": 180, "text": "total number"}, {"st": 188, "ed": 190, "text": "convergence rate"}, {"st": 192, "ed": 194, "text": "error bound"}, {"st": 206, "ed": 208, "text": "feature selection"}, {"st": 215, "ed": 217, "text": "case study"}, {"st": 233, "ed": 235, "text": "tasks including"}, {"st": 245, "ed": 247, "text": "real world"}, {"st": 248, "ed": 250, "text": "empirical evidence"}]
[{"st": 10, "ed": 12, "text": "large scale"}, {"st": 12, "ed": 14, "text": "multi label"}, {"st": 18, "ed": 20, "text": "multi label"}, {"st": 22, "ed": 24, "text": "distributional semantics"}, {"st": 33, "ed": 35, "text": "distributional semantics"}, {"st": 37, "ed": 39, "text": "skip gram"}, {"st": 47, "ed": 49, "text": "word embeddings"}, {"st": 50, "ed": 52, "text": "natural language"}, {"st": 77, "ed": 79, "text": "embedding methods"}, {"st": 81, "ed": 83, "text": "multi label"}, {"st": 87, "ed": 89, "text": "embedding methods"}, {"st": 92, "ed": 94, "text": "learning representations"}, {"st": 138, "ed": 142, "text": "extensive set of experiments"}, {"st": 146, "ed": 148, "text": "benchmark datasets"}, {"st": 168, "ed": 170, "text": "large scale"}, {"st": 170, "ed": 172, "text": "multi label"}, {"st": 175, "ed": 178, "text": "end to end"}, {"st": 182, "ed": 184, "text": "joint learning"}, {"st": 201, "ed": 203, "text": "input features"}, {"st": 205, "ed": 207, "text": "gradient based"}]
[{"st": 6, "ed": 8, "text": "practical applications"}, {"st": 15, "ed": 17, "text": "existing methods"}, {"st": 30, "ed": 32, "text": "based approach"}, {"st": 52, "ed": 54, "text": "real world"}, {"st": 114, "ed": 116, "text": "rnn model"}, {"st": 119, "ed": 121, "text": "attention model"}, {"st": 144, "ed": 146, "text": "external memory"}]
[{"st": 46, "ed": 48, "text": "problem instances"}, {"st": 69, "ed": 71, "text": "machine learning"}, {"st": 104, "ed": 106, "text": "learned jointly"}, {"st": 108, "ed": 110, "text": "neural networks"}, {"st": 115, "ed": 117, "text": "directly optimizing"}, {"st": 127, "ed": 129, "text": "empirical study"}, {"st": 142, "ed": 144, "text": "neural networks"}]
[{"st": 0, "ed": 5, "text": "graph based semi supervised learning"}, {"st": 23, "ed": 25, "text": "iterative optimization"}, {"st": 74, "ed": 77, "text": "peer to peer"}, {"st": 116, "ed": 118, "text": "highly efficient"}, {"st": 154, "ed": 156, "text": "big data"}, {"st": 159, "ed": 161, "text": "distance matrix"}, {"st": 162, "ed": 164, "text": "parallel computing"}, {"st": 165, "ed": 168, "text": "locality sensitive hashing"}, {"st": 172, "ed": 174, "text": "large datasets"}, {"st": 177, "ed": 179, "text": "promising results"}]
[{"st": 8, "ed": 10, "text": "supervised learning"}, {"st": 17, "ed": 19, "text": "big data"}, {"st": 51, "ed": 53, "text": "fuzzy logic"}, {"st": 54, "ed": 56, "text": "supervised learning"}, {"st": 60, "ed": 62, "text": "main challenges"}, {"st": 80, "ed": 82, "text": "supervised learning"}, {"st": 83, "ed": 85, "text": "fuzzy logic"}, {"st": 89, "ed": 91, "text": "feature representation"}, {"st": 92, "ed": 94, "text": "hamming distance"}, {"st": 95, "ed": 97, "text": "hash function"}, {"st": 126, "ed": 128, "text": "hamming distance"}, {"st": 128, "ed": 130, "text": "hash function"}, {"st": 178, "ed": 180, "text": "main contribution"}, {"st": 187, "ed": 189, "text": "supervised learning"}, {"st": 198, "ed": 200, "text": "feature space"}, {"st": 205, "ed": 207, "text": "hash table"}]
[{"st": 1, "ed": 3, "text": "recent advances"}, {"st": 4, "ed": 7, "text": "deep reinforcement learning"}, {"st": 10, "ed": 12, "text": "learning agents"}, {"st": 18, "ed": 20, "text": "complex tasks"}, {"st": 20, "ed": 22, "text": "existing algorithms"}, {"st": 82, "ed": 84, "text": "previous approaches"}, {"st": 136, "ed": 138, "text": "representational power"}, {"st": 139, "ed": 142, "text": "deep neural networks"}, {"st": 146, "ed": 148, "text": "complex tasks"}, {"st": 203, "ed": 205, "text": "reinforcement learning"}]
[{"st": 18, "ed": 20, "text": "recent approaches"}, {"st": 35, "ed": 37, "text": "sparse rewards"}, {"st": 74, "ed": 77, "text": "deep reinforcement learning"}, {"st": 79, "ed": 81, "text": "model based"}, {"st": 99, "ed": 101, "text": "sparse rewards"}, {"st": 110, "ed": 112, "text": "significantly outperforms"}, {"st": 112, "ed": 114, "text": "previous methods"}, {"st": 119, "ed": 121, "text": "based approach"}, {"st": 122, "ed": 125, "text": "deep q networks"}]
[{"st": 2, "ed": 6, "text": "recurrent neural networks rnns"}, {"st": 11, "ed": 13, "text": "notoriously difficult"}, {"st": 23, "ed": 26, "text": "vanishing and exploding"}, {"st": 37, "ed": 40, "text": "simple yet effective"}, {"st": 58, "ed": 60, "text": "multi resolution"}, {"st": 62, "ed": 64, "text": "skip connections"}, {"st": 100, "ed": 102, "text": "tasks involving"}, {"st": 135, "ed": 137, "text": "skip connections"}, {"st": 150, "ed": 152, "text": "recurrent neural"}, {"st": 160, "ed": 164, "text": "available at https github.com"}]
[{"st": 1, "ed": 4, "text": "deep reinforcement learning"}]
[{"st": 0, "ed": 2, "text": "random projection"}, {"st": 12, "ed": 14, "text": "machine learning"}, {"st": 58, "ed": 60, "text": "random projection"}]
[{"st": 44, "ed": 46, "text": "meta learning"}, {"st": 61, "ed": 63, "text": "multi agent"}, {"st": 82, "ed": 84, "text": "meta learning"}, {"st": 94, "ed": 96, "text": "few shot"}]
[{"st": 3, "ed": 5, "text": "domain knowledge"}, {"st": 21, "ed": 23, "text": "statistical analysis"}, {"st": 61, "ed": 63, "text": "optimization algorithms"}, {"st": 68, "ed": 70, "text": "loss minimization"}, {"st": 100, "ed": 102, "text": "theoretical analyses"}, {"st": 115, "ed": 117, "text": "convergence rate"}, {"st": 136, "ed": 138, "text": "prior information"}, {"st": 162, "ed": 164, "text": "significant improvement"}, {"st": 165, "ed": 167, "text": "generalization performance"}]
[{"st": 4, "ed": 6, "text": "natural environment"}, {"st": 32, "ed": 34, "text": "random walk"}, {"st": 51, "ed": 53, "text": "relevant information"}, {"st": 62, "ed": 64, "text": "mental representation"}, {"st": 65, "ed": 67, "text": "random walk"}, {"st": 70, "ed": 74, "text": "markov chain monte carlo"}, {"st": 114, "ed": 116, "text": "power law"}, {"st": 169, "ed": 173, "text": "markov chain monte carlo"}]
[{"st": 2, "ed": 4, "text": "reinforcement learning"}, {"st": 44, "ed": 46, "text": "recently introduced"}, {"st": 49, "ed": 51, "text": "representation learning"}, {"st": 78, "ed": 80, "text": "existing algorithms"}, {"st": 91, "ed": 93, "text": "handcrafted features"}, {"st": 114, "ed": 116, "text": "recent successes"}, {"st": 118, "ed": 121, "text": "deep reinforcement learning"}]
[{"st": 0, "ed": 3, "text": "deep reinforcement learning"}, {"st": 23, "ed": 25, "text": "domains including"}, {"st": 29, "ed": 31, "text": "raw image"}, {"st": 42, "ed": 44, "text": "typically assume"}, {"st": 74, "ed": 77, "text": "deep reinforcement learning"}, {"st": 98, "ed": 100, "text": "partially observed"}, {"st": 106, "ed": 108, "text": "partially observed"}, {"st": 108, "ed": 110, "text": "reinforcement learning"}, {"st": 119, "ed": 121, "text": "strong baseline"}]
[{"st": 3, "ed": 5, "text": "deep learning"}, {"st": 12, "ed": 14, "text": "safety critical"}, {"st": 19, "ed": 21, "text": "formal verification"}, {"st": 22, "ed": 24, "text": "neural network"}, {"st": 115, "ed": 117, "text": "mixed integer"}, {"st": 130, "ed": 133, "text": "branch and bound"}]
[{"st": 73, "ed": 75, "text": "learned representations"}, {"st": 96, "ed": 98, "text": "learned features"}, {"st": 137, "ed": 139, "text": "handwritten digits"}, {"st": 139, "ed": 141, "text": "natural images"}, {"st": 142, "ed": 144, "text": "classical music"}, {"st": 152, "ed": 154, "text": "baseline method"}]
[{"st": 7, "ed": 9, "text": "artificial intelligence"}, {"st": 20, "ed": 22, "text": "representation learning"}, {"st": 46, "ed": 48, "text": "von neumann"}, {"st": 51, "ed": 54, "text": "artificial neural network"}, {"st": 86, "ed": 88, "text": "learning paradigm"}, {"st": 108, "ed": 110, "text": "input output"}, {"st": 113, "ed": 115, "text": "learning algorithms"}, {"st": 135, "ed": 137, "text": "neural network"}, {"st": 168, "ed": 170, "text": "neural network"}, {"st": 174, "ed": 177, "text": "deep neural network"}, {"st": 179, "ed": 182, "text": "short term memory"}, {"st": 191, "ed": 194, "text": "efficiency and scalability"}]
[{"st": 115, "ed": 117, "text": "base classifier"}, {"st": 135, "ed": 138, "text": "online feature selection"}, {"st": 147, "ed": 150, "text": "the paper presents"}, {"st": 155, "ed": 157, "text": "ensemble learning"}, {"st": 161, "ed": 163, "text": "active learning"}, {"st": 190, "ed": 192, "text": "real world"}, {"st": 213, "ed": 215, "text": "concept drift"}, {"st": 231, "ed": 233, "text": "significant reduction"}]
[{"st": 4, "ed": 6, "text": "reward function"}, {"st": 11, "ed": 13, "text": "don t"}, {"st": 24, "ed": 26, "text": "reward function"}, {"st": 83, "ed": 85, "text": "reward functions"}, {"st": 151, "ed": 153, "text": "empirical results"}, {"st": 165, "ed": 167, "text": "reward functions"}]
[{"st": 3, "ed": 7, "text": "recurrent neural network rnn"}, {"st": 14, "ed": 16, "text": "echo state"}, {"st": 30, "ed": 32, "text": "great success"}, {"st": 33, "ed": 35, "text": "time series"}, {"st": 39, "ed": 41, "text": "time series"}, {"st": 47, "ed": 50, "text": "single hidden layer"}, {"st": 70, "ed": 72, "text": "echo state"}, {"st": 90, "ed": 92, "text": "time series"}, {"st": 99, "ed": 101, "text": "time series"}, {"st": 107, "ed": 109, "text": "echo state"}, {"st": 123, "ed": 125, "text": "random projection"}, {"st": 128, "ed": 130, "text": "echo state"}, {"st": 133, "ed": 135, "text": "lower dimensional"}, {"st": 137, "ed": 139, "text": "low dimensional"}, {"st": 214, "ed": 216, "text": "theoretical analyses"}, {"st": 227, "ed": 229, "text": "echo state"}, {"st": 247, "ed": 249, "text": "real world"}, {"st": 249, "ed": 251, "text": "time series"}]
[{"st": 19, "ed": 21, "text": "solved efficiently"}, {"st": 35, "ed": 37, "text": "hidden layer"}, {"st": 45, "ed": 47, "text": "hidden layer"}, {"st": 52, "ed": 54, "text": "output layer"}, {"st": 69, "ed": 71, "text": "hidden layer"}]
[{"st": 39, "ed": 41, "text": "neural network"}, {"st": 50, "ed": 52, "text": "training data"}, {"st": 62, "ed": 64, "text": "neural network"}, {"st": 91, "ed": 93, "text": "computer vision"}, {"st": 99, "ed": 101, "text": "point clouds"}, {"st": 119, "ed": 121, "text": "synthetic data"}, {"st": 137, "ed": 140, "text": "continuous and discrete"}]
[{"st": 10, "ed": 12, "text": "reinforcement learning"}, {"st": 36, "ed": 38, "text": "theoretical properties"}, {"st": 52, "ed": 55, "text": "markov decision processes"}, {"st": 68, "ed": 70, "text": "sample efficiency"}, {"st": 111, "ed": 113, "text": "partially observable"}]
[{"st": 1, "ed": 4, "text": "deep q network"}, {"st": 19, "ed": 22, "text": "deep reinforcement learning"}, {"st": 27, "ed": 29, "text": "complex systems"}, {"st": 49, "ed": 51, "text": "software engineering"}, {"st": 110, "ed": 112, "text": "computational performance"}]
[{"st": 89, "ed": 91, "text": "performance guarantees"}, {"st": 116, "ed": 118, "text": "user preferences"}, {"st": 135, "ed": 137, "text": "theoretical analysis"}, {"st": 170, "ed": 172, "text": "empirical evaluation"}]
[{"st": 0, "ed": 2, "text": "posterior sampling"}, {"st": 3, "ed": 5, "text": "reinforcement learning"}, {"st": 16, "ed": 19, "text": "markov decision process"}, {"st": 38, "ed": 40, "text": "optimal policy"}, {"st": 46, "ed": 48, "text": "special case"}, {"st": 106, "ed": 108, "text": "random variables"}, {"st": 134, "ed": 136, "text": "regret bound"}, {"st": 164, "ed": 166, "text": "continuous state"}]
[{"st": 0, "ed": 2, "text": "policy optimization"}, {"st": 5, "ed": 7, "text": "great promise"}, {"st": 12, "ed": 14, "text": "imitation learning"}, {"st": 31, "ed": 33, "text": "model based"}, {"st": 36, "ed": 38, "text": "sample efficiency"}, {"st": 77, "ed": 79, "text": "continuous action"}, {"st": 116, "ed": 118, "text": "policy gradient"}, {"st": 120, "ed": 122, "text": "score function"}, {"st": 135, "ed": 137, "text": "significant gains"}, {"st": 138, "ed": 140, "text": "sample complexity"}, {"st": 165, "ed": 168, "text": "discrete and continuous"}]
[{"st": 97, "ed": 99, "text": "elderly care"}, {"st": 114, "ed": 116, "text": "prediction accuracy"}, {"st": 126, "ed": 128, "text": "expert knowledge"}, {"st": 128, "ed": 130, "text": "rough set"}]
[{"st": 8, "ed": 10, "text": "recent successes"}, {"st": 46, "ed": 48, "text": "continuous action"}, {"st": 64, "ed": 66, "text": "neural architecture"}, {"st": 82, "ed": 84, "text": "approach achieves"}, {"st": 91, "ed": 93, "text": "network outputs"}, {"st": 97, "ed": 100, "text": "degrees of freedom"}, {"st": 102, "ed": 104, "text": "a level"}, {"st": 134, "ed": 137, "text": "deep q network"}, {"st": 156, "ed": 158, "text": "empirical results"}, {"st": 202, "ed": 204, "text": "continuous control"}, {"st": 206, "ed": 208, "text": "deterministic policy"}]
[{"st": 8, "ed": 10, "text": "intensive care"}, {"st": 58, "ed": 60, "text": "continuous state"}, {"st": 91, "ed": 93, "text": "intensive care"}, {"st": 96, "ed": 98, "text": "decision making"}]
[{"st": 5, "ed": 7, "text": "reinforcement learning"}, {"st": 94, "ed": 97, "text": "deep reinforcement learning"}]
[{"st": 45, "ed": 47, "text": "policy optimization"}]
[{"st": 0, "ed": 2, "text": "reinforcement learning"}, {"st": 42, "ed": 44, "text": "recent years"}, {"st": 44, "ed": 46, "text": "leading edge"}, {"st": 58, "ed": 61, "text": "deep reinforcement learning"}, {"st": 69, "ed": 71, "text": "policy optimization"}, {"st": 75, "ed": 77, "text": "deterministic policy"}, {"st": 96, "ed": 98, "text": "continuous control"}, {"st": 101, "ed": 104, "text": "deep reinforcement learning"}]
[{"st": 104, "ed": 107, "text": "online learning algorithms"}]
[{"st": 1, "ed": 3, "text": "experience replay"}, {"st": 8, "ed": 12, "text": "deep reinforcement learning rl"}, {"st": 32, "ed": 34, "text": "experience replay"}, {"st": 38, "ed": 40, "text": "learning process"}, {"st": 51, "ed": 53, "text": "experience replay"}, {"st": 59, "ed": 61, "text": "deep rl"}, {"st": 74, "ed": 76, "text": "experience replay"}, {"st": 93, "ed": 95, "text": "hyper parameter"}, {"st": 120, "ed": 122, "text": "empirical study"}, {"st": 123, "ed": 125, "text": "experience replay"}]
[{"st": 7, "ed": 9, "text": "widely studied"}, {"st": 28, "ed": 30, "text": "domain specific"}, {"st": 51, "ed": 53, "text": "recently achieved"}, {"st": 61, "ed": 63, "text": "tabula rasa"}, {"st": 63, "ed": 65, "text": "reinforcement learning"}, {"st": 85, "ed": 87, "text": "tabula rasa"}, {"st": 100, "ed": 102, "text": "domain knowledge"}]
[{"st": 1, "ed": 3, "text": "off policy"}, {"st": 3, "ed": 5, "text": "temporal difference"}, {"st": 11, "ed": 13, "text": "reinforcement learning"}, {"st": 40, "ed": 42, "text": "non linearity"}, {"st": 58, "ed": 60, "text": "off policy"}, {"st": 113, "ed": 115, "text": "empirical results"}, {"st": 137, "ed": 139, "text": "computational complexity"}]
[{"st": 5, "ed": 7, "text": "cloud computing"}, {"st": 176, "ed": 178, "text": "machine learning"}, {"st": 200, "ed": 202, "text": "traditional methods"}, {"st": 203, "ed": 205, "text": "significantly reduces"}]
[{"st": 0, "ed": 2, "text": "neural networks"}, {"st": 9, "ed": 11, "text": "real world"}, {"st": 21, "ed": 23, "text": "neural networks"}, {"st": 31, "ed": 33, "text": "neural networks"}, {"st": 58, "ed": 60, "text": "neural networks"}, {"st": 62, "ed": 66, "text": "rectified linear unit relu"}, {"st": 66, "ed": 68, "text": "activation functions"}, {"st": 71, "ed": 74, "text": "layer by layer"}, {"st": 125, "ed": 127, "text": "neural network"}, {"st": 152, "ed": 154, "text": "randomly generated"}, {"st": 155, "ed": 157, "text": "neural network"}]
[{"st": 32, "ed": 35, "text": "multi task learning"}, {"st": 39, "ed": 42, "text": "support vector regression"}, {"st": 44, "ed": 46, "text": "joint learning"}, {"st": 56, "ed": 59, "text": "multi task learning"}, {"st": 76, "ed": 78, "text": "time series"}, {"st": 82, "ed": 85, "text": "dynamic time warping"}, {"st": 85, "ed": 87, "text": "distance measure"}, {"st": 108, "ed": 110, "text": "time series"}, {"st": 129, "ed": 131, "text": "nancy france"}, {"st": 131, "ed": 133, "text": "public transport"}]
[{"st": 3, "ed": 6, "text": "deep reinforcement learning"}, {"st": 11, "ed": 13, "text": "deep learning"}, {"st": 22, "ed": 25, "text": "deep reinforcement learning"}, {"st": 29, "ed": 31, "text": "efficiently learn"}, {"st": 49, "ed": 51, "text": "learning speed"}, {"st": 54, "ed": 56, "text": "sample efficiency"}]
[{"st": 7, "ed": 9, "text": "human intelligence"}, {"st": 38, "ed": 40, "text": "intelligent systems"}, {"st": 140, "ed": 142, "text": "agent learns"}, {"st": 179, "ed": 182, "text": "continuous and discrete"}, {"st": 222, "ed": 224, "text": "generalization performance"}, {"st": 230, "ed": 232, "text": "input signal"}, {"st": 252, "ed": 254, "text": "building block"}, {"st": 257, "ed": 259, "text": "intelligent systems"}]
[{"st": 3, "ed": 6, "text": "deep reinforcement learning"}, {"st": 11, "ed": 13, "text": "complex tasks"}, {"st": 43, "ed": 46, "text": "orders of magnitude"}, {"st": 82, "ed": 84, "text": "automatically generated"}, {"st": 89, "ed": 91, "text": "transfer learning"}]
[{"st": 2, "ed": 4, "text": "user preferences"}, {"st": 22, "ed": 24, "text": "recent studies"}, {"st": 30, "ed": 32, "text": "randomly chosen"}, {"st": 44, "ed": 46, "text": "learning algorithms"}, {"st": 146, "ed": 148, "text": "learning algorithm"}, {"st": 159, "ed": 161, "text": "near optimal"}]
[{"st": 1, "ed": 3, "text": "recent years"}, {"st": 40, "ed": 42, "text": "generative models"}, {"st": 46, "ed": 48, "text": "computer vision"}, {"st": 52, "ed": 54, "text": "generative models"}, {"st": 60, "ed": 62, "text": "framework called"}]
[{"st": 4, "ed": 7, "text": "deep neural networks"}, {"st": 9, "ed": 11, "text": "application areas"}, {"st": 18, "ed": 20, "text": "network architecture"}, {"st": 43, "ed": 45, "text": "neural network"}, {"st": 48, "ed": 50, "text": "evolutionary algorithms"}, {"st": 51, "ed": 53, "text": "reinforcement learning"}, {"st": 69, "ed": 71, "text": "search space"}, {"st": 76, "ed": 78, "text": "a 10"}, {"st": 116, "ed": 118, "text": "hand crafted"}, {"st": 149, "ed": 151, "text": "neural networks"}, {"st": 166, "ed": 168, "text": "neural architectures"}, {"st": 174, "ed": 176, "text": "problem specific"}, {"st": 176, "ed": 178, "text": "neural architecture"}, {"st": 178, "ed": 180, "text": "search spaces"}, {"st": 205, "ed": 207, "text": "image processing"}, {"st": 207, "ed": 209, "text": "reinforcement learning"}, {"st": 219, "ed": 221, "text": "problem specific"}]
[{"st": 0, "ed": 2, "text": "feature engineering"}, {"st": 12, "ed": 14, "text": "data science"}, {"st": 20, "ed": 22, "text": "feature learning"}, {"st": 31, "ed": 33, "text": "relevant features"}, {"st": 39, "ed": 41, "text": "predictive analytics"}, {"st": 55, "ed": 57, "text": "rule based"}, {"st": 76, "ed": 78, "text": "proposed approach"}, {"st": 93, "ed": 96, "text": "deep neural network"}, {"st": 97, "ed": 99, "text": "automatically learns"}]
[{"st": 5, "ed": 7, "text": "deep learning"}, {"st": 68, "ed": 71, "text": "ability to learn"}, {"st": 92, "ed": 94, "text": "commonsense reasoning"}, {"st": 105, "ed": 107, "text": "audit trail"}, {"st": 128, "ed": 130, "text": "deep learning"}, {"st": 144, "ed": 146, "text": "catastrophic forgetting"}, {"st": 147, "ed": 149, "text": "deep learning"}, {"st": 181, "ed": 183, "text": "deep learning"}, {"st": 232, "ed": 234, "text": "deep learning"}]
[{"st": 3, "ed": 5, "text": "open world"}, {"st": 47, "ed": 49, "text": "unseen classes"}, {"st": 55, "ed": 57, "text": "prior knowledge"}, {"st": 113, "ed": 115, "text": "unseen classes"}, {"st": 157, "ed": 159, "text": "distance function"}]
[{"st": 0, "ed": 2, "text": "geometric analysis"}, {"st": 17, "ed": 19, "text": "input data"}, {"st": 20, "ed": 22, "text": "machine learning"}, {"st": 89, "ed": 91, "text": "computationally feasible"}, {"st": 131, "ed": 133, "text": "intrinsic dimension"}, {"st": 151, "ed": 153, "text": "social network"}]
[{"st": 10, "ed": 12, "text": "generalization error"}, {"st": 32, "ed": 34, "text": "generalization error"}, {"st": 52, "ed": 54, "text": "bias variance"}, {"st": 57, "ed": 59, "text": "generalization error"}, {"st": 90, "ed": 92, "text": "bias variance"}, {"st": 95, "ed": 97, "text": "generalization error"}, {"st": 147, "ed": 149, "text": "bias variance"}, {"st": 151, "ed": 153, "text": "highly competitive"}, {"st": 163, "ed": 165, "text": "concept drift"}, {"st": 166, "ed": 168, "text": "real world"}]
[{"st": 25, "ed": 27, "text": "deep structured"}, {"st": 27, "ed": 29, "text": "machine learning"}, {"st": 72, "ed": 74, "text": "transfer learning"}]
[{"st": 4, "ed": 6, "text": "decision boundary"}, {"st": 36, "ed": 38, "text": "recent methods"}]
[{"st": 34, "ed": 36, "text": "accurately predict"}, {"st": 36, "ed": 38, "text": "patient specific"}, {"st": 64, "ed": 66, "text": "recurrent network"}, {"st": 97, "ed": 100, "text": "publicly available datasets"}]
[{"st": 2, "ed": 4, "text": "decision making"}, {"st": 7, "ed": 9, "text": "autonomous driving"}, {"st": 30, "ed": 33, "text": "deep reinforcement learning"}, {"st": 36, "ed": 38, "text": "decision making"}]
[{"st": 0, "ed": 2, "text": "multi view"}, {"st": 9, "ed": 11, "text": "machine learning"}, {"st": 13, "ed": 15, "text": "multi view"}, {"st": 18, "ed": 20, "text": "multi view"}, {"st": 34, "ed": 36, "text": "cross view"}, {"st": 44, "ed": 46, "text": "neural architecture"}, {"st": 47, "ed": 49, "text": "multi view"}, {"st": 65, "ed": 67, "text": "neural architecture"}, {"st": 102, "ed": 104, "text": "cross view"}, {"st": 111, "ed": 113, "text": "attention mechanism"}, {"st": 126, "ed": 128, "text": "multi view"}, {"st": 141, "ed": 143, "text": "multi view"}, {"st": 156, "ed": 158, "text": "multi view"}, {"st": 178, "ed": 180, "text": "multi view"}]
[{"st": 15, "ed": 17, "text": "reinforcement learning"}, {"st": 25, "ed": 27, "text": "key challenge"}, {"st": 70, "ed": 72, "text": "without sacrificing"}, {"st": 93, "ed": 95, "text": "off policy"}, {"st": 96, "ed": 98, "text": "method called"}, {"st": 107, "ed": 109, "text": "multi task"}, {"st": 109, "ed": 111, "text": "reinforcement learning"}, {"st": 133, "ed": 135, "text": "atari games"}, {"st": 173, "ed": 175, "text": "multi task"}]
[{"st": 13, "ed": 16, "text": "sequential decision making"}, {"st": 80, "ed": 82, "text": "neural network"}, {"st": 87, "ed": 89, "text": "significantly improve"}, {"st": 130, "ed": 132, "text": "significant improvement"}]
[{"st": 27, "ed": 29, "text": "computer science"}, {"st": 80, "ed": 84, "text": "real and synthetic data"}, {"st": 94, "ed": 97, "text": "short term memory"}]
[{"st": 8, "ed": 10, "text": "wasserstein distance"}, {"st": 14, "ed": 16, "text": "wasserstein distance"}, {"st": 31, "ed": 33, "text": "metric space"}, {"st": 58, "ed": 60, "text": "wasserstein distance"}]
[{"st": 3, "ed": 5, "text": "meta learning"}, {"st": 10, "ed": 12, "text": "reinforcement learning"}, {"st": 21, "ed": 23, "text": "loss function"}, {"st": 36, "ed": 38, "text": "achieve high"}, {"st": 83, "ed": 85, "text": "empirical results"}, {"st": 89, "ed": 91, "text": "policy gradient"}, {"st": 93, "ed": 95, "text": "faster learning"}, {"st": 105, "ed": 107, "text": "policy gradient"}, {"st": 118, "ed": 120, "text": "loss function"}]
[{"st": 14, "ed": 16, "text": "united states"}, {"st": 41, "ed": 43, "text": "decision maker"}, {"st": 111, "ed": 113, "text": "decision maker"}, {"st": 125, "ed": 127, "text": "article presents"}, {"st": 140, "ed": 142, "text": "problems involving"}, {"st": 151, "ed": 153, "text": "false positives"}, {"st": 154, "ed": 156, "text": "false negatives"}, {"st": 178, "ed": 182, "text": "attack on pearl harbor"}, {"st": 198, "ed": 202, "text": "sir arthur conan doyle"}, {"st": 238, "ed": 240, "text": "valuable information"}]
[{"st": 4, "ed": 6, "text": "reinforcement learning"}, {"st": 29, "ed": 32, "text": "markov decision processes"}, {"st": 35, "ed": 38, "text": "markov decision processes"}, {"st": 107, "ed": 109, "text": "task performance"}, {"st": 137, "ed": 139, "text": "reinforcement learning"}, {"st": 160, "ed": 162, "text": "learning algorithm"}, {"st": 166, "ed": 168, "text": "learning algorithm"}, {"st": 214, "ed": 216, "text": "decision making"}]
[{"st": 5, "ed": 7, "text": "learning algorithm"}, {"st": 8, "ed": 10, "text": "experience replay"}, {"st": 30, "ed": 32, "text": "low dimensional"}, {"st": 37, "ed": 39, "text": "batch size"}, {"st": 50, "ed": 52, "text": "reward signal"}, {"st": 69, "ed": 71, "text": "low dimensional"}]
[{"st": 0, "ed": 2, "text": "hierarchical classification"}, {"st": 4, "ed": 7, "text": "multi class classification"}, {"st": 12, "ed": 14, "text": "class labels"}, {"st": 31, "ed": 33, "text": "hierarchical classification"}, {"st": 43, "ed": 45, "text": "hierarchical classification"}, {"st": 54, "ed": 56, "text": "mathcal o"}, {"st": 62, "ed": 64, "text": "bayes optimal"}, {"st": 81, "ed": 83, "text": "loss function"}, {"st": 84, "ed": 86, "text": "bayes optimal"}, {"st": 95, "ed": 97, "text": "mathcal o"}, {"st": 116, "ed": 118, "text": "hierarchical classification"}]
[{"st": 7, "ed": 9, "text": "policy gradient"}, {"st": 25, "ed": 27, "text": "policy gradients"}, {"st": 43, "ed": 45, "text": "low variance"}, {"st": 61, "ed": 63, "text": "radial basis"}, {"st": 95, "ed": 97, "text": "sample based"}, {"st": 100, "ed": 102, "text": "policy gradients"}, {"st": 104, "ed": 106, "text": "existing results"}, {"st": 107, "ed": 109, "text": "sample based"}, {"st": 124, "ed": 126, "text": "policy gradient"}]
[{"st": 8, "ed": 10, "text": "neural networks"}, {"st": 68, "ed": 70, "text": "special case"}, {"st": 73, "ed": 76, "text": "degrees of freedom"}, {"st": 90, "ed": 92, "text": "loss function"}, {"st": 102, "ed": 106, "text": "mnist and cifar 10"}, {"st": 116, "ed": 118, "text": "adversarial training"}, {"st": 155, "ed": 157, "text": "exploding gradients"}]
[{"st": 3, "ed": 5, "text": "multi agent"}, {"st": 5, "ed": 7, "text": "reinforcement learning"}, {"st": 23, "ed": 25, "text": "reward function"}, {"st": 28, "ed": 30, "text": "hidden state"}, {"st": 47, "ed": 49, "text": "observed behavior"}, {"st": 90, "ed": 92, "text": "hidden state"}, {"st": 122, "ed": 124, "text": "hidden states"}]
[{"st": 9, "ed": 11, "text": "video games"}, {"st": 20, "ed": 22, "text": "prior knowledge"}, {"st": 68, "ed": 70, "text": "video game"}, {"st": 77, "ed": 79, "text": "visual information"}, {"st": 93, "ed": 95, "text": "prior knowledge"}]
[{"st": 23, "ed": 25, "text": "machine learning"}, {"st": 116, "ed": 118, "text": "bi directional"}, {"st": 136, "ed": 138, "text": "self organizing"}, {"st": 165, "ed": 167, "text": "machine learning"}, {"st": 190, "ed": 192, "text": "intrinsic motivation"}, {"st": 192, "ed": 194, "text": "lifelong learning"}, {"st": 195, "ed": 197, "text": "world model"}, {"st": 198, "ed": 200, "text": "free energy"}, {"st": 203, "ed": 205, "text": "machine learning"}]
[{"st": 23, "ed": 25, "text": "algorithmic framework"}, {"st": 41, "ed": 43, "text": "causal models"}, {"st": 124, "ed": 126, "text": "existing solutions"}, {"st": 127, "ed": 129, "text": "special cases"}, {"st": 133, "ed": 135, "text": "efficient algorithms"}]
[{"st": 29, "ed": 31, "text": "real world"}, {"st": 42, "ed": 44, "text": "continuous state"}, {"st": 63, "ed": 65, "text": "feature space"}, {"st": 76, "ed": 78, "text": "representation learning"}, {"st": 98, "ed": 100, "text": "deep learning"}, {"st": 113, "ed": 115, "text": "latent space"}, {"st": 150, "ed": 152, "text": "learned representations"}]
[{"st": 11, "ed": 13, "text": "time series"}, {"st": 30, "ed": 32, "text": "time series"}, {"st": 49, "ed": 51, "text": "machine learning"}, {"st": 87, "ed": 89, "text": "catastrophic forgetting"}, {"st": 114, "ed": 116, "text": "real world"}, {"st": 122, "ed": 124, "text": "proposed method"}, {"st": 129, "ed": 131, "text": "computational resources"}]
[{"st": 6, "ed": 8, "text": "neural network"}, {"st": 28, "ed": 30, "text": "potential energy"}, {"st": 34, "ed": 36, "text": "molecular dynamics"}]
[{"st": 10, "ed": 12, "text": "reinforcement learning"}, {"st": 68, "ed": 70, "text": "expected reward"}, {"st": 143, "ed": 146, "text": "ability to learn"}, {"st": 155, "ed": 157, "text": "significantly improved"}, {"st": 160, "ed": 162, "text": "continuous control"}]
[{"st": 3, "ed": 5, "text": "recent literature"}, {"st": 47, "ed": 49, "text": "near optimal"}, {"st": 55, "ed": 58, "text": "upper confidence bound"}, {"st": 60, "ed": 62, "text": "thompson sampling"}, {"st": 77, "ed": 79, "text": "near optimal"}, {"st": 86, "ed": 88, "text": "thompson sampling"}, {"st": 109, "ed": 111, "text": "thompson sampling"}, {"st": 115, "ed": 117, "text": "armed bandits"}]
[{"st": 1, "ed": 3, "text": "anomaly detection"}, {"st": 8, "ed": 10, "text": "point based"}, {"st": 21, "ed": 23, "text": "point based"}, {"st": 27, "ed": 29, "text": "real world"}, {"st": 44, "ed": 46, "text": "point based"}, {"st": 63, "ed": 65, "text": "mathematical model"}, {"st": 99, "ed": 101, "text": "point based"}, {"st": 124, "ed": 126, "text": "multi dimensional"}]
[{"st": 3, "ed": 5, "text": "off policy"}, {"st": 12, "ed": 14, "text": "deterministic policy"}, {"st": 62, "ed": 64, "text": "policy gradient"}, {"st": 113, "ed": 115, "text": "significantly improves"}, {"st": 116, "ed": 118, "text": "sample efficiency"}, {"st": 124, "ed": 126, "text": "reinforcement learning"}]
[{"st": 0, "ed": 3, "text": "deep reinforcement learning"}, {"st": 14, "ed": 16, "text": "video game"}, {"st": 21, "ed": 23, "text": "video game"}, {"st": 23, "ed": 25, "text": "agents learn"}, {"st": 25, "ed": 28, "text": "end to end"}, {"st": 101, "ed": 103, "text": "feature vectors"}, {"st": 109, "ed": 111, "text": "fixed length"}, {"st": 112, "ed": 114, "text": "feature vector"}]
[{"st": 55, "ed": 57, "text": "non trivial"}]
[{"st": 23, "ed": 25, "text": "fiber optic"}, {"st": 55, "ed": 57, "text": "neural network"}, {"st": 64, "ed": 66, "text": "structural information"}]
[{"st": 25, "ed": 27, "text": "intrusion detection"}, {"st": 51, "ed": 53, "text": "generalization ability"}, {"st": 63, "ed": 65, "text": "original input"}, {"st": 71, "ed": 74, "text": "linear discriminant analysis"}, {"st": 76, "ed": 78, "text": "reduction technique"}, {"st": 103, "ed": 105, "text": "reduction technique"}, {"st": 114, "ed": 116, "text": "cyber attack"}, {"st": 125, "ed": 127, "text": "input features"}, {"st": 131, "ed": 133, "text": "classification accuracy"}, {"st": 136, "ed": 139, "text": "training and testing"}, {"st": 150, "ed": 153, "text": "artificial neural network"}]
[{"st": 0, "ed": 2, "text": "geometric algebra"}, {"st": 12, "ed": 14, "text": "geometric algebra"}, {"st": 31, "ed": 33, "text": "geometric algebra"}, {"st": 55, "ed": 57, "text": "geometric algebra"}, {"st": 97, "ed": 99, "text": "geometric algebra"}, {"st": 127, "ed": 129, "text": "geometric algebra"}]
[{"st": 40, "ed": 42, "text": "activation functions"}, {"st": 44, "ed": 46, "text": "neural networks"}, {"st": 114, "ed": 116, "text": "activation functions"}]
[{"st": 29, "ed": 32, "text": "primary visual cortex"}, {"st": 52, "ed": 54, "text": "feedforward neural"}, {"st": 84, "ed": 86, "text": "unsupervised manner"}, {"st": 120, "ed": 122, "text": "receptive field"}, {"st": 129, "ed": 131, "text": "lateral connections"}, {"st": 141, "ed": 143, "text": "spatial features"}]
[{"st": 30, "ed": 33, "text": "taking into account"}, {"st": 76, "ed": 78, "text": "biologically plausible"}]
[{"st": 6, "ed": 8, "text": "x ray"}, {"st": 25, "ed": 28, "text": "artificial neural network"}, {"st": 40, "ed": 42, "text": "x ray"}, {"st": 61, "ed": 63, "text": "image processing"}, {"st": 63, "ed": 65, "text": "feature extraction"}, {"st": 66, "ed": 69, "text": "artificial neural network"}, {"st": 72, "ed": 74, "text": "x ray"}, {"st": 84, "ed": 86, "text": "extracted features"}, {"st": 122, "ed": 125, "text": "artificial neural network"}, {"st": 136, "ed": 138, "text": "x ray"}, {"st": 145, "ed": 147, "text": "learning rate"}, {"st": 150, "ed": 152, "text": "hidden units"}, {"st": 157, "ed": 159, "text": "feature vectors"}, {"st": 163, "ed": 165, "text": "a level"}]
[{"st": 4, "ed": 6, "text": "feature learning"}, {"st": 7, "ed": 9, "text": "computer vision"}, {"st": 12, "ed": 14, "text": "neural networks"}, {"st": 18, "ed": 20, "text": "object recognition"}, {"st": 38, "ed": 40, "text": "visual tasks"}, {"st": 48, "ed": 50, "text": "living organisms"}, {"st": 54, "ed": 56, "text": "visual perception"}, {"st": 97, "ed": 99, "text": "class labels"}, {"st": 130, "ed": 132, "text": "class label"}, {"st": 135, "ed": 137, "text": "visual tasks"}, {"st": 138, "ed": 140, "text": "scene recognition"}, {"st": 140, "ed": 142, "text": "object recognition"}]
[{"st": 27, "ed": 29, "text": "image classification"}, {"st": 34, "ed": 37, "text": "deep convolutional networks"}, {"st": 40, "ed": 42, "text": "competitive results"}, {"st": 43, "ed": 45, "text": "hand crafted"}, {"st": 87, "ed": 90, "text": "hundreds of thousands"}, {"st": 102, "ed": 104, "text": "large scale"}, {"st": 113, "ed": 115, "text": "time consuming"}, {"st": 149, "ed": 151, "text": "higher level"}, {"st": 164, "ed": 166, "text": "frame level"}, {"st": 174, "ed": 176, "text": "highly efficient"}, {"st": 176, "ed": 179, "text": "mixture of experts"}, {"st": 186, "ed": 189, "text": "short term memory"}]
[{"st": 12, "ed": 14, "text": "feature spaces"}, {"st": 15, "ed": 18, "text": "convolutional neural networks"}, {"st": 36, "ed": 38, "text": "neural networks"}, {"st": 54, "ed": 56, "text": "feature maps"}, {"st": 73, "ed": 75, "text": "statistical properties"}, {"st": 76, "ed": 78, "text": "natural images"}, {"st": 103, "ed": 105, "text": "deep representations"}]
[{"st": 18, "ed": 21, "text": "convolutional neural network"}, {"st": 31, "ed": 33, "text": "rgb image"}, {"st": 35, "ed": 38, "text": "end to end"}, {"st": 73, "ed": 75, "text": "large scale"}, {"st": 115, "ed": 117, "text": "transfer learning"}, {"st": 118, "ed": 120, "text": "large scale"}, {"st": 128, "ed": 130, "text": "high level"}, {"st": 137, "ed": 139, "text": "motion blur"}, {"st": 144, "ed": 146, "text": "point based"}]
[{"st": 20, "ed": 22, "text": "neural networks"}, {"st": 23, "ed": 25, "text": "pattern recognition"}, {"st": 36, "ed": 38, "text": "network architecture"}, {"st": 42, "ed": 44, "text": "pattern recognition"}]
[{"st": 0, "ed": 3, "text": "self organizing map"}, {"st": 11, "ed": 13, "text": "tasks including"}, {"st": 20, "ed": 22, "text": "high dimensional"}, {"st": 35, "ed": 37, "text": "low dimensional"}, {"st": 55, "ed": 57, "text": "weight vectors"}, {"st": 81, "ed": 83, "text": "low dimensional"}, {"st": 92, "ed": 94, "text": "high dimensional"}, {"st": 120, "ed": 122, "text": "high dimensional"}]
[{"st": 31, "ed": 33, "text": "feature extraction"}, {"st": 35, "ed": 37, "text": "machine learning"}, {"st": 70, "ed": 72, "text": "neural activity"}, {"st": 98, "ed": 100, "text": "feature extraction"}, {"st": 192, "ed": 195, "text": "independent component analysis"}, {"st": 204, "ed": 206, "text": "noise reduction"}, {"st": 218, "ed": 220, "text": "feature vector"}, {"st": 254, "ed": 256, "text": "machine learning"}, {"st": 257, "ed": 259, "text": "neural networks"}, {"st": 261, "ed": 264, "text": "support vector machines"}]
[{"st": 1, "ed": 3, "text": "human brain"}, {"st": 10, "ed": 12, "text": "visual processing"}, {"st": 18, "ed": 20, "text": "object recognition"}, {"st": 37, "ed": 39, "text": "human brain"}, {"st": 91, "ed": 93, "text": "low level"}, {"st": 93, "ed": 95, "text": "visual input"}, {"st": 115, "ed": 117, "text": "human brain"}, {"st": 120, "ed": 122, "text": "low level"}, {"st": 122, "ed": 124, "text": "visual input"}, {"st": 157, "ed": 160, "text": "convolutional neural networks"}, {"st": 169, "ed": 171, "text": "accurately predict"}, {"st": 228, "ed": 233, "text": "computer vision and machine learning"}]
[{"st": 10, "ed": 12, "text": "classification methods"}, {"st": 20, "ed": 23, "text": "convolutional neural network"}, {"st": 27, "ed": 29, "text": "object class"}, {"st": 77, "ed": 79, "text": "success rate"}, {"st": 87, "ed": 89, "text": "great potential"}, {"st": 90, "ed": 93, "text": "convolutional neural networks"}, {"st": 94, "ed": 96, "text": "deep learning"}]
[{"st": 1, "ed": 3, "text": "fine art"}, {"st": 53, "ed": 55, "text": "visual perception"}, {"st": 59, "ed": 61, "text": "face recognition"}, {"st": 71, "ed": 73, "text": "biologically inspired"}, {"st": 88, "ed": 91, "text": "deep neural network"}, {"st": 134, "ed": 137, "text": "artificial neural networks"}]
[{"st": 20, "ed": 22, "text": "social media"}, {"st": 28, "ed": 31, "text": "images and videos"}, {"st": 35, "ed": 37, "text": "image analysis"}, {"st": 66, "ed": 68, "text": "computer vision"}, {"st": 83, "ed": 85, "text": "deep learning"}, {"st": 86, "ed": 89, "text": "convolutional neural networks"}]
[{"st": 0, "ed": 4, "text": "convolutional neural networks cnns"}, {"st": 9, "ed": 11, "text": "computer vision"}, {"st": 17, "ed": 19, "text": "image classification"}, {"st": 23, "ed": 25, "text": "computational effort"}, {"st": 37, "ed": 39, "text": "recent developments"}, {"st": 91, "ed": 93, "text": "low power"}, {"st": 109, "ed": 111, "text": "classification accuracy"}, {"st": 203, "ed": 205, "text": "significantly outperforms"}]
[{"st": 15, "ed": 17, "text": "public health"}, {"st": 38, "ed": 41, "text": "artificial neural networks"}]
[{"st": 60, "ed": 62, "text": "neural network"}, {"st": 68, "ed": 70, "text": "residual learning"}]
[{"st": 4, "ed": 7, "text": "artificial intelligence ai"}, {"st": 8, "ed": 10, "text": "deep learning"}, {"st": 15, "ed": 18, "text": "convolutional neural networks"}, {"st": 69, "ed": 71, "text": "cognitive science"}, {"st": 76, "ed": 78, "text": "conceptual blending"}]
[{"st": 0, "ed": 2, "text": "fuzzy logic"}, {"st": 4, "ed": 6, "text": "powerful tool"}, {"st": 31, "ed": 33, "text": "fuzzy logic"}, {"st": 43, "ed": 46, "text": "multiple instance learning"}, {"st": 102, "ed": 104, "text": "fuzzy inference"}, {"st": 106, "ed": 108, "text": "neural networks"}, {"st": 128, "ed": 130, "text": "multiple instance"}, {"st": 132, "ed": 134, "text": "fuzzy inference"}, {"st": 158, "ed": 160, "text": "multiple instance"}, {"st": 162, "ed": 164, "text": "fuzzy inference"}, {"st": 184, "ed": 186, "text": "learning algorithm"}, {"st": 209, "ed": 211, "text": "benchmark datasets"}]
[{"st": 0, "ed": 2, "text": "visual cortex"}, {"st": 4, "ed": 6, "text": "multi level"}, {"st": 11, "ed": 13, "text": "biologically inspired"}, {"st": 22, "ed": 24, "text": "larger scale"}, {"st": 138, "ed": 140, "text": "associative memory"}, {"st": 254, "ed": 256, "text": "activation functions"}]
[{"st": 0, "ed": 4, "text": "deep convolutional neural networks"}, {"st": 18, "ed": 20, "text": "improved performance"}, {"st": 24, "ed": 26, "text": "image analysis"}]
[{"st": 91, "ed": 93, "text": "distributed representation"}, {"st": 158, "ed": 160, "text": "input space"}, {"st": 241, "ed": 243, "text": "selection algorithm"}, {"st": 246, "ed": 249, "text": "learning and inference"}]
[{"st": 7, "ed": 9, "text": "texture synthesis"}, {"st": 10, "ed": 12, "text": "style transfer"}, {"st": 14, "ed": 17, "text": "convolutional neural networks"}, {"st": 58, "ed": 60, "text": "parameter tuning"}, {"st": 74, "ed": 77, "text": "convolutional neural networks"}]
[{"st": 9, "ed": 11, "text": "processing units"}, {"st": 12, "ed": 14, "text": "large scale"}, {"st": 25, "ed": 27, "text": "software architecture"}]
[{"st": 23, "ed": 25, "text": "image analysis"}, {"st": 31, "ed": 33, "text": "hand crafted"}, {"st": 33, "ed": 35, "text": "image analysis"}, {"st": 51, "ed": 53, "text": "multiple objects"}, {"st": 62, "ed": 64, "text": "hand crafted"}, {"st": 81, "ed": 83, "text": "deep learning"}, {"st": 92, "ed": 94, "text": "hand crafted"}, {"st": 104, "ed": 106, "text": "object localization"}, {"st": 123, "ed": 125, "text": "object localization"}, {"st": 130, "ed": 132, "text": "time consuming"}, {"st": 151, "ed": 153, "text": "weak supervision"}, {"st": 177, "ed": 179, "text": "synthetic data"}, {"st": 185, "ed": 187, "text": "expert knowledge"}, {"st": 194, "ed": 196, "text": "weak supervision"}, {"st": 205, "ed": 207, "text": "synthetic data"}]
[{"st": 76, "ed": 78, "text": "peripheral vision"}, {"st": 104, "ed": 106, "text": "peripheral vision"}, {"st": 111, "ed": 113, "text": "neural network"}, {"st": 142, "ed": 144, "text": "peripheral vision"}]
[{"st": 4, "ed": 6, "text": "primal dual"}, {"st": 22, "ed": 25, "text": "deep neural network"}, {"st": 29, "ed": 31, "text": "primal dual"}, {"st": 31, "ed": 33, "text": "optimization method"}, {"st": 48, "ed": 52, "text": "trained end to end"}, {"st": 79, "ed": 81, "text": "proposed method"}, {"st": 82, "ed": 84, "text": "low dose"}, {"st": 84, "ed": 86, "text": "computed tomography"}, {"st": 89, "ed": 91, "text": "total variation"}, {"st": 93, "ed": 95, "text": "deep learning"}, {"st": 96, "ed": 98, "text": "post processing"}, {"st": 131, "ed": 133, "text": "post processing"}, {"st": 136, "ed": 138, "text": "substantial improvement"}, {"st": 140, "ed": 142, "text": "structural similarity"}]
[{"st": 0, "ed": 2, "text": "recent advances"}, {"st": 3, "ed": 5, "text": "deep learning"}, {"st": 10, "ed": 12, "text": "great potential"}, {"st": 30, "ed": 33, "text": "deep neural network"}, {"st": 35, "ed": 37, "text": "specifically designed"}, {"st": 119, "ed": 121, "text": "ct scans"}]
[{"st": 7, "ed": 9, "text": "important information"}, {"st": 17, "ed": 19, "text": "deep learning"}, {"st": 22, "ed": 24, "text": "successfully applied"}, {"st": 30, "ed": 32, "text": "image classification"}, {"st": 33, "ed": 35, "text": "voice recognition"}, {"st": 40, "ed": 42, "text": "significantly improve"}, {"st": 49, "ed": 51, "text": "deep learning"}, {"st": 76, "ed": 79, "text": "deep neural network"}]
[{"st": 1, "ed": 3, "text": "visual recognition"}, {"st": 22, "ed": 24, "text": "challenging task"}, {"st": 57, "ed": 59, "text": "self organizing"}, {"st": 59, "ed": 61, "text": "neural network"}, {"st": 69, "ed": 71, "text": "rgb d"}, {"st": 138, "ed": 140, "text": "self organizing"}, {"st": 169, "ed": 171, "text": "classification results"}, {"st": 173, "ed": 175, "text": "benchmark dataset"}]
[{"st": 6, "ed": 8, "text": "image compression"}, {"st": 51, "ed": 53, "text": "image compression"}, {"st": 64, "ed": 66, "text": "optimization techniques"}, {"st": 94, "ed": 96, "text": "differential evolution"}, {"st": 131, "ed": 133, "text": "efficient exploration"}, {"st": 142, "ed": 144, "text": "optimal solution"}, {"st": 158, "ed": 160, "text": "approach produces"}]
[{"st": 0, "ed": 2, "text": "neural networks"}, {"st": 41, "ed": 43, "text": "neural network"}, {"st": 45, "ed": 47, "text": "power law"}, {"st": 65, "ed": 67, "text": "cifar 10"}, {"st": 78, "ed": 80, "text": "phase transition"}, {"st": 90, "ed": 92, "text": "power law"}, {"st": 109, "ed": 111, "text": "network size"}, {"st": 117, "ed": 119, "text": "critical point"}]
[{"st": 1, "ed": 4, "text": "spatio temporal data"}, {"st": 8, "ed": 10, "text": "challenging task"}, {"st": 15, "ed": 17, "text": "temporal information"}, {"st": 18, "ed": 21, "text": "convolutional neural networks"}, {"st": 27, "ed": 29, "text": "feature extractors"}, {"st": 30, "ed": 32, "text": "transfer learning"}, {"st": 43, "ed": 45, "text": "temporal information"}, {"st": 49, "ed": 52, "text": "hand crafted features"}, {"st": 53, "ed": 56, "text": "recurrent neural networks"}, {"st": 74, "ed": 77, "text": "spatio temporal data"}, {"st": 78, "ed": 81, "text": "hand crafted features"}, {"st": 92, "ed": 94, "text": "neural network"}, {"st": 111, "ed": 113, "text": "feature extraction"}, {"st": 115, "ed": 119, "text": "deep convolutional neural networks"}, {"st": 156, "ed": 158, "text": "classification results"}, {"st": 171, "ed": 173, "text": "spatio temporal"}, {"st": 181, "ed": 183, "text": "feed forward"}]
[{"st": 12, "ed": 14, "text": "neural networks"}, {"st": 21, "ed": 23, "text": "scale invariant"}, {"st": 26, "ed": 28, "text": "multi scale"}, {"st": 44, "ed": 46, "text": "neural network"}, {"st": 52, "ed": 54, "text": "cifar 10"}, {"st": 56, "ed": 58, "text": "proposed method"}, {"st": 88, "ed": 90, "text": "fewer parameters"}]
[{"st": 7, "ed": 9, "text": "increasingly important"}, {"st": 22, "ed": 24, "text": "low power"}, {"st": 33, "ed": 35, "text": "artificial intelligence"}, {"st": 41, "ed": 43, "text": "neural networks"}, {"st": 44, "ed": 46, "text": "machine learning"}, {"st": 54, "ed": 56, "text": "parallel computing"}, {"st": 70, "ed": 72, "text": "machine learning"}, {"st": 75, "ed": 77, "text": "autonomous driving"}, {"st": 79, "ed": 81, "text": "spoken language"}, {"st": 94, "ed": 96, "text": "great potential"}, {"st": 105, "ed": 107, "text": "neural network"}, {"st": 129, "ed": 131, "text": "neural network"}, {"st": 144, "ed": 147, "text": "deep neural networks"}]
[{"st": 5, "ed": 7, "text": "neural network"}, {"st": 8, "ed": 10, "text": "reinforcement learning"}, {"st": 55, "ed": 57, "text": "audio visual"}, {"st": 77, "ed": 80, "text": "recurrent neural networks"}, {"st": 120, "ed": 122, "text": "proposed method"}, {"st": 134, "ed": 136, "text": "parameter values"}]
[{"st": 19, "ed": 21, "text": "existing approaches"}, {"st": 34, "ed": 36, "text": "ad hoc"}, {"st": 63, "ed": 65, "text": "parameter sharing"}, {"st": 90, "ed": 92, "text": "efficiently learn"}, {"st": 96, "ed": 98, "text": "competitive performance"}, {"st": 116, "ed": 119, "text": "convolutional neural networks"}, {"st": 122, "ed": 124, "text": "extensive experiments"}]
[{"st": 45, "ed": 49, "text": "content based image retrieval"}, {"st": 61, "ed": 63, "text": "feature selection"}, {"st": 69, "ed": 72, "text": "principal component analysis"}, {"st": 73, "ed": 75, "text": "image classification"}, {"st": 78, "ed": 80, "text": "multilayer perceptrons"}, {"st": 84, "ed": 86, "text": "self organizing"}]
[{"st": 1, "ed": 3, "text": "image analysis"}, {"st": 26, "ed": 28, "text": "current methods"}, {"st": 39, "ed": 41, "text": "computational intelligence"}, {"st": 94, "ed": 96, "text": "case study"}, {"st": 108, "ed": 110, "text": "magnetic resonance"}, {"st": 117, "ed": 119, "text": "ground truth"}, {"st": 130, "ed": 132, "text": "classification results"}, {"st": 151, "ed": 153, "text": "white matter"}, {"st": 164, "ed": 166, "text": "mr images"}]
[{"st": 3, "ed": 6, "text": "world health organization"}, {"st": 6, "ed": 8, "text": "breast cancer"}, {"st": 22, "ed": 24, "text": "breast cancer"}, {"st": 34, "ed": 36, "text": "economic development"}, {"st": 51, "ed": 53, "text": "early detection"}, {"st": 57, "ed": 60, "text": "point of view"}, {"st": 108, "ed": 110, "text": "multi resolution"}, {"st": 137, "ed": 140, "text": "detection and classification"}, {"st": 255, "ed": 258, "text": "save a lot"}, {"st": 262, "ed": 264, "text": "learning process"}]
[{"st": 25, "ed": 27, "text": "visual input"}, {"st": 69, "ed": 71, "text": "neural network"}, {"st": 117, "ed": 119, "text": "humanoid robot"}, {"st": 138, "ed": 140, "text": "prediction error"}, {"st": 156, "ed": 158, "text": "incomplete data"}]
[{"st": 0, "ed": 2, "text": "conformal prediction"}, {"st": 52, "ed": 54, "text": "conformal prediction"}, {"st": 65, "ed": 67, "text": "nearest neighbor"}, {"st": 69, "ed": 72, "text": "support vector machine"}, {"st": 72, "ed": 74, "text": "ridge regression"}, {"st": 75, "ed": 77, "text": "conformal prediction"}, {"st": 106, "ed": 108, "text": "conformal prediction"}, {"st": 173, "ed": 175, "text": "linear model"}, {"st": 190, "ed": 192, "text": "conformal prediction"}]
[{"st": 7, "ed": 9, "text": "small sample"}, {"st": 54, "ed": 56, "text": "ell 2"}, {"st": 97, "ed": 99, "text": "graph based"}, {"st": 129, "ed": 131, "text": "edge weights"}, {"st": 186, "ed": 188, "text": "feature space"}]
[{"st": 1, "ed": 4, "text": "supervised and unsupervised"}, {"st": 5, "ed": 7, "text": "positive definite"}, {"st": 14, "ed": 16, "text": "infinite dimensional"}, {"st": 16, "ed": 18, "text": "feature spaces"}, {"st": 20, "ed": 22, "text": "computational cost"}, {"st": 52, "ed": 54, "text": "sparsity inducing"}, {"st": 58, "ed": 60, "text": "l1 norm"}, {"st": 85, "ed": 88, "text": "directed acyclic graph"}, {"st": 102, "ed": 105, "text": "multiple kernel learning"}, {"st": 126, "ed": 128, "text": "extensive simulations"}, {"st": 129, "ed": 131, "text": "synthetic datasets"}, {"st": 143, "ed": 145, "text": "feature space"}, {"st": 146, "ed": 148, "text": "sparsity inducing"}]
[{"st": 44, "ed": 47, "text": "mean square error"}, {"st": 78, "ed": 80, "text": "low rank"}, {"st": 126, "ed": 128, "text": "massive data"}]
[{"st": 3, "ed": 5, "text": "least square"}, {"st": 5, "ed": 7, "text": "linear regression"}, {"st": 12, "ed": 14, "text": "ell 1"}, {"st": 31, "ed": 33, "text": "asymptotic analysis"}, {"st": 40, "ed": 42, "text": "low dimensional"}, {"st": 48, "ed": 50, "text": "regularization parameter"}]
[{"st": 2, "ed": 4, "text": "reinforcement learning"}, {"st": 9, "ed": 11, "text": "decision making"}, {"st": 44, "ed": 46, "text": "dynamic programming"}]
[{"st": 3, "ed": 5, "text": "machine learning"}, {"st": 25, "ed": 27, "text": "low level"}, {"st": 30, "ed": 32, "text": "problem solving"}, {"st": 55, "ed": 57, "text": "hierarchical clustering"}, {"st": 68, "ed": 70, "text": "natural language"}]
[{"st": 3, "ed": 5, "text": "nonparametric bayesian"}, {"st": 56, "ed": 58, "text": "factor analysis"}]
[{"st": 6, "ed": 8, "text": "large scale"}, {"st": 13, "ed": 15, "text": "ell 2"}, {"st": 41, "ed": 43, "text": "ell 2"}, {"st": 93, "ed": 95, "text": "approximate solution"}, {"st": 147, "ed": 149, "text": "weight vector"}, {"st": 203, "ed": 206, "text": "batch and online"}]
[{"st": 3, "ed": 5, "text": "exponential families"}, {"st": 46, "ed": 48, "text": "strong convexity"}, {"st": 51, "ed": 53, "text": "exponential families"}, {"st": 56, "ed": 58, "text": "generalization ability"}, {"st": 74, "ed": 76, "text": "exponential families"}]
[{"st": 8, "ed": 10, "text": "machine learning"}, {"st": 44, "ed": 46, "text": "unseen data"}]
[{"st": 7, "ed": 9, "text": "social sciences"}, {"st": 23, "ed": 25, "text": "latent variables"}, {"st": 69, "ed": 71, "text": "gaussian process"}, {"st": 79, "ed": 81, "text": "latent variables"}, {"st": 85, "ed": 87, "text": "gaussian process"}, {"st": 87, "ed": 89, "text": "latent variable"}, {"st": 99, "ed": 101, "text": "without compromising"}, {"st": 101, "ed": 105, "text": "markov chain monte carlo"}]
[{"st": 35, "ed": 37, "text": "total number"}, {"st": 56, "ed": 58, "text": "dynamic programming"}, {"st": 69, "ed": 71, "text": "real world"}]
[{"st": 25, "ed": 27, "text": "similarity measure"}, {"st": 52, "ed": 54, "text": "latent variable"}, {"st": 67, "ed": 69, "text": "generalization properties"}, {"st": 79, "ed": 82, "text": "support vector machine"}, {"st": 89, "ed": 92, "text": "hidden markov models"}, {"st": 99, "ed": 102, "text": "discrete and continuous"}, {"st": 115, "ed": 117, "text": "generalization error"}]
[{"st": 0, "ed": 2, "text": "recent research"}, {"st": 3, "ed": 6, "text": "multiple kernel learning"}, {"st": 43, "ed": 46, "text": "multiple kernel learning"}, {"st": 75, "ed": 78, "text": "multiple kernel learning"}, {"st": 84, "ed": 86, "text": "rademacher complexity"}, {"st": 89, "ed": 91, "text": "generalization error"}]
[{"st": 53, "ed": 55, "text": "probabilistic model"}, {"st": 59, "ed": 61, "text": "random walk"}, {"st": 83, "ed": 85, "text": "closed form"}]
[{"st": 24, "ed": 26, "text": "pattern recognition"}, {"st": 28, "ed": 30, "text": "neural networks"}, {"st": 31, "ed": 34, "text": "support vector machines"}, {"st": 62, "ed": 65, "text": "support vector machine"}, {"st": 77, "ed": 79, "text": "data fusion"}, {"st": 119, "ed": 122, "text": "orders of magnitude"}, {"st": 129, "ed": 131, "text": "floating point"}]
[{"st": 20, "ed": 23, "text": "statistical learning theory"}, {"st": 72, "ed": 74, "text": "sufficient conditions"}]
[{"st": 7, "ed": 9, "text": "boosting algorithms"}, {"st": 10, "ed": 12, "text": "multi class"}, {"st": 45, "ed": 47, "text": "exhaustive search"}, {"st": 77, "ed": 79, "text": "large datasets"}, {"st": 81, "ed": 83, "text": "exhaustive search"}, {"st": 142, "ed": 144, "text": "large datasets"}, {"st": 155, "ed": 157, "text": "preliminary results"}, {"st": 162, "ed": 164, "text": "large datasets"}, {"st": 191, "ed": 193, "text": "g 20"}, {"st": 212, "ed": 214, "text": "multi class"}]
[{"st": 8, "ed": 10, "text": "optimal control"}, {"st": 13, "ed": 15, "text": "approximate inference"}, {"st": 43, "ed": 45, "text": "optimal control"}, {"st": 57, "ed": 59, "text": "theoretical insights"}, {"st": 63, "ed": 65, "text": "reinforcement learning"}, {"st": 73, "ed": 75, "text": "off policy"}, {"st": 77, "ed": 80, "text": "discrete and continuous"}]
[{"st": 0, "ed": 3, "text": "multiple kernel learning"}, {"st": 4, "ed": 6, "text": "structured sparsity"}, {"st": 7, "ed": 10, "text": "multi task learning"}, {"st": 39, "ed": 41, "text": "based regularization"}, {"st": 46, "ed": 48, "text": "structured sparsity"}, {"st": 87, "ed": 89, "text": "probabilistic model"}, {"st": 97, "ed": 99, "text": "learning algorithms"}, {"st": 112, "ed": 114, "text": "numerical experiments"}, {"st": 115, "ed": 117, "text": "ell 2"}, {"st": 120, "ed": 122, "text": "elastic net"}, {"st": 123, "ed": 125, "text": "achieve comparable"}, {"st": 140, "ed": 142, "text": "ell 2"}, {"st": 145, "ed": 147, "text": "elastic net"}, {"st": 161, "ed": 163, "text": "elastic net"}]
[{"st": 1, "ed": 3, "text": "clustering schemes"}, {"st": 8, "ed": 10, "text": "objective function"}, {"st": 43, "ed": 45, "text": "clustering schemes"}, {"st": 66, "ed": 68, "text": "clustering algorithms"}, {"st": 126, "ed": 128, "text": "clustering schemes"}, {"st": 190, "ed": 192, "text": "clustering schemes"}]
[{"st": 22, "ed": 24, "text": "linear combination"}, {"st": 33, "ed": 35, "text": "signal processing"}, {"st": 35, "ed": 37, "text": "applications including"}, {"st": 66, "ed": 68, "text": "previously unseen"}, {"st": 75, "ed": 77, "text": "l 2"}, {"st": 103, "ed": 105, "text": "statistical learning"}, {"st": 109, "ed": 111, "text": "generalization bounds"}, {"st": 132, "ed": 134, "text": "l 2"}, {"st": 154, "ed": 156, "text": "generalization bound"}, {"st": 160, "ed": 162, "text": "o sqrt"}, {"st": 187, "ed": 190, "text": "l 1 norm"}, {"st": 229, "ed": 231, "text": "o sqrt"}]
[{"st": 0, "ed": 2, "text": "statistical models"}, {"st": 15, "ed": 17, "text": "machine learning"}, {"st": 31, "ed": 33, "text": "statistical models"}, {"st": 41, "ed": 43, "text": "statistical models"}, {"st": 45, "ed": 47, "text": "recently gained"}, {"st": 57, "ed": 59, "text": "complex data"}, {"st": 105, "ed": 107, "text": "computationally tractable"}, {"st": 117, "ed": 120, "text": "deep belief network"}, {"st": 128, "ed": 130, "text": "natural image"}, {"st": 140, "ed": 142, "text": "natural image"}, {"st": 149, "ed": 151, "text": "qualitative results"}, {"st": 157, "ed": 159, "text": "provide evidence"}, {"st": 171, "ed": 173, "text": "natural images"}]
[{"st": 7, "ed": 9, "text": "inverse covariance"}, {"st": 13, "ed": 15, "text": "likelihood function"}, {"st": 42, "ed": 45, "text": "maximum likelihood estimation"}, {"st": 52, "ed": 54, "text": "significantly faster"}, {"st": 66, "ed": 68, "text": "coordinate descent"}, {"st": 85, "ed": 87, "text": "based method"}, {"st": 99, "ed": 101, "text": "regularization terms"}, {"st": 104, "ed": 106, "text": "ell 1"}]
[{"st": 5, "ed": 7, "text": "sparsity inducing"}, {"st": 7, "ed": 9, "text": "regularization terms"}, {"st": 25, "ed": 27, "text": "submodular functions"}, {"st": 78, "ed": 80, "text": "regularization terms"}, {"st": 82, "ed": 84, "text": "prior knowledge"}, {"st": 104, "ed": 106, "text": "optimization algorithms"}, {"st": 111, "ed": 113, "text": "theoretical guarantees"}, {"st": 123, "ed": 125, "text": "submodular functions"}, {"st": 136, "ed": 138, "text": "total variation"}, {"st": 168, "ed": 171, "text": "change point detection"}]
[{"st": 8, "ed": 10, "text": "partially observed"}, {"st": 11, "ed": 13, "text": "low rank"}, {"st": 17, "ed": 20, "text": "received much attention"}, {"st": 24, "ed": 26, "text": "trace norm"}, {"st": 35, "ed": 37, "text": "low rank"}, {"st": 42, "ed": 44, "text": "trace norm"}, {"st": 62, "ed": 64, "text": "rademacher complexity"}, {"st": 81, "ed": 83, "text": "recently published"}]
[{"st": 3, "ed": 5, "text": "sparse signal"}, {"st": 5, "ed": 7, "text": "recovery problem"}, {"st": 28, "ed": 30, "text": "existing algorithms"}, {"st": 34, "ed": 36, "text": "temporal correlations"}, {"st": 53, "ed": 55, "text": "bayesian learning"}, {"st": 79, "ed": 81, "text": "existing algorithms"}, {"st": 114, "ed": 117, "text": "global and local"}, {"st": 120, "ed": 122, "text": "cost function"}, {"st": 127, "ed": 129, "text": "cost function"}, {"st": 136, "ed": 138, "text": "global minimum"}, {"st": 147, "ed": 149, "text": "extensive experiments"}]
[{"st": 1, "ed": 3, "text": "multi label"}, {"st": 11, "ed": 13, "text": "existing works"}, {"st": 30, "ed": 32, "text": "training process"}, {"st": 46, "ed": 48, "text": "feature space"}, {"st": 58, "ed": 60, "text": "multi label"}, {"st": 64, "ed": 66, "text": "group sparsity"}, {"st": 84, "ed": 86, "text": "training data"}, {"st": 96, "ed": 98, "text": "group sparse"}, {"st": 152, "ed": 154, "text": "low rank"}, {"st": 200, "ed": 202, "text": "group sparse"}, {"st": 246, "ed": 248, "text": "real datasets"}, {"st": 257, "ed": 260, "text": "effectiveness and efficiency"}]
[{"st": 2, "ed": 4, "text": "generalization error"}]
[]
[{"st": 96, "ed": 99, "text": "learning to rank"}, {"st": 116, "ed": 118, "text": "prior works"}, {"st": 131, "ed": 134, "text": "expectation maximization algorithm"}, {"st": 139, "ed": 142, "text": "k means clustering"}, {"st": 150, "ed": 152, "text": "rank minimization"}, {"st": 182, "ed": 184, "text": "empirical evaluation"}, {"st": 191, "ed": 193, "text": "simulated data"}, {"st": 209, "ed": 211, "text": "prediction performance"}, {"st": 241, "ed": 243, "text": "mathematical model"}, {"st": 244, "ed": 247, "text": "sheds light on"}]
[{"st": 52, "ed": 56, "text": "restricted boltzmann machine rbm"}, {"st": 87, "ed": 90, "text": "multiple instance learning"}]
[{"st": 10, "ed": 12, "text": "decision making"}, {"st": 29, "ed": 31, "text": "margin based"}, {"st": 31, "ed": 33, "text": "machine learning"}, {"st": 89, "ed": 91, "text": "learning algorithm"}, {"st": 102, "ed": 104, "text": "attention mechanism"}, {"st": 111, "ed": 113, "text": "margin based"}, {"st": 113, "ed": 116, "text": "online learning algorithm"}, {"st": 134, "ed": 136, "text": "o sqrt"}]
[{"st": 0, "ed": 2, "text": "nearest neighbor"}, {"st": 2, "ed": 4, "text": "k nn"}, {"st": 9, "ed": 11, "text": "machine learning"}, {"st": 12, "ed": 14, "text": "data mining"}, {"st": 56, "ed": 58, "text": "statistical analysis"}, {"st": 65, "ed": 67, "text": "k nn"}, {"st": 92, "ed": 94, "text": "finite sample"}, {"st": 140, "ed": 142, "text": "finite sample"}]
[{"st": 38, "ed": 40, "text": "convex objective"}, {"st": 73, "ed": 75, "text": "strongly convex"}, {"st": 108, "ed": 111, "text": "convex loss functions"}]
[{"st": 7, "ed": 9, "text": "pac bayesian"}, {"st": 30, "ed": 32, "text": "convex functions"}, {"st": 35, "ed": 37, "text": "random variables"}, {"st": 76, "ed": 78, "text": "pac bayesian"}, {"st": 86, "ed": 88, "text": "pac bayesian"}, {"st": 101, "ed": 103, "text": "pac bayesian"}, {"st": 105, "ed": 107, "text": "regret bounds"}, {"st": 114, "ed": 116, "text": "regret bound"}, {"st": 126, "ed": 128, "text": "regret bounds"}, {"st": 141, "ed": 143, "text": "potential applications"}, {"st": 144, "ed": 146, "text": "pac bayesian"}, {"st": 154, "ed": 156, "text": "reinforcement learning"}]
[{"st": 11, "ed": 13, "text": "exploration exploitation"}, {"st": 35, "ed": 37, "text": "pac bayesian"}]
[{"st": 6, "ed": 8, "text": "generalization error"}, {"st": 9, "ed": 11, "text": "time series"}, {"st": 21, "ed": 23, "text": "predict future"}, {"st": 46, "ed": 48, "text": "time series"}, {"st": 74, "ed": 76, "text": "rademacher complexity"}]
[{"st": 1, "ed": 3, "text": "exploration exploitation"}, {"st": 74, "ed": 76, "text": "infinite dimensional"}, {"st": 81, "ed": 83, "text": "finite dimensional"}]
[{"st": 14, "ed": 16, "text": "positive definite"}, {"st": 22, "ed": 24, "text": "learning algorithms"}, {"st": 43, "ed": 45, "text": "inner product"}, {"st": 48, "ed": 50, "text": "large scale"}, {"st": 64, "ed": 66, "text": "large scale"}, {"st": 70, "ed": 72, "text": "extremely high"}, {"st": 114, "ed": 117, "text": "theoretical and empirical"}, {"st": 136, "ed": 138, "text": "random projections"}, {"st": 158, "ed": 160, "text": "training speed"}]
[{"st": 67, "ed": 69, "text": "training examples"}, {"st": 80, "ed": 82, "text": "positive results"}]
[{"st": 7, "ed": 9, "text": "convex function"}, {"st": 20, "ed": 22, "text": "optimization problem"}, {"st": 30, "ed": 32, "text": "greedy algorithm"}, {"st": 76, "ed": 79, "text": "scale to large"}, {"st": 86, "ed": 88, "text": "matrix completion"}, {"st": 89, "ed": 91, "text": "collaborative filtering"}, {"st": 93, "ed": 96, "text": "low rank matrix"}]
[{"st": 2, "ed": 5, "text": "online learning algorithms"}, {"st": 22, "ed": 24, "text": "online algorithm"}, {"st": 54, "ed": 56, "text": "computationally efficient"}, {"st": 56, "ed": 58, "text": "online algorithm"}, {"st": 59, "ed": 61, "text": "collaborative filtering"}, {"st": 62, "ed": 64, "text": "trace norm"}, {"st": 73, "ed": 75, "text": "open question"}, {"st": 80, "ed": 82, "text": "online learning"}]
[{"st": 4, "ed": 6, "text": "online learning"}, {"st": 9, "ed": 11, "text": "decision maker"}, {"st": 32, "ed": 34, "text": "decision maker"}, {"st": 90, "ed": 92, "text": "decision maker"}, {"st": 98, "ed": 101, "text": "multi armed bandits"}, {"st": 104, "ed": 106, "text": "decision maker"}, {"st": 126, "ed": 128, "text": "non trivial"}]
[{"st": 5, "ed": 8, "text": "sequential decision making"}, {"st": 20, "ed": 22, "text": "special cases"}, {"st": 26, "ed": 28, "text": "reinforcement learning"}, {"st": 66, "ed": 68, "text": "near optimal"}, {"st": 72, "ed": 74, "text": "decision problem"}, {"st": 81, "ed": 83, "text": "bayes optimal"}, {"st": 114, "ed": 116, "text": "reinforcement learning"}]
[{"st": 9, "ed": 11, "text": "trace norm"}, {"st": 21, "ed": 23, "text": "trace norm"}, {"st": 27, "ed": 29, "text": "sampling distribution"}, {"st": 36, "ed": 39, "text": "row and column"}, {"st": 74, "ed": 76, "text": "sampling distribution"}, {"st": 82, "ed": 84, "text": "true distribution"}]
[{"st": 1, "ed": 3, "text": "compressive sensing"}, {"st": 11, "ed": 13, "text": "sparse signal"}, {"st": 32, "ed": 34, "text": "real world"}, {"st": 108, "ed": 110, "text": "signal recovery"}]
[{"st": 17, "ed": 19, "text": "group lasso"}, {"st": 30, "ed": 32, "text": "convex optimization"}, {"st": 55, "ed": 57, "text": "optimization methods"}, {"st": 70, "ed": 72, "text": "sample size"}, {"st": 87, "ed": 89, "text": "fixed points"}, {"st": 129, "ed": 131, "text": "optimization algorithm"}]
[{"st": 14, "ed": 16, "text": "training instances"}, {"st": 57, "ed": 59, "text": "ensemble learning"}, {"st": 82, "ed": 84, "text": "real life"}, {"st": 84, "ed": 86, "text": "survival analysis"}, {"st": 129, "ed": 132, "text": "real life data"}, {"st": 135, "ed": 137, "text": "training sets"}]
[{"st": 11, "ed": 13, "text": "learning algorithm"}, {"st": 16, "ed": 18, "text": "small change"}, {"st": 29, "ed": 31, "text": "training sample"}, {"st": 54, "ed": 56, "text": "uniform convergence"}, {"st": 89, "ed": 91, "text": "learning algorithm"}, {"st": 137, "ed": 139, "text": "batch setting"}, {"st": 169, "ed": 171, "text": "gradient based"}, {"st": 263, "ed": 265, "text": "online algorithms"}]
[{"st": 5, "ed": 7, "text": "generalization bound"}, {"st": 16, "ed": 18, "text": "structured sparsity"}, {"st": 27, "ed": 29, "text": "norm regularization"}, {"st": 32, "ed": 34, "text": "group lasso"}, {"st": 38, "ed": 40, "text": "group lasso"}, {"st": 43, "ed": 46, "text": "multiple kernel learning"}, {"st": 54, "ed": 56, "text": "competitive results"}, {"st": 72, "ed": 74, "text": "infinite dimensional"}, {"st": 85, "ed": 88, "text": "multiple kernel learning"}]
[{"st": 5, "ed": 7, "text": "structured sparsity"}, {"st": 10, "ed": 12, "text": "sparse linear"}, {"st": 30, "ed": 32, "text": "group lasso"}, {"st": 40, "ed": 42, "text": "group lasso"}, {"st": 50, "ed": 52, "text": "detailed analysis"}, {"st": 72, "ed": 74, "text": "latent variables"}, {"st": 96, "ed": 98, "text": "simulated data"}, {"st": 103, "ed": 105, "text": "breast cancer"}]
[{"st": 28, "ed": 30, "text": "observed variables"}, {"st": 41, "ed": 43, "text": "observed variables"}, {"st": 47, "ed": 49, "text": "sparse matrix"}, {"st": 55, "ed": 57, "text": "observed variables"}, {"st": 59, "ed": 61, "text": "low rank"}, {"st": 67, "ed": 69, "text": "latent variables"}, {"st": 89, "ed": 92, "text": "guaranteed to converge"}, {"st": 101, "ed": 103, "text": "significantly faster"}, {"st": 137, "ed": 139, "text": "observed variables"}]
[{"st": 42, "ed": 44, "text": "training examples"}, {"st": 73, "ed": 75, "text": "efficient implementation"}]
[{"st": 8, "ed": 10, "text": "linear optimization"}, {"st": 37, "ed": 39, "text": "sqrt t"}, {"st": 60, "ed": 62, "text": "convex geometry"}, {"st": 80, "ed": 82, "text": "linear bandits"}, {"st": 91, "ed": 93, "text": "linear optimization"}, {"st": 93, "ed": 96, "text": "with expert advice"}, {"st": 113, "ed": 116, "text": "armed bandit problem"}, {"st": 116, "ed": 119, "text": "with expert advice"}]
[{"st": 0, "ed": 3, "text": "latent dirichlet allocation"}, {"st": 4, "ed": 6, "text": "discrete data"}, {"st": 53, "ed": 55, "text": "topic models"}, {"st": 71, "ed": 73, "text": "approximate inference"}, {"st": 101, "ed": 103, "text": "gaussian process"}, {"st": 103, "ed": 105, "text": "latent variable"}, {"st": 109, "ed": 111, "text": "topic model"}]
[{"st": 18, "ed": 20, "text": "hierarchical bayesian"}, {"st": 41, "ed": 43, "text": "clustering methods"}, {"st": 53, "ed": 56, "text": "k means clustering"}, {"st": 68, "ed": 70, "text": "k means"}, {"st": 78, "ed": 80, "text": "gibbs sampling"}, {"st": 89, "ed": 91, "text": "clustering algorithm"}, {"st": 105, "ed": 107, "text": "k means"}, {"st": 166, "ed": 168, "text": "clustering algorithm"}, {"st": 173, "ed": 176, "text": "number of clusters"}]
[{"st": 16, "ed": 18, "text": "graphical model"}, {"st": 19, "ed": 21, "text": "ell 1"}, {"st": 30, "ed": 32, "text": "precision matrix"}, {"st": 78, "ed": 80, "text": "precision matrix"}, {"st": 135, "ed": 137, "text": "block coordinate"}, {"st": 165, "ed": 167, "text": "precision matrix"}, {"st": 182, "ed": 185, "text": "block coordinate descent"}]
[{"st": 16, "ed": 19, "text": "principal component analysis"}, {"st": 25, "ed": 29, "text": "synthetic and real data"}, {"st": 40, "ed": 42, "text": "factor analysis"}, {"st": 46, "ed": 48, "text": "theoretical results"}, {"st": 69, "ed": 71, "text": "computational requirements"}]
[{"st": 6, "ed": 8, "text": "potential applications"}, {"st": 15, "ed": 17, "text": "social network"}, {"st": 37, "ed": 39, "text": "machine learning"}, {"st": 43, "ed": 45, "text": "current approaches"}, {"st": 53, "ed": 55, "text": "standard classification"}, {"st": 75, "ed": 77, "text": "real world"}, {"st": 80, "ed": 82, "text": "kernel based"}, {"st": 93, "ed": 95, "text": "existing approaches"}, {"st": 106, "ed": 108, "text": "existing approaches"}, {"st": 128, "ed": 130, "text": "recent developments"}, {"st": 131, "ed": 133, "text": "fuzzy set"}, {"st": 143, "ed": 149, "text": "experiments on synthetic and real world"}]
[{"st": 0, "ed": 3, "text": "probabilistic graphical models"}, {"st": 81, "ed": 83, "text": "conditional independence"}, {"st": 91, "ed": 94, "text": "inference and learning"}, {"st": 129, "ed": 131, "text": "social science"}, {"st": 131, "ed": 133, "text": "control theory"}, {"st": 133, "ed": 135, "text": "image processing"}, {"st": 140, "ed": 142, "text": "structure learning"}, {"st": 175, "ed": 177, "text": "structure learning"}]
[{"st": 0, "ed": 2, "text": "metric learning"}, {"st": 20, "ed": 22, "text": "metric learning"}, {"st": 63, "ed": 65, "text": "metric learning"}, {"st": 78, "ed": 80, "text": "distance function"}, {"st": 92, "ed": 94, "text": "random forest"}, {"st": 99, "ed": 101, "text": "distance function"}, {"st": 139, "ed": 141, "text": "proposed method"}, {"st": 172, "ed": 174, "text": "worst case"}]
[{"st": 0, "ed": 2, "text": "functional data"}, {"st": 16, "ed": 18, "text": "real valued"}, {"st": 42, "ed": 44, "text": "functional data"}, {"st": 70, "ed": 72, "text": "clustering approach"}]
[{"st": 1, "ed": 3, "text": "classification methods"}, {"st": 16, "ed": 18, "text": "machine learning"}, {"st": 25, "ed": 27, "text": "classification techniques"}, {"st": 52, "ed": 54, "text": "concept drift"}, {"st": 172, "ed": 174, "text": "evaluation criteria"}, {"st": 188, "ed": 190, "text": "proposed approach"}]
[{"st": 7, "ed": 9, "text": "sample size"}, {"st": 16, "ed": 18, "text": "based method"}, {"st": 36, "ed": 38, "text": "performance metrics"}, {"st": 42, "ed": 44, "text": "sample size"}, {"st": 59, "ed": 61, "text": "cross validation"}, {"st": 87, "ed": 89, "text": "empirical evidence"}, {"st": 110, "ed": 114, "text": "university of california irvine"}, {"st": 114, "ed": 116, "text": "machine learning"}, {"st": 127, "ed": 129, "text": "sample size"}, {"st": 148, "ed": 150, "text": "sample sizes"}, {"st": 182, "ed": 185, "text": "curse of dimensionality"}, {"st": 187, "ed": 189, "text": "feature selection"}]
[{"st": 5, "ed": 7, "text": "learning problems"}, {"st": 68, "ed": 70, "text": "decision theoretic"}, {"st": 74, "ed": 76, "text": "reinforcement learning"}, {"st": 86, "ed": 88, "text": "multi stage"}, {"st": 92, "ed": 94, "text": "learning agent"}, {"st": 201, "ed": 203, "text": "performance bounds"}]
[{"st": 12, "ed": 15, "text": "spike and slab"}, {"st": 15, "ed": 17, "text": "sparse coding"}, {"st": 31, "ed": 34, "text": "spike and slab"}, {"st": 39, "ed": 41, "text": "exact inference"}, {"st": 50, "ed": 52, "text": "variational inference"}, {"st": 63, "ed": 65, "text": "approximate inference"}, {"st": 86, "ed": 88, "text": "inference procedure"}, {"st": 101, "ed": 103, "text": "training set"}, {"st": 118, "ed": 120, "text": "supervised learning"}, {"st": 123, "ed": 125, "text": "sparse coding"}, {"st": 130, "ed": 132, "text": "cifar 10"}, {"st": 140, "ed": 143, "text": "semi supervised learning"}, {"st": 177, "ed": 179, "text": "hierarchical models"}, {"st": 179, "ed": 181, "text": "transfer learning"}]
[{"st": 0, "ed": 3, "text": "support vector machines"}, {"st": 6, "ed": 8, "text": "large margin"}, {"st": 8, "ed": 10, "text": "nearest neighbor"}, {"st": 16, "ed": 18, "text": "learning algorithms"}, {"st": 52, "ed": 54, "text": "metric learning"}, {"st": 60, "ed": 62, "text": "metric learning"}, {"st": 140, "ed": 142, "text": "preliminary experiments"}, {"st": 146, "ed": 148, "text": "benchmark datasets"}, {"st": 154, "ed": 156, "text": "compares favorably"}]
[{"st": 10, "ed": 12, "text": "k means"}, {"st": 40, "ed": 42, "text": "spectral clustering"}, {"st": 156, "ed": 158, "text": "objective function"}, {"st": 159, "ed": 161, "text": "spectral clustering"}, {"st": 173, "ed": 175, "text": "spectral clustering"}, {"st": 189, "ed": 191, "text": "empirical results"}, {"st": 209, "ed": 211, "text": "transfer learning"}]
[{"st": 26, "ed": 28, "text": "parameter free"}, {"st": 30, "ed": 32, "text": "inference procedure"}, {"st": 36, "ed": 39, "text": "hidden markov models"}, {"st": 51, "ed": 53, "text": "discrete state"}, {"st": 55, "ed": 57, "text": "discrete state"}]
[{"st": 3, "ed": 5, "text": "reinforcement learning"}, {"st": 6, "ed": 8, "text": "real world"}, {"st": 18, "ed": 20, "text": "learning algorithms"}, {"st": 43, "ed": 45, "text": "real world"}, {"st": 53, "ed": 55, "text": "supervised classification"}, {"st": 58, "ed": 60, "text": "optimal policy"}, {"st": 80, "ed": 82, "text": "error correcting"}, {"st": 191, "ed": 193, "text": "benchmark problems"}]
[{"st": 0, "ed": 2, "text": "mixture models"}, {"st": 7, "ed": 9, "text": "applied statistics"}, {"st": 10, "ed": 12, "text": "machine learning"}, {"st": 31, "ed": 33, "text": "local search"}, {"st": 36, "ed": 38, "text": "em algorithm"}, {"st": 55, "ed": 57, "text": "sample complexity"}, {"st": 72, "ed": 75, "text": "method of moments"}, {"st": 77, "ed": 79, "text": "parameter estimation"}, {"st": 86, "ed": 88, "text": "mixture models"}, {"st": 92, "ed": 94, "text": "multi view"}, {"st": 114, "ed": 116, "text": "unsupervised learning"}, {"st": 118, "ed": 120, "text": "mixture models"}, {"st": 125, "ed": 127, "text": "previous works"}]
[{"st": 0, "ed": 2, "text": "positive definite"}, {"st": 2, "ed": 4, "text": "operator valued"}, {"st": 18, "ed": 20, "text": "multi output"}, {"st": 31, "ed": 33, "text": "linear combination"}, {"st": 34, "ed": 36, "text": "infinite dimensional"}, {"st": 36, "ed": 38, "text": "operator valued"}, {"st": 44, "ed": 46, "text": "functional data"}, {"st": 59, "ed": 62, "text": "kernel ridge regression"}, {"st": 76, "ed": 78, "text": "optimization problem"}, {"st": 87, "ed": 89, "text": "kernel learning"}, {"st": 90, "ed": 92, "text": "operator valued"}, {"st": 103, "ed": 105, "text": "operator valued"}, {"st": 105, "ed": 107, "text": "kernel learning"}, {"st": 114, "ed": 116, "text": "linear operator"}]
[{"st": 6, "ed": 8, "text": "gaussian process"}, {"st": 17, "ed": 20, "text": "branch and bound"}, {"st": 26, "ed": 28, "text": "ucb algorithm"}, {"st": 61, "ed": 63, "text": "frac 1"}, {"st": 63, "ed": 65, "text": "sqrt t"}, {"st": 121, "ed": 123, "text": "search space"}, {"st": 135, "ed": 137, "text": "objective function"}]
[{"st": 0, "ed": 2, "text": "differential privacy"}, {"st": 25, "ed": 27, "text": "finite dimensional"}, {"st": 52, "ed": 54, "text": "gaussian process"}, {"st": 72, "ed": 74, "text": "gaussian process"}, {"st": 77, "ed": 79, "text": "noise level"}, {"st": 96, "ed": 99, "text": "kernel density estimation"}, {"st": 100, "ed": 103, "text": "support vector machines"}]
[{"st": 2, "ed": 5, "text": "distance metric learning"}, {"st": 8, "ed": 10, "text": "side information"}, {"st": 22, "ed": 24, "text": "real world"}, {"st": 30, "ed": 32, "text": "side information"}, {"st": 61, "ed": 63, "text": "distance metric"}, {"st": 67, "ed": 69, "text": "robust optimization"}, {"st": 71, "ed": 73, "text": "worst case"}, {"st": 85, "ed": 87, "text": "learning task"}, {"st": 90, "ed": 92, "text": "combinatorial optimization"}, {"st": 109, "ed": 111, "text": "efficient learning"}, {"st": 121, "ed": 123, "text": "worst case"}, {"st": 123, "ed": 125, "text": "convergence rate"}, {"st": 132, "ed": 134, "text": "optimization problems"}, {"st": 146, "ed": 148, "text": "empirical study"}, {"st": 157, "ed": 159, "text": "proposed method"}]
[{"st": 2, "ed": 4, "text": "gaussian process"}, {"st": 4, "ed": 6, "text": "topic models"}, {"st": 11, "ed": 13, "text": "topic models"}, {"st": 34, "ed": 36, "text": "topic models"}, {"st": 40, "ed": 42, "text": "gaussian process"}, {"st": 57, "ed": 59, "text": "kernel matrix"}, {"st": 89, "ed": 91, "text": "topic modeling"}]
[{"st": 0, "ed": 2, "text": "hierarchical structure"}, {"st": 12, "ed": 14, "text": "hierarchical clustering"}, {"st": 18, "ed": 20, "text": "domain experts"}, {"st": 55, "ed": 57, "text": "hierarchical clustering"}, {"st": 88, "ed": 90, "text": "computationally efficient"}, {"st": 101, "ed": 103, "text": "marginal likelihood"}, {"st": 128, "ed": 130, "text": "hierarchical clustering"}]
[]
[{"st": 48, "ed": 50, "text": "noise free"}, {"st": 74, "ed": 76, "text": "probability density"}, {"st": 100, "ed": 102, "text": "theoretical analysis"}, {"st": 112, "ed": 114, "text": "low noise"}, {"st": 124, "ed": 126, "text": "empirical results"}, {"st": 128, "ed": 131, "text": "real world data"}]
[{"st": 4, "ed": 7, "text": "probabilistic graphical models"}, {"st": 31, "ed": 33, "text": "efficient inference"}, {"st": 85, "ed": 87, "text": "significant computational"}, {"st": 89, "ed": 91, "text": "training data"}, {"st": 109, "ed": 111, "text": "learning objective"}, {"st": 118, "ed": 120, "text": "log likelihood"}, {"st": 153, "ed": 156, "text": "structure and parameters"}]
[{"st": 8, "ed": 10, "text": "intrinsic dimension"}, {"st": 19, "ed": 21, "text": "maximum likelihood"}, {"st": 30, "ed": 32, "text": "regularization scheme"}, {"st": 51, "ed": 53, "text": "convergence properties"}, {"st": 60, "ed": 63, "text": "simulated and real"}, {"st": 77, "ed": 79, "text": "intrinsic dimension"}]
[{"st": 6, "ed": 9, "text": "hierarchical dirichlet process"}, {"st": 9, "ed": 12, "text": "hidden markov model"}, {"st": 79, "ed": 81, "text": "interpretable models"}, {"st": 84, "ed": 86, "text": "prior information"}, {"st": 100, "ed": 102, "text": "posterior sampling"}, {"st": 104, "ed": 106, "text": "efficient inference"}, {"st": 128, "ed": 130, "text": "synthetic data"}]
[{"st": 4, "ed": 6, "text": "graphical model"}, {"st": 53, "ed": 55, "text": "asymptotic analysis"}, {"st": 64, "ed": 66, "text": "real datasets"}, {"st": 110, "ed": 112, "text": "statistical model"}]
[{"st": 0, "ed": 2, "text": "exponential family"}, {"st": 4, "ed": 7, "text": "principal component analysis"}, {"st": 16, "ed": 18, "text": "recent years"}, {"st": 31, "ed": 33, "text": "squared loss"}, {"st": 46, "ed": 48, "text": "exponential family"}, {"st": 48, "ed": 50, "text": "multi view"}, {"st": 55, "ed": 57, "text": "least squares"}, {"st": 58, "ed": 61, "text": "canonical correlation analysis"}, {"st": 69, "ed": 71, "text": "matrix factorization"}, {"st": 106, "ed": 108, "text": "methods outperform"}]
[{"st": 4, "ed": 6, "text": "boosting algorithm"}, {"st": 26, "ed": 28, "text": "weak learners"}, {"st": 48, "ed": 50, "text": "multi class"}, {"st": 75, "ed": 77, "text": "extensive experiments"}, {"st": 78, "ed": 80, "text": "multi class"}, {"st": 106, "ed": 108, "text": "deep learning"}]
[{"st": 7, "ed": 9, "text": "theoretical analysis"}, {"st": 12, "ed": 14, "text": "higher order"}, {"st": 38, "ed": 40, "text": "distance based"}, {"st": 47, "ed": 49, "text": "large scale"}, {"st": 63, "ed": 65, "text": "efficient algorithms"}]
[{"st": 10, "ed": 13, "text": "log partition function"}, {"st": 15, "ed": 18, "text": "markov random field"}, {"st": 37, "ed": 39, "text": "linear combination"}, {"st": 56, "ed": 58, "text": "belief propagation"}, {"st": 62, "ed": 64, "text": "convex combination"}, {"st": 88, "ed": 90, "text": "convex set"}, {"st": 97, "ed": 99, "text": "mean field"}, {"st": 104, "ed": 106, "text": "mean field"}]
[{"st": 6, "ed": 8, "text": "unlabeled data"}, {"st": 10, "ed": 12, "text": "unlabeled data"}, {"st": 14, "ed": 16, "text": "increasing attention"}, {"st": 26, "ed": 28, "text": "semi supervised"}, {"st": 28, "ed": 30, "text": "kernel learning"}, {"st": 38, "ed": 40, "text": "unlabeled data"}, {"st": 42, "ed": 44, "text": "least squares"}, {"st": 53, "ed": 55, "text": "kernel matrix"}, {"st": 71, "ed": 73, "text": "proposed algorithm"}, {"st": 87, "ed": 89, "text": "labeled data"}, {"st": 98, "ed": 100, "text": "closed form"}, {"st": 108, "ed": 110, "text": "proposed method"}, {"st": 131, "ed": 133, "text": "extensive experiments"}, {"st": 135, "ed": 137, "text": "benchmark datasets"}, {"st": 143, "ed": 145, "text": "parameter free"}, {"st": 146, "ed": 148, "text": "kernel learning"}, {"st": 151, "ed": 153, "text": "comparable performance"}, {"st": 154, "ed": 156, "text": "fine tuned"}, {"st": 157, "ed": 159, "text": "regularization methods"}, {"st": 164, "ed": 167, "text": "multiple kernel learning"}]
[{"st": 5, "ed": 7, "text": "mixture model"}, {"st": 14, "ed": 16, "text": "gibbs sampling"}, {"st": 43, "ed": 46, "text": "taking advantage of"}, {"st": 52, "ed": 54, "text": "empirically demonstrate"}, {"st": 73, "ed": 75, "text": "clustering techniques"}]
[{"st": 2, "ed": 4, "text": "reinforcement learning"}, {"st": 9, "ed": 11, "text": "decision making"}, {"st": 21, "ed": 23, "text": "risk management"}, {"st": 81, "ed": 83, "text": "conditional probability"}, {"st": 95, "ed": 97, "text": "learning algorithm"}]
[{"st": 31, "ed": 33, "text": "maximum likelihood"}, {"st": 33, "ed": 35, "text": "monte carlo"}, {"st": 71, "ed": 73, "text": "objective function"}]
[{"st": 0, "ed": 3, "text": "gaussian processes gps"}, {"st": 60, "ed": 62, "text": "gaussian process"}, {"st": 81, "ed": 83, "text": "basis point"}, {"st": 108, "ed": 110, "text": "expectation propagation"}, {"st": 156, "ed": 158, "text": "basis point"}, {"st": 160, "ed": 162, "text": "gaussian distribution"}, {"st": 174, "ed": 176, "text": "basis point"}, {"st": 194, "ed": 196, "text": "experiments demonstrate"}, {"st": 199, "ed": 201, "text": "outperforms previous"}, {"st": 202, "ed": 204, "text": "classification methods"}, {"st": 205, "ed": 207, "text": "benchmark datasets"}]
[{"st": 26, "ed": 28, "text": "relational learning"}, {"st": 30, "ed": 32, "text": "matrix factorization"}, {"st": 44, "ed": 46, "text": "random walk"}, {"st": 78, "ed": 80, "text": "predictive model"}, {"st": 92, "ed": 94, "text": "side information"}]
[{"st": 7, "ed": 9, "text": "sample size"}, {"st": 16, "ed": 18, "text": "important role"}, {"st": 26, "ed": 28, "text": "asymptotic analysis"}, {"st": 30, "ed": 32, "text": "marginal likelihood"}, {"st": 46, "ed": 48, "text": "sample size"}, {"st": 80, "ed": 82, "text": "marginal likelihood"}]
[{"st": 9, "ed": 12, "text": "semi supervised learning"}, {"st": 36, "ed": 38, "text": "fast approximate"}, {"st": 93, "ed": 95, "text": "face recognition"}, {"st": 96, "ed": 99, "text": "optical character recognition"}, {"st": 116, "ed": 118, "text": "unlike previous"}, {"st": 124, "ed": 126, "text": "method yields"}]
[{"st": 0, "ed": 2, "text": "gaussian processes"}, {"st": 5, "ed": 7, "text": "building blocks"}, {"st": 58, "ed": 60, "text": "gp regression"}, {"st": 75, "ed": 77, "text": "posterior inference"}, {"st": 85, "ed": 87, "text": "expectation propagation"}]
[{"st": 13, "ed": 15, "text": "conditional independence"}, {"st": 36, "ed": 38, "text": "random variables"}, {"st": 53, "ed": 55, "text": "great importance"}, {"st": 94, "ed": 96, "text": "learning strategy"}, {"st": 102, "ed": 104, "text": "graphical model"}, {"st": 105, "ed": 107, "text": "l1 regularization"}, {"st": 121, "ed": 123, "text": "efficient implementation"}, {"st": 125, "ed": 128, "text": "block coordinate descent"}, {"st": 161, "ed": 163, "text": "biologically plausible"}]
[{"st": 71, "ed": 73, "text": "source separation"}, {"st": 75, "ed": 78, "text": "takes into account"}]
[{"st": 2, "ed": 4, "text": "latent variable"}, {"st": 12, "ed": 14, "text": "latent variables"}, {"st": 32, "ed": 34, "text": "closely related"}, {"st": 35, "ed": 37, "text": "causal discovery"}, {"st": 78, "ed": 80, "text": "gaussian process"}, {"st": 80, "ed": 82, "text": "latent variable"}, {"st": 104, "ed": 106, "text": "gaussian process"}, {"st": 143, "ed": 145, "text": "causal discovery"}, {"st": 160, "ed": 162, "text": "linear transformation"}]
[{"st": 16, "ed": 18, "text": "labeled training"}, {"st": 28, "ed": 30, "text": "training set"}, {"st": 50, "ed": 52, "text": "worst case"}, {"st": 73, "ed": 75, "text": "special cases"}, {"st": 110, "ed": 112, "text": "audio visual"}, {"st": 119, "ed": 121, "text": "recently proposed"}, {"st": 121, "ed": 123, "text": "multi modal"}]
[{"st": 40, "ed": 42, "text": "hidden state"}, {"st": 65, "ed": 67, "text": "side information"}, {"st": 86, "ed": 88, "text": "side information"}, {"st": 93, "ed": 95, "text": "recognition performance"}]
[{"st": 8, "ed": 10, "text": "local search"}, {"st": 17, "ed": 19, "text": "exponential families"}, {"st": 24, "ed": 26, "text": "mixture models"}, {"st": 31, "ed": 34, "text": "expectation maximization em"}, {"st": 35, "ed": 37, "text": "clustering technique"}, {"st": 70, "ed": 72, "text": "maximum likelihood"}, {"st": 79, "ed": 81, "text": "exponential families"}, {"st": 82, "ed": 84, "text": "bregman divergences"}, {"st": 121, "ed": 123, "text": "k means"}, {"st": 148, "ed": 150, "text": "cross entropy"}, {"st": 172, "ed": 174, "text": "weight update"}, {"st": 183, "ed": 185, "text": "special case"}, {"st": 195, "ed": 197, "text": "weight update"}]
[{"st": 61, "ed": 63, "text": "simultaneously learns"}, {"st": 96, "ed": 98, "text": "proposed method"}, {"st": 111, "ed": 113, "text": "prediction performance"}]
[{"st": 0, "ed": 3, "text": "hidden markov models"}, {"st": 9, "ed": 11, "text": "co occurrence"}, {"st": 43, "ed": 45, "text": "significantly reduces"}, {"st": 46, "ed": 50, "text": "number of model parameters"}, {"st": 58, "ed": 60, "text": "sample complexity"}]
[{"st": 89, "ed": 91, "text": "search space"}, {"st": 103, "ed": 105, "text": "search space"}, {"st": 125, "ed": 127, "text": "proposed algorithm"}, {"st": 127, "ed": 129, "text": "significantly outperforms"}]
[{"st": 9, "ed": 11, "text": "sample complexity"}, {"st": 12, "ed": 14, "text": "large margin"}, {"st": 16, "ed": 18, "text": "l2 regularization"}, {"st": 48, "ed": 50, "text": "sample complexity"}, {"st": 77, "ed": 79, "text": "gaussian distributions"}, {"st": 92, "ed": 94, "text": "sample complexity"}, {"st": 95, "ed": 97, "text": "large margin"}, {"st": 126, "ed": 128, "text": "linear classifiers"}, {"st": 155, "ed": 157, "text": "large margin"}, {"st": 160, "ed": 162, "text": "learning rules"}, {"st": 171, "ed": 173, "text": "sample complexity"}]
[{"st": 9, "ed": 12, "text": "multi armed bandit"}, {"st": 15, "ed": 17, "text": "machine learning"}, {"st": 54, "ed": 57, "text": "upper confidence bound"}, {"st": 109, "ed": 111, "text": "low complexity"}]
[{"st": 1, "ed": 3, "text": "statistical models"}, {"st": 7, "ed": 9, "text": "information science"}, {"st": 28, "ed": 30, "text": "latent variables"}, {"st": 35, "ed": 37, "text": "asymptotic analysis"}, {"st": 42, "ed": 44, "text": "important role"}, {"st": 47, "ed": 49, "text": "learning process"}, {"st": 83, "ed": 85, "text": "prediction accuracy"}, {"st": 95, "ed": 97, "text": "latent variables"}, {"st": 104, "ed": 106, "text": "quantitative evaluation"}, {"st": 133, "ed": 135, "text": "maximum likelihood"}]
[{"st": 9, "ed": 11, "text": "learning task"}, {"st": 44, "ed": 46, "text": "training error"}, {"st": 47, "ed": 49, "text": "linearly separable"}, {"st": 75, "ed": 77, "text": "weight update"}, {"st": 149, "ed": 151, "text": "efficient learning"}, {"st": 164, "ed": 166, "text": "linear programming"}, {"st": 180, "ed": 182, "text": "learning problems"}, {"st": 186, "ed": 188, "text": "convex optimization"}, {"st": 220, "ed": 222, "text": "weight update"}]
[{"st": 3, "ed": 5, "text": "linear optimization"}, {"st": 12, "ed": 14, "text": "decision maker"}, {"st": 23, "ed": 25, "text": "decision maker"}, {"st": 73, "ed": 75, "text": "decision maker"}, {"st": 80, "ed": 82, "text": "partial information"}, {"st": 133, "ed": 135, "text": "existing results"}, {"st": 148, "ed": 150, "text": "optimal regret"}, {"st": 161, "ed": 163, "text": "weighted average"}]
[{"st": 10, "ed": 12, "text": "convex relaxation"}, {"st": 17, "ed": 19, "text": "ell 2"}, {"st": 35, "ed": 37, "text": "elastic net"}, {"st": 48, "ed": 50, "text": "elastic net"}, {"st": 69, "ed": 71, "text": "elastic net"}]
[{"st": 0, "ed": 4, "text": "multi armed bandit problems"}, {"st": 10, "ed": 12, "text": "sequential decision"}, {"st": 15, "ed": 17, "text": "exploration exploitation"}, {"st": 51, "ed": 53, "text": "bandit problems"}, {"st": 58, "ed": 60, "text": "exploration exploitation"}, {"st": 78, "ed": 81, "text": "multi armed bandit"}, {"st": 138, "ed": 140, "text": "contextual bandit"}]
[{"st": 11, "ed": 13, "text": "natural extension"}, {"st": 19, "ed": 22, "text": "k means clustering"}, {"st": 26, "ed": 28, "text": "proposed method"}, {"st": 31, "ed": 33, "text": "hierarchical clustering"}, {"st": 36, "ed": 38, "text": "multi level"}, {"st": 43, "ed": 45, "text": "computationally efficient"}]
[{"st": 1, "ed": 3, "text": "binary classification"}, {"st": 12, "ed": 14, "text": "loss function"}, {"st": 23, "ed": 25, "text": "loss function"}, {"st": 30, "ed": 32, "text": "learning algorithms"}, {"st": 34, "ed": 37, "text": "support vector machine"}, {"st": 42, "ed": 44, "text": "loss function"}, {"st": 57, "ed": 59, "text": "learning algorithm"}, {"st": 64, "ed": 66, "text": "loss function"}, {"st": 82, "ed": 84, "text": "learning algorithms"}, {"st": 86, "ed": 88, "text": "loss functions"}, {"st": 89, "ed": 91, "text": "widely applied"}, {"st": 92, "ed": 95, "text": "real world data"}, {"st": 98, "ed": 100, "text": "statistical properties"}, {"st": 102, "ed": 104, "text": "learning algorithms"}, {"st": 139, "ed": 141, "text": "maximum margin"}, {"st": 144, "ed": 146, "text": "learning algorithm"}, {"st": 185, "ed": 187, "text": "maximum margin"}, {"st": 199, "ed": 201, "text": "robust optimization"}, {"st": 208, "ed": 210, "text": "statistical properties"}, {"st": 211, "ed": 213, "text": "learning algorithms"}, {"st": 245, "ed": 247, "text": "level set"}, {"st": 260, "ed": 262, "text": "statistical properties"}, {"st": 263, "ed": 265, "text": "learning algorithms"}]
[{"st": 3, "ed": 5, "text": "topic modeling"}, {"st": 13, "ed": 15, "text": "clustering problem"}, {"st": 26, "ed": 28, "text": "latent factors"}, {"st": 51, "ed": 53, "text": "representational power"}, {"st": 61, "ed": 63, "text": "unsupervised learning"}, {"st": 92, "ed": 95, "text": "simple and efficient"}, {"st": 109, "ed": 111, "text": "mixture models"}, {"st": 114, "ed": 117, "text": "latent dirichlet allocation"}, {"st": 159, "ed": 161, "text": "correlation analysis"}, {"st": 169, "ed": 171, "text": "low order"}, {"st": 206, "ed": 208, "text": "latent factors"}, {"st": 210, "ed": 213, "text": "number of topics"}, {"st": 217, "ed": 219, "text": "d dimensional"}]
[{"st": 8, "ed": 10, "text": "empirical loss"}, {"st": 43, "ed": 45, "text": "approximation error"}, {"st": 48, "ed": 50, "text": "computational effort"}]
[{"st": 17, "ed": 19, "text": "mixture model"}, {"st": 20, "ed": 23, "text": "each data point"}, {"st": 28, "ed": 30, "text": "mixture components"}, {"st": 55, "ed": 58, "text": "gaussian mixture model"}, {"st": 83, "ed": 86, "text": "learning and inference"}, {"st": 106, "ed": 109, "text": "dirichlet process mixture"}, {"st": 110, "ed": 113, "text": "latent dirichlet allocation"}, {"st": 122, "ed": 124, "text": "topic modeling"}]
[{"st": 21, "ed": 23, "text": "class imbalance"}, {"st": 24, "ed": 26, "text": "cost sensitive"}, {"st": 27, "ed": 30, "text": "learning to rank"}, {"st": 53, "ed": 55, "text": "surrogate loss"}, {"st": 75, "ed": 78, "text": "a sufficient condition"}, {"st": 87, "ed": 89, "text": "surrogate loss"}, {"st": 97, "ed": 99, "text": "exponential loss"}, {"st": 100, "ed": 102, "text": "logistic loss"}, {"st": 107, "ed": 109, "text": "hinge loss"}, {"st": 117, "ed": 119, "text": "hinge loss"}, {"st": 121, "ed": 123, "text": "hinge loss"}, {"st": 135, "ed": 137, "text": "exponential loss"}, {"st": 138, "ed": 140, "text": "logistic loss"}, {"st": 147, "ed": 149, "text": "surrogate loss"}, {"st": 163, "ed": 165, "text": "surrogate loss"}, {"st": 169, "ed": 171, "text": "surrogate loss"}]
[{"st": 7, "ed": 9, "text": "conformal prediction"}, {"st": 18, "ed": 20, "text": "conformal prediction"}, {"st": 21, "ed": 23, "text": "cross validation"}]
[{"st": 10, "ed": 13, "text": "empirical risk minimization"}, {"st": 13, "ed": 15, "text": "learning algorithm"}, {"st": 20, "ed": 22, "text": "learning theory"}, {"st": 31, "ed": 33, "text": "error bounds"}, {"st": 56, "ed": 58, "text": "asymptotic analysis"}, {"st": 62, "ed": 64, "text": "generalization error"}, {"st": 72, "ed": 74, "text": "window function"}, {"st": 86, "ed": 88, "text": "least squares"}, {"st": 100, "ed": 102, "text": "least squares"}]
[{"st": 20, "ed": 23, "text": "multivariate normal distribution"}, {"st": 76, "ed": 78, "text": "optimal design"}, {"st": 96, "ed": 98, "text": "optimal solution"}]
[{"st": 6, "ed": 8, "text": "optimal control"}, {"st": 13, "ed": 15, "text": "path integral"}, {"st": 23, "ed": 25, "text": "sample based"}, {"st": 41, "ed": 43, "text": "approximate solution"}, {"st": 76, "ed": 78, "text": "sample based"}, {"st": 90, "ed": 92, "text": "numerical examples"}, {"st": 98, "ed": 100, "text": "sample efficiency"}]
[{"st": 2, "ed": 5, "text": "linear discriminant analysis"}, {"st": 24, "ed": 26, "text": "bayes optimal"}, {"st": 59, "ed": 61, "text": "training sample"}, {"st": 77, "ed": 79, "text": "generalization ability"}, {"st": 101, "ed": 103, "text": "random matrix"}, {"st": 156, "ed": 158, "text": "generalization ability"}, {"st": 186, "ed": 188, "text": "generalization error"}]
[{"st": 0, "ed": 2, "text": "structured prediction"}, {"st": 16, "ed": 18, "text": "predictive power"}, {"st": 21, "ed": 23, "text": "computational resources"}, {"st": 29, "ed": 31, "text": "output spaces"}, {"st": 39, "ed": 41, "text": "structured prediction"}, {"st": 47, "ed": 49, "text": "complex models"}, {"st": 78, "ed": 80, "text": "structured output"}, {"st": 89, "ed": 92, "text": "learning and inference"}, {"st": 106, "ed": 109, "text": "convex loss function"}, {"st": 126, "ed": 128, "text": "generalization bounds"}, {"st": 159, "ed": 161, "text": "large scale"}, {"st": 169, "ed": 171, "text": "handwriting recognition"}, {"st": 178, "ed": 180, "text": "structured prediction"}]
[{"st": 3, "ed": 5, "text": "metric learning"}, {"st": 6, "ed": 8, "text": "significantly improved"}, {"st": 14, "ed": 17, "text": "k nearest neighbor"}, {"st": 18, "ed": 21, "text": "support vector machines"}, {"st": 31, "ed": 33, "text": "classification algorithms"}, {"st": 35, "ed": 37, "text": "distance metrics"}, {"st": 44, "ed": 46, "text": "empirical analysis"}, {"st": 56, "ed": 58, "text": "metric learning"}, {"st": 60, "ed": 62, "text": "pre processing"}, {"st": 89, "ed": 91, "text": "support vector"}, {"st": 91, "ed": 93, "text": "metric learning"}, {"st": 122, "ed": 125, "text": "benchmark data sets"}, {"st": 141, "ed": 143, "text": "metric learning"}]
[{"st": 5, "ed": 7, "text": "linear optimization"}, {"st": 15, "ed": 17, "text": "worst case"}, {"st": 45, "ed": 47, "text": "worst case"}, {"st": 54, "ed": 56, "text": "worst case"}, {"st": 56, "ed": 58, "text": "regret bounds"}, {"st": 74, "ed": 76, "text": "prior knowledge"}, {"st": 107, "ed": 109, "text": "online learning"}, {"st": 172, "ed": 174, "text": "potential applications"}, {"st": 178, "ed": 180, "text": "time series"}]
[{"st": 7, "ed": 9, "text": "recent advances"}, {"st": 17, "ed": 19, "text": "clustering results"}, {"st": 30, "ed": 32, "text": "clustering algorithm"}, {"st": 72, "ed": 74, "text": "current approaches"}, {"st": 128, "ed": 130, "text": "semi supervised"}, {"st": 172, "ed": 175, "text": "semi supervised clustering"}]
[{"st": 7, "ed": 10, "text": "change point detection"}, {"st": 26, "ed": 29, "text": "change point detection"}, {"st": 50, "ed": 52, "text": "change point"}, {"st": 63, "ed": 65, "text": "observed data"}, {"st": 69, "ed": 71, "text": "proposed method"}, {"st": 89, "ed": 91, "text": "low dimensional"}, {"st": 99, "ed": 101, "text": "streaming data"}, {"st": 144, "ed": 146, "text": "recent results"}, {"st": 158, "ed": 160, "text": "missing data"}, {"st": 164, "ed": 166, "text": "point clouds"}, {"st": 166, "ed": 168, "text": "online optimization"}, {"st": 169, "ed": 172, "text": "change point detection"}, {"st": 184, "ed": 186, "text": "proposed approach"}]
[{"st": 1, "ed": 3, "text": "statistical model"}, {"st": 27, "ed": 29, "text": "fisher information"}, {"st": 42, "ed": 44, "text": "statistical models"}, {"st": 46, "ed": 48, "text": "free energy"}, {"st": 57, "ed": 59, "text": "marginal likelihood"}, {"st": 86, "ed": 88, "text": "free energy"}, {"st": 124, "ed": 126, "text": "statistical models"}, {"st": 144, "ed": 146, "text": "free energy"}, {"st": 148, "ed": 150, "text": "training samples"}, {"st": 175, "ed": 177, "text": "log likelihood"}, {"st": 180, "ed": 182, "text": "posterior distribution"}, {"st": 205, "ed": 207, "text": "asymptotic expansion"}, {"st": 210, "ed": 212, "text": "free energy"}, {"st": 215, "ed": 217, "text": "statistical model"}, {"st": 237, "ed": 239, "text": "true distribution"}]
[{"st": 12, "ed": 14, "text": "machine learning"}, {"st": 22, "ed": 24, "text": "training examples"}, {"st": 63, "ed": 65, "text": "training examples"}, {"st": 67, "ed": 69, "text": "training examples"}, {"st": 71, "ed": 73, "text": "fast convergence"}]
[{"st": 13, "ed": 15, "text": "missing entries"}, {"st": 25, "ed": 27, "text": "machine learning"}, {"st": 41, "ed": 43, "text": "generative model"}, {"st": 44, "ed": 47, "text": "mixture of gaussians"}, {"st": 50, "ed": 52, "text": "conditional expectation"}, {"st": 63, "ed": 65, "text": "gaussian mixture"}, {"st": 70, "ed": 72, "text": "missing values"}, {"st": 80, "ed": 82, "text": "spanning tree"}, {"st": 104, "ed": 106, "text": "generative model"}, {"st": 110, "ed": 112, "text": "missing values"}]
[{"st": 5, "ed": 7, "text": "sparse coding"}, {"st": 34, "ed": 36, "text": "sparse linear"}, {"st": 47, "ed": 49, "text": "infinite dimensional"}, {"st": 64, "ed": 66, "text": "transfer learning"}, {"st": 79, "ed": 81, "text": "generalization error"}, {"st": 87, "ed": 89, "text": "numerical experiments"}, {"st": 94, "ed": 96, "text": "real datasets"}, {"st": 122, "ed": 124, "text": "learning task"}]
[{"st": 10, "ed": 12, "text": "optimal transport"}, {"st": 33, "ed": 35, "text": "optimal transport"}, {"st": 39, "ed": 41, "text": "learning theory"}, {"st": 54, "ed": 56, "text": "unsupervised learning"}, {"st": 56, "ed": 58, "text": "k means"}, {"st": 89, "ed": 91, "text": "convergence rate"}, {"st": 99, "ed": 101, "text": "unlike existing"}]
[{"st": 0, "ed": 2, "text": "metric learning"}, {"st": 14, "ed": 16, "text": "generalization ability"}, {"st": 49, "ed": 51, "text": "generalization bounds"}, {"st": 69, "ed": 72, "text": "sufficient condition for"}, {"st": 73, "ed": 75, "text": "metric learning"}, {"st": 84, "ed": 86, "text": "proposed framework"}, {"st": 96, "ed": 98, "text": "metric learning"}]
[{"st": 23, "ed": 25, "text": "k means"}, {"st": 34, "ed": 36, "text": "previous results"}, {"st": 37, "ed": 39, "text": "k means"}, {"st": 49, "ed": 51, "text": "k means"}, {"st": 61, "ed": 63, "text": "higher order"}, {"st": 78, "ed": 80, "text": "k means"}]
[{"st": 27, "ed": 29, "text": "multiple classes"}, {"st": 86, "ed": 88, "text": "convex analysis"}]
[{"st": 2, "ed": 4, "text": "functional neuroimaging"}, {"st": 32, "ed": 34, "text": "machine learning"}, {"st": 35, "ed": 37, "text": "transfer learning"}]
[{"st": 4, "ed": 6, "text": "pair wise"}, {"st": 89, "ed": 91, "text": "rank aggregation"}, {"st": 100, "ed": 102, "text": "pair wise"}, {"st": 108, "ed": 110, "text": "random walk"}, {"st": 173, "ed": 175, "text": "pair wise"}, {"st": 190, "ed": 192, "text": "pair wise"}, {"st": 199, "ed": 201, "text": "pair wise"}, {"st": 201, "ed": 203, "text": "marginal probabilities"}, {"st": 223, "ed": 225, "text": "finite sample"}, {"st": 225, "ed": 227, "text": "error rates"}, {"st": 286, "ed": 288, "text": "randomly chosen"}]
[{"st": 2, "ed": 6, "text": "multi armed bandit problem"}, {"st": 11, "ed": 13, "text": "reward distributions"}, {"st": 22, "ed": 24, "text": "bandit problem"}, {"st": 56, "ed": 58, "text": "regret bounds"}]
[{"st": 63, "ed": 66, "text": "mild cognitive impairment"}, {"st": 130, "ed": 133, "text": "sufficient condition for"}, {"st": 177, "ed": 181, "text": "synthetic and real data"}, {"st": 185, "ed": 188, "text": "effectiveness and efficiency"}]
[{"st": 7, "ed": 9, "text": "convergence rate"}, {"st": 65, "ed": 67, "text": "near optimal"}, {"st": 71, "ed": 73, "text": "strongly convex"}, {"st": 86, "ed": 89, "text": "boolean valued function"}, {"st": 133, "ed": 136, "text": "boolean valued function"}, {"st": 138, "ed": 140, "text": "convergence rate"}]
[{"st": 4, "ed": 6, "text": "modus operandi"}, {"st": 7, "ed": 10, "text": "multi task learning"}, {"st": 119, "ed": 121, "text": "worst case"}, {"st": 142, "ed": 145, "text": "synthetic and real"}]
[{"st": 14, "ed": 16, "text": "low dimensional"}, {"st": 18, "ed": 20, "text": "joint inference"}, {"st": 46, "ed": 48, "text": "method called"}, {"st": 48, "ed": 52, "text": "canonical correlation analysis cca"}, {"st": 56, "ed": 59, "text": "canonical correlation analysis"}, {"st": 88, "ed": 90, "text": "document classification"}]
[{"st": 7, "ed": 9, "text": "social network"}, {"st": 53, "ed": 55, "text": "unseen data"}, {"st": 58, "ed": 60, "text": "efficient algorithms"}, {"st": 68, "ed": 70, "text": "ranking loss"}, {"st": 78, "ed": 80, "text": "ranking loss"}, {"st": 108, "ed": 114, "text": "experiments on synthetic and real world"}, {"st": 129, "ed": 131, "text": "predictive power"}]
[{"st": 0, "ed": 2, "text": "super resolution"}, {"st": 8, "ed": 10, "text": "low resolution"}, {"st": 68, "ed": 70, "text": "natural images"}, {"st": 81, "ed": 83, "text": "large scale"}, {"st": 89, "ed": 91, "text": "visual quality"}, {"st": 100, "ed": 102, "text": "gibbs sampling"}, {"st": 113, "ed": 115, "text": "large scale"}, {"st": 124, "ed": 126, "text": "variational bayes"}]
[{"st": 90, "ed": 92, "text": "time consuming"}, {"st": 94, "ed": 96, "text": "error prone"}, {"st": 103, "ed": 105, "text": "linear programming"}, {"st": 105, "ed": 108, "text": "support vector machines"}, {"st": 113, "ed": 116, "text": "multiple kernel learning"}, {"st": 127, "ed": 129, "text": "feature spaces"}, {"st": 142, "ed": 144, "text": "feature spaces"}, {"st": 147, "ed": 149, "text": "feature spaces"}, {"st": 157, "ed": 159, "text": "decision boundary"}, {"st": 166, "ed": 168, "text": "spatio temporal"}, {"st": 178, "ed": 180, "text": "feature spaces"}]
[{"st": 1, "ed": 3, "text": "natural language"}, {"st": 65, "ed": 67, "text": "supervised learning"}, {"st": 69, "ed": 71, "text": "unlabeled data"}, {"st": 76, "ed": 78, "text": "labeled data"}, {"st": 86, "ed": 88, "text": "new york"}, {"st": 117, "ed": 119, "text": "supervised learning"}, {"st": 142, "ed": 144, "text": "low dimensional"}, {"st": 144, "ed": 146, "text": "hidden state"}, {"st": 156, "ed": 159, "text": "hidden markov model"}, {"st": 161, "ed": 164, "text": "latent dirichlet allocation"}]
[{"st": 3, "ed": 6, "text": "markov random field"}, {"st": 24, "ed": 26, "text": "prediction error"}, {"st": 39, "ed": 41, "text": "np hard"}, {"st": 48, "ed": 51, "text": "markov random fields"}, {"st": 58, "ed": 61, "text": "semi supervised learning"}, {"st": 70, "ed": 72, "text": "approximation algorithm"}, {"st": 83, "ed": 85, "text": "message passing"}, {"st": 89, "ed": 92, "text": "markov random fields"}]
[{"st": 12, "ed": 14, "text": "binary data"}, {"st": 34, "ed": 36, "text": "binary data"}, {"st": 63, "ed": 65, "text": "ising model"}, {"st": 73, "ed": 75, "text": "binary data"}, {"st": 105, "ed": 108, "text": "exploratory data analysis"}, {"st": 146, "ed": 148, "text": "simulated data"}]
[{"st": 3, "ed": 7, "text": "markov random fields mrfs"}, {"st": 8, "ed": 10, "text": "np hard"}, {"st": 14, "ed": 17, "text": "a posteriori map"}, {"st": 23, "ed": 25, "text": "cost functions"}, {"st": 50, "ed": 53, "text": "bethe free energy"}, {"st": 63, "ed": 65, "text": "stationary points"}, {"st": 68, "ed": 70, "text": "technique called"}, {"st": 101, "ed": 103, "text": "global optimization"}]
[{"st": 5, "ed": 7, "text": "temporal difference"}, {"st": 7, "ed": 9, "text": "policy evaluation"}, {"st": 26, "ed": 28, "text": "risk management"}, {"st": 49, "ed": 51, "text": "linear function"}, {"st": 60, "ed": 62, "text": "a 4"}, {"st": 63, "ed": 65, "text": "continuous state"}]
[{"st": 13, "ed": 15, "text": "semi supervised"}, {"st": 15, "ed": 17, "text": "domain adaptation"}, {"st": 29, "ed": 31, "text": "marginal distributions"}, {"st": 82, "ed": 85, "text": "real world data"}, {"st": 90, "ed": 92, "text": "proposed approach"}]
[{"st": 23, "ed": 25, "text": "worst case"}, {"st": 31, "ed": 33, "text": "worst case"}, {"st": 83, "ed": 85, "text": "learning rate"}, {"st": 106, "ed": 108, "text": "worst case"}, {"st": 121, "ed": 123, "text": "constant factor"}, {"st": 127, "ed": 129, "text": "without sacrificing"}, {"st": 131, "ed": 133, "text": "worst case"}]
[{"st": 5, "ed": 8, "text": "independent component analysis"}, {"st": 64, "ed": 66, "text": "mutual information"}, {"st": 88, "ed": 91, "text": "kernel density estimation"}, {"st": 100, "ed": 102, "text": "component analysis"}]
[{"st": 16, "ed": 19, "text": "labeled and unlabeled"}, {"st": 47, "ed": 49, "text": "fixed point"}, {"st": 74, "ed": 76, "text": "fixed points"}, {"st": 131, "ed": 133, "text": "classification tasks"}, {"st": 134, "ed": 137, "text": "labeled and unlabeled"}, {"st": 147, "ed": 149, "text": "min max"}, {"st": 151, "ed": 153, "text": "dna sequence"}]
[{"st": 8, "ed": 10, "text": "clustering quality"}, {"st": 24, "ed": 26, "text": "clustering algorithm"}, {"st": 34, "ed": 36, "text": "ground truth"}, {"st": 118, "ed": 121, "text": "number of clusters"}, {"st": 127, "ed": 129, "text": "mutual information"}, {"st": 164, "ed": 166, "text": "class labels"}, {"st": 169, "ed": 172, "text": "encoder and decoder"}, {"st": 185, "ed": 187, "text": "conditional probabilities"}, {"st": 189, "ed": 191, "text": "class labels"}, {"st": 210, "ed": 212, "text": "class labels"}]
[{"st": 37, "ed": 39, "text": "hidden variables"}, {"st": 104, "ed": 106, "text": "hierarchical latent"}, {"st": 136, "ed": 138, "text": "demonstrate empirically"}]
[{"st": 17, "ed": 19, "text": "generalization error"}, {"st": 32, "ed": 34, "text": "learning algorithm"}, {"st": 84, "ed": 86, "text": "uniform convergence"}, {"st": 87, "ed": 89, "text": "generalization error"}, {"st": 111, "ed": 113, "text": "learning algorithms"}, {"st": 125, "ed": 127, "text": "generalization error"}, {"st": 135, "ed": 137, "text": "learning algorithms"}]
[{"st": 31, "ed": 33, "text": "finite mixture"}, {"st": 37, "ed": 39, "text": "expectation maximization"}]
[{"st": 26, "ed": 28, "text": "naive bayes"}, {"st": 67, "ed": 69, "text": "closed form"}, {"st": 95, "ed": 97, "text": "discriminative power"}, {"st": 122, "ed": 124, "text": "naive bayes"}, {"st": 144, "ed": 146, "text": "sample based"}, {"st": 146, "ed": 148, "text": "discriminative power"}]
[{"st": 9, "ed": 11, "text": "classification rules"}, {"st": 17, "ed": 19, "text": "highly accurate"}, {"st": 28, "ed": 30, "text": "boosting algorithm"}, {"st": 103, "ed": 105, "text": "prior knowledge"}, {"st": 121, "ed": 123, "text": "unlabeled examples"}]
[{"st": 0, "ed": 2, "text": "reinforcement learning"}, {"st": 7, "ed": 9, "text": "real world"}, {"st": 11, "ed": 13, "text": "domain knowledge"}, {"st": 26, "ed": 28, "text": "hidden state"}, {"st": 31, "ed": 35, "text": "partially observable markov decision"}, {"st": 88, "ed": 90, "text": "framework called"}, {"st": 93, "ed": 96, "text": "markov decision process"}, {"st": 133, "ed": 135, "text": "reinforcement learning"}, {"st": 166, "ed": 168, "text": "domain knowledge"}]
[{"st": 0, "ed": 2, "text": "active learning"}, {"st": 16, "ed": 18, "text": "active learning"}]
[{"st": 3, "ed": 6, "text": "log partition function"}, {"st": 14, "ed": 16, "text": "approximate inference"}, {"st": 18, "ed": 20, "text": "decision theory"}, {"st": 34, "ed": 37, "text": "log partition function"}, {"st": 58, "ed": 60, "text": "special case"}, {"st": 77, "ed": 80, "text": "bethe free energy"}, {"st": 85, "ed": 87, "text": "desirable properties"}, {"st": 95, "ed": 97, "text": "global minimum"}, {"st": 100, "ed": 102, "text": "global minimum"}, {"st": 112, "ed": 114, "text": "global minimum"}, {"st": 124, "ed": 126, "text": "fixed points"}, {"st": 127, "ed": 129, "text": "belief propagation"}, {"st": 140, "ed": 142, "text": "fixed points"}]
[{"st": 45, "ed": 47, "text": "low cost"}, {"st": 65, "ed": 67, "text": "countable set"}, {"st": 87, "ed": 89, "text": "low cost"}, {"st": 114, "ed": 116, "text": "non trivial"}, {"st": 136, "ed": 138, "text": "statistical physics"}, {"st": 152, "ed": 154, "text": "closed form"}, {"st": 163, "ed": 165, "text": "real data"}]
[{"st": 5, "ed": 8, "text": "online convex optimization"}, {"st": 21, "ed": 23, "text": "regret bounds"}, {"st": 39, "ed": 41, "text": "online optimization"}, {"st": 63, "ed": 65, "text": "regret bounds"}, {"st": 76, "ed": 78, "text": "practical scenarios"}]
[{"st": 79, "ed": 82, "text": "multi armed bandits"}, {"st": 93, "ed": 95, "text": "cumulative reward"}, {"st": 110, "ed": 112, "text": "linear bandits"}, {"st": 118, "ed": 120, "text": "low dimensional"}, {"st": 145, "ed": 147, "text": "near optimal"}]
[{"st": 0, "ed": 2, "text": "bayesian optimization"}, {"st": 5, "ed": 7, "text": "successfully applied"}, {"st": 15, "ed": 17, "text": "user interfaces"}, {"st": 82, "ed": 84, "text": "invariance properties"}, {"st": 98, "ed": 100, "text": "theoretical analysis"}, {"st": 102, "ed": 104, "text": "empirical results"}, {"st": 117, "ed": 119, "text": "intrinsic dimensionality"}, {"st": 141, "ed": 143, "text": "mixed integer"}, {"st": 143, "ed": 145, "text": "linear programming"}]
[{"st": 24, "ed": 26, "text": "variational approximation"}, {"st": 27, "ed": 29, "text": "expectation propagation"}]
[{"st": 21, "ed": 23, "text": "previously unseen"}, {"st": 26, "ed": 28, "text": "domain invariant"}, {"st": 28, "ed": 30, "text": "component analysis"}, {"st": 32, "ed": 34, "text": "kernel based"}, {"st": 34, "ed": 36, "text": "optimization algorithm"}, {"st": 53, "ed": 56, "text": "input and output"}, {"st": 60, "ed": 62, "text": "analysis shows"}, {"st": 68, "ed": 70, "text": "generalization ability"}, {"st": 82, "ed": 87, "text": "synthetic and real world datasets"}, {"st": 92, "ed": 94, "text": "invariant features"}, {"st": 96, "ed": 98, "text": "classifier performance"}]
[{"st": 60, "ed": 62, "text": "heavy tailed"}]
[{"st": 6, "ed": 8, "text": "supervised classification"}, {"st": 33, "ed": 35, "text": "marginal likelihood"}, {"st": 41, "ed": 43, "text": "marginal likelihood"}, {"st": 53, "ed": 55, "text": "marginal likelihood"}, {"st": 82, "ed": 84, "text": "conditional distributions"}, {"st": 100, "ed": 102, "text": "marginal likelihood"}, {"st": 115, "ed": 119, "text": "number of model parameters"}]
[]
[{"st": 0, "ed": 2, "text": "dag models"}, {"st": 3, "ed": 5, "text": "hidden variables"}, {"st": 21, "ed": 23, "text": "dag models"}, {"st": 58, "ed": 60, "text": "latent variable"}, {"st": 67, "ed": 69, "text": "observed variables"}, {"st": 84, "ed": 87, "text": "singular value decomposition"}, {"st": 114, "ed": 116, "text": "latent variable"}]
[{"st": 1, "ed": 3, "text": "operator valued"}, {"st": 6, "ed": 8, "text": "received increasing"}, {"st": 11, "ed": 13, "text": "machine learning"}, {"st": 14, "ed": 16, "text": "functional data"}, {"st": 20, "ed": 23, "text": "multi task learning"}, {"st": 49, "ed": 51, "text": "operator valued"}, {"st": 52, "ed": 54, "text": "feature space"}, {"st": 66, "ed": 68, "text": "least squares"}, {"st": 90, "ed": 92, "text": "proposed method"}]
[{"st": 42, "ed": 45, "text": "discrete and continuous"}, {"st": 56, "ed": 60, "text": "reproducing kernel hilbert spaces"}, {"st": 64, "ed": 66, "text": "operator valued"}]
[{"st": 0, "ed": 2, "text": "matrix approximation"}, {"st": 7, "ed": 9, "text": "machine learning"}, {"st": 11, "ed": 13, "text": "accurate prediction"}, {"st": 15, "ed": 17, "text": "recommendation systems"}, {"st": 17, "ed": 19, "text": "text mining"}, {"st": 32, "ed": 34, "text": "partially observed"}, {"st": 43, "ed": 45, "text": "matrix approximation"}, {"st": 57, "ed": 59, "text": "low rank"}, {"st": 69, "ed": 71, "text": "weighted sum"}, {"st": 72, "ed": 74, "text": "low rank"}, {"st": 83, "ed": 85, "text": "low rank"}, {"st": 91, "ed": 93, "text": "prediction accuracy"}]
[{"st": 11, "ed": 13, "text": "prior methods"}, {"st": 53, "ed": 55, "text": "training procedure"}, {"st": 69, "ed": 71, "text": "generative model"}, {"st": 75, "ed": 77, "text": "variational approximation"}, {"st": 86, "ed": 88, "text": "recurrent networks"}, {"st": 117, "ed": 119, "text": "outperforms previous"}, {"st": 125, "ed": 127, "text": "approximate inference"}]
[{"st": 1, "ed": 4, "text": "support vector machine"}, {"st": 19, "ed": 21, "text": "pattern recognition"}, {"st": 31, "ed": 33, "text": "linear combination"}, {"st": 34, "ed": 36, "text": "kernel functions"}, {"st": 42, "ed": 44, "text": "training data"}, {"st": 86, "ed": 88, "text": "probabilistic model"}, {"st": 89, "ed": 91, "text": "functional form"}, {"st": 97, "ed": 99, "text": "achieves comparable"}, {"st": 99, "ed": 101, "text": "recognition accuracy"}, {"st": 128, "ed": 130, "text": "type ii"}, {"st": 130, "ed": 132, "text": "maximum likelihood"}, {"st": 168, "ed": 170, "text": "variational inference"}, {"st": 173, "ed": 175, "text": "posterior distribution"}, {"st": 192, "ed": 196, "text": "synthetic and real world"}]
[{"st": 0, "ed": 2, "text": "feature selection"}, {"st": 19, "ed": 21, "text": "classification performance"}, {"st": 72, "ed": 74, "text": "statistical analysis"}, {"st": 134, "ed": 136, "text": "worst case"}]
[{"st": 0, "ed": 2, "text": "recent theoretical"}, {"st": 5, "ed": 7, "text": "random projection"}, {"st": 10, "ed": 12, "text": "dimensionality reduction"}, {"st": 28, "ed": 30, "text": "wide variety"}]
[{"st": 7, "ed": 9, "text": "bayesian network"}, {"st": 61, "ed": 63, "text": "prediction tasks"}, {"st": 70, "ed": 72, "text": "predictive distributions"}, {"st": 77, "ed": 80, "text": "joint probability distribution"}, {"st": 103, "ed": 105, "text": "marginal likelihood"}, {"st": 138, "ed": 140, "text": "marginal likelihood"}, {"st": 180, "ed": 182, "text": "marginal likelihood"}, {"st": 197, "ed": 199, "text": "marginal likelihood"}]
[{"st": 31, "ed": 33, "text": "local optimum"}, {"st": 36, "ed": 38, "text": "objective functions"}, {"st": 68, "ed": 71, "text": "minimum message length"}, {"st": 78, "ed": 80, "text": "k means"}, {"st": 109, "ed": 113, "text": "markov chain monte carlo"}, {"st": 120, "ed": 122, "text": "gibbs sampling"}, {"st": 139, "ed": 141, "text": "multi modal"}]
[{"st": 1, "ed": 3, "text": "feature selection"}, {"st": 20, "ed": 22, "text": "feature selection"}, {"st": 29, "ed": 31, "text": "classification regression"}, {"st": 33, "ed": 35, "text": "feature selection"}, {"st": 43, "ed": 45, "text": "recently proposed"}, {"st": 45, "ed": 47, "text": "maximum entropy"}, {"st": 62, "ed": 64, "text": "support vector"}, {"st": 64, "ed": 66, "text": "classification regression"}, {"st": 67, "ed": 69, "text": "exponential family"}, {"st": 77, "ed": 79, "text": "feature selection"}, {"st": 83, "ed": 85, "text": "linear classification"}, {"st": 91, "ed": 93, "text": "proposed approach"}, {"st": 95, "ed": 97, "text": "substantial improvements"}, {"st": 107, "ed": 109, "text": "feature selection"}, {"st": 119, "ed": 122, "text": "degrees of freedom"}]
[{"st": 7, "ed": 9, "text": "reduction techniques"}, {"st": 12, "ed": 15, "text": "k nearest neighbor"}, {"st": 48, "ed": 50, "text": "np hard"}, {"st": 95, "ed": 97, "text": "selection algorithms"}, {"st": 121, "ed": 123, "text": "nearest neighbor"}, {"st": 128, "ed": 130, "text": "recent results"}, {"st": 170, "ed": 172, "text": "statistical analysis"}]
[{"st": 4, "ed": 6, "text": "likelihood function"}, {"st": 16, "ed": 18, "text": "likelihood function"}, {"st": 28, "ed": 30, "text": "probabilistic framework"}, {"st": 36, "ed": 38, "text": "cross entropy"}, {"st": 56, "ed": 59, "text": "support vector machine"}, {"st": 65, "ed": 67, "text": "maximum margin"}]
[{"st": 1, "ed": 3, "text": "policy gradient"}, {"st": 9, "ed": 11, "text": "reinforcement learning"}, {"st": 35, "ed": 37, "text": "policy gradient"}, {"st": 54, "ed": 56, "text": "highly effective"}, {"st": 56, "ed": 58, "text": "policy gradient"}, {"st": 61, "ed": 63, "text": "policy gradients"}, {"st": 70, "ed": 72, "text": "recently proposed"}, {"st": 72, "ed": 74, "text": "policy search"}, {"st": 76, "ed": 78, "text": "low variance"}, {"st": 83, "ed": 85, "text": "importance sampling"}, {"st": 117, "ed": 119, "text": "proposed method"}, {"st": 121, "ed": 123, "text": "theoretical analysis"}]
[{"st": 1, "ed": 3, "text": "relation extraction"}, {"st": 12, "ed": 14, "text": "machine learning"}, {"st": 20, "ed": 22, "text": "manual annotation"}, {"st": 27, "ed": 29, "text": "distant supervision"}, {"st": 89, "ed": 91, "text": "structured data"}, {"st": 111, "ed": 113, "text": "matrix factorization"}, {"st": 128, "ed": 130, "text": "higher accuracy"}, {"st": 162, "ed": 164, "text": "structured data"}, {"st": 172, "ed": 174, "text": "approach outperforms"}, {"st": 178, "ed": 180, "text": "distant supervision"}]
[{"st": 6, "ed": 8, "text": "hidden layer"}, {"st": 10, "ed": 12, "text": "multilayer perceptron"}]
[{"st": 5, "ed": 7, "text": "active learning"}, {"st": 23, "ed": 25, "text": "recent results"}, {"st": 53, "ed": 55, "text": "selection algorithm"}, {"st": 73, "ed": 76, "text": "simple and efficient"}, {"st": 85, "ed": 87, "text": "selection algorithm"}, {"st": 134, "ed": 136, "text": "active learning"}, {"st": 161, "ed": 163, "text": "active learning"}]
[{"st": 12, "ed": 14, "text": "least squares"}, {"st": 36, "ed": 38, "text": "operator theory"}, {"st": 84, "ed": 86, "text": "optimization problem"}]
[{"st": 0, "ed": 2, "text": "efficient online"}, {"st": 5, "ed": 7, "text": "loss functions"}, {"st": 13, "ed": 15, "text": "large scale"}, {"st": 34, "ed": 36, "text": "generalization performance"}, {"st": 37, "ed": 40, "text": "online learning algorithms"}, {"st": 52, "ed": 54, "text": "generalization bounds"}, {"st": 55, "ed": 57, "text": "online algorithms"}, {"st": 64, "ed": 66, "text": "directly applied"}, {"st": 136, "ed": 138, "text": "online algorithms"}, {"st": 144, "ed": 146, "text": "natural extension"}, {"st": 168, "ed": 170, "text": "online algorithm"}]
[{"st": 0, "ed": 3, "text": "neural language models"}, {"st": 6, "ed": 9, "text": "recurrent neural networks"}, {"st": 18, "ed": 20, "text": "character level"}, {"st": 72, "ed": 74, "text": "training data"}, {"st": 119, "ed": 121, "text": "rnn model"}, {"st": 134, "ed": 136, "text": "impulse response"}, {"st": 146, "ed": 148, "text": "learning rates"}, {"st": 175, "ed": 177, "text": "large datasets"}, {"st": 180, "ed": 182, "text": "regularization methods"}, {"st": 215, "ed": 217, "text": "microsoft research"}, {"st": 274, "ed": 276, "text": "internal representations"}]
[{"st": 0, "ed": 2, "text": "current methods"}, {"st": 7, "ed": 9, "text": "latent variables"}, {"st": 28, "ed": 30, "text": "generalization performance"}, {"st": 39, "ed": 41, "text": "posterior distributions"}, {"st": 55, "ed": 57, "text": "latent variables"}, {"st": 75, "ed": 77, "text": "variational bayes"}, {"st": 89, "ed": 91, "text": "posterior distributions"}, {"st": 99, "ed": 101, "text": "latent variables"}, {"st": 105, "ed": 107, "text": "without resorting"}, {"st": 134, "ed": 137, "text": "expectation maximization algorithm"}, {"st": 158, "ed": 160, "text": "domains including"}, {"st": 160, "ed": 162, "text": "unsupervised clustering"}]
[{"st": 49, "ed": 51, "text": "negative log"}, {"st": 63, "ed": 65, "text": "off line"}, {"st": 95, "ed": 97, "text": "off line"}, {"st": 100, "ed": 102, "text": "loss bounds"}]
[{"st": 5, "ed": 7, "text": "significantly improve"}, {"st": 13, "ed": 15, "text": "machine learning"}, {"st": 50, "ed": 52, "text": "learning algorithms"}, {"st": 57, "ed": 59, "text": "models including"}, {"st": 59, "ed": 61, "text": "decision trees"}, {"st": 66, "ed": 68, "text": "naive bayes"}, {"st": 84, "ed": 86, "text": "em algorithm"}, {"st": 92, "ed": 94, "text": "naive bayes"}, {"st": 97, "ed": 100, "text": "real world data"}]
[{"st": 10, "ed": 12, "text": "dag models"}, {"st": 41, "ed": 43, "text": "wishart distribution"}, {"st": 52, "ed": 54, "text": "positive definite"}, {"st": 54, "ed": 56, "text": "symmetric matrix"}, {"st": 57, "ed": 59, "text": "random variables"}, {"st": 72, "ed": 74, "text": "wishart distribution"}]
[{"st": 7, "ed": 9, "text": "bayesian network"}, {"st": 61, "ed": 63, "text": "prediction tasks"}, {"st": 70, "ed": 72, "text": "predictive distributions"}, {"st": 77, "ed": 80, "text": "joint probability distribution"}, {"st": 103, "ed": 105, "text": "marginal likelihood"}, {"st": 138, "ed": 140, "text": "marginal likelihood"}, {"st": 180, "ed": 182, "text": "marginal likelihood"}, {"st": 197, "ed": 199, "text": "marginal likelihood"}]
[{"st": 23, "ed": 25, "text": "probabilistic models"}]
[{"st": 1, "ed": 3, "text": "real valued"}, {"st": 4, "ed": 6, "text": "time series"}, {"st": 47, "ed": 49, "text": "graphical model"}, {"st": 52, "ed": 55, "text": "hidden markov model"}, {"st": 68, "ed": 71, "text": "discrete and continuous"}, {"st": 84, "ed": 86, "text": "exact inference"}, {"st": 97, "ed": 99, "text": "variational inference"}, {"st": 111, "ed": 114, "text": "discrete and continuous"}]
[{"st": 14, "ed": 16, "text": "fast approximate"}, {"st": 57, "ed": 59, "text": "recent developments"}, {"st": 60, "ed": 62, "text": "approximate bayesian"}, {"st": 114, "ed": 116, "text": "theoretical bounds"}]
[{"st": 7, "ed": 10, "text": "support vector machines"}, {"st": 18, "ed": 20, "text": "loss function"}, {"st": 35, "ed": 37, "text": "finite sample"}]
[{"st": 18, "ed": 20, "text": "training set"}, {"st": 24, "ed": 26, "text": "object classification"}, {"st": 45, "ed": 48, "text": "support vector machine"}]
[{"st": 16, "ed": 19, "text": "undirected graphical models"}, {"st": 21, "ed": 23, "text": "hidden variables"}, {"st": 25, "ed": 27, "text": "exponential families"}, {"st": 33, "ed": 35, "text": "chain graphs"}, {"st": 37, "ed": 39, "text": "hidden variables"}, {"st": 50, "ed": 52, "text": "exponential families"}, {"st": 57, "ed": 59, "text": "hidden variables"}, {"st": 61, "ed": 63, "text": "exponential families"}, {"st": 84, "ed": 86, "text": "automatically generate"}]
[{"st": 10, "ed": 12, "text": "statistical inference"}, {"st": 16, "ed": 19, "text": "minimum description length"}, {"st": 23, "ed": 26, "text": "minimum message length"}, {"st": 65, "ed": 67, "text": "empirical results"}, {"st": 75, "ed": 77, "text": "approach yields"}, {"st": 78, "ed": 80, "text": "accurate predictions"}, {"st": 85, "ed": 87, "text": "empirical results"}]
[{"st": 12, "ed": 14, "text": "exponential family"}, {"st": 17, "ed": 19, "text": "generalized linear"}, {"st": 45, "ed": 47, "text": "exponential family"}, {"st": 86, "ed": 88, "text": "probability density"}, {"st": 150, "ed": 152, "text": "likelihood based"}, {"st": 168, "ed": 170, "text": "sample size"}, {"st": 180, "ed": 183, "text": "mean square error"}]
[{"st": 4, "ed": 6, "text": "belief networks"}, {"st": 8, "ed": 10, "text": "random variables"}, {"st": 13, "ed": 15, "text": "conditional probabilities"}, {"st": 26, "ed": 28, "text": "large networks"}, {"st": 30, "ed": 32, "text": "probabilistic inference"}, {"st": 61, "ed": 63, "text": "marginal probabilities"}, {"st": 69, "ed": 72, "text": "rates of convergence"}, {"st": 91, "ed": 93, "text": "transfer function"}, {"st": 96, "ed": 98, "text": "conditional probability"}]
[{"st": 3, "ed": 6, "text": "undirected graphical models"}, {"st": 29, "ed": 31, "text": "widely studied"}, {"st": 58, "ed": 60, "text": "mean field"}, {"st": 72, "ed": 74, "text": "learning algorithm"}, {"st": 94, "ed": 96, "text": "mean field"}, {"st": 114, "ed": 116, "text": "variational methods"}, {"st": 122, "ed": 124, "text": "multi modal"}, {"st": 133, "ed": 136, "text": "inference and learning"}]
[{"st": 23, "ed": 25, "text": "clustering algorithms"}, {"st": 26, "ed": 30, "text": "expectation maximization em algorithm"}, {"st": 37, "ed": 39, "text": "em algorithm"}, {"st": 42, "ed": 44, "text": "k means"}, {"st": 53, "ed": 55, "text": "naive bayes"}, {"st": 69, "ed": 72, "text": "real and synthetic"}, {"st": 77, "ed": 79, "text": "em algorithm"}, {"st": 79, "ed": 81, "text": "significantly outperforms"}, {"st": 96, "ed": 98, "text": "final solution"}, {"st": 121, "ed": 123, "text": "marginal distribution"}, {"st": 143, "ed": 145, "text": "learned models"}]
[]
[{"st": 2, "ed": 4, "text": "latent feature"}, {"st": 12, "ed": 14, "text": "grows exponentially"}, {"st": 19, "ed": 21, "text": "input data"}, {"st": 41, "ed": 43, "text": "map inference"}, {"st": 46, "ed": 48, "text": "latent feature"}, {"st": 60, "ed": 62, "text": "submodular function"}, {"st": 86, "ed": 88, "text": "submodular function"}, {"st": 94, "ed": 96, "text": "greedy algorithm"}, {"st": 111, "ed": 113, "text": "scales linearly"}, {"st": 118, "ed": 120, "text": "input data"}]
[{"st": 2, "ed": 6, "text": "prediction with expert advice"}, {"st": 16, "ed": 20, "text": "prediction with expert advice"}, {"st": 68, "ed": 72, "text": "prediction with expert advice"}, {"st": 74, "ed": 76, "text": "o sqrt"}]
[{"st": 7, "ed": 9, "text": "machine learning"}, {"st": 45, "ed": 47, "text": "large datasets"}, {"st": 56, "ed": 58, "text": "machine learning"}, {"st": 81, "ed": 83, "text": "unsupervised clustering"}, {"st": 87, "ed": 91, "text": "expectation maximization em algorithm"}, {"st": 95, "ed": 98, "text": "hidden markov model"}, {"st": 114, "ed": 116, "text": "training instances"}, {"st": 138, "ed": 141, "text": "dynamic time warping"}, {"st": 170, "ed": 172, "text": "training instances"}, {"st": 209, "ed": 211, "text": "classification accuracy"}, {"st": 219, "ed": 221, "text": "training set"}, {"st": 237, "ed": 239, "text": "proposed approach"}]
[{"st": 3, "ed": 5, "text": "learning problems"}, {"st": 13, "ed": 15, "text": "highly dependent"}, {"st": 51, "ed": 53, "text": "high dimensional"}, {"st": 87, "ed": 89, "text": "time series"}, {"st": 101, "ed": 103, "text": "time series"}, {"st": 114, "ed": 116, "text": "time series"}, {"st": 132, "ed": 134, "text": "shannon entropy"}, {"st": 149, "ed": 151, "text": "time series"}, {"st": 164, "ed": 166, "text": "optimal control"}]
[{"st": 13, "ed": 15, "text": "large datasets"}, {"st": 44, "ed": 46, "text": "rule based"}]
[{"st": 25, "ed": 27, "text": "iterative procedure"}, {"st": 63, "ed": 65, "text": "fixed size"}, {"st": 83, "ed": 85, "text": "gaussian process"}, {"st": 85, "ed": 88, "text": "upper confidence bound"}, {"st": 89, "ed": 91, "text": "pure exploration"}, {"st": 101, "ed": 103, "text": "pure exploration"}, {"st": 170, "ed": 173, "text": "real and synthetic"}]
[{"st": 3, "ed": 5, "text": "strongly convex"}, {"st": 18, "ed": 20, "text": "computationally expensive"}, {"st": 26, "ed": 29, "text": "stochastic gradient descent"}, {"st": 45, "ed": 48, "text": "stochastic gradient descent"}, {"st": 68, "ed": 70, "text": "objective function"}, {"st": 89, "ed": 91, "text": "strongly convex"}, {"st": 95, "ed": 97, "text": "total number"}, {"st": 112, "ed": 114, "text": "convergence rate"}, {"st": 132, "ed": 134, "text": "optimization problem"}, {"st": 159, "ed": 161, "text": "real world"}, {"st": 163, "ed": 165, "text": "empirical results"}]
[{"st": 18, "ed": 20, "text": "density function"}, {"st": 71, "ed": 73, "text": "importance sampling"}, {"st": 74, "ed": 76, "text": "statistical inference"}, {"st": 79, "ed": 81, "text": "closely related"}, {"st": 86, "ed": 88, "text": "covariate shift"}, {"st": 89, "ed": 91, "text": "transfer learning"}, {"st": 116, "ed": 118, "text": "density function"}, {"st": 136, "ed": 138, "text": "inverse problem"}, {"st": 174, "ed": 176, "text": "kernel methods"}, {"st": 180, "ed": 182, "text": "kernel based"}, {"st": 212, "ed": 214, "text": "theoretical analysis"}, {"st": 218, "ed": 220, "text": "convergence rates"}, {"st": 222, "ed": 224, "text": "gaussian kernel"}, {"st": 240, "ed": 242, "text": "d dimensional"}, {"st": 258, "ed": 261, "text": "semi supervised learning"}, {"st": 263, "ed": 265, "text": "covariate shift"}]
[{"st": 0, "ed": 3, "text": "support vector machines"}, {"st": 34, "ed": 36, "text": "feature selection"}, {"st": 51, "ed": 53, "text": "feature selection"}, {"st": 68, "ed": 70, "text": "geometric properties"}, {"st": 76, "ed": 78, "text": "feature sets"}, {"st": 80, "ed": 82, "text": "statistically significant"}, {"st": 93, "ed": 95, "text": "linear regression"}, {"st": 99, "ed": 101, "text": "geometric properties"}, {"st": 105, "ed": 107, "text": "proposed algorithm"}, {"st": 108, "ed": 110, "text": "excellent results"}, {"st": 113, "ed": 115, "text": "text data"}, {"st": 149, "ed": 151, "text": "feature selection"}]
[{"st": 4, "ed": 8, "text": "multi armed bandit problem"}, {"st": 10, "ed": 12, "text": "prior distribution"}, {"st": 26, "ed": 28, "text": "regret bounds"}, {"st": 67, "ed": 69, "text": "thompson sampling"}, {"st": 81, "ed": 83, "text": "prior distribution"}, {"st": 107, "ed": 109, "text": "prior distribution"}, {"st": 121, "ed": 123, "text": "frac 1"}, {"st": 169, "ed": 171, "text": "thompson sampling"}, {"st": 181, "ed": 183, "text": "thompson sampling"}]
[{"st": 7, "ed": 9, "text": "similarity graph"}, {"st": 12, "ed": 14, "text": "subspace learning"}, {"st": 18, "ed": 20, "text": "similarity graph"}, {"st": 44, "ed": 46, "text": "similarity graph"}, {"st": 48, "ed": 50, "text": "distance based"}, {"st": 52, "ed": 54, "text": "linear representation"}, {"st": 57, "ed": 59, "text": "existing works"}, {"st": 74, "ed": 76, "text": "distance based"}, {"st": 86, "ed": 88, "text": "linear representation"}, {"st": 99, "ed": 101, "text": "linear representation"}, {"st": 126, "ed": 128, "text": "linear representation"}, {"st": 134, "ed": 136, "text": "linear representation"}, {"st": 142, "ed": 144, "text": "proposed algorithm"}, {"st": 147, "ed": 150, "text": "each data point"}, {"st": 187, "ed": 189, "text": "subspace learning"}]
[{"st": 24, "ed": 26, "text": "feature set"}, {"st": 85, "ed": 87, "text": "feature selection"}]
[{"st": 0, "ed": 2, "text": "semi supervised"}, {"st": 6, "ed": 8, "text": "prior knowledge"}, {"st": 10, "ed": 12, "text": "decision process"}, {"st": 23, "ed": 26, "text": "semi supervised clustering"}, {"st": 34, "ed": 36, "text": "proposed method"}, {"st": 45, "ed": 47, "text": "clustering algorithm"}, {"st": 49, "ed": 51, "text": "squared loss"}, {"st": 51, "ed": 53, "text": "mutual information"}, {"st": 62, "ed": 64, "text": "proposed method"}, {"st": 65, "ed": 67, "text": "computationally efficient"}, {"st": 79, "ed": 81, "text": "proposed method"}, {"st": 85, "ed": 87, "text": "tuning parameters"}, {"st": 108, "ed": 110, "text": "proposed method"}]
[{"st": 11, "ed": 13, "text": "real valued"}, {"st": 40, "ed": 42, "text": "distributed representation"}, {"st": 88, "ed": 90, "text": "mixture models"}]
[{"st": 1, "ed": 3, "text": "fully connected"}, {"st": 4, "ed": 7, "text": "convolutional neural networks"}, {"st": 19, "ed": 21, "text": "wide variety"}, {"st": 25, "ed": 27, "text": "speech recognition"}, {"st": 27, "ed": 29, "text": "image classification"}, {"st": 29, "ed": 31, "text": "natural language"}, {"st": 35, "ed": 37, "text": "classification tasks"}, {"st": 40, "ed": 42, "text": "deep learning"}, {"st": 46, "ed": 48, "text": "activation function"}, {"st": 79, "ed": 81, "text": "margin based"}, {"st": 95, "ed": 97, "text": "neural nets"}, {"st": 100, "ed": 102, "text": "prior art"}, {"st": 117, "ed": 119, "text": "significant gains"}, {"st": 121, "ed": 123, "text": "deep learning"}, {"st": 124, "ed": 127, "text": "mnist cifar 10"}, {"st": 131, "ed": 133, "text": "representation learning"}, {"st": 136, "ed": 138, "text": "expression recognition"}]
[{"st": 1, "ed": 3, "text": "machine learning"}, {"st": 10, "ed": 12, "text": "training examples"}, {"st": 32, "ed": 34, "text": "training examples"}, {"st": 53, "ed": 55, "text": "error bound"}]
[{"st": 6, "ed": 8, "text": "missing data"}, {"st": 11, "ed": 13, "text": "statistical learning"}, {"st": 22, "ed": 24, "text": "tree based"}, {"st": 38, "ed": 40, "text": "recently proposed"}, {"st": 43, "ed": 45, "text": "decision trees"}, {"st": 50, "ed": 53, "text": "takes advantage of"}, {"st": 66, "ed": 68, "text": "real data"}, {"st": 71, "ed": 73, "text": "proposed method"}, {"st": 101, "ed": 103, "text": "predictive performance"}]
[{"st": 11, "ed": 14, "text": "inference and learning"}, {"st": 18, "ed": 20, "text": "multiple layers"}, {"st": 44, "ed": 46, "text": "latent variables"}, {"st": 74, "ed": 76, "text": "significant speedups"}, {"st": 84, "ed": 86, "text": "monte carlo"}, {"st": 88, "ed": 90, "text": "gradient based"}]
[{"st": 12, "ed": 14, "text": "training data"}, {"st": 34, "ed": 36, "text": "method called"}, {"st": 47, "ed": 49, "text": "instance labels"}, {"st": 58, "ed": 60, "text": "large margin"}, {"st": 63, "ed": 65, "text": "existing works"}, {"st": 83, "ed": 85, "text": "integer programming"}, {"st": 100, "ed": 102, "text": "alternating optimization"}, {"st": 110, "ed": 112, "text": "extensive experiments"}, {"st": 113, "ed": 115, "text": "standard datasets"}]
[{"st": 2, "ed": 4, "text": "efficient learning"}, {"st": 8, "ed": 10, "text": "poorly understood"}, {"st": 10, "ed": 13, "text": "states and actions"}, {"st": 22, "ed": 24, "text": "efficient exploration"}, {"st": 24, "ed": 26, "text": "posterior sampling"}, {"st": 27, "ed": 29, "text": "reinforcement learning"}, {"st": 49, "ed": 51, "text": "prior distribution"}, {"st": 52, "ed": 55, "text": "markov decision processes"}, {"st": 79, "ed": 81, "text": "conceptually simple"}, {"st": 81, "ed": 83, "text": "computationally efficient"}, {"st": 89, "ed": 91, "text": "prior knowledge"}, {"st": 155, "ed": 157, "text": "reinforcement learning"}, {"st": 164, "ed": 167, "text": "significantly outperforms existing"}]
[{"st": 0, "ed": 2, "text": "differential privacy"}, {"st": 47, "ed": 49, "text": "posterior distribution"}, {"st": 66, "ed": 68, "text": "differential privacy"}, {"st": 114, "ed": 116, "text": "posterior sampling"}, {"st": 118, "ed": 120, "text": "differentially private"}, {"st": 125, "ed": 127, "text": "decision theoretic"}]
[{"st": 15, "ed": 17, "text": "random variable"}, {"st": 25, "ed": 27, "text": "important role"}, {"st": 40, "ed": 42, "text": "learning theory"}, {"st": 43, "ed": 45, "text": "generalization bounds"}]
[{"st": 3, "ed": 5, "text": "sampling scheme"}, {"st": 39, "ed": 41, "text": "excess risk"}, {"st": 42, "ed": 44, "text": "compare favorably"}]
[{"st": 4, "ed": 6, "text": "increasingly popular"}, {"st": 32, "ed": 34, "text": "multi relational"}, {"st": 57, "ed": 59, "text": "benchmark datasets"}, {"st": 68, "ed": 70, "text": "prediction results"}]
[{"st": 28, "ed": 30, "text": "low dimensional"}, {"st": 44, "ed": 46, "text": "low dimensional"}, {"st": 54, "ed": 56, "text": "subset selection"}, {"st": 79, "ed": 81, "text": "low dimensional"}, {"st": 85, "ed": 87, "text": "feature learning"}, {"st": 93, "ed": 95, "text": "proposed approach"}, {"st": 115, "ed": 117, "text": "low dimensional"}, {"st": 137, "ed": 139, "text": "convex optimization"}, {"st": 143, "ed": 145, "text": "spectral radius"}]
[{"st": 6, "ed": 8, "text": "based method"}, {"st": 13, "ed": 15, "text": "policy evaluation"}, {"st": 17, "ed": 19, "text": "least squares"}, {"st": 19, "ed": 21, "text": "temporal difference"}, {"st": 48, "ed": 50, "text": "convergence rate"}, {"st": 53, "ed": 55, "text": "proposed method"}, {"st": 78, "ed": 81, "text": "rate of convergence"}, {"st": 94, "ed": 96, "text": "low complexity"}, {"st": 110, "ed": 112, "text": "performance bounds"}, {"st": 123, "ed": 125, "text": "low complexity"}, {"st": 134, "ed": 136, "text": "big data"}, {"st": 146, "ed": 148, "text": "low complexity"}, {"st": 150, "ed": 152, "text": "least squares"}, {"st": 178, "ed": 180, "text": "traffic signal"}, {"st": 194, "ed": 196, "text": "low complexity"}, {"st": 198, "ed": 200, "text": "least squares"}, {"st": 206, "ed": 208, "text": "contextual bandits"}, {"st": 210, "ed": 212, "text": "large scale"}]
[{"st": 13, "ed": 15, "text": "social network"}, {"st": 54, "ed": 56, "text": "social network"}, {"st": 84, "ed": 86, "text": "prior information"}, {"st": 144, "ed": 146, "text": "marginal distribution"}, {"st": 168, "ed": 170, "text": "inference algorithm"}, {"st": 194, "ed": 196, "text": "link prediction"}, {"st": 198, "ed": 202, "text": "synthetic and real world"}]
[{"st": 0, "ed": 2, "text": "representation learning"}, {"st": 6, "ed": 8, "text": "deep learning"}, {"st": 10, "ed": 12, "text": "widely applied"}, {"st": 21, "ed": 23, "text": "labeled data"}, {"st": 26, "ed": 28, "text": "classification performance"}, {"st": 29, "ed": 32, "text": "deep neural network"}, {"st": 36, "ed": 38, "text": "learned features"}, {"st": 55, "ed": 57, "text": "ensemble methods"}, {"st": 60, "ed": 62, "text": "classification performance"}]
[{"st": 39, "ed": 41, "text": "theoretical framework"}, {"st": 55, "ed": 57, "text": "objective function"}, {"st": 107, "ed": 109, "text": "similarity measure"}]
[{"st": 0, "ed": 2, "text": "prior knowledge"}, {"st": 7, "ed": 9, "text": "predictive performance"}, {"st": 10, "ed": 12, "text": "learning algorithms"}, {"st": 35, "ed": 37, "text": "recently introduced"}, {"st": 46, "ed": 48, "text": "additional information"}, {"st": 71, "ed": 73, "text": "prior knowledge"}]
[{"st": 1, "ed": 3, "text": "l1 regularized"}, {"st": 4, "ed": 7, "text": "maximum likelihood estimator"}, {"st": 20, "ed": 22, "text": "inverse covariance"}, {"st": 32, "ed": 35, "text": "markov random field"}, {"st": 48, "ed": 50, "text": "optimization problem"}, {"st": 113, "ed": 117, "text": "synthetic and real world"}]
[{"st": 1, "ed": 3, "text": "latent variable"}, {"st": 10, "ed": 12, "text": "gradient based"}, {"st": 25, "ed": 27, "text": "computationally efficient"}, {"st": 43, "ed": 45, "text": "latent variable"}, {"st": 51, "ed": 53, "text": "low rank"}, {"st": 53, "ed": 55, "text": "linear regression"}, {"st": 74, "ed": 77, "text": "rates of convergence"}, {"st": 83, "ed": 85, "text": "empirical evaluation"}]
[{"st": 7, "ed": 9, "text": "multi task"}, {"st": 15, "ed": 17, "text": "operator valued"}, {"st": 21, "ed": 23, "text": "multi task"}, {"st": 34, "ed": 37, "text": "multi task learning"}, {"st": 45, "ed": 47, "text": "multi task"}, {"st": 47, "ed": 49, "text": "kernel regression"}, {"st": 58, "ed": 60, "text": "infinite dimensional"}, {"st": 87, "ed": 89, "text": "operator valued"}, {"st": 100, "ed": 102, "text": "multi task"}, {"st": 102, "ed": 104, "text": "kernel regression"}]
[{"st": 26, "ed": 30, "text": "multi armed bandit problem"}, {"st": 43, "ed": 45, "text": "sample complexity"}, {"st": 60, "ed": 62, "text": "large scale"}, {"st": 72, "ed": 74, "text": "total number"}, {"st": 98, "ed": 101, "text": "multi armed bandit"}, {"st": 119, "ed": 122, "text": "best arm identification"}, {"st": 126, "ed": 128, "text": "sample complexity"}, {"st": 142, "ed": 145, "text": "multi armed bandits"}, {"st": 165, "ed": 167, "text": "sample complexity"}, {"st": 181, "ed": 183, "text": "problem instances"}, {"st": 185, "ed": 187, "text": "sample complexity"}]
[{"st": 1, "ed": 3, "text": "machine learning"}, {"st": 35, "ed": 37, "text": "invasive species"}, {"st": 54, "ed": 56, "text": "machine learning"}, {"st": 56, "ed": 58, "text": "based approaches"}, {"st": 118, "ed": 120, "text": "machine learning"}]
[{"st": 0, "ed": 2, "text": "multivariate regression"}, {"st": 36, "ed": 38, "text": "multivariate regression"}, {"st": 52, "ed": 54, "text": "proposed method"}, {"st": 56, "ed": 58, "text": "multivariate regression"}, {"st": 65, "ed": 67, "text": "log likelihood"}, {"st": 90, "ed": 92, "text": "inverse covariance"}]
[{"st": 4, "ed": 6, "text": "classification problems"}, {"st": 12, "ed": 14, "text": "domain adaptation"}, {"st": 18, "ed": 21, "text": "training and testing"}, {"st": 49, "ed": 51, "text": "labeled examples"}, {"st": 72, "ed": 75, "text": "labeled training data"}, {"st": 88, "ed": 90, "text": "domain adaptation"}, {"st": 136, "ed": 138, "text": "domain adaptation"}]
[{"st": 9, "ed": 12, "text": "deep neural networks"}, {"st": 19, "ed": 21, "text": "complex valued"}, {"st": 54, "ed": 56, "text": "unsupervised learning"}, {"st": 72, "ed": 74, "text": "unlabeled examples"}, {"st": 80, "ed": 82, "text": "supervised learning"}]
[{"st": 9, "ed": 11, "text": "semi supervised"}, {"st": 40, "ed": 44, "text": "canonical correlation analysis cca"}, {"st": 45, "ed": 47, "text": "unlabeled data"}, {"st": 82, "ed": 84, "text": "recent theoretical"}, {"st": 91, "ed": 93, "text": "random features"}, {"st": 95, "ed": 97, "text": "kernel regression"}, {"st": 112, "ed": 114, "text": "consistently outperforms"}, {"st": 121, "ed": 124, "text": "semi supervised learning"}, {"st": 126, "ed": 128, "text": "predictive performance"}, {"st": 136, "ed": 138, "text": "wide variety"}, {"st": 139, "ed": 141, "text": "real world"}]
[{"st": 2, "ed": 4, "text": "large scale"}, {"st": 4, "ed": 7, "text": "markov decision processes"}, {"st": 16, "ed": 18, "text": "previous studies"}, {"st": 34, "ed": 36, "text": "dynamic programming"}, {"st": 39, "ed": 41, "text": "medium sized"}, {"st": 46, "ed": 49, "text": "curse of dimensionality"}, {"st": 52, "ed": 54, "text": "real life"}, {"st": 68, "ed": 70, "text": "reinforcement learning"}, {"st": 80, "ed": 83, "text": "approximate dynamic programming"}, {"st": 88, "ed": 90, "text": "fixed point"}, {"st": 94, "ed": 96, "text": "large scale"}, {"st": 102, "ed": 104, "text": "proposed method"}]
[{"st": 16, "ed": 18, "text": "machine learning"}, {"st": 18, "ed": 20, "text": "pattern recognition"}, {"st": 41, "ed": 43, "text": "metric learning"}, {"st": 60, "ed": 62, "text": "machine learning"}, {"st": 75, "ed": 77, "text": "systematic review"}, {"st": 79, "ed": 81, "text": "metric learning"}, {"st": 95, "ed": 97, "text": "mahalanobis distance"}, {"st": 97, "ed": 99, "text": "metric learning"}, {"st": 122, "ed": 124, "text": "metric learning"}, {"st": 124, "ed": 126, "text": "similarity learning"}, {"st": 136, "ed": 138, "text": "semi supervised"}, {"st": 138, "ed": 140, "text": "metric learning"}, {"st": 140, "ed": 142, "text": "metric learning"}, {"st": 158, "ed": 160, "text": "metric learning"}, {"st": 161, "ed": 163, "text": "structured data"}, {"st": 166, "ed": 168, "text": "distance learning"}, {"st": 179, "ed": 181, "text": "metric learning"}]
[{"st": 28, "ed": 30, "text": "latent variables"}, {"st": 35, "ed": 38, "text": "hidden markov models"}, {"st": 38, "ed": 41, "text": "gaussian mixture models"}, {"st": 56, "ed": 58, "text": "observed variables"}, {"st": 77, "ed": 79, "text": "hidden variables"}, {"st": 87, "ed": 89, "text": "observed variables"}, {"st": 117, "ed": 119, "text": "finite sample"}, {"st": 136, "ed": 138, "text": "structural properties"}, {"st": 145, "ed": 147, "text": "sample complexity"}, {"st": 157, "ed": 159, "text": "observed variables"}]
[{"st": 2, "ed": 4, "text": "supervised learning"}, {"st": 6, "ed": 10, "text": "multiple instance learning mil"}, {"st": 50, "ed": 52, "text": "instance labels"}, {"st": 75, "ed": 77, "text": "numerous applications"}, {"st": 102, "ed": 104, "text": "theoretical analysis"}, {"st": 125, "ed": 127, "text": "sample complexity"}, {"st": 152, "ed": 154, "text": "learning algorithm"}, {"st": 160, "ed": 162, "text": "supervised learning"}, {"st": 182, "ed": 184, "text": "supervised learning"}, {"st": 191, "ed": 193, "text": "computational complexity"}]
[{"st": 0, "ed": 2, "text": "machine learning"}, {"st": 4, "ed": 6, "text": "multi label"}, {"st": 6, "ed": 8, "text": "document classification"}, {"st": 35, "ed": 37, "text": "total number"}, {"st": 67, "ed": 69, "text": "real world"}, {"st": 80, "ed": 82, "text": "topic models"}, {"st": 83, "ed": 85, "text": "multi label"}, {"st": 103, "ed": 105, "text": "discriminative models"}, {"st": 109, "ed": 111, "text": "classification problems"}, {"st": 123, "ed": 126, "text": "generative and discriminative"}, {"st": 150, "ed": 152, "text": "generative models"}, {"st": 153, "ed": 155, "text": "achieve competitive"}, {"st": 155, "ed": 158, "text": "multi label classification"}]
[{"st": 22, "ed": 24, "text": "face recognition"}, {"st": 43, "ed": 46, "text": "k nearest neighbor"}, {"st": 55, "ed": 57, "text": "low dimensional"}, {"st": 67, "ed": 69, "text": "kernel methods"}, {"st": 70, "ed": 73, "text": "k nearest neighbor"}, {"st": 76, "ed": 78, "text": "latent variables"}, {"st": 86, "ed": 89, "text": "k nearest neighbor"}]
[{"st": 25, "ed": 28, "text": "hidden markov model"}, {"st": 60, "ed": 62, "text": "most probable"}, {"st": 66, "ed": 68, "text": "most probable"}, {"st": 71, "ed": 74, "text": "taking advantage of"}, {"st": 78, "ed": 80, "text": "markov property"}, {"st": 87, "ed": 89, "text": "most probable"}]
[{"st": 14, "ed": 16, "text": "low dimensional"}, {"st": 39, "ed": 41, "text": "low dimensional"}, {"st": 72, "ed": 74, "text": "closed form"}, {"st": 99, "ed": 101, "text": "response variable"}, {"st": 136, "ed": 138, "text": "latent variable"}, {"st": 143, "ed": 146, "text": "expectation maximization em"}, {"st": 156, "ed": 158, "text": "maximum likelihood"}, {"st": 176, "ed": 178, "text": "inference procedures"}, {"st": 194, "ed": 196, "text": "factor analysis"}, {"st": 198, "ed": 200, "text": "proposed framework"}, {"st": 214, "ed": 216, "text": "method outperforms"}]
[{"st": 11, "ed": 13, "text": "finite dimensional"}, {"st": 26, "ed": 28, "text": "recent theoretical"}, {"st": 34, "ed": 36, "text": "statistical learning"}, {"st": 52, "ed": 54, "text": "inverse problem"}, {"st": 57, "ed": 59, "text": "machine learning"}]
[{"st": 15, "ed": 17, "text": "mixture model"}, {"st": 24, "ed": 26, "text": "parameter estimation"}, {"st": 51, "ed": 53, "text": "maximum likelihood"}, {"st": 80, "ed": 84, "text": "expectation maximization em algorithm"}, {"st": 107, "ed": 109, "text": "maximum likelihood"}]
[{"st": 8, "ed": 10, "text": "linear classifier"}, {"st": 42, "ed": 44, "text": "analysis shows"}, {"st": 69, "ed": 71, "text": "classification performance"}, {"st": 99, "ed": 101, "text": "labeled data"}, {"st": 112, "ed": 114, "text": "unlabeled examples"}, {"st": 133, "ed": 135, "text": "theoretically analyze"}, {"st": 169, "ed": 172, "text": "labeled and unlabeled"}, {"st": 178, "ed": 180, "text": "text classification"}, {"st": 184, "ed": 187, "text": "semi supervised learning"}, {"st": 189, "ed": 191, "text": "supervised methods"}, {"st": 194, "ed": 196, "text": "labeled examples"}, {"st": 231, "ed": 233, "text": "classification performance"}]
[{"st": 10, "ed": 12, "text": "online learning"}, {"st": 93, "ed": 95, "text": "expected reward"}, {"st": 132, "ed": 135, "text": "taking into account"}, {"st": 151, "ed": 154, "text": "online learning algorithms"}, {"st": 175, "ed": 177, "text": "expected reward"}, {"st": 205, "ed": 207, "text": "theoretical framework"}, {"st": 212, "ed": 214, "text": "practical applications"}, {"st": 215, "ed": 217, "text": "big data"}, {"st": 218, "ed": 220, "text": "event detection"}]
[{"st": 1, "ed": 3, "text": "deep learning"}, {"st": 9, "ed": 11, "text": "representation learning"}, {"st": 15, "ed": 17, "text": "sparse coding"}, {"st": 18, "ed": 20, "text": "auto encoders"}, {"st": 21, "ed": 24, "text": "convolutional neural networks"}, {"st": 26, "ed": 28, "text": "building blocks"}, {"st": 38, "ed": 40, "text": "building block"}, {"st": 44, "ed": 46, "text": "proposed method"}, {"st": 61, "ed": 63, "text": "hidden units"}, {"st": 70, "ed": 72, "text": "hidden units"}, {"st": 96, "ed": 98, "text": "hidden units"}, {"st": 110, "ed": 112, "text": "nearest neighbors"}, {"st": 119, "ed": 121, "text": "building blocks"}, {"st": 127, "ed": 129, "text": "proposed method"}, {"st": 137, "ed": 139, "text": "deep model"}, {"st": 144, "ed": 147, "text": "deep belief networks"}]
[{"st": 2, "ed": 5, "text": "blind source separation"}, {"st": 24, "ed": 29, "text": "non negative matrix factorization nmf"}, {"st": 98, "ed": 100, "text": "non negativity"}, {"st": 127, "ed": 129, "text": "proposed algorithm"}, {"st": 135, "ed": 137, "text": "component analysis"}, {"st": 163, "ed": 165, "text": "numerical experiments"}, {"st": 167, "ed": 169, "text": "wide variety"}]
[{"st": 3, "ed": 6, "text": "probabilistic graphical models"}, {"st": 12, "ed": 14, "text": "maximum likelihood"}, {"st": 66, "ed": 68, "text": "exact bayesian"}, {"st": 79, "ed": 81, "text": "maximum likelihood"}, {"st": 101, "ed": 103, "text": "log likelihood"}, {"st": 170, "ed": 172, "text": "maximum likelihood"}]
[{"st": 6, "ed": 8, "text": "parameter learning"}, {"st": 10, "ed": 13, "text": "markov random fields"}, {"st": 57, "ed": 59, "text": "log linear"}, {"st": 69, "ed": 71, "text": "sufficient statistics"}]
[{"st": 8, "ed": 11, "text": "online convex optimization"}, {"st": 13, "ed": 15, "text": "von neumann"}, {"st": 22, "ed": 24, "text": "optimal regret"}, {"st": 29, "ed": 31, "text": "closely related"}, {"st": 51, "ed": 53, "text": "joint distributions"}, {"st": 78, "ed": 80, "text": "optimal regret"}, {"st": 109, "ed": 111, "text": "expected loss"}, {"st": 139, "ed": 141, "text": "online learning"}, {"st": 153, "ed": 155, "text": "learning algorithm"}]
[{"st": 5, "ed": 7, "text": "low rank"}, {"st": 15, "ed": 17, "text": "noisy observations"}, {"st": 34, "ed": 36, "text": "collaborative filtering"}, {"st": 48, "ed": 50, "text": "low complexity"}, {"st": 74, "ed": 76, "text": "performance guarantees"}]
[{"st": 5, "ed": 7, "text": "learning problems"}, {"st": 36, "ed": 38, "text": "prior knowledge"}, {"st": 51, "ed": 53, "text": "based regularization"}, {"st": 62, "ed": 64, "text": "statistical properties"}, {"st": 90, "ed": 92, "text": "strongly convex"}, {"st": 143, "ed": 145, "text": "regret bounds"}, {"st": 146, "ed": 149, "text": "multi task learning"}, {"st": 149, "ed": 151, "text": "multi class"}]
[{"st": 97, "ed": 99, "text": "regret bounds"}, {"st": 101, "ed": 103, "text": "online setting"}, {"st": 104, "ed": 106, "text": "rademacher complexity"}]
[{"st": 2, "ed": 4, "text": "practical applications"}, {"st": 36, "ed": 38, "text": "clustering results"}, {"st": 52, "ed": 54, "text": "clustering algorithms"}, {"st": 67, "ed": 69, "text": "cost function"}, {"st": 126, "ed": 128, "text": "proposed framework"}, {"st": 137, "ed": 139, "text": "clustering algorithms"}, {"st": 141, "ed": 143, "text": "k means"}, {"st": 144, "ed": 146, "text": "spectral clustering"}, {"st": 152, "ed": 156, "text": "synthetic and real data"}, {"st": 160, "ed": 162, "text": "proposed framework"}, {"st": 168, "ed": 170, "text": "clustering algorithms"}]
[{"st": 5, "ed": 7, "text": "clustering algorithms"}, {"st": 9, "ed": 11, "text": "k means"}, {"st": 15, "ed": 17, "text": "clustering results"}, {"st": 51, "ed": 53, "text": "clustering algorithms"}, {"st": 100, "ed": 102, "text": "k means"}, {"st": 104, "ed": 106, "text": "clustering approaches"}, {"st": 126, "ed": 129, "text": "block coordinate descent"}, {"st": 134, "ed": 136, "text": "iterative algorithms"}, {"st": 137, "ed": 139, "text": "convergence guarantees"}, {"st": 142, "ed": 144, "text": "computational complexity"}, {"st": 156, "ed": 158, "text": "clustering algorithms"}, {"st": 185, "ed": 188, "text": "synthetic and real"}]
[{"st": 8, "ed": 10, "text": "partially observed"}, {"st": 117, "ed": 119, "text": "convex optimization"}, {"st": 129, "ed": 132, "text": "problem of recovering"}, {"st": 134, "ed": 136, "text": "low rank"}, {"st": 140, "ed": 142, "text": "sparse matrix"}, {"st": 144, "ed": 146, "text": "partially observed"}, {"st": 166, "ed": 168, "text": "sufficient conditions"}, {"st": 208, "ed": 211, "text": "number of clusters"}]
[{"st": 5, "ed": 8, "text": "inverse reinforcement learning"}, {"st": 26, "ed": 29, "text": "inverse reinforcement learning"}, {"st": 35, "ed": 37, "text": "posterior distribution"}, {"st": 61, "ed": 63, "text": "statistical methods"}, {"st": 64, "ed": 67, "text": "inverse reinforcement learning"}, {"st": 99, "ed": 101, "text": "significantly improved"}]
[{"st": 1, "ed": 3, "text": "classification methods"}, {"st": 8, "ed": 10, "text": "unlabeled data"}, {"st": 33, "ed": 35, "text": "classification methods"}, {"st": 47, "ed": 49, "text": "existing methods"}, {"st": 87, "ed": 89, "text": "nearest neighbor"}]
[{"st": 5, "ed": 7, "text": "signal processing"}, {"st": 8, "ed": 10, "text": "machine learning"}, {"st": 16, "ed": 18, "text": "sparse linear"}, {"st": 57, "ed": 59, "text": "sparse coding"}, {"st": 60, "ed": 63, "text": "sparse dictionary learning"}, {"st": 70, "ed": 72, "text": "local minima"}, {"st": 84, "ed": 86, "text": "probabilistic model"}, {"st": 95, "ed": 97, "text": "sparse coding"}, {"st": 99, "ed": 101, "text": "local minimum"}, {"st": 110, "ed": 113, "text": "takes into account"}]
[{"st": 19, "ed": 21, "text": "moving objects"}, {"st": 118, "ed": 120, "text": "similarity graph"}, {"st": 132, "ed": 134, "text": "synthetic data"}]
[{"st": 9, "ed": 11, "text": "sparse representations"}, {"st": 22, "ed": 24, "text": "proposed approach"}, {"st": 33, "ed": 35, "text": "temporal information"}, {"st": 46, "ed": 48, "text": "generalization bounds"}, {"st": 53, "ed": 55, "text": "sparse coding"}, {"st": 59, "ed": 61, "text": "sample complexity"}, {"st": 64, "ed": 66, "text": "l1 norm"}, {"st": 67, "ed": 69, "text": "kernel function"}, {"st": 78, "ed": 80, "text": "sparse codes"}, {"st": 81, "ed": 83, "text": "significantly improves"}, {"st": 89, "ed": 92, "text": "scale to large"}, {"st": 101, "ed": 103, "text": "proposed approach"}, {"st": 107, "ed": 110, "text": "accuracy and speed"}, {"st": 115, "ed": 117, "text": "real data"}, {"st": 124, "ed": 126, "text": "proposed approach"}, {"st": 131, "ed": 133, "text": "semi supervised"}]
[{"st": 11, "ed": 16, "text": "non negative matrix factorization nmf"}, {"st": 56, "ed": 58, "text": "finite set"}, {"st": 72, "ed": 74, "text": "highly scalable"}, {"st": 101, "ed": 103, "text": "distributed memory"}]
[{"st": 2, "ed": 4, "text": "latent structure"}, {"st": 6, "ed": 8, "text": "observed variables"}, {"st": 15, "ed": 17, "text": "existing approaches"}, {"st": 27, "ed": 29, "text": "hidden states"}, {"st": 39, "ed": 41, "text": "based approach"}, {"st": 49, "ed": 51, "text": "key contribution"}, {"st": 62, "ed": 64, "text": "marginal distribution"}, {"st": 75, "ed": 77, "text": "nuclear norm"}, {"st": 94, "ed": 97, "text": "divide and conquer"}, {"st": 101, "ed": 103, "text": "latent tree"}, {"st": 104, "ed": 107, "text": "under mild conditions"}, {"st": 125, "ed": 127, "text": "proposed approach"}, {"st": 127, "ed": 130, "text": "compares favorably to"}, {"st": 133, "ed": 135, "text": "real world"}]
[{"st": 0, "ed": 2, "text": "feature selection"}, {"st": 14, "ed": 16, "text": "feature selection"}, {"st": 36, "ed": 38, "text": "real world"}, {"st": 58, "ed": 60, "text": "l1 regularization"}, {"st": 65, "ed": 67, "text": "squared loss"}, {"st": 69, "ed": 71, "text": "mutual information"}, {"st": 76, "ed": 78, "text": "numerical results"}]
[{"st": 1, "ed": 3, "text": "machine learning"}, {"st": 7, "ed": 9, "text": "large scale"}, {"st": 9, "ed": 11, "text": "real world"}, {"st": 14, "ed": 16, "text": "search engines"}, {"st": 70, "ed": 72, "text": "feature extraction"}, {"st": 127, "ed": 129, "text": "cost sensitive"}]
[{"st": 107, "ed": 109, "text": "statistical power"}]
[{"st": 28, "ed": 30, "text": "classification problems"}, {"st": 37, "ed": 39, "text": "semi supervised"}, {"st": 104, "ed": 106, "text": "low cost"}, {"st": 186, "ed": 188, "text": "closed form"}, {"st": 245, "ed": 249, "text": "real world data sets"}]
[{"st": 8, "ed": 11, "text": "dirichlet process mixture"}, {"st": 42, "ed": 44, "text": "exact inference"}, {"st": 55, "ed": 57, "text": "kernel machines"}, {"st": 68, "ed": 70, "text": "efficient learning"}, {"st": 88, "ed": 90, "text": "mixture model"}]
[{"st": 29, "ed": 31, "text": "generalization error"}, {"st": 55, "ed": 57, "text": "weight vector"}, {"st": 73, "ed": 75, "text": "soft margin"}, {"st": 82, "ed": 84, "text": "previously published"}, {"st": 92, "ed": 94, "text": "soft margin"}, {"st": 142, "ed": 144, "text": "low cost"}, {"st": 144, "ed": 146, "text": "soft margin"}, {"st": 157, "ed": 159, "text": "soft margin"}, {"st": 184, "ed": 186, "text": "previously published"}, {"st": 186, "ed": 188, "text": "margin based"}, {"st": 211, "ed": 213, "text": "classification error"}]
[{"st": 6, "ed": 9, "text": "multiple kernel learning"}, {"st": 14, "ed": 16, "text": "multivariate regression"}, {"st": 48, "ed": 50, "text": "highly scalable"}, {"st": 64, "ed": 67, "text": "input and output"}, {"st": 86, "ed": 88, "text": "causal inference"}, {"st": 108, "ed": 110, "text": "granger causality"}, {"st": 115, "ed": 117, "text": "extensive empirical"}, {"st": 121, "ed": 123, "text": "theoretical analyses"}]
[{"st": 15, "ed": 17, "text": "exploration exploitation"}, {"st": 31, "ed": 33, "text": "decision maker"}, {"st": 56, "ed": 58, "text": "social networks"}, {"st": 71, "ed": 73, "text": "efficient algorithms"}, {"st": 75, "ed": 77, "text": "upper confidence"}, {"st": 82, "ed": 84, "text": "additional information"}, {"st": 105, "ed": 107, "text": "social networks"}, {"st": 109, "ed": 111, "text": "real datasets"}, {"st": 113, "ed": 115, "text": "learning rate"}]
[{"st": 1, "ed": 3, "text": "recent years"}, {"st": 7, "ed": 9, "text": "random walks"}, {"st": 20, "ed": 22, "text": "machine learning"}, {"st": 51, "ed": 53, "text": "transition matrix"}, {"st": 57, "ed": 59, "text": "random walk"}, {"st": 67, "ed": 70, "text": "kernel density estimation"}, {"st": 73, "ed": 75, "text": "random walk"}, {"st": 82, "ed": 84, "text": "transition matrix"}, {"st": 100, "ed": 102, "text": "de facto"}, {"st": 115, "ed": 117, "text": "without sacrificing"}, {"st": 123, "ed": 126, "text": "benchmark data sets"}]
[{"st": 7, "ed": 10, "text": "learning to rank"}, {"st": 54, "ed": 56, "text": "bregman divergences"}, {"st": 65, "ed": 67, "text": "recently shown"}, {"st": 105, "ed": 107, "text": "bregman divergence"}, {"st": 112, "ed": 114, "text": "parameter estimation"}, {"st": 133, "ed": 135, "text": "global optimum"}, {"st": 141, "ed": 143, "text": "empirical results"}, {"st": 144, "ed": 146, "text": "benchmark datasets"}]
[{"st": 6, "ed": 8, "text": "probabilistic models"}, {"st": 30, "ed": 32, "text": "matrix decomposition"}, {"st": 55, "ed": 58, "text": "context free grammar"}, {"st": 61, "ed": 63, "text": "wide variety"}, {"st": 102, "ed": 104, "text": "greedy search"}, {"st": 114, "ed": 116, "text": "raw data"}, {"st": 126, "ed": 128, "text": "proposed method"}, {"st": 134, "ed": 136, "text": "synthetic data"}, {"st": 155, "ed": 157, "text": "image patches"}, {"st": 157, "ed": 159, "text": "motion capture"}]
[{"st": 38, "ed": 40, "text": "policy evaluation"}, {"st": 54, "ed": 56, "text": "bias variance"}, {"st": 70, "ed": 72, "text": "empirical evidence"}, {"st": 77, "ed": 79, "text": "learning problems"}, {"st": 85, "ed": 87, "text": "previous approaches"}]
[{"st": 7, "ed": 9, "text": "real world"}, {"st": 18, "ed": 20, "text": "large scale"}, {"st": 33, "ed": 35, "text": "main contribution"}, {"st": 42, "ed": 44, "text": "variational inference"}, {"st": 86, "ed": 88, "text": "marginal probabilities"}]
[{"st": 3, "ed": 5, "text": "imitation learning"}, {"st": 42, "ed": 44, "text": "imitation learning"}, {"st": 89, "ed": 91, "text": "imitation learning"}, {"st": 93, "ed": 95, "text": "active learning"}, {"st": 112, "ed": 114, "text": "non stationary"}, {"st": 127, "ed": 129, "text": "imitation learning"}, {"st": 154, "ed": 156, "text": "highly effective"}]
[{"st": 89, "ed": 91, "text": "message passing"}]
[{"st": 0, "ed": 2, "text": "latent variable"}, {"st": 16, "ed": 18, "text": "current approaches"}, {"st": 23, "ed": 25, "text": "conditional probability"}, {"st": 31, "ed": 33, "text": "local search"}, {"st": 39, "ed": 41, "text": "tensor algebra"}, {"st": 47, "ed": 49, "text": "latent variable"}, {"st": 92, "ed": 94, "text": "local minimum"}, {"st": 115, "ed": 118, "text": "orders of magnitude"}, {"st": 134, "ed": 136, "text": "parameter learning"}, {"st": 157, "ed": 160, "text": "synthetic and real"}]
[{"st": 33, "ed": 35, "text": "complex data"}, {"st": 38, "ed": 40, "text": "multiple modalities"}, {"st": 61, "ed": 63, "text": "mixture model"}, {"st": 76, "ed": 78, "text": "learning scheme"}, {"st": 84, "ed": 87, "text": "number of clusters"}, {"st": 133, "ed": 135, "text": "positive results"}, {"st": 150, "ed": 152, "text": "image data"}, {"st": 169, "ed": 172, "text": "directions for future"}]
[{"st": 7, "ed": 9, "text": "reinforcement learning"}, {"st": 11, "ed": 14, "text": "online convex optimization"}, {"st": 36, "ed": 38, "text": "convex functions"}, {"st": 41, "ed": 43, "text": "unlike traditional"}, {"st": 55, "ed": 57, "text": "dual space"}, {"st": 80, "ed": 82, "text": "generating function"}, {"st": 91, "ed": 93, "text": "proximal gradient"}, {"st": 94, "ed": 97, "text": "temporal difference td"}, {"st": 103, "ed": 105, "text": "bregman divergences"}, {"st": 115, "ed": 117, "text": "bregman divergences"}, {"st": 125, "ed": 127, "text": "mahalanobis distance"}, {"st": 141, "ed": 143, "text": "reinforcement learning"}, {"st": 152, "ed": 154, "text": "fixed points"}, {"st": 156, "ed": 158, "text": "l1 regularized"}, {"st": 163, "ed": 165, "text": "computational cost"}, {"st": 166, "ed": 168, "text": "previous methods"}, {"st": 180, "ed": 182, "text": "reinforcement learning"}, {"st": 185, "ed": 188, "text": "discrete and continuous"}]
[{"st": 8, "ed": 10, "text": "linear programming"}, {"st": 23, "ed": 26, "text": "value function approximation"}, {"st": 40, "ed": 42, "text": "feature selection"}, {"st": 43, "ed": 46, "text": "value function approximation"}, {"st": 50, "ed": 52, "text": "performance guarantees"}, {"st": 92, "ed": 94, "text": "l1 regularized"}, {"st": 95, "ed": 97, "text": "linear programming"}, {"st": 144, "ed": 146, "text": "reinforcement learning"}]
[{"st": 25, "ed": 28, "text": "a posteriori map"}, {"st": 56, "ed": 58, "text": "machine learning"}, {"st": 69, "ed": 71, "text": "marginal probabilities"}, {"st": 93, "ed": 95, "text": "belief propagation"}, {"st": 97, "ed": 99, "text": "low order"}, {"st": 134, "ed": 136, "text": "exact inference"}, {"st": 150, "ed": 152, "text": "expressive power"}, {"st": 157, "ed": 159, "text": "empirically demonstrate"}]
[{"st": 0, "ed": 2, "text": "latent variable"}, {"st": 33, "ed": 35, "text": "job satisfaction"}, {"st": 36, "ed": 38, "text": "social sciences"}, {"st": 80, "ed": 82, "text": "latent variables"}, {"st": 93, "ed": 95, "text": "latent variables"}, {"st": 124, "ed": 126, "text": "canonical correlation"}, {"st": 145, "ed": 147, "text": "structure learning"}, {"st": 166, "ed": 168, "text": "explicitly modeling"}, {"st": 193, "ed": 195, "text": "efficiently learn"}, {"st": 203, "ed": 205, "text": "synthetic data"}, {"st": 218, "ed": 220, "text": "100 000"}, {"st": 224, "ed": 227, "text": "national health service"}]
[{"st": 0, "ed": 2, "text": "active learning"}, {"st": 4, "ed": 6, "text": "increasingly important"}, {"st": 22, "ed": 24, "text": "labeled data"}, {"st": 29, "ed": 31, "text": "decision boundary"}, {"st": 67, "ed": 69, "text": "class conditional"}, {"st": 86, "ed": 88, "text": "random variable"}, {"st": 98, "ed": 100, "text": "main contribution"}, {"st": 142, "ed": 144, "text": "kernel density"}, {"st": 176, "ed": 178, "text": "error reduction"}, {"st": 184, "ed": 186, "text": "kernel density"}]
[{"st": 2, "ed": 4, "text": "machine learning"}, {"st": 19, "ed": 21, "text": "unsupervised clustering"}, {"st": 31, "ed": 33, "text": "network structure"}, {"st": 36, "ed": 38, "text": "social network"}, {"st": 87, "ed": 89, "text": "learning algorithms"}, {"st": 110, "ed": 112, "text": "computationally expensive"}, {"st": 123, "ed": 125, "text": "method called"}, {"st": 140, "ed": 142, "text": "significantly reduced"}, {"st": 144, "ed": 146, "text": "empirical study"}, {"st": 159, "ed": 161, "text": "proposed method"}]
[{"st": 9, "ed": 11, "text": "structure learning"}, {"st": 15, "ed": 17, "text": "large scale"}, {"st": 42, "ed": 44, "text": "framework called"}, {"st": 45, "ed": 47, "text": "large scale"}, {"st": 96, "ed": 98, "text": "structure learning"}, {"st": 136, "ed": 138, "text": "large scale"}, {"st": 138, "ed": 140, "text": "network structure"}, {"st": 140, "ed": 142, "text": "learning algorithms"}, {"st": 146, "ed": 148, "text": "greedy search"}, {"st": 152, "ed": 154, "text": "comparable results"}, {"st": 157, "ed": 159, "text": "benchmark datasets"}, {"st": 161, "ed": 163, "text": "precision recall"}, {"st": 175, "ed": 177, "text": "large scale"}]
[{"st": 13, "ed": 15, "text": "existing methods"}, {"st": 21, "ed": 23, "text": "trace norm"}, {"st": 23, "ed": 25, "text": "nuclear norm"}, {"st": 63, "ed": 65, "text": "trace norm"}, {"st": 76, "ed": 78, "text": "simulated data"}, {"st": 81, "ed": 83, "text": "large scale"}, {"st": 90, "ed": 92, "text": "improved accuracy"}, {"st": 100, "ed": 102, "text": "provide theoretical"}]
[{"st": 4, "ed": 6, "text": "recommender systems"}, {"st": 28, "ed": 30, "text": "content information"}, {"st": 33, "ed": 35, "text": "matrix factorization"}, {"st": 42, "ed": 44, "text": "matrix factorization"}]
[{"st": 6, "ed": 8, "text": "supervised learning"}, {"st": 17, "ed": 19, "text": "similarity function"}, {"st": 34, "ed": 37, "text": "multi class classification"}, {"st": 49, "ed": 51, "text": "supervised learning"}, {"st": 57, "ed": 59, "text": "previously proposed"}, {"st": 72, "ed": 74, "text": "supervised learning"}, {"st": 85, "ed": 87, "text": "efficient algorithms"}, {"st": 88, "ed": 90, "text": "supervised learning"}, {"st": 106, "ed": 108, "text": "learning problems"}, {"st": 109, "ed": 111, "text": "real valued"}, {"st": 133, "ed": 135, "text": "real valued"}, {"st": 154, "ed": 156, "text": "recovery guarantees"}, {"st": 169, "ed": 171, "text": "learning algorithms"}, {"st": 195, "ed": 197, "text": "selection algorithm"}, {"st": 199, "ed": 201, "text": "significantly higher"}, {"st": 204, "ed": 206, "text": "baseline methods"}]
[{"st": 4, "ed": 7, "text": "self organizing map"}, {"st": 20, "ed": 22, "text": "initialization methods"}, {"st": 40, "ed": 42, "text": "random initialization"}, {"st": 49, "ed": 51, "text": "principal component"}]
[{"st": 3, "ed": 5, "text": "binary classification"}, {"st": 27, "ed": 29, "text": "highly dependent"}, {"st": 35, "ed": 37, "text": "time series"}, {"st": 63, "ed": 65, "text": "time series"}, {"st": 71, "ed": 73, "text": "binary classification"}, {"st": 87, "ed": 89, "text": "theoretical results"}, {"st": 92, "ed": 98, "text": "experiments on synthetic and real world"}]
[{"st": 4, "ed": 7, "text": "hierarchical dirichlet process"}, {"st": 73, "ed": 76, "text": "stochastic variational inference"}, {"st": 86, "ed": 88, "text": "selection method"}, {"st": 94, "ed": 96, "text": "efficient inference"}, {"st": 112, "ed": 114, "text": "new york"}]
[{"st": 19, "ed": 21, "text": "directed graph"}, {"st": 23, "ed": 25, "text": "finite set"}, {"st": 42, "ed": 44, "text": "generative model"}, {"st": 65, "ed": 67, "text": "low dimensional"}, {"st": 85, "ed": 87, "text": "theoretical results"}, {"st": 108, "ed": 110, "text": "real data"}]
[{"st": 7, "ed": 9, "text": "kernel bandwidth"}, {"st": 12, "ed": 14, "text": "learning algorithms"}, {"st": 56, "ed": 58, "text": "principled approach"}]
[{"st": 5, "ed": 7, "text": "feature selection"}, {"st": 50, "ed": 52, "text": "feature space"}, {"st": 57, "ed": 59, "text": "relative error"}, {"st": 65, "ed": 67, "text": "feature space"}, {"st": 78, "ed": 80, "text": "worst case"}, {"st": 97, "ed": 99, "text": "feature space"}, {"st": 102, "ed": 104, "text": "open problem"}, {"st": 111, "ed": 113, "text": "extensive experiments"}, {"st": 114, "ed": 116, "text": "real world"}]
[{"st": 5, "ed": 8, "text": "multi task learning"}, {"st": 16, "ed": 18, "text": "spatial temporal"}, {"st": 21, "ed": 23, "text": "natural resource"}, {"st": 35, "ed": 37, "text": "gaussian processes"}, {"st": 79, "ed": 81, "text": "random variables"}, {"st": 92, "ed": 95, "text": "multi task learning"}, {"st": 123, "ed": 126, "text": "multi task learning"}, {"st": 150, "ed": 153, "text": "publicly available datasets"}, {"st": 154, "ed": 156, "text": "natural resource"}]
[{"st": 16, "ed": 18, "text": "probabilistic inference"}, {"st": 19, "ed": 21, "text": "missing values"}, {"st": 67, "ed": 69, "text": "building block"}, {"st": 70, "ed": 72, "text": "deep learning"}, {"st": 75, "ed": 77, "text": "desirable properties"}, {"st": 106, "ed": 108, "text": "inference engine"}, {"st": 113, "ed": 115, "text": "variational inference"}]
[{"st": 5, "ed": 7, "text": "sparse representation"}, {"st": 19, "ed": 21, "text": "image processing"}, {"st": 26, "ed": 28, "text": "sparse representations"}, {"st": 33, "ed": 35, "text": "recently gained"}, {"st": 54, "ed": 56, "text": "learning algorithms"}, {"st": 125, "ed": 127, "text": "learning algorithm"}]
[{"st": 0, "ed": 2, "text": "recent advances"}, {"st": 11, "ed": 13, "text": "recurrent networks"}, {"st": 39, "ed": 41, "text": "latent state"}, {"st": 47, "ed": 50, "text": "scale to large"}, {"st": 61, "ed": 63, "text": "fast approximate"}, {"st": 85, "ed": 87, "text": "latent state"}, {"st": 95, "ed": 97, "text": "auto encoder"}, {"st": 116, "ed": 118, "text": "latent state"}, {"st": 128, "ed": 131, "text": "kullback leibler divergence"}, {"st": 133, "ed": 135, "text": "variational approximation"}, {"st": 165, "ed": 167, "text": "qualitative results"}]
[{"st": 10, "ed": 13, "text": "markov decision process"}, {"st": 32, "ed": 34, "text": "regret bounds"}, {"st": 51, "ed": 53, "text": "tilde o"}, {"st": 84, "ed": 86, "text": "regret bounds"}, {"st": 89, "ed": 91, "text": "reinforcement learning"}, {"st": 108, "ed": 111, "text": "computationally efficient algorithm"}, {"st": 112, "ed": 114, "text": "posterior sampling"}, {"st": 115, "ed": 117, "text": "reinforcement learning"}]
[{"st": 42, "ed": 44, "text": "machine learning"}, {"st": 46, "ed": 48, "text": "current approaches"}, {"st": 130, "ed": 132, "text": "learning algorithms"}, {"st": 146, "ed": 148, "text": "learning algorithms"}, {"st": 158, "ed": 160, "text": "training instances"}, {"st": 165, "ed": 167, "text": "multilayer perceptrons"}]
[{"st": 15, "ed": 17, "text": "matrix completion"}, {"st": 25, "ed": 27, "text": "np hard"}, {"st": 39, "ed": 41, "text": "nuclear norm"}, {"st": 97, "ed": 99, "text": "optimal solutions"}, {"st": 114, "ed": 116, "text": "parameter free"}, {"st": 145, "ed": 148, "text": "degrees of freedom"}, {"st": 151, "ed": 153, "text": "low rank"}, {"st": 172, "ed": 174, "text": "recovery guarantees"}, {"st": 191, "ed": 193, "text": "cost function"}, {"st": 196, "ed": 198, "text": "stationary point"}, {"st": 201, "ed": 203, "text": "global optimum"}, {"st": 205, "ed": 207, "text": "cost function"}, {"st": 220, "ed": 222, "text": "computer vision"}, {"st": 229, "ed": 231, "text": "collaborative filtering"}]
[{"st": 41, "ed": 43, "text": "acquisition function"}, {"st": 51, "ed": 53, "text": "differential entropy"}, {"st": 86, "ed": 88, "text": "fully bayesian"}, {"st": 101, "ed": 105, "text": "synthetic and real world"}, {"st": 105, "ed": 107, "text": "applications including"}, {"st": 107, "ed": 109, "text": "optimization problems"}, {"st": 110, "ed": 112, "text": "machine learning"}, {"st": 126, "ed": 128, "text": "significant gains"}]
[{"st": 13, "ed": 15, "text": "machine learning"}, {"st": 42, "ed": 44, "text": "learning algorithm"}, {"st": 51, "ed": 53, "text": "kernel machines"}, {"st": 55, "ed": 57, "text": "case study"}, {"st": 71, "ed": 74, "text": "kernel ridge regression"}, {"st": 79, "ed": 81, "text": "least squares"}]
[{"st": 7, "ed": 9, "text": "generative models"}, {"st": 21, "ed": 23, "text": "generative model"}, {"st": 31, "ed": 33, "text": "discriminative model"}, {"st": 44, "ed": 46, "text": "training data"}, {"st": 50, "ed": 52, "text": "training procedure"}, {"st": 90, "ed": 92, "text": "training data"}, {"st": 110, "ed": 112, "text": "multilayer perceptrons"}, {"st": 130, "ed": 132, "text": "approximate inference"}, {"st": 140, "ed": 142, "text": "experiments demonstrate"}, {"st": 148, "ed": 151, "text": "qualitative and quantitative"}]
[{"st": 3, "ed": 5, "text": "decision trees"}, {"st": 9, "ed": 11, "text": "random forests"}, {"st": 15, "ed": 18, "text": "classification and regression"}, {"st": 20, "ed": 22, "text": "machine learning"}, {"st": 24, "ed": 26, "text": "random forests"}, {"st": 26, "ed": 28, "text": "achieve competitive"}, {"st": 28, "ed": 30, "text": "predictive performance"}, {"st": 32, "ed": 34, "text": "computationally efficient"}, {"st": 43, "ed": 45, "text": "real world"}, {"st": 50, "ed": 52, "text": "random forest"}, {"st": 57, "ed": 59, "text": "random forest"}, {"st": 78, "ed": 80, "text": "random forests"}, {"st": 83, "ed": 85, "text": "training data"}, {"st": 90, "ed": 92, "text": "achieve comparable"}, {"st": 110, "ed": 112, "text": "decision trees"}, {"st": 124, "ed": 126, "text": "online fashion"}, {"st": 145, "ed": 147, "text": "achieve competitive"}, {"st": 147, "ed": 149, "text": "predictive performance"}, {"st": 153, "ed": 155, "text": "random forests"}, {"st": 160, "ed": 162, "text": "random forests"}]
[{"st": 5, "ed": 7, "text": "extensively studied"}, {"st": 16, "ed": 18, "text": "low rank"}, {"st": 43, "ed": 45, "text": "big data"}, {"st": 57, "ed": 59, "text": "online algorithm"}, {"st": 63, "ed": 65, "text": "large scale"}, {"st": 70, "ed": 72, "text": "matrix decomposition"}, {"st": 111, "ed": 113, "text": "matrix factorization"}, {"st": 163, "ed": 165, "text": "sample size"}, {"st": 195, "ed": 197, "text": "stationary point"}, {"st": 199, "ed": 201, "text": "expected loss"}, {"st": 206, "ed": 208, "text": "encouraging results"}, {"st": 221, "ed": 223, "text": "nuclear norm"}]
[{"st": 3, "ed": 5, "text": "representation learning"}, {"st": 21, "ed": 23, "text": "denoising autoencoders"}, {"st": 39, "ed": 41, "text": "coarse grained"}, {"st": 55, "ed": 57, "text": "fine grained"}, {"st": 62, "ed": 64, "text": "denoising autoencoder"}, {"st": 95, "ed": 97, "text": "original input"}, {"st": 101, "ed": 103, "text": "denoising autoencoder"}, {"st": 111, "ed": 113, "text": "fine tuning"}, {"st": 124, "ed": 126, "text": "cifar 10"}]
[{"st": 0, "ed": 3, "text": "stochastic variational inference"}, {"st": 20, "ed": 22, "text": "variational distribution"}, {"st": 33, "ed": 35, "text": "stochastic optimization"}, {"st": 70, "ed": 72, "text": "natural gradient"}, {"st": 82, "ed": 84, "text": "moving average"}, {"st": 101, "ed": 103, "text": "computational cost"}, {"st": 137, "ed": 140, "text": "mean squared error"}, {"st": 149, "ed": 152, "text": "latent dirichlet allocation"}]
[{"st": 0, "ed": 3, "text": "empirical risk minimization"}, {"st": 7, "ed": 9, "text": "learning rule"}, {"st": 10, "ed": 12, "text": "statistical learning"}, {"st": 21, "ed": 23, "text": "unknown distribution"}, {"st": 64, "ed": 67, "text": "rates of convergence"}, {"st": 69, "ed": 71, "text": "excess risk"}, {"st": 76, "ed": 78, "text": "sample size"}, {"st": 86, "ed": 88, "text": "sufficient conditions"}, {"st": 116, "ed": 120, "text": "prediction with expert advice"}]
[{"st": 0, "ed": 3, "text": "stochastic gradient descent"}, {"st": 54, "ed": 56, "text": "prior knowledge"}, {"st": 61, "ed": 63, "text": "optimal solution"}, {"st": 82, "ed": 84, "text": "kernel based"}, {"st": 84, "ed": 87, "text": "stochastic gradient descent"}, {"st": 112, "ed": 114, "text": "online learning"}, {"st": 131, "ed": 134, "text": "rates of convergence"}]
[{"st": 66, "ed": 68, "text": "standard benchmarks"}, {"st": 83, "ed": 85, "text": "achieve comparable"}]
[{"st": 12, "ed": 14, "text": "machine learning"}, {"st": 28, "ed": 30, "text": "partial information"}, {"st": 36, "ed": 38, "text": "machine learning"}, {"st": 67, "ed": 69, "text": "machine learning"}, {"st": 74, "ed": 76, "text": "positive definite"}, {"st": 89, "ed": 91, "text": "gaussian process"}, {"st": 115, "ed": 117, "text": "machine learning"}]
[{"st": 5, "ed": 7, "text": "computer science"}, {"st": 46, "ed": 48, "text": "patient specific"}, {"st": 62, "ed": 64, "text": "algorithmic framework"}, {"st": 67, "ed": 69, "text": "decision problem"}, {"st": 95, "ed": 97, "text": "learning algorithm"}, {"st": 164, "ed": 166, "text": "algorithmic framework"}, {"st": 185, "ed": 187, "text": "learning algorithm"}, {"st": 199, "ed": 202, "text": "decision support systems"}]
[{"st": 3, "ed": 5, "text": "optimal control"}, {"st": 18, "ed": 20, "text": "optimal control"}, {"st": 21, "ed": 23, "text": "computationally expensive"}, {"st": 39, "ed": 41, "text": "posterior sampling"}, {"st": 91, "ed": 93, "text": "web server"}]
[{"st": 1, "ed": 3, "text": "machine learning"}, {"st": 20, "ed": 22, "text": "massive data"}, {"st": 34, "ed": 36, "text": "stochastic optimization"}, {"st": 52, "ed": 54, "text": "risk minimization"}, {"st": 58, "ed": 60, "text": "saddle point"}, {"st": 60, "ed": 63, "text": "problem and propose"}, {"st": 65, "ed": 67, "text": "distributed stochastic"}, {"st": 75, "ed": 78, "text": "rate of convergence"}, {"st": 80, "ed": 82, "text": "analysis shows"}, {"st": 97, "ed": 99, "text": "empirical evaluations"}, {"st": 101, "ed": 103, "text": "proposed algorithm"}, {"st": 113, "ed": 115, "text": "optimization algorithms"}]
[{"st": 8, "ed": 10, "text": "parameter estimation"}, {"st": 11, "ed": 13, "text": "latent tree"}, {"st": 25, "ed": 28, "text": "divide and conquer"}, {"st": 45, "ed": 47, "text": "structure learning"}, {"st": 52, "ed": 55, "text": "minimum spanning tree"}, {"st": 61, "ed": 63, "text": "parameter learning"}, {"st": 67, "ed": 70, "text": "method of moments"}, {"st": 91, "ed": 93, "text": "sample complexity"}, {"st": 99, "ed": 101, "text": "latent tree"}, {"st": 106, "ed": 108, "text": "gaussian distributions"}, {"st": 114, "ed": 116, "text": "parallel algorithm"}, {"st": 140, "ed": 142, "text": "experiments confirm"}, {"st": 150, "ed": 152, "text": "large datasets"}, {"st": 157, "ed": 159, "text": "proposed algorithm"}]
[{"st": 4, "ed": 6, "text": "sample efficient"}, {"st": 18, "ed": 20, "text": "bayesian optimization"}, {"st": 26, "ed": 28, "text": "exploration strategy"}, {"st": 32, "ed": 34, "text": "acquisition function"}, {"st": 104, "ed": 106, "text": "outperforms existing"}, {"st": 110, "ed": 113, "text": "real and synthetic"}, {"st": 113, "ed": 115, "text": "problems including"}, {"st": 139, "ed": 141, "text": "acquisition function"}]
[{"st": 7, "ed": 9, "text": "sample complexity"}, {"st": 52, "ed": 54, "text": "parameter settings"}]
[{"st": 8, "ed": 10, "text": "linear models"}, {"st": 58, "ed": 60, "text": "estimation error"}, {"st": 79, "ed": 81, "text": "non trivial"}, {"st": 104, "ed": 106, "text": "existing methods"}, {"st": 112, "ed": 114, "text": "statistical analysis"}]
[{"st": 18, "ed": 21, "text": "semi supervised learning"}, {"st": 38, "ed": 41, "text": "semi supervised learning"}, {"st": 42, "ed": 44, "text": "generative models"}, {"st": 55, "ed": 57, "text": "labelled data"}, {"st": 77, "ed": 80, "text": "deep generative models"}, {"st": 81, "ed": 83, "text": "approximate bayesian"}, {"st": 85, "ed": 87, "text": "recent advances"}, {"st": 88, "ed": 90, "text": "variational methods"}, {"st": 95, "ed": 97, "text": "significant improvements"}, {"st": 100, "ed": 102, "text": "highly competitive"}]
[{"st": 56, "ed": 58, "text": "main contribution"}, {"st": 89, "ed": 91, "text": "machine learning"}, {"st": 93, "ed": 95, "text": "probability distributions"}, {"st": 96, "ed": 99, "text": "a reproducing kernel"}, {"st": 138, "ed": 140, "text": "domain adaptation"}]
[{"st": 6, "ed": 8, "text": "margin based"}, {"st": 8, "ed": 10, "text": "active learning"}, {"st": 40, "ed": 42, "text": "low noise"}, {"st": 77, "ed": 79, "text": "margin based"}, {"st": 79, "ed": 81, "text": "active learning"}, {"st": 84, "ed": 86, "text": "noise conditions"}, {"st": 124, "ed": 126, "text": "sample complexity"}, {"st": 159, "ed": 161, "text": "d dimensional"}, {"st": 165, "ed": 167, "text": "carefully designed"}]
[{"st": 6, "ed": 8, "text": "machine learning"}, {"st": 30, "ed": 32, "text": "data point"}, {"st": 65, "ed": 68, "text": "divide and conquer"}, {"st": 68, "ed": 70, "text": "learning scheme"}, {"st": 76, "ed": 78, "text": "mathcal o"}, {"st": 153, "ed": 155, "text": "significant speedup"}, {"st": 167, "ed": 169, "text": "subspace clustering"}, {"st": 172, "ed": 174, "text": "competitive performance"}]
[{"st": 7, "ed": 9, "text": "imitation learning"}, {"st": 10, "ed": 12, "text": "structured prediction"}, {"st": 43, "ed": 45, "text": "imitation learning"}, {"st": 59, "ed": 61, "text": "existing results"}, {"st": 69, "ed": 71, "text": "imitation learning"}, {"st": 87, "ed": 89, "text": "provide theoretical"}, {"st": 115, "ed": 117, "text": "existing techniques"}]
[{"st": 48, "ed": 50, "text": "empirical evidence"}, {"st": 54, "ed": 57, "text": "amazon mechanical turk"}, {"st": 107, "ed": 109, "text": "error rates"}, {"st": 118, "ed": 120, "text": "error rates"}, {"st": 130, "ed": 132, "text": "noise levels"}]
[{"st": 2, "ed": 4, "text": "online learning"}, {"st": 6, "ed": 9, "text": "markov decision process"}, {"st": 13, "ed": 15, "text": "side information"}, {"st": 26, "ed": 28, "text": "clinical trials"}, {"st": 28, "ed": 30, "text": "recommendation systems"}, {"st": 58, "ed": 60, "text": "side information"}, {"st": 65, "ed": 68, "text": "computationally efficient algorithm"}, {"st": 76, "ed": 78, "text": "o sqrt"}, {"st": 95, "ed": 97, "text": "regret bound"}]
[{"st": 2, "ed": 4, "text": "reinforcement learning"}, {"st": 6, "ed": 9, "text": "markov decision processes"}, {"st": 17, "ed": 19, "text": "transition probabilities"}, {"st": 43, "ed": 45, "text": "thompson sampling"}, {"st": 47, "ed": 49, "text": "reinforcement learning"}, {"st": 54, "ed": 56, "text": "regret bound"}, {"st": 63, "ed": 65, "text": "result shows"}, {"st": 85, "ed": 87, "text": "prior distributions"}, {"st": 99, "ed": 101, "text": "closed form"}, {"st": 110, "ed": 112, "text": "constant factor"}, {"st": 128, "ed": 130, "text": "kullback leibler"}]
[{"st": 14, "ed": 16, "text": "machine learning"}, {"st": 24, "ed": 26, "text": "hyper parameters"}, {"st": 53, "ed": 55, "text": "theoretical analysis"}, {"st": 68, "ed": 70, "text": "cumulative regret"}, {"st": 75, "ed": 77, "text": "gaussian processes"}, {"st": 80, "ed": 82, "text": "hyper parameters"}, {"st": 94, "ed": 96, "text": "acquisition function"}, {"st": 109, "ed": 111, "text": "hyper parameter"}]
[{"st": 26, "ed": 28, "text": "input space"}, {"st": 37, "ed": 39, "text": "proposed method"}, {"st": 62, "ed": 64, "text": "existing methods"}, {"st": 68, "ed": 70, "text": "wide applicability"}, {"st": 71, "ed": 73, "text": "strong assumptions"}, {"st": 91, "ed": 93, "text": "proposed method"}]
[{"st": 6, "ed": 8, "text": "least square"}, {"st": 8, "ed": 11, "text": "support vector machines"}, {"st": 75, "ed": 77, "text": "feature space"}, {"st": 108, "ed": 110, "text": "proposed method"}]
[{"st": 2, "ed": 4, "text": "ell 1"}, {"st": 15, "ed": 17, "text": "linear model"}, {"st": 37, "ed": 40, "text": "takes into account"}, {"st": 58, "ed": 60, "text": "trace norm"}, {"st": 63, "ed": 65, "text": "convex surrogate"}, {"st": 87, "ed": 89, "text": "optimization algorithm"}, {"st": 92, "ed": 94, "text": "least squares"}, {"st": 102, "ed": 104, "text": "synthetic data"}, {"st": 114, "ed": 116, "text": "competing methods"}]
[{"st": 20, "ed": 22, "text": "combinatorial optimization"}, {"st": 25, "ed": 27, "text": "feature selection"}, {"st": 29, "ed": 31, "text": "convex relaxation"}, {"st": 36, "ed": 38, "text": "ell 1"}, {"st": 57, "ed": 59, "text": "prior knowledge"}, {"st": 67, "ed": 69, "text": "ell 1"}, {"st": 101, "ed": 103, "text": "unsupervised learning"}, {"st": 106, "ed": 109, "text": "principal component analysis"}, {"st": 115, "ed": 117, "text": "supervised learning"}]
[{"st": 33, "ed": 35, "text": "missing data"}, {"st": 105, "ed": 108, "text": "low dimensional manifold"}, {"st": 132, "ed": 134, "text": "low dimensional"}, {"st": 171, "ed": 173, "text": "conditional distribution"}, {"st": 206, "ed": 208, "text": "inverse kinematics"}]
[{"st": 0, "ed": 2, "text": "bias variance"}, {"st": 5, "ed": 7, "text": "expected error"}, {"st": 11, "ed": 13, "text": "classification problems"}, {"st": 38, "ed": 40, "text": "survival analysis"}, {"st": 46, "ed": 48, "text": "bias variance"}, {"st": 51, "ed": 53, "text": "expected error"}, {"st": 66, "ed": 68, "text": "l1 regularized"}, {"st": 78, "ed": 80, "text": "experiments demonstrate"}]
[{"st": 0, "ed": 2, "text": "user preferences"}, {"st": 24, "ed": 26, "text": "collaborative filtering"}, {"st": 72, "ed": 74, "text": "collaborative filtering"}]
[{"st": 38, "ed": 41, "text": "multiple kernel learning"}, {"st": 43, "ed": 45, "text": "computational cost"}, {"st": 45, "ed": 47, "text": "scales linearly"}, {"st": 88, "ed": 90, "text": "computationally efficient"}, {"st": 92, "ed": 94, "text": "low variance"}, {"st": 100, "ed": 102, "text": "importance sampling"}, {"st": 146, "ed": 148, "text": "base kernels"}, {"st": 169, "ed": 171, "text": "computational cost"}, {"st": 178, "ed": 180, "text": "optimal solution"}, {"st": 202, "ed": 205, "text": "simulated and real"}]
[{"st": 1, "ed": 4, "text": "latent dirichlet allocation"}, {"st": 31, "ed": 33, "text": "variable selection"}, {"st": 37, "ed": 39, "text": "statistical modeling"}, {"st": 78, "ed": 80, "text": "latent topic"}]
[{"st": 10, "ed": 12, "text": "recently proposed"}, {"st": 12, "ed": 14, "text": "structured sparsity"}, {"st": 42, "ed": 44, "text": "prior knowledge"}, {"st": 57, "ed": 59, "text": "combinatorial optimization"}, {"st": 65, "ed": 67, "text": "convex optimization"}, {"st": 95, "ed": 97, "text": "latent representations"}, {"st": 100, "ed": 102, "text": "group lasso"}]
[{"st": 4, "ed": 6, "text": "crucial step"}, {"st": 7, "ed": 9, "text": "spectral clustering"}, {"st": 11, "ed": 16, "text": "graph based semi supervised learning"}, {"st": 18, "ed": 20, "text": "spectral methods"}, {"st": 31, "ed": 33, "text": "k nn"}, {"st": 37, "ed": 39, "text": "poor performance"}, {"st": 50, "ed": 52, "text": "spectral methods"}, {"st": 112, "ed": 114, "text": "k nn"}, {"st": 151, "ed": 155, "text": "unsupervised and semi supervised"}, {"st": 157, "ed": 161, "text": "synthetic and real data"}]
[{"st": 0, "ed": 3, "text": "approximate dynamic programming"}, {"st": 20, "ed": 23, "text": "approximate dynamic programming"}, {"st": 31, "ed": 34, "text": "curse of dimensionality"}, {"st": 49, "ed": 51, "text": "optimization problem"}, {"st": 67, "ed": 69, "text": "theoretical guarantees"}, {"st": 77, "ed": 79, "text": "l1 norm"}, {"st": 83, "ed": 85, "text": "empirical evaluation"}, {"st": 90, "ed": 92, "text": "theoretical guarantees"}]
[{"st": 1, "ed": 3, "text": "natural gradient"}, {"st": 52, "ed": 54, "text": "natural gradient"}, {"st": 63, "ed": 65, "text": "parameter space"}, {"st": 70, "ed": 72, "text": "natural gradient"}, {"st": 92, "ed": 94, "text": "natural gradient"}]
[{"st": 5, "ed": 7, "text": "structured output"}, {"st": 26, "ed": 28, "text": "operator valued"}, {"st": 41, "ed": 43, "text": "special cases"}, {"st": 52, "ed": 54, "text": "operator valued"}, {"st": 74, "ed": 76, "text": "output space"}, {"st": 117, "ed": 120, "text": "takes into account"}, {"st": 144, "ed": 146, "text": "structured output"}, {"st": 156, "ed": 158, "text": "kernel based"}, {"st": 158, "ed": 160, "text": "structured output"}]
[{"st": 1, "ed": 4, "text": "taking into account"}, {"st": 21, "ed": 23, "text": "observed variables"}, {"st": 27, "ed": 29, "text": "causal model"}, {"st": 32, "ed": 34, "text": "excellent performance"}, {"st": 104, "ed": 107, "text": "provide sufficient conditions"}, {"st": 132, "ed": 134, "text": "causal structure"}, {"st": 141, "ed": 143, "text": "causal model"}, {"st": 169, "ed": 171, "text": "exhaustive search"}]
[{"st": 9, "ed": 11, "text": "hierarchical bayesian"}, {"st": 13, "ed": 15, "text": "infinite dimensional"}, {"st": 15, "ed": 18, "text": "dynamic bayesian networks"}, {"st": 70, "ed": 72, "text": "network topology"}, {"st": 76, "ed": 78, "text": "video game"}]
[{"st": 6, "ed": 8, "text": "partially observable"}, {"st": 8, "ed": 10, "text": "random field"}, {"st": 38, "ed": 40, "text": "complex dynamics"}, {"st": 46, "ed": 48, "text": "hidden state"}]
[{"st": 0, "ed": 3, "text": "temporal difference td"}, {"st": 8, "ed": 10, "text": "predictive state"}, {"st": 21, "ed": 23, "text": "partially observable"}, {"st": 25, "ed": 27, "text": "previous research"}, {"st": 34, "ed": 36, "text": "dynamical systems"}, {"st": 53, "ed": 55, "text": "dynamical systems"}]
[{"st": 15, "ed": 17, "text": "random projection"}, {"st": 22, "ed": 24, "text": "intrinsic dimension"}, {"st": 74, "ed": 76, "text": "low dimensional"}]
[{"st": 6, "ed": 8, "text": "bayesian network"}, {"st": 26, "ed": 28, "text": "posterior probability"}, {"st": 41, "ed": 43, "text": "posterior probabilities"}, {"st": 107, "ed": 109, "text": "posterior probability"}, {"st": 118, "ed": 120, "text": "posterior probabilities"}]
[{"st": 2, "ed": 5, "text": "hidden markov models"}, {"st": 11, "ed": 13, "text": "generative models"}, {"st": 29, "ed": 31, "text": "computationally expensive"}, {"st": 33, "ed": 35, "text": "learning algorithm"}, {"st": 41, "ed": 43, "text": "log likelihood"}, {"st": 69, "ed": 71, "text": "contrastive divergence"}, {"st": 93, "ed": 96, "text": "undirected graphical models"}, {"st": 109, "ed": 111, "text": "time series"}]
[{"st": 3, "ed": 5, "text": "online learning"}, {"st": 13, "ed": 15, "text": "large data"}, {"st": 65, "ed": 67, "text": "prediction accuracy"}, {"st": 81, "ed": 83, "text": "data stream"}, {"st": 85, "ed": 87, "text": "gaussian distribution"}, {"st": 132, "ed": 134, "text": "real data"}, {"st": 138, "ed": 140, "text": "point set"}, {"st": 186, "ed": 188, "text": "predictive accuracy"}]
[{"st": 8, "ed": 11, "text": "gaussian graphical models"}, {"st": 34, "ed": 36, "text": "convex optimization"}, {"st": 46, "ed": 48, "text": "map estimation"}, {"st": 75, "ed": 77, "text": "l1 regularization"}, {"st": 103, "ed": 105, "text": "exact inference"}, {"st": 138, "ed": 140, "text": "variational inference"}, {"st": 154, "ed": 157, "text": "real world data"}, {"st": 158, "ed": 160, "text": "motion capture"}, {"st": 183, "ed": 185, "text": "baseline methods"}]
[{"st": 1, "ed": 3, "text": "domain knowledge"}, {"st": 22, "ed": 24, "text": "domain knowledge"}, {"st": 36, "ed": 38, "text": "explicitly model"}, {"st": 60, "ed": 62, "text": "domain knowledge"}, {"st": 96, "ed": 100, "text": "synthetic and real world"}]
[{"st": 12, "ed": 14, "text": "multiple source"}, {"st": 43, "ed": 45, "text": "multiple source"}, {"st": 152, "ed": 154, "text": "sentiment analysis"}]
[{"st": 4, "ed": 6, "text": "recently developed"}, {"st": 6, "ed": 8, "text": "parameter learning"}, {"st": 46, "ed": 48, "text": "maximum likelihood"}, {"st": 52, "ed": 54, "text": "analysis shows"}]
[{"st": 59, "ed": 61, "text": "joint distribution"}, {"st": 69, "ed": 71, "text": "theoretical result"}, {"st": 110, "ed": 112, "text": "method works"}]
[{"st": 31, "ed": 33, "text": "latent feature"}, {"st": 51, "ed": 53, "text": "latent features"}]
[{"st": 12, "ed": 14, "text": "learning algorithms"}, {"st": 24, "ed": 26, "text": "training data"}, {"st": 47, "ed": 49, "text": "linear combinations"}, {"st": 51, "ed": 53, "text": "base kernels"}, {"st": 77, "ed": 79, "text": "l2 regularization"}, {"st": 103, "ed": 105, "text": "optimization problem"}, {"st": 109, "ed": 111, "text": "iterative algorithm"}, {"st": 119, "ed": 121, "text": "theoretical analysis"}, {"st": 148, "ed": 151, "text": "kernel ridge regression"}, {"st": 162, "ed": 164, "text": "l1 regularization"}, {"st": 180, "ed": 182, "text": "larger scale"}, {"st": 185, "ed": 187, "text": "l2 regularization"}, {"st": 194, "ed": 196, "text": "significant improvements"}]
[{"st": 2, "ed": 5, "text": "undirected graphical models"}, {"st": 11, "ed": 13, "text": "mean field"}, {"st": 29, "ed": 31, "text": "objective function"}, {"st": 35, "ed": 37, "text": "mean field"}, {"st": 61, "ed": 63, "text": "block coordinate"}, {"st": 85, "ed": 87, "text": "exponential family"}, {"st": 103, "ed": 106, "text": "advantages and disadvantages"}]
[{"st": 3, "ed": 5, "text": "objective function"}, {"st": 8, "ed": 10, "text": "unlabeled data"}, {"st": 18, "ed": 20, "text": "objective function"}, {"st": 75, "ed": 77, "text": "supervised learning"}, {"st": 95, "ed": 97, "text": "comparable accuracy"}, {"st": 103, "ed": 105, "text": "supervised learning"}, {"st": 108, "ed": 110, "text": "structural constraints"}, {"st": 112, "ed": 115, "text": "semi supervised learning"}]
[{"st": 0, "ed": 2, "text": "latent dirichlet"}, {"st": 4, "ed": 6, "text": "topic modeling"}, {"st": 9, "ed": 11, "text": "latent variable"}, {"st": 20, "ed": 22, "text": "learning algorithms"}, {"st": 26, "ed": 28, "text": "recent years"}, {"st": 30, "ed": 32, "text": "gibbs sampling"}, {"st": 32, "ed": 34, "text": "variational inference"}, {"st": 100, "ed": 102, "text": "comparable accuracy"}, {"st": 108, "ed": 110, "text": "computationally efficient"}, {"st": 117, "ed": 119, "text": "comparative study"}, {"st": 123, "ed": 125, "text": "topic models"}, {"st": 132, "ed": 134, "text": "text corpora"}]
[{"st": 12, "ed": 15, "text": "multi armed bandit"}, {"st": 73, "ed": 76, "text": "best arm identification"}]
[{"st": 6, "ed": 8, "text": "thompson sampling"}, {"st": 12, "ed": 16, "text": "multi armed bandit problem"}, {"st": 68, "ed": 70, "text": "optimal policies"}]
[{"st": 15, "ed": 17, "text": "batch setting"}, {"st": 29, "ed": 31, "text": "rademacher complexity"}, {"st": 59, "ed": 61, "text": "substantially improve"}, {"st": 123, "ed": 125, "text": "preliminary experiments"}]
[{"st": 5, "ed": 9, "text": "reproducing kernel hilbert space"}, {"st": 12, "ed": 14, "text": "conditional distributions"}, {"st": 24, "ed": 26, "text": "loss function"}, {"st": 95, "ed": 97, "text": "convergence rates"}, {"st": 135, "ed": 138, "text": "a logarithmic factor"}, {"st": 142, "ed": 144, "text": "method achieves"}, {"st": 155, "ed": 157, "text": "reinforcement learning"}, {"st": 161, "ed": 163, "text": "shows significant"}]
[{"st": 1, "ed": 3, "text": "feature selection"}, {"st": 19, "ed": 21, "text": "existing works"}, {"st": 34, "ed": 36, "text": "feature selection"}, {"st": 50, "ed": 52, "text": "feature selection"}, {"st": 66, "ed": 68, "text": "feature selection"}, {"st": 70, "ed": 72, "text": "main contributions"}, {"st": 85, "ed": 87, "text": "feature selection"}, {"st": 96, "ed": 98, "text": "feature selection"}, {"st": 99, "ed": 101, "text": "parameter estimation"}, {"st": 115, "ed": 117, "text": "large scale"}, {"st": 118, "ed": 120, "text": "numerical results"}, {"st": 125, "ed": 128, "text": "method compares favorably"}, {"st": 132, "ed": 134, "text": "synthetic data"}, {"st": 135, "ed": 137, "text": "real world"}]
[{"st": 17, "ed": 19, "text": "compression scheme"}, {"st": 32, "ed": 34, "text": "compression scheme"}, {"st": 39, "ed": 41, "text": "compactness theorem"}, {"st": 49, "ed": 51, "text": "compression scheme"}, {"st": 76, "ed": 78, "text": "compression scheme"}, {"st": 88, "ed": 90, "text": "compactness theorem"}, {"st": 119, "ed": 121, "text": "compression scheme"}, {"st": 136, "ed": 138, "text": "compression scheme"}]
[{"st": 0, "ed": 2, "text": "sparse coding"}, {"st": 15, "ed": 17, "text": "source separation"}, {"st": 19, "ed": 21, "text": "inverse problems"}, {"st": 44, "ed": 46, "text": "training data"}, {"st": 52, "ed": 54, "text": "sparse coding"}, {"st": 91, "ed": 93, "text": "sparse coding"}, {"st": 99, "ed": 101, "text": "approximation error"}, {"st": 122, "ed": 124, "text": "sparse coding"}]
[{"st": 5, "ed": 7, "text": "auto encoder"}, {"st": 42, "ed": 44, "text": "deep learning"}, {"st": 46, "ed": 48, "text": "auto encoder"}, {"st": 112, "ed": 114, "text": "auto encoder"}, {"st": 142, "ed": 144, "text": "maximum likelihood"}, {"st": 156, "ed": 158, "text": "recently proposed"}, {"st": 158, "ed": 160, "text": "sampling algorithm"}, {"st": 163, "ed": 165, "text": "auto encoder"}]
[{"st": 98, "ed": 100, "text": "finite sample"}, {"st": 100, "ed": 102, "text": "error bound"}, {"st": 123, "ed": 125, "text": "proposed method"}]
[{"st": 9, "ed": 12, "text": "positive or negative"}, {"st": 19, "ed": 21, "text": "scoring function"}, {"st": 31, "ed": 34, "text": "positive and negative"}, {"st": 47, "ed": 49, "text": "widely studied"}, {"st": 56, "ed": 58, "text": "algorithmic framework"}, {"st": 98, "ed": 100, "text": "regret bounds"}, {"st": 109, "ed": 111, "text": "regret bounds"}, {"st": 142, "ed": 144, "text": "regret bounds"}, {"st": 239, "ed": 241, "text": "low noise"}]
[{"st": 6, "ed": 8, "text": "compressed sensing"}, {"st": 53, "ed": 55, "text": "ell 2"}, {"st": 55, "ed": 57, "text": "ell 1"}]
[{"st": 1, "ed": 3, "text": "spectral clustering"}, {"st": 5, "ed": 7, "text": "image segmentation"}, {"st": 17, "ed": 19, "text": "pairwise similarities"}, {"st": 42, "ed": 44, "text": "spectral clustering"}]
[{"st": 1, "ed": 3, "text": "training data"}, {"st": 6, "ed": 8, "text": "domain knowledge"}, {"st": 13, "ed": 15, "text": "learning algorithm"}, {"st": 44, "ed": 46, "text": "learning algorithms"}, {"st": 55, "ed": 57, "text": "learning algorithm"}, {"st": 75, "ed": 77, "text": "probability distributions"}, {"st": 92, "ed": 94, "text": "improved accuracy"}, {"st": 98, "ed": 100, "text": "training sets"}]
[{"st": 4, "ed": 6, "text": "sample complexity"}, {"st": 9, "ed": 11, "text": "structure learning"}, {"st": 15, "ed": 17, "text": "main result"}, {"st": 58, "ed": 60, "text": "parameter estimation"}, {"st": 63, "ed": 65, "text": "network structure"}, {"st": 96, "ed": 98, "text": "maximum likelihood"}, {"st": 132, "ed": 134, "text": "generating distribution"}]
[{"st": 5, "ed": 7, "text": "change detection"}, {"st": 13, "ed": 15, "text": "recently proposed"}, {"st": 25, "ed": 27, "text": "change detection"}, {"st": 104, "ed": 106, "text": "data stream"}, {"st": 110, "ed": 112, "text": "synthetic data"}]
[{"st": 30, "ed": 33, "text": "undirected graphical models"}, {"st": 34, "ed": 36, "text": "maximum margin"}, {"st": 56, "ed": 59, "text": "undirected graphical model"}, {"st": 79, "ed": 81, "text": "maximum margin"}, {"st": 87, "ed": 89, "text": "bayesian network"}, {"st": 94, "ed": 96, "text": "approximate solution"}, {"st": 109, "ed": 111, "text": "generalization performance"}, {"st": 131, "ed": 133, "text": "prior knowledge"}, {"st": 137, "ed": 139, "text": "causal model"}, {"st": 144, "ed": 146, "text": "discriminative learning"}]
[{"st": 16, "ed": 19, "text": "multiple instance learning"}, {"st": 31, "ed": 33, "text": "feature vectors"}, {"st": 41, "ed": 43, "text": "labeled instances"}, {"st": 51, "ed": 53, "text": "instance level"}, {"st": 79, "ed": 81, "text": "probabilistic model"}, {"st": 113, "ed": 117, "text": "synthetic and real world"}]
[{"st": 0, "ed": 3, "text": "boosted decision trees"}, {"st": 19, "ed": 21, "text": "well calibrated"}, {"st": 21, "ed": 23, "text": "posterior probabilities"}, {"st": 26, "ed": 28, "text": "squared error"}, {"st": 32, "ed": 34, "text": "empirically demonstrate"}, {"st": 61, "ed": 63, "text": "log loss"}, {"st": 77, "ed": 79, "text": "log loss"}, {"st": 91, "ed": 93, "text": "poor performance"}, {"st": 96, "ed": 98, "text": "complex models"}, {"st": 109, "ed": 111, "text": "significantly improve"}]
[{"st": 3, "ed": 5, "text": "undirected models"}, {"st": 8, "ed": 10, "text": "real world"}, {"st": 20, "ed": 22, "text": "marginal distributions"}, {"st": 108, "ed": 110, "text": "natural language"}, {"st": 121, "ed": 123, "text": "performs comparably"}]
[{"st": 5, "ed": 7, "text": "approximate inference"}, {"st": 18, "ed": 20, "text": "existing algorithms"}, {"st": 22, "ed": 24, "text": "belief propagation"}, {"st": 38, "ed": 40, "text": "mean field"}, {"st": 50, "ed": 53, "text": "bethe free energy"}, {"st": 89, "ed": 91, "text": "spin glass"}]
[{"st": 6, "ed": 8, "text": "policy gradient"}, {"st": 9, "ed": 13, "text": "partially observable markov decision"}, {"st": 24, "ed": 26, "text": "finite state"}, {"st": 83, "ed": 86, "text": "temporal difference td"}, {"st": 88, "ed": 90, "text": "linear function"}, {"st": 115, "ed": 118, "text": "markov decision processes"}, {"st": 158, "ed": 160, "text": "finite state"}]
[{"st": 8, "ed": 10, "text": "convex function"}, {"st": 13, "ed": 15, "text": "convex set"}, {"st": 106, "ed": 108, "text": "sqrt t"}, {"st": 109, "ed": 111, "text": "convex functions"}, {"st": 116, "ed": 118, "text": "strongly convex"}, {"st": 120, "ed": 122, "text": "special cases"}, {"st": 162, "ed": 164, "text": "frac 1"}, {"st": 173, "ed": 175, "text": "convex optimization"}, {"st": 180, "ed": 182, "text": "active learning"}]
[{"st": 8, "ed": 11, "text": "functional magnetic resonance"}, {"st": 21, "ed": 24, "text": "general linear model"}, {"st": 32, "ed": 34, "text": "supervised learning"}, {"st": 66, "ed": 68, "text": "prior knowledge"}, {"st": 90, "ed": 92, "text": "non linearities"}, {"st": 101, "ed": 103, "text": "least square"}, {"st": 115, "ed": 117, "text": "linear models"}]
[{"st": 9, "ed": 11, "text": "supervised learning"}, {"st": 17, "ed": 19, "text": "semi supervised"}, {"st": 35, "ed": 37, "text": "input space"}]
[{"st": 8, "ed": 10, "text": "machine learning"}, {"st": 24, "ed": 26, "text": "maximum entropy"}, {"st": 40, "ed": 42, "text": "mutual information"}, {"st": 59, "ed": 61, "text": "mutual information"}, {"st": 64, "ed": 66, "text": "maximum entropy"}, {"st": 87, "ed": 89, "text": "generalization bounds"}, {"st": 92, "ed": 94, "text": "iterative algorithms"}, {"st": 122, "ed": 124, "text": "maximum entropy"}]
[{"st": 7, "ed": 9, "text": "algebraic geometry"}, {"st": 73, "ed": 75, "text": "hidden variables"}]
[{"st": 20, "ed": 22, "text": "linear regression"}]
[{"st": 4, "ed": 6, "text": "principal components"}, {"st": 7, "ed": 9, "text": "discrete data"}, {"st": 24, "ed": 27, "text": "latent semantic analysis"}, {"st": 96, "ed": 98, "text": "prediction task"}, {"st": 99, "ed": 102, "text": "support vector machines"}]
[{"st": 10, "ed": 14, "text": "reproducing kernel hilbert spaces"}, {"st": 18, "ed": 20, "text": "gaussian process"}, {"st": 30, "ed": 33, "text": "undirected graphical models"}, {"st": 48, "ed": 50, "text": "optimization problem"}]
[{"st": 1, "ed": 3, "text": "recent years"}, {"st": 23, "ed": 25, "text": "computationally expensive"}, {"st": 55, "ed": 57, "text": "search algorithm"}, {"st": 73, "ed": 75, "text": "hidden variables"}, {"st": 77, "ed": 79, "text": "network structure"}, {"st": 100, "ed": 102, "text": "hidden variables"}]
[{"st": 3, "ed": 6, "text": "undirected graphical models"}, {"st": 7, "ed": 9, "text": "posterior distributions"}, {"st": 22, "ed": 24, "text": "undirected models"}, {"st": 29, "ed": 33, "text": "markov chain monte carlo"}, {"st": 59, "ed": 61, "text": "posterior distributions"}, {"st": 90, "ed": 93, "text": "coronary heart disease"}, {"st": 111, "ed": 113, "text": "sampling scheme"}, {"st": 120, "ed": 122, "text": "mean field"}, {"st": 149, "ed": 152, "text": "markov random field"}]
[{"st": 8, "ed": 10, "text": "labeled data"}, {"st": 20, "ed": 22, "text": "active learning"}, {"st": 33, "ed": 35, "text": "training sample"}, {"st": 72, "ed": 74, "text": "pure exploration"}, {"st": 77, "ed": 79, "text": "active learning"}, {"st": 93, "ed": 95, "text": "fixed budget"}, {"st": 176, "ed": 178, "text": "round robin"}, {"st": 198, "ed": 200, "text": "empirical performance"}, {"st": 213, "ed": 215, "text": "significantly outperforms"}]
[{"st": 43, "ed": 45, "text": "special case"}, {"st": 64, "ed": 66, "text": "fisher information"}, {"st": 79, "ed": 81, "text": "logistic regression"}]
[{"st": 9, "ed": 11, "text": "time series"}, {"st": 53, "ed": 55, "text": "learning algorithms"}, {"st": 129, "ed": 132, "text": "structure and parameters"}]
[{"st": 19, "ed": 21, "text": "machine learning"}, {"st": 24, "ed": 26, "text": "training data"}, {"st": 34, "ed": 36, "text": "prior knowledge"}]
[{"st": 2, "ed": 4, "text": "dynamical systems"}, {"st": 29, "ed": 32, "text": "hidden markov models"}, {"st": 34, "ed": 36, "text": "dynamical systems"}, {"st": 79, "ed": 81, "text": "hidden states"}, {"st": 88, "ed": 90, "text": "higher level"}, {"st": 94, "ed": 97, "text": "hidden markov models"}, {"st": 132, "ed": 135, "text": "inference and learning"}, {"st": 144, "ed": 146, "text": "mean field"}, {"st": 170, "ed": 172, "text": "american football"}]
[{"st": 33, "ed": 36, "text": "number of clusters"}, {"st": 64, "ed": 66, "text": "similarity matrix"}, {"st": 95, "ed": 97, "text": "distance measure"}, {"st": 100, "ed": 102, "text": "principal components"}, {"st": 109, "ed": 111, "text": "principal components"}, {"st": 124, "ed": 126, "text": "fuzzy clustering"}]
[{"st": 2, "ed": 4, "text": "variational inference"}, {"st": 14, "ed": 16, "text": "variational approximations"}, {"st": 51, "ed": 53, "text": "mean field"}, {"st": 94, "ed": 96, "text": "empirical results"}, {"st": 109, "ed": 111, "text": "clustering algorithm"}]
[{"st": 141, "ed": 143, "text": "models including"}]
[{"st": 0, "ed": 2, "text": "recent research"}, {"st": 4, "ed": 6, "text": "significant progress"}, {"st": 11, "ed": 13, "text": "log partition"}, {"st": 15, "ed": 17, "text": "exponential family"}, {"st": 34, "ed": 36, "text": "marginal probabilities"}, {"st": 51, "ed": 53, "text": "marginal probabilities"}, {"st": 109, "ed": 111, "text": "convex optimization"}, {"st": 113, "ed": 115, "text": "optimization problems"}, {"st": 121, "ed": 123, "text": "log partition"}]
[{"st": 5, "ed": 7, "text": "ell 1"}, {"st": 8, "ed": 10, "text": "multi task"}, {"st": 10, "ed": 12, "text": "structure learning"}, {"st": 55, "ed": 57, "text": "statistical efficiency"}, {"st": 58, "ed": 61, "text": "multi task learning"}, {"st": 72, "ed": 75, "text": "block coordinate descent"}, {"st": 85, "ed": 87, "text": "positive definite"}, {"st": 93, "ed": 95, "text": "synthetic data"}, {"st": 102, "ed": 105, "text": "real world data"}, {"st": 107, "ed": 111, "text": "functional magnetic resonance imaging"}]
[{"st": 5, "ed": 7, "text": "statistical physics"}, {"st": 8, "ed": 10, "text": "computer science"}, {"st": 17, "ed": 19, "text": "marginal probabilities"}, {"st": 24, "ed": 26, "text": "belief propagation"}, {"st": 28, "ed": 30, "text": "message passing"}, {"st": 66, "ed": 68, "text": "fixed points"}, {"st": 81, "ed": 84, "text": "sufficient condition for"}, {"st": 88, "ed": 90, "text": "belief propagation"}, {"st": 90, "ed": 92, "text": "fixed point"}, {"st": 114, "ed": 116, "text": "belief propagation"}]
[{"st": 18, "ed": 20, "text": "machine learning"}, {"st": 28, "ed": 30, "text": "edinburgh scotland"}]
[{"st": 11, "ed": 14, "text": "problem of classifying"}, {"st": 14, "ed": 16, "text": "d dimensional"}, {"st": 75, "ed": 77, "text": "majority rule"}, {"st": 102, "ed": 104, "text": "real data"}, {"st": 127, "ed": 129, "text": "error rates"}]
[{"st": 50, "ed": 52, "text": "regret bounds"}, {"st": 91, "ed": 93, "text": "numerical experiments"}]
[{"st": 1, "ed": 3, "text": "metric learning"}, {"st": 4, "ed": 6, "text": "similarity learning"}, {"st": 40, "ed": 42, "text": "generalization bounds"}, {"st": 79, "ed": 81, "text": "generalization bounds"}, {"st": 83, "ed": 85, "text": "similarity learning"}, {"st": 102, "ed": 104, "text": "similarity learning"}, {"st": 105, "ed": 108, "text": "l 1 norm"}, {"st": 134, "ed": 136, "text": "rademacher complexity"}]
[{"st": 15, "ed": 17, "text": "basis functions"}, {"st": 26, "ed": 28, "text": "policy evaluation"}, {"st": 33, "ed": 35, "text": "convergence rate"}, {"st": 51, "ed": 53, "text": "random projections"}, {"st": 63, "ed": 65, "text": "finite sample"}, {"st": 68, "ed": 70, "text": "proposed method"}, {"st": 90, "ed": 92, "text": "empirical results"}]
[{"st": 7, "ed": 9, "text": "online learning"}, {"st": 52, "ed": 54, "text": "real world"}, {"st": 60, "ed": 62, "text": "non stationary"}, {"st": 86, "ed": 88, "text": "non stationary"}, {"st": 105, "ed": 107, "text": "min max"}, {"st": 119, "ed": 121, "text": "worst case"}, {"st": 141, "ed": 143, "text": "linear functions"}]
[{"st": 10, "ed": 12, "text": "anomaly detection"}, {"st": 29, "ed": 32, "text": "support vector machines"}, {"st": 59, "ed": 61, "text": "kernel density"}, {"st": 65, "ed": 67, "text": "input space"}, {"st": 77, "ed": 79, "text": "large margin"}, {"st": 81, "ed": 83, "text": "kernel density"}, {"st": 110, "ed": 114, "text": "sloan digital sky survey"}, {"st": 118, "ed": 120, "text": "particle physics"}, {"st": 126, "ed": 128, "text": "proposed framework"}, {"st": 129, "ed": 131, "text": "real world"}]
[{"st": 0, "ed": 2, "text": "decision tree"}, {"st": 8, "ed": 11, "text": "classification and regression"}, {"st": 12, "ed": 14, "text": "machine learning"}, {"st": 22, "ed": 24, "text": "prior distribution"}, {"st": 25, "ed": 27, "text": "decision trees"}, {"st": 31, "ed": 33, "text": "posterior inference"}, {"st": 44, "ed": 46, "text": "decision tree"}, {"st": 46, "ed": 48, "text": "learning algorithms"}, {"st": 68, "ed": 70, "text": "posterior distribution"}, {"st": 81, "ed": 83, "text": "monte carlo"}, {"st": 92, "ed": 96, "text": "markov chain monte carlo"}, {"st": 102, "ed": 104, "text": "monte carlo"}, {"st": 123, "ed": 125, "text": "demonstrate empirically"}]
[{"st": 39, "ed": 41, "text": "low dimensional"}, {"st": 41, "ed": 43, "text": "linear subspace"}, {"st": 56, "ed": 58, "text": "dimensionality reduction"}, {"st": 61, "ed": 63, "text": "posterior distribution"}, {"st": 76, "ed": 79, "text": "orders of magnitude"}, {"st": 101, "ed": 103, "text": "random projection"}, {"st": 123, "ed": 125, "text": "convergence rates"}, {"st": 147, "ed": 149, "text": "real data"}]
[{"st": 9, "ed": 11, "text": "machine learning"}, {"st": 12, "ed": 14, "text": "signal processing"}, {"st": 17, "ed": 20, "text": "support vector machine"}, {"st": 35, "ed": 37, "text": "optimization problems"}, {"st": 48, "ed": 50, "text": "ell 2"}, {"st": 51, "ed": 53, "text": "soft margin"}, {"st": 65, "ed": 67, "text": "optimal solutions"}, {"st": 75, "ed": 77, "text": "optimization algorithms"}, {"st": 99, "ed": 101, "text": "theoretical insights"}, {"st": 148, "ed": 150, "text": "support vectors"}]
[{"st": 2, "ed": 4, "text": "real world"}, {"st": 4, "ed": 6, "text": "classification problems"}, {"st": 9, "ed": 11, "text": "training examples"}, {"st": 39, "ed": 41, "text": "class label"}, {"st": 65, "ed": 67, "text": "class conditional"}, {"st": 89, "ed": 91, "text": "noise levels"}, {"st": 113, "ed": 115, "text": "class conditional"}, {"st": 143, "ed": 145, "text": "class conditional"}, {"st": 200, "ed": 203, "text": "rate of convergence"}, {"st": 217, "ed": 219, "text": "rule based"}, {"st": 220, "ed": 222, "text": "surrogate loss"}, {"st": 232, "ed": 234, "text": "classification problem"}]
[{"st": 11, "ed": 13, "text": "image segmentation"}, {"st": 17, "ed": 20, "text": "change point detection"}, {"st": 40, "ed": 43, "text": "change point detection"}, {"st": 43, "ed": 45, "text": "k means"}, {"st": 45, "ed": 47, "text": "spectral clustering"}, {"st": 51, "ed": 53, "text": "main goal"}, {"st": 95, "ed": 97, "text": "metric learning"}, {"st": 100, "ed": 102, "text": "large margin"}, {"st": 102, "ed": 104, "text": "structured prediction"}, {"st": 115, "ed": 117, "text": "convex optimization"}, {"st": 121, "ed": 123, "text": "solved efficiently"}, {"st": 137, "ed": 139, "text": "significantly improve"}, {"st": 149, "ed": 151, "text": "image segmentation"}]
[{"st": 0, "ed": 3, "text": "the paper presents"}, {"st": 8, "ed": 11, "text": "support vector regression"}, {"st": 14, "ed": 17, "text": "support vector machines"}, {"st": 36, "ed": 38, "text": "complex valued"}, {"st": 45, "ed": 47, "text": "complex data"}, {"st": 65, "ed": 67, "text": "complex data"}, {"st": 70, "ed": 72, "text": "feature space"}, {"st": 92, "ed": 94, "text": "recently developed"}, {"st": 99, "ed": 103, "text": "reproducing kernel hilbert spaces"}, {"st": 180, "ed": 182, "text": "proposed framework"}, {"st": 243, "ed": 245, "text": "significant computational"}, {"st": 246, "ed": 248, "text": "experiments demonstrate"}, {"st": 252, "ed": 254, "text": "proposed framework"}, {"st": 257, "ed": 259, "text": "classification tasks"}]
[{"st": 12, "ed": 14, "text": "quality assessment"}, {"st": 33, "ed": 35, "text": "pair wise"}]
[{"st": 8, "ed": 10, "text": "monte carlo"}, {"st": 14, "ed": 16, "text": "monte carlo"}, {"st": 22, "ed": 24, "text": "bayes optimal"}, {"st": 99, "ed": 101, "text": "significant computational"}]
[{"st": 6, "ed": 9, "text": "markov decision processes"}, {"st": 10, "ed": 12, "text": "finite state"}, {"st": 18, "ed": 20, "text": "probability distributions"}, {"st": 21, "ed": 23, "text": "loss functions"}, {"st": 51, "ed": 53, "text": "square root"}, {"st": 63, "ed": 65, "text": "transition probabilities"}]
[{"st": 7, "ed": 9, "text": "decision making"}, {"st": 35, "ed": 37, "text": "supervised setting"}, {"st": 46, "ed": 48, "text": "labeled data"}, {"st": 102, "ed": 104, "text": "conditional independence"}, {"st": 149, "ed": 151, "text": "linear approximation"}, {"st": 153, "ed": 156, "text": "maximum likelihood estimator"}, {"st": 177, "ed": 180, "text": "simulated and real"}, {"st": 185, "ed": 187, "text": "higher accuracy"}, {"st": 198, "ed": 200, "text": "starting point"}, {"st": 201, "ed": 203, "text": "majority voting"}, {"st": 206, "ed": 208, "text": "maximum likelihood"}]
[{"st": 4, "ed": 6, "text": "topic modeling"}, {"st": 46, "ed": 48, "text": "highly efficient"}, {"st": 54, "ed": 56, "text": "random projections"}, {"st": 83, "ed": 85, "text": "mild assumptions"}, {"st": 94, "ed": 96, "text": "key insight"}, {"st": 120, "ed": 122, "text": "sample complexity"}, {"st": 134, "ed": 136, "text": "computational complexity"}, {"st": 138, "ed": 140, "text": "random projection"}, {"st": 141, "ed": 143, "text": "scales linearly"}, {"st": 158, "ed": 164, "text": "experiments on synthetic and real world"}, {"st": 167, "ed": 170, "text": "qualitative and quantitative"}]
[{"st": 12, "ed": 14, "text": "maximum margin"}, {"st": 18, "ed": 20, "text": "step size"}, {"st": 32, "ed": 34, "text": "step size"}, {"st": 64, "ed": 66, "text": "step sizes"}, {"st": 83, "ed": 85, "text": "exponential loss"}]
[{"st": 6, "ed": 8, "text": "inverse covariance"}, {"st": 14, "ed": 16, "text": "precision matrix"}, {"st": 34, "ed": 36, "text": "computationally intensive"}, {"st": 39, "ed": 41, "text": "approximate inference"}, {"st": 43, "ed": 45, "text": "message passing"}, {"st": 76, "ed": 78, "text": "marginal likelihood"}, {"st": 84, "ed": 86, "text": "parameter estimates"}, {"st": 130, "ed": 132, "text": "message passing"}, {"st": 135, "ed": 137, "text": "proposed algorithm"}, {"st": 141, "ed": 143, "text": "computationally efficient"}, {"st": 187, "ed": 189, "text": "local neighborhood"}, {"st": 206, "ed": 208, "text": "convergence rate"}, {"st": 222, "ed": 224, "text": "maximum likelihood"}, {"st": 225, "ed": 227, "text": "extensive numerical"}, {"st": 227, "ed": 229, "text": "experiments demonstrate"}, {"st": 230, "ed": 232, "text": "improved performance"}, {"st": 251, "ed": 254, "text": "maximum likelihood estimator"}]
[{"st": 13, "ed": 15, "text": "supervised learning"}, {"st": 43, "ed": 45, "text": "recent results"}]
[{"st": 41, "ed": 43, "text": "convex function"}, {"st": 55, "ed": 57, "text": "group lasso"}, {"st": 60, "ed": 63, "text": "multi task learning"}, {"st": 101, "ed": 104, "text": "support vector machines"}]
[{"st": 11, "ed": 13, "text": "smooth function"}, {"st": 18, "ed": 20, "text": "point wise"}, {"st": 36, "ed": 38, "text": "fixed budget"}, {"st": 38, "ed": 41, "text": "best arm identification"}, {"st": 43, "ed": 46, "text": "multi armed bandit"}, {"st": 68, "ed": 70, "text": "bayesian optimization"}, {"st": 98, "ed": 101, "text": "number of arms"}, {"st": 125, "ed": 127, "text": "practical applications"}, {"st": 130, "ed": 132, "text": "machine learning"}, {"st": 133, "ed": 136, "text": "the paper presents"}, {"st": 140, "ed": 142, "text": "proposed approach"}, {"st": 142, "ed": 144, "text": "thompson sampling"}, {"st": 145, "ed": 147, "text": "bayesian optimization"}, {"st": 158, "ed": 161, "text": "best arm identification"}]
[{"st": 11, "ed": 13, "text": "data fusion"}, {"st": 38, "ed": 40, "text": "data fusion"}]
[{"st": 18, "ed": 20, "text": "large scale"}, {"st": 20, "ed": 22, "text": "l1 regularized"}, {"st": 32, "ed": 34, "text": "convergence rate"}, {"st": 41, "ed": 43, "text": "low rank"}, {"st": 46, "ed": 48, "text": "quasi newton"}, {"st": 59, "ed": 61, "text": "active set"}, {"st": 76, "ed": 78, "text": "working set"}, {"st": 108, "ed": 110, "text": "highly competitive"}, {"st": 112, "ed": 114, "text": "recently proposed"}, {"st": 122, "ed": 124, "text": "logistic regression"}, {"st": 126, "ed": 128, "text": "inverse covariance"}]
[{"st": 11, "ed": 13, "text": "reinforcement learning"}, {"st": 14, "ed": 16, "text": "approximate bayesian"}, {"st": 28, "ed": 30, "text": "prior distribution"}, {"st": 35, "ed": 37, "text": "generative models"}, {"st": 46, "ed": 48, "text": "probabilistic model"}, {"st": 72, "ed": 74, "text": "reinforcement learning"}, {"st": 109, "ed": 111, "text": "experimentally demonstrate"}, {"st": 138, "ed": 140, "text": "sufficient statistics"}]
[{"st": 1, "ed": 3, "text": "machine learning"}, {"st": 7, "ed": 9, "text": "learning algorithm"}, {"st": 19, "ed": 21, "text": "performance metric"}, {"st": 48, "ed": 50, "text": "performance metric"}, {"st": 55, "ed": 57, "text": "supervised learning"}, {"st": 64, "ed": 66, "text": "empirical analysis"}, {"st": 82, "ed": 84, "text": "classification accuracy"}]
[{"st": 8, "ed": 10, "text": "recovery problem"}, {"st": 24, "ed": 27, "text": "constrained optimization problem"}, {"st": 29, "ed": 31, "text": "least squares"}, {"st": 33, "ed": 35, "text": "maximum likelihood"}, {"st": 56, "ed": 59, "text": "constrained optimization problem"}, {"st": 63, "ed": 65, "text": "convex relaxation"}, {"st": 80, "ed": 82, "text": "proposed approach"}, {"st": 86, "ed": 88, "text": "synthetic data"}]
[{"st": 4, "ed": 6, "text": "pattern classification"}, {"st": 16, "ed": 18, "text": "nearest neighbor"}, {"st": 73, "ed": 75, "text": "positive integer"}, {"st": 76, "ed": 78, "text": "m 1"}, {"st": 96, "ed": 98, "text": "prediction error"}, {"st": 100, "ed": 102, "text": "nearest neighbor"}, {"st": 152, "ed": 154, "text": "recent years"}, {"st": 194, "ed": 196, "text": "nearest neighbor"}, {"st": 220, "ed": 222, "text": "recent results"}, {"st": 242, "ed": 244, "text": "nearest neighbor"}]
[{"st": 8, "ed": 10, "text": "multi class"}, {"st": 14, "ed": 16, "text": "generalization performance"}, {"st": 30, "ed": 32, "text": "existing algorithms"}, {"st": 35, "ed": 38, "text": "directed acyclic graph"}, {"st": 51, "ed": 53, "text": "previous approaches"}, {"st": 71, "ed": 73, "text": "support vectors"}, {"st": 95, "ed": 97, "text": "generalization ability"}, {"st": 100, "ed": 102, "text": "cross validation"}, {"st": 121, "ed": 123, "text": "performance measure"}, {"st": 144, "ed": 147, "text": "directed acyclic graph"}, {"st": 174, "ed": 176, "text": "significantly higher"}, {"st": 213, "ed": 215, "text": "times faster"}]
[{"st": 0, "ed": 4, "text": "restricted boltzmann machines rbms"}, {"st": 5, "ed": 7, "text": "generative models"}, {"st": 28, "ed": 31, "text": "unsupervised pre training"}, {"st": 41, "ed": 43, "text": "time series"}, {"st": 58, "ed": 60, "text": "temporal dependencies"}, {"st": 62, "ed": 64, "text": "hidden layer"}, {"st": 122, "ed": 124, "text": "denoising autoencoders"}, {"st": 159, "ed": 161, "text": "significant improvement"}, {"st": 179, "ed": 181, "text": "error reduction"}, {"st": 182, "ed": 184, "text": "motion capture"}, {"st": 222, "ed": 224, "text": "motion capture"}, {"st": 257, "ed": 259, "text": "prediction error"}, {"st": 285, "ed": 287, "text": "observed data"}]
[{"st": 0, "ed": 4, "text": "multiple instance learning mil"}, {"st": 17, "ed": 19, "text": "instance labels"}, {"st": 24, "ed": 26, "text": "supervised learning"}, {"st": 37, "ed": 39, "text": "additional assumptions"}, {"st": 108, "ed": 110, "text": "training set"}, {"st": 145, "ed": 147, "text": "proposed approach"}]
[{"st": 5, "ed": 7, "text": "collaborative filtering"}, {"st": 26, "ed": 28, "text": "generative model"}, {"st": 39, "ed": 41, "text": "xbox live"}, {"st": 43, "ed": 45, "text": "unlike previous"}, {"st": 68, "ed": 70, "text": "random graph"}, {"st": 81, "ed": 83, "text": "large scale"}, {"st": 92, "ed": 95, "text": "stochastic gradient descent"}, {"st": 96, "ed": 98, "text": "mean field"}, {"st": 98, "ed": 100, "text": "variational inference"}, {"st": 101, "ed": 103, "text": "random graph"}, {"st": 105, "ed": 107, "text": "fine grained"}]
[{"st": 9, "ed": 11, "text": "generative modeling"}, {"st": 13, "ed": 17, "text": "multiple instance learning mil"}, {"st": 23, "ed": 25, "text": "training instances"}, {"st": 62, "ed": 64, "text": "motor unit"}, {"st": 99, "ed": 101, "text": "generative models"}]
[{"st": 0, "ed": 2, "text": "graph based"}, {"st": 5, "ed": 7, "text": "powerful tool"}, {"st": 21, "ed": 23, "text": "computational complexity"}, {"st": 44, "ed": 46, "text": "large scale"}, {"st": 75, "ed": 77, "text": "recently introduced"}, {"st": 120, "ed": 122, "text": "bregman divergences"}, {"st": 138, "ed": 140, "text": "bregman divergence"}, {"st": 158, "ed": 160, "text": "significantly improve"}, {"st": 169, "ed": 171, "text": "proposed framework"}, {"st": 173, "ed": 175, "text": "text categorization"}]
[{"st": 6, "ed": 8, "text": "powerful tools"}, {"st": 23, "ed": 25, "text": "approximate inference"}, {"st": 39, "ed": 41, "text": "hinge loss"}, {"st": 41, "ed": 44, "text": "markov random fields"}, {"st": 44, "ed": 46, "text": "hl mrfs"}, {"st": 58, "ed": 60, "text": "continuous variables"}, {"st": 71, "ed": 73, "text": "hl mrfs"}, {"st": 77, "ed": 80, "text": "fast and accurate"}, {"st": 86, "ed": 88, "text": "inference algorithm"}, {"st": 99, "ed": 101, "text": "hl mrfs"}, {"st": 106, "ed": 108, "text": "hl mrfs"}, {"st": 116, "ed": 118, "text": "hl mrfs"}, {"st": 122, "ed": 124, "text": "predictive performance"}]
[{"st": 4, "ed": 7, "text": "multi task learning"}, {"st": 8, "ed": 11, "text": "attracted much attention"}, {"st": 20, "ed": 22, "text": "group lasso"}, {"st": 58, "ed": 61, "text": "multi task learning"}, {"st": 70, "ed": 72, "text": "convex relaxation"}, {"st": 104, "ed": 106, "text": "ridge regression"}, {"st": 127, "ed": 129, "text": "approach produces"}, {"st": 130, "ed": 132, "text": "asymptotically optimal"}, {"st": 136, "ed": 138, "text": "multitask learning"}, {"st": 158, "ed": 160, "text": "significantly outperforms"}, {"st": 160, "ed": 162, "text": "group lasso"}, {"st": 164, "ed": 166, "text": "multi stage"}]
[{"st": 32, "ed": 34, "text": "base learner"}, {"st": 47, "ed": 49, "text": "empirical evaluation"}, {"st": 78, "ed": 80, "text": "boosting algorithm"}]
[{"st": 0, "ed": 2, "text": "predictive state"}, {"st": 54, "ed": 56, "text": "learning algorithms"}, {"st": 106, "ed": 109, "text": "a reproducing kernel"}, {"st": 117, "ed": 119, "text": "kernel methods"}, {"st": 147, "ed": 149, "text": "learned models"}]
[{"st": 6, "ed": 8, "text": "reinforcement learning"}, {"st": 22, "ed": 24, "text": "empirical evidence"}, {"st": 27, "ed": 29, "text": "improve performance"}, {"st": 31, "ed": 33, "text": "reinforcement learning"}, {"st": 48, "ed": 50, "text": "multi task"}, {"st": 55, "ed": 57, "text": "reinforcement learning"}, {"st": 66, "ed": 68, "text": "unknown distribution"}, {"st": 70, "ed": 72, "text": "finite set"}, {"st": 73, "ed": 76, "text": "markov decision processes"}, {"st": 93, "ed": 95, "text": "sample complexity"}, {"st": 110, "ed": 112, "text": "multi task"}, {"st": 125, "ed": 127, "text": "negative transfer"}, {"st": 129, "ed": 131, "text": "worst case"}, {"st": 134, "ed": 136, "text": "sample complexity"}]
[{"st": 2, "ed": 4, "text": "convex relaxations"}, {"st": 10, "ed": 13, "text": "the past decade"}, {"st": 21, "ed": 23, "text": "discriminative models"}, {"st": 39, "ed": 41, "text": "convex relaxations"}, {"st": 51, "ed": 53, "text": "bregman divergence"}, {"st": 79, "ed": 81, "text": "optimization methods"}]
[{"st": 6, "ed": 9, "text": "multi armed bandit"}, {"st": 12, "ed": 14, "text": "active learning"}, {"st": 27, "ed": 29, "text": "active learning"}, {"st": 30, "ed": 33, "text": "multi armed bandits"}, {"st": 39, "ed": 41, "text": "confidence bounds"}, {"st": 47, "ed": 50, "text": "multi armed bandit"}, {"st": 68, "ed": 70, "text": "sampling distribution"}, {"st": 94, "ed": 96, "text": "sampling distribution"}, {"st": 103, "ed": 105, "text": "active learning"}, {"st": 115, "ed": 117, "text": "confidence bounds"}, {"st": 124, "ed": 126, "text": "previously proposed"}, {"st": 126, "ed": 128, "text": "active learning"}]
[{"st": 0, "ed": 2, "text": "matching pursuit"}, {"st": 9, "ed": 11, "text": "feature construction"}, {"st": 28, "ed": 30, "text": "expert knowledge"}, {"st": 79, "ed": 81, "text": "policy evaluation"}]
[{"st": 6, "ed": 10, "text": "multiple instance learning mil"}, {"st": 48, "ed": 50, "text": "weakly supervised"}, {"st": 59, "ed": 62, "text": "max margin learning"}, {"st": 64, "ed": 66, "text": "efficient inference"}, {"st": 74, "ed": 76, "text": "proposed framework"}]
[{"st": 13, "ed": 15, "text": "discrete variables"}, {"st": 21, "ed": 23, "text": "previous approaches"}, {"st": 28, "ed": 30, "text": "expectation maximization"}, {"st": 43, "ed": 45, "text": "monte carlo"}, {"st": 66, "ed": 69, "text": "method of moments"}, {"st": 143, "ed": 145, "text": "training data"}, {"st": 145, "ed": 147, "text": "ground truth"}]
[{"st": 2, "ed": 5, "text": "stochastic variational inference"}, {"st": 6, "ed": 8, "text": "gaussian process"}, {"st": 14, "ed": 17, "text": "gaussian process gp"}, {"st": 70, "ed": 72, "text": "latent variable"}]
[{"st": 2, "ed": 4, "text": "maximum likelihood"}, {"st": 7, "ed": 10, "text": "gaussian graphical models"}, {"st": 13, "ed": 15, "text": "ell 2"}, {"st": 29, "ed": 31, "text": "ell 1"}, {"st": 39, "ed": 41, "text": "optimization problem"}, {"st": 50, "ed": 52, "text": "closed form"}, {"st": 61, "ed": 64, "text": "singular value decomposition"}, {"st": 113, "ed": 115, "text": "regularization parameter"}, {"st": 129, "ed": 131, "text": "sample complexity"}, {"st": 169, "ed": 171, "text": "promising results"}]
[{"st": 10, "ed": 12, "text": "current methods"}, {"st": 26, "ed": 28, "text": "variational inference"}, {"st": 55, "ed": 57, "text": "multitask learning"}, {"st": 67, "ed": 69, "text": "parameter estimation"}, {"st": 75, "ed": 77, "text": "conditional independence"}, {"st": 93, "ed": 95, "text": "functional neuroimaging"}, {"st": 125, "ed": 127, "text": "jointly learns"}, {"st": 128, "ed": 130, "text": "weight matrix"}, {"st": 133, "ed": 135, "text": "inverse covariance"}, {"st": 146, "ed": 148, "text": "proposed approach"}, {"st": 149, "ed": 151, "text": "strong baseline"}, {"st": 155, "ed": 157, "text": "predictive performance"}]
[{"st": 0, "ed": 2, "text": "structured prediction"}, {"st": 61, "ed": 63, "text": "predictive models"}, {"st": 64, "ed": 66, "text": "achieve high"}, {"st": 87, "ed": 89, "text": "np hard"}, {"st": 140, "ed": 142, "text": "prediction accuracy"}, {"st": 146, "ed": 148, "text": "fully connected"}]
[{"st": 3, "ed": 5, "text": "probabilistic model"}, {"st": 8, "ed": 10, "text": "latent variables"}, {"st": 27, "ed": 29, "text": "latent variables"}, {"st": 33, "ed": 35, "text": "latent variables"}, {"st": 69, "ed": 71, "text": "latent variable"}, {"st": 118, "ed": 120, "text": "latent variables"}, {"st": 127, "ed": 129, "text": "nearest neighbour"}, {"st": 143, "ed": 145, "text": "hash codes"}, {"st": 155, "ed": 157, "text": "hash codes"}, {"st": 166, "ed": 168, "text": "latent feature"}]
[{"st": 0, "ed": 2, "text": "semi supervised"}, {"st": 23, "ed": 26, "text": "number of clusters"}, {"st": 41, "ed": 44, "text": "dirichlet process mixture"}, {"st": 52, "ed": 55, "text": "number of clusters"}, {"st": 81, "ed": 83, "text": "kernel based"}, {"st": 94, "ed": 97, "text": "number of clusters"}, {"st": 113, "ed": 115, "text": "key insight"}, {"st": 123, "ed": 125, "text": "kernel matrix"}, {"st": 140, "ed": 142, "text": "theoretical properties"}, {"st": 164, "ed": 168, "text": "synthetic and real world"}]
[{"st": 9, "ed": 12, "text": "markov decision process"}, {"st": 23, "ed": 25, "text": "basis functions"}, {"st": 48, "ed": 50, "text": "kalman filter"}, {"st": 62, "ed": 64, "text": "kalman filter"}, {"st": 79, "ed": 81, "text": "standard benchmark"}]
[{"st": 11, "ed": 13, "text": "finite set"}, {"st": 55, "ed": 57, "text": "expected reward"}, {"st": 60, "ed": 62, "text": "linear function"}, {"st": 69, "ed": 73, "text": "reproducing kernel hilbert space"}, {"st": 80, "ed": 82, "text": "ucb algorithm"}, {"st": 85, "ed": 87, "text": "cumulative regret"}, {"st": 93, "ed": 95, "text": "contextual bandits"}, {"st": 105, "ed": 107, "text": "special case"}, {"st": 117, "ed": 119, "text": "regret bound"}, {"st": 149, "ed": 151, "text": "regret bound"}]
[{"st": 2, "ed": 5, "text": "with expert advice"}, {"st": 15, "ed": 17, "text": "class labels"}, {"st": 29, "ed": 31, "text": "real applications"}, {"st": 51, "ed": 53, "text": "active learning"}, {"st": 53, "ed": 56, "text": "with expert advice"}, {"st": 79, "ed": 81, "text": "accurate prediction"}, {"st": 107, "ed": 109, "text": "active learning"}, {"st": 109, "ed": 112, "text": "with expert advice"}, {"st": 121, "ed": 123, "text": "weighted average"}, {"st": 132, "ed": 134, "text": "active learning"}]
[{"st": 8, "ed": 10, "text": "generalization bounds"}, {"st": 12, "ed": 14, "text": "learning process"}, {"st": 22, "ed": 24, "text": "generalization bounds"}, {"st": 27, "ed": 30, "text": "rate of convergence"}, {"st": 49, "ed": 51, "text": "learning process"}, {"st": 54, "ed": 56, "text": "generalization bounds"}, {"st": 88, "ed": 90, "text": "generalization bounds"}, {"st": 138, "ed": 140, "text": "empirical risk"}, {"st": 160, "ed": 162, "text": "learning process"}]
[{"st": 25, "ed": 27, "text": "recent years"}, {"st": 54, "ed": 57, "text": "gaussian graphical models"}, {"st": 71, "ed": 73, "text": "gibbs sampler"}, {"st": 79, "ed": 81, "text": "significant computational"}, {"st": 97, "ed": 99, "text": "monte carlo"}, {"st": 109, "ed": 111, "text": "fully bayesian"}, {"st": 141, "ed": 143, "text": "simulated data"}, {"st": 146, "ed": 148, "text": "real life"}]
[{"st": 2, "ed": 4, "text": "nearest neighbor"}, {"st": 19, "ed": 21, "text": "nearest neighbor"}, {"st": 64, "ed": 66, "text": "decision rule"}, {"st": 80, "ed": 82, "text": "training set"}]
[{"st": 0, "ed": 4, "text": "nonnegative matrix factorization nmf"}, {"st": 44, "ed": 46, "text": "source separation"}, {"st": 101, "ed": 103, "text": "least squares"}]
[{"st": 11, "ed": 13, "text": "spectral clustering"}, {"st": 21, "ed": 23, "text": "kernel based"}, {"st": 28, "ed": 30, "text": "least squares"}, {"st": 30, "ed": 33, "text": "support vector machine"}, {"st": 36, "ed": 38, "text": "spectral clustering"}, {"st": 42, "ed": 44, "text": "kernel pca"}, {"st": 62, "ed": 64, "text": "high dimensional"}, {"st": 88, "ed": 90, "text": "error correcting"}, {"st": 121, "ed": 123, "text": "tuning parameters"}, {"st": 125, "ed": 128, "text": "number of clusters"}, {"st": 140, "ed": 142, "text": "spectral clustering"}, {"st": 190, "ed": 192, "text": "cholesky decomposition"}, {"st": 194, "ed": 196, "text": "l 0"}, {"st": 198, "ed": 200, "text": "l 0"}, {"st": 202, "ed": 204, "text": "group lasso"}, {"st": 218, "ed": 220, "text": "large scale"}, {"st": 227, "ed": 229, "text": "hierarchical clustering"}, {"st": 232, "ed": 234, "text": "clustering method"}, {"st": 237, "ed": 239, "text": "real world"}, {"st": 242, "ed": 244, "text": "image segmentation"}, {"st": 246, "ed": 248, "text": "time series"}, {"st": 249, "ed": 251, "text": "document clustering"}, {"st": 252, "ed": 254, "text": "big data"}]
[{"st": 14, "ed": 16, "text": "bandit problems"}, {"st": 19, "ed": 21, "text": "online learning"}, {"st": 63, "ed": 65, "text": "expected reward"}, {"st": 216, "ed": 218, "text": "online learning"}]
[{"st": 3, "ed": 5, "text": "learning scheme"}, {"st": 24, "ed": 26, "text": "accurate prediction"}, {"st": 43, "ed": 45, "text": "convergence rate"}, {"st": 70, "ed": 72, "text": "convergence rate"}, {"st": 90, "ed": 92, "text": "convergence rate"}, {"st": 98, "ed": 101, "text": "a logarithmic factor"}, {"st": 117, "ed": 120, "text": "classification and regression"}, {"st": 125, "ed": 127, "text": "generalization error"}]
[{"st": 4, "ed": 6, "text": "theoretical analysis"}, {"st": 16, "ed": 18, "text": "decision theoretic"}, {"st": 27, "ed": 29, "text": "f measure"}, {"st": 45, "ed": 47, "text": "performance metrics"}, {"st": 62, "ed": 64, "text": "conditional probability"}, {"st": 83, "ed": 85, "text": "recent results"}, {"st": 102, "ed": 104, "text": "performance metric"}, {"st": 132, "ed": 134, "text": "computational requirements"}, {"st": 157, "ed": 159, "text": "empirical results"}, {"st": 162, "ed": 164, "text": "benchmark datasets"}, {"st": 172, "ed": 174, "text": "decision theoretic"}]
[{"st": 17, "ed": 19, "text": "prediction accuracy"}, {"st": 72, "ed": 74, "text": "unseen data"}, {"st": 142, "ed": 144, "text": "recently proposed"}, {"st": 174, "ed": 176, "text": "classification tasks"}, {"st": 177, "ed": 179, "text": "large scale"}, {"st": 179, "ed": 182, "text": "publicly available datasets"}]
[{"st": 19, "ed": 21, "text": "random variables"}, {"st": 29, "ed": 31, "text": "k 1"}, {"st": 70, "ed": 72, "text": "random variables"}, {"st": 168, "ed": 170, "text": "asymptotically optimal"}, {"st": 188, "ed": 190, "text": "finite horizon"}, {"st": 190, "ed": 192, "text": "regret bounds"}]
[{"st": 4, "ed": 6, "text": "estimation error"}, {"st": 26, "ed": 28, "text": "loss function"}, {"st": 41, "ed": 43, "text": "estimation error"}, {"st": 61, "ed": 63, "text": "estimation error"}, {"st": 79, "ed": 81, "text": "estimation error"}, {"st": 106, "ed": 109, "text": "convex loss functions"}, {"st": 110, "ed": 112, "text": "least squares"}, {"st": 123, "ed": 125, "text": "important role"}, {"st": 137, "ed": 139, "text": "sample complexity"}, {"st": 168, "ed": 170, "text": "sample complexity"}, {"st": 171, "ed": 173, "text": "estimation error"}]
[{"st": 1, "ed": 3, "text": "gaussian process"}, {"st": 3, "ed": 5, "text": "latent variable"}, {"st": 33, "ed": 36, "text": "spike and slab"}, {"st": 45, "ed": 47, "text": "variational inference"}, {"st": 64, "ed": 66, "text": "principled approach"}, {"st": 97, "ed": 99, "text": "multi view"}, {"st": 99, "ed": 101, "text": "gaussian processes"}, {"st": 113, "ed": 116, "text": "spike and slab"}, {"st": 121, "ed": 123, "text": "principled approach"}, {"st": 129, "ed": 131, "text": "latent space"}, {"st": 150, "ed": 152, "text": "cross modal"}]
[{"st": 18, "ed": 21, "text": "multi armed bandit"}, {"st": 28, "ed": 30, "text": "finite horizon"}, {"st": 32, "ed": 34, "text": "convergence rates"}, {"st": 123, "ed": 125, "text": "exploration exploitation"}]
[{"st": 5, "ed": 7, "text": "scoring function"}, {"st": 8, "ed": 10, "text": "structure learning"}, {"st": 16, "ed": 18, "text": "traditional approaches"}, {"st": 19, "ed": 21, "text": "score based"}, {"st": 21, "ed": 23, "text": "structure learning"}, {"st": 45, "ed": 47, "text": "conditional independence"}, {"st": 60, "ed": 62, "text": "scoring function"}, {"st": 87, "ed": 89, "text": "sample complexity"}, {"st": 112, "ed": 114, "text": "generating distribution"}, {"st": 139, "ed": 141, "text": "search algorithm"}, {"st": 151, "ed": 153, "text": "empirical results"}, {"st": 164, "ed": 166, "text": "linear programming"}, {"st": 184, "ed": 186, "text": "finite sample"}]
[{"st": 10, "ed": 12, "text": "training points"}, {"st": 41, "ed": 43, "text": "rademacher complexity"}, {"st": 98, "ed": 100, "text": "statistical learning"}, {"st": 102, "ed": 104, "text": "rademacher complexity"}, {"st": 106, "ed": 108, "text": "rademacher complexity"}]
[{"st": 4, "ed": 7, "text": "deep neural network"}, {"st": 12, "ed": 14, "text": "unknown distribution"}, {"st": 33, "ed": 35, "text": "generator network"}, {"st": 63, "ed": 66, "text": "maximum mean discrepancy"}, {"st": 107, "ed": 109, "text": "generator network"}, {"st": 142, "ed": 144, "text": "generalization error"}]
[{"st": 6, "ed": 8, "text": "class classification"}, {"st": 40, "ed": 42, "text": "class classification"}, {"st": 49, "ed": 51, "text": "main goal"}, {"st": 77, "ed": 79, "text": "hinge loss"}, {"st": 96, "ed": 98, "text": "frac 1"}, {"st": 106, "ed": 108, "text": "convex surrogate"}, {"st": 117, "ed": 119, "text": "frac 1"}, {"st": 125, "ed": 127, "text": "lower dimensional"}, {"st": 149, "ed": 151, "text": "frac 1"}]
[{"st": 42, "ed": 45, "text": "real world data"}, {"st": 48, "ed": 50, "text": "population genetics"}, {"st": 93, "ed": 95, "text": "subset selection"}, {"st": 97, "ed": 99, "text": "partially observed"}, {"st": 114, "ed": 116, "text": "computational efficiency"}, {"st": 116, "ed": 118, "text": "sample complexity"}, {"st": 131, "ed": 133, "text": "desired properties"}, {"st": 157, "ed": 160, "text": "low rank matrix"}, {"st": 180, "ed": 182, "text": "analysis shows"}, {"st": 188, "ed": 190, "text": "input data"}, {"st": 205, "ed": 207, "text": "low rank"}, {"st": 226, "ed": 228, "text": "relative error"}, {"st": 237, "ed": 239, "text": "matrix approximation"}, {"st": 245, "ed": 248, "text": "theoretical and empirical"}, {"st": 258, "ed": 260, "text": "random sampling"}, {"st": 264, "ed": 266, "text": "highly correlated"}]
[{"st": 4, "ed": 6, "text": "bandit problem"}, {"st": 52, "ed": 54, "text": "cumulative regret"}, {"st": 73, "ed": 75, "text": "cumulative regret"}, {"st": 79, "ed": 81, "text": "armed bandits"}, {"st": 85, "ed": 87, "text": "simple regret"}, {"st": 98, "ed": 100, "text": "near optimal"}, {"st": 143, "ed": 145, "text": "near optimal"}]
[{"st": 0, "ed": 4, "text": "nonnegative matrix factorization nmf"}, {"st": 64, "ed": 66, "text": "random projections"}, {"st": 139, "ed": 141, "text": "random projections"}]
[{"st": 0, "ed": 2, "text": "additive models"}, {"st": 23, "ed": 25, "text": "low dimensional"}, {"st": 52, "ed": 54, "text": "domain specific"}, {"st": 61, "ed": 64, "text": "large scale problems"}, {"st": 69, "ed": 71, "text": "related tasks"}, {"st": 73, "ed": 75, "text": "additive models"}, {"st": 87, "ed": 89, "text": "training data"}, {"st": 95, "ed": 98, "text": "multi task learning"}, {"st": 107, "ed": 109, "text": "additive models"}, {"st": 118, "ed": 120, "text": "key idea"}, {"st": 146, "ed": 149, "text": "sparse dictionary learning"}, {"st": 159, "ed": 161, "text": "sparse coding"}, {"st": 162, "ed": 164, "text": "transfer function"}, {"st": 175, "ed": 177, "text": "matching pursuit"}, {"st": 188, "ed": 190, "text": "existing results"}, {"st": 206, "ed": 209, "text": "real world data"}, {"st": 213, "ed": 216, "text": "compares favorably to"}, {"st": 216, "ed": 218, "text": "baseline methods"}, {"st": 236, "ed": 238, "text": "training data"}, {"st": 249, "ed": 251, "text": "additive models"}]
[{"st": 43, "ed": 45, "text": "theoretically sound"}, {"st": 60, "ed": 62, "text": "hierarchical models"}, {"st": 110, "ed": 112, "text": "regret bounds"}, {"st": 113, "ed": 115, "text": "log loss"}, {"st": 119, "ed": 121, "text": "hierarchical models"}, {"st": 141, "ed": 143, "text": "log loss"}, {"st": 143, "ed": 145, "text": "regret bound"}, {"st": 164, "ed": 166, "text": "regret bounds"}, {"st": 188, "ed": 190, "text": "feature selection"}]
[{"st": 0, "ed": 2, "text": "kernel matrices"}, {"st": 23, "ed": 25, "text": "large datasets"}, {"st": 32, "ed": 34, "text": "kernel matrices"}, {"st": 45, "ed": 47, "text": "sampling algorithm"}, {"st": 75, "ed": 77, "text": "kernel matrix"}, {"st": 84, "ed": 86, "text": "numerical experiments"}, {"st": 88, "ed": 93, "text": "synthetic and real world datasets"}, {"st": 105, "ed": 107, "text": "sampling methods"}]
[{"st": 7, "ed": 9, "text": "spectral methods"}, {"st": 48, "ed": 50, "text": "prior information"}, {"st": 73, "ed": 75, "text": "dynamical systems"}, {"st": 81, "ed": 83, "text": "supervised learning"}, {"st": 89, "ed": 91, "text": "prior knowledge"}, {"st": 100, "ed": 102, "text": "spectral methods"}, {"st": 103, "ed": 105, "text": "special cases"}, {"st": 110, "ed": 112, "text": "linear regression"}, {"st": 139, "ed": 141, "text": "linear regression"}]
[{"st": 20, "ed": 22, "text": "neural network"}, {"st": 39, "ed": 41, "text": "free energy"}, {"st": 59, "ed": 61, "text": "comparable performance"}, {"st": 84, "ed": 86, "text": "linear regression"}, {"st": 98, "ed": 100, "text": "exploration exploitation"}]
[{"st": 34, "ed": 37, "text": "multi armed bandit"}, {"st": 62, "ed": 64, "text": "bandit algorithms"}, {"st": 92, "ed": 94, "text": "epsilon greedy"}, {"st": 191, "ed": 194, "text": "exploration and exploitation"}, {"st": 206, "ed": 208, "text": "time series"}, {"st": 221, "ed": 223, "text": "contextual bandit"}]
[{"st": 23, "ed": 25, "text": "multitask learning"}, {"st": 37, "ed": 39, "text": "special case"}, {"st": 51, "ed": 53, "text": "representation learning"}, {"st": 77, "ed": 79, "text": "representation learning"}, {"st": 90, "ed": 92, "text": "sample size"}, {"st": 102, "ed": 104, "text": "potential applications"}, {"st": 109, "ed": 111, "text": "feature learning"}, {"st": 112, "ed": 116, "text": "reproducing kernel hilbert spaces"}]
[{"st": 0, "ed": 2, "text": "spectral clustering"}, {"st": 17, "ed": 19, "text": "based clustering"}, {"st": 20, "ed": 22, "text": "existing methods"}, {"st": 66, "ed": 68, "text": "continuous relaxation"}, {"st": 79, "ed": 81, "text": "recently proposed"}, {"st": 89, "ed": 91, "text": "poor performance"}, {"st": 99, "ed": 101, "text": "continuous relaxation"}, {"st": 112, "ed": 114, "text": "minimization problem"}, {"st": 123, "ed": 125, "text": "method outperforms"}, {"st": 126, "ed": 128, "text": "existing approaches"}]
[{"st": 4, "ed": 6, "text": "prior information"}, {"st": 25, "ed": 27, "text": "spectral clustering"}, {"st": 35, "ed": 37, "text": "recently proposed"}, {"st": 38, "ed": 40, "text": "spectral clustering"}, {"st": 59, "ed": 61, "text": "continuous optimization"}, {"st": 73, "ed": 75, "text": "spectral clustering"}, {"st": 103, "ed": 105, "text": "efficient implementation"}]
[{"st": 7, "ed": 9, "text": "machine learning"}, {"st": 15, "ed": 17, "text": "classification problems"}]
[{"st": 3, "ed": 6, "text": "change point detection"}, {"st": 27, "ed": 29, "text": "likelihood ratio"}, {"st": 45, "ed": 47, "text": "performance metrics"}, {"st": 64, "ed": 66, "text": "highly accurate"}, {"st": 83, "ed": 85, "text": "random projection"}, {"st": 94, "ed": 96, "text": "expander graph"}, {"st": 110, "ed": 112, "text": "real data"}]
[{"st": 1, "ed": 3, "text": "classification problems"}, {"st": 21, "ed": 23, "text": "performance measures"}, {"st": 45, "ed": 47, "text": "learning algorithms"}, {"st": 59, "ed": 61, "text": "large scale"}, {"st": 61, "ed": 63, "text": "optimization methods"}, {"st": 79, "ed": 81, "text": "performance measures"}, {"st": 109, "ed": 111, "text": "linear functions"}, {"st": 119, "ed": 121, "text": "performance measures"}, {"st": 123, "ed": 125, "text": "f measure"}, {"st": 145, "ed": 147, "text": "optimization techniques"}, {"st": 150, "ed": 152, "text": "point based"}, {"st": 156, "ed": 158, "text": "performance measures"}, {"st": 163, "ed": 165, "text": "primal dual"}, {"st": 182, "ed": 184, "text": "convergence guarantees"}, {"st": 185, "ed": 187, "text": "significant speedups"}, {"st": 188, "ed": 190, "text": "existing methods"}, {"st": 203, "ed": 205, "text": "accurate predictions"}]
[{"st": 27, "ed": 30, "text": "multi label classification"}, {"st": 78, "ed": 81, "text": "stochastic gradient descent"}, {"st": 163, "ed": 166, "text": "stochastic gradient descent"}, {"st": 180, "ed": 182, "text": "uniform convergence"}, {"st": 191, "ed": 193, "text": "structural properties"}, {"st": 212, "ed": 214, "text": "cutting plane"}]
[{"st": 24, "ed": 26, "text": "real world"}, {"st": 32, "ed": 34, "text": "open problems"}, {"st": 83, "ed": 85, "text": "weak classifiers"}, {"st": 108, "ed": 110, "text": "exponential loss"}, {"st": 150, "ed": 152, "text": "open problems"}, {"st": 165, "ed": 167, "text": "convergence properties"}, {"st": 182, "ed": 184, "text": "convergence rate"}, {"st": 205, "ed": 207, "text": "significantly smaller"}, {"st": 231, "ed": 233, "text": "generalization performance"}]
[{"st": 20, "ed": 22, "text": "closed form"}]
[{"st": 5, "ed": 7, "text": "probabilistic model"}, {"st": 8, "ed": 10, "text": "online learning"}, {"st": 37, "ed": 39, "text": "unlike traditional"}, {"st": 54, "ed": 56, "text": "thompson sampling"}, {"st": 109, "ed": 111, "text": "classification tasks"}, {"st": 112, "ed": 114, "text": "logistic regression"}]
[{"st": 0, "ed": 4, "text": "nonnegative matrix factorization nmf"}, {"st": 7, "ed": 9, "text": "dimension reduction"}]
[]
[{"st": 4, "ed": 6, "text": "bayesian optimization"}, {"st": 23, "ed": 25, "text": "bayesian optimization"}, {"st": 33, "ed": 35, "text": "global optimization"}, {"st": 39, "ed": 41, "text": "time consuming"}, {"st": 50, "ed": 52, "text": "bayesian optimization"}]
[{"st": 11, "ed": 13, "text": "multi class"}, {"st": 64, "ed": 66, "text": "subset selection"}, {"st": 101, "ed": 103, "text": "method outperforms"}]
[{"st": 2, "ed": 4, "text": "online optimization"}, {"st": 26, "ed": 28, "text": "learning algorithms"}, {"st": 31, "ed": 33, "text": "recently introduced"}, {"st": 43, "ed": 45, "text": "machine learning"}, {"st": 49, "ed": 51, "text": "lipschitz continuous"}, {"st": 64, "ed": 66, "text": "learning algorithm"}, {"st": 83, "ed": 85, "text": "lipschitz continuous"}]
[{"st": 1, "ed": 3, "text": "statistical analysis"}, {"st": 72, "ed": 74, "text": "sample complexity"}, {"st": 187, "ed": 189, "text": "error correcting"}, {"st": 199, "ed": 201, "text": "previously proposed"}]
[{"st": 0, "ed": 2, "text": "decision tree"}, {"st": 9, "ed": 11, "text": "data stream"}, {"st": 15, "ed": 17, "text": "confidence intervals"}, {"st": 43, "ed": 45, "text": "decision tree"}, {"st": 71, "ed": 73, "text": "statistical analysis"}, {"st": 86, "ed": 88, "text": "confidence intervals"}, {"st": 100, "ed": 102, "text": "confidence intervals"}, {"st": 108, "ed": 110, "text": "decision tree"}, {"st": 129, "ed": 131, "text": "confidence intervals"}, {"st": 155, "ed": 157, "text": "decision tree"}, {"st": 169, "ed": 171, "text": "theoretical guarantee"}, {"st": 182, "ed": 184, "text": "decision tree"}, {"st": 191, "ed": 195, "text": "real and synthetic data"}, {"st": 221, "ed": 223, "text": "active learning"}, {"st": 236, "ed": 238, "text": "recent methods"}]
[{"st": 8, "ed": 10, "text": "predictive performance"}, {"st": 97, "ed": 99, "text": "complexity bounds"}]
[{"st": 0, "ed": 3, "text": "empirical risk minimization"}, {"st": 10, "ed": 12, "text": "loss functions"}, {"st": 29, "ed": 31, "text": "loss functions"}, {"st": 49, "ed": 51, "text": "convex surrogate"}, {"st": 55, "ed": 57, "text": "loss functions"}, {"st": 69, "ed": 71, "text": "loss functions"}, {"st": 143, "ed": 145, "text": "loss function"}, {"st": 153, "ed": 155, "text": "empirical results"}, {"st": 173, "ed": 175, "text": "real world"}, {"st": 179, "ed": 182, "text": "tens of thousands"}, {"st": 186, "ed": 188, "text": "improved performance"}]
[{"st": 0, "ed": 2, "text": "existing approaches"}, {"st": 19, "ed": 21, "text": "discrete optimization"}, {"st": 38, "ed": 40, "text": "computational complexity"}, {"st": 49, "ed": 51, "text": "transfer function"}, {"st": 93, "ed": 95, "text": "standard backpropagation"}]
[{"st": 79, "ed": 81, "text": "least squares"}, {"st": 96, "ed": 99, "text": "method of moments"}, {"st": 128, "ed": 131, "text": "method of moments"}, {"st": 158, "ed": 161, "text": "method of moments"}]
[{"st": 10, "ed": 12, "text": "training data"}, {"st": 14, "ed": 16, "text": "linear regression"}, {"st": 44, "ed": 46, "text": "linear regression"}, {"st": 61, "ed": 63, "text": "conjugate prior"}, {"st": 64, "ed": 68, "text": "expectation maximization em algorithm"}, {"st": 75, "ed": 77, "text": "maximum likelihood"}, {"st": 84, "ed": 86, "text": "linear regression"}, {"st": 93, "ed": 95, "text": "em algorithm"}, {"st": 102, "ed": 104, "text": "em algorithm"}]
[{"st": 0, "ed": 3, "text": "statistical machine learning"}, {"st": 5, "ed": 7, "text": "important role"}, {"st": 14, "ed": 16, "text": "main goal"}, {"st": 17, "ed": 20, "text": "statistical machine learning"}, {"st": 47, "ed": 49, "text": "kernel methods"}, {"st": 56, "ed": 59, "text": "a reproducing kernel"}, {"st": 65, "ed": 68, "text": "statistical machine learning"}, {"st": 92, "ed": 94, "text": "kernel learning"}]
[{"st": 2, "ed": 4, "text": "logistic regression"}, {"st": 28, "ed": 31, "text": "log partition function"}, {"st": 47, "ed": 49, "text": "distributed stochastic"}, {"st": 51, "ed": 53, "text": "based optimization"}, {"st": 60, "ed": 62, "text": "logistic regression"}, {"st": 90, "ed": 92, "text": "objective functions"}, {"st": 96, "ed": 98, "text": "machine learning"}, {"st": 137, "ed": 139, "text": "real world"}]
[{"st": 7, "ed": 9, "text": "machine learning"}, {"st": 21, "ed": 23, "text": "problems including"}, {"st": 23, "ed": 25, "text": "sparse coding"}, {"st": 26, "ed": 28, "text": "matrix completion"}, {"st": 98, "ed": 100, "text": "alternating minimization"}, {"st": 113, "ed": 115, "text": "stationary points"}, {"st": 130, "ed": 132, "text": "provide theoretical"}, {"st": 133, "ed": 135, "text": "empirical evidence"}, {"st": 136, "ed": 138, "text": "alternating minimization"}, {"st": 140, "ed": 142, "text": "random initialization"}, {"st": 171, "ed": 173, "text": "stationary points"}, {"st": 178, "ed": 180, "text": "dictionary learning"}, {"st": 193, "ed": 195, "text": "alternating minimization"}]
[{"st": 1, "ed": 3, "text": "variational autoencoder"}, {"st": 6, "ed": 8, "text": "generative model"}, {"st": 10, "ed": 12, "text": "latent variables"}, {"st": 47, "ed": 49, "text": "outer product"}, {"st": 87, "ed": 89, "text": "multivariate gaussian"}, {"st": 124, "ed": 126, "text": "continuous data"}, {"st": 131, "ed": 133, "text": "variational autoencoder"}, {"st": 156, "ed": 158, "text": "dependency structure"}]
[{"st": 0, "ed": 2, "text": "gaussian process"}, {"st": 26, "ed": 28, "text": "additive models"}, {"st": 35, "ed": 38, "text": "generalized linear models"}, {"st": 64, "ed": 66, "text": "linear combination"}, {"st": 79, "ed": 81, "text": "linear combination"}, {"st": 91, "ed": 93, "text": "gaussian processes"}, {"st": 126, "ed": 128, "text": "approximate inference"}]
[{"st": 11, "ed": 13, "text": "multi label"}, {"st": 15, "ed": 17, "text": "existing approaches"}, {"st": 27, "ed": 29, "text": "learning process"}, {"st": 131, "ed": 134, "text": "empirical risk minimization"}, {"st": 154, "ed": 156, "text": "generalization error"}, {"st": 166, "ed": 168, "text": "trace norm"}, {"st": 175, "ed": 177, "text": "extensive experiments"}, {"st": 179, "ed": 181, "text": "benchmark datasets"}]
[{"st": 0, "ed": 2, "text": "predictive models"}, {"st": 37, "ed": 39, "text": "risk prediction"}, {"st": 66, "ed": 68, "text": "cost sensitive"}, {"st": 97, "ed": 99, "text": "multi layer"}, {"st": 112, "ed": 114, "text": "feature vector"}, {"st": 145, "ed": 147, "text": "real world"}, {"st": 154, "ed": 156, "text": "intensive care"}, {"st": 179, "ed": 181, "text": "prediction accuracy"}]
[{"st": 0, "ed": 3, "text": "stochastic gradient descent"}, {"st": 10, "ed": 12, "text": "large scale"}, {"st": 12, "ed": 14, "text": "machine learning"}, {"st": 22, "ed": 24, "text": "online learning"}, {"st": 62, "ed": 65, "text": "stochastic gradient descent"}, {"st": 74, "ed": 76, "text": "weight vector"}, {"st": 97, "ed": 99, "text": "weight vector"}, {"st": 103, "ed": 105, "text": "stability selection"}, {"st": 131, "ed": 134, "text": "exploration and exploitation"}, {"st": 140, "ed": 142, "text": "numerical experiments"}, {"st": 146, "ed": 149, "text": "compares favorably with"}, {"st": 155, "ed": 157, "text": "prediction accuracy"}]
[{"st": 4, "ed": 6, "text": "challenging problems"}, {"st": 8, "ed": 10, "text": "online learning"}, {"st": 22, "ed": 24, "text": "sparse models"}, {"st": 169, "ed": 171, "text": "convergence analysis"}, {"st": 176, "ed": 178, "text": "loss functions"}, {"st": 185, "ed": 187, "text": "classification task"}, {"st": 190, "ed": 192, "text": "l 2"}, {"st": 200, "ed": 202, "text": "extensive experiments"}, {"st": 203, "ed": 205, "text": "classification task"}, {"st": 206, "ed": 209, "text": "batch and online"}, {"st": 230, "ed": 232, "text": "predictive performance"}, {"st": 242, "ed": 244, "text": "significant computational"}]
[{"st": 7, "ed": 9, "text": "thompson sampling"}, {"st": 40, "ed": 42, "text": "posterior distribution"}, {"st": 130, "ed": 134, "text": "synthetic and real world"}]
[{"st": 18, "ed": 20, "text": "neural networks"}, {"st": 30, "ed": 32, "text": "random forest"}, {"st": 38, "ed": 40, "text": "neural network"}, {"st": 58, "ed": 60, "text": "prior knowledge"}, {"st": 96, "ed": 100, "text": "synthetic and real data"}, {"st": 104, "ed": 106, "text": "excellent performance"}]
[{"st": 56, "ed": 58, "text": "significant reduction"}, {"st": 107, "ed": 109, "text": "special cases"}, {"st": 132, "ed": 134, "text": "highly nonlinear"}]
[{"st": 45, "ed": 47, "text": "gaussian processes"}, {"st": 52, "ed": 55, "text": "deep neural networks"}, {"st": 75, "ed": 77, "text": "standard benchmark"}, {"st": 87, "ed": 89, "text": "low fidelity"}]
[{"st": 21, "ed": 23, "text": "predictive performance"}, {"st": 50, "ed": 52, "text": "additional assumptions"}, {"st": 55, "ed": 57, "text": "ground truth"}, {"st": 87, "ed": 89, "text": "consensus clustering"}]
[{"st": 5, "ed": 8, "text": "multi view learning"}, {"st": 25, "ed": 27, "text": "pre processing"}, {"st": 54, "ed": 56, "text": "multi view"}, {"st": 65, "ed": 67, "text": "fine tune"}, {"st": 75, "ed": 77, "text": "multi view"}, {"st": 116, "ed": 118, "text": "real world"}, {"st": 127, "ed": 129, "text": "multi view"}, {"st": 135, "ed": 137, "text": "proposed algorithm"}]
[{"st": 1, "ed": 3, "text": "machine learning"}, {"st": 23, "ed": 25, "text": "machine learning"}, {"st": 57, "ed": 59, "text": "kernel machines"}, {"st": 71, "ed": 73, "text": "kernel functions"}, {"st": 105, "ed": 107, "text": "semi definite"}, {"st": 112, "ed": 114, "text": "kernel machines"}, {"st": 115, "ed": 117, "text": "classification regression"}, {"st": 117, "ed": 119, "text": "anomaly detection"}, {"st": 120, "ed": 122, "text": "low dimensional"}, {"st": 131, "ed": 133, "text": "numerical experiments"}]
[{"st": 21, "ed": 23, "text": "low dimensional"}, {"st": 39, "ed": 41, "text": "big data"}, {"st": 76, "ed": 78, "text": "semi supervised"}, {"st": 81, "ed": 83, "text": "multi task"}, {"st": 84, "ed": 86, "text": "multi view"}, {"st": 99, "ed": 101, "text": "semi supervised"}, {"st": 103, "ed": 105, "text": "proposed framework"}, {"st": 110, "ed": 112, "text": "multi task"}, {"st": 113, "ed": 115, "text": "multi view"}, {"st": 145, "ed": 147, "text": "efficiently solved"}, {"st": 149, "ed": 151, "text": "alternating optimization"}, {"st": 160, "ed": 162, "text": "explicitly model"}, {"st": 172, "ed": 174, "text": "nearest neighbor"}, {"st": 181, "ed": 183, "text": "multi task"}, {"st": 184, "ed": 186, "text": "multi view"}, {"st": 205, "ed": 207, "text": "multiple tasks"}, {"st": 225, "ed": 227, "text": "baseline methods"}]
[{"st": 37, "ed": 39, "text": "prior distribution"}, {"st": 71, "ed": 73, "text": "prior distribution"}, {"st": 96, "ed": 98, "text": "prior distribution"}, {"st": 121, "ed": 124, "text": "mean square error"}, {"st": 174, "ed": 178, "text": "expectation maximization em algorithm"}]
[{"st": 60, "ed": 62, "text": "random forest"}, {"st": 81, "ed": 83, "text": "random forest"}]
[{"st": 10, "ed": 12, "text": "random forests"}, {"st": 45, "ed": 47, "text": "class conditional"}, {"st": 129, "ed": 131, "text": "active learning"}, {"st": 131, "ed": 134, "text": "semi supervised learning"}]
[{"st": 4, "ed": 6, "text": "linear optimization"}, {"st": 62, "ed": 64, "text": "previous works"}, {"st": 67, "ed": 69, "text": "regret bound"}, {"st": 111, "ed": 113, "text": "linear optimization"}, {"st": 113, "ed": 116, "text": "with expert advice"}, {"st": 127, "ed": 129, "text": "armed bandit"}, {"st": 147, "ed": 149, "text": "computationally efficient"}, {"st": 152, "ed": 154, "text": "optimal regret"}, {"st": 180, "ed": 183, "text": "computationally efficient algorithm"}]
[{"st": 11, "ed": 13, "text": "regret bounds"}, {"st": 25, "ed": 27, "text": "carefully designed"}, {"st": 31, "ed": 33, "text": "weight sharing"}, {"st": 70, "ed": 72, "text": "weight sharing"}]
[{"st": 3, "ed": 6, "text": "simple yet effective"}, {"st": 23, "ed": 25, "text": "shortest path"}, {"st": 39, "ed": 41, "text": "distance based"}, {"st": 41, "ed": 43, "text": "supervised learning"}, {"st": 46, "ed": 48, "text": "nearest neighbor"}, {"st": 62, "ed": 64, "text": "large data"}, {"st": 73, "ed": 75, "text": "nearest neighbor"}, {"st": 78, "ed": 80, "text": "shortest path"}, {"st": 89, "ed": 91, "text": "extremely large"}, {"st": 108, "ed": 110, "text": "large scale"}]
[{"st": 153, "ed": 155, "text": "near optimal"}]
[{"st": 13, "ed": 15, "text": "structured sparsity"}, {"st": 20, "ed": 22, "text": "structural information"}, {"st": 31, "ed": 33, "text": "widely adopted"}, {"st": 43, "ed": 45, "text": "group lasso"}, {"st": 73, "ed": 75, "text": "optimization method"}, {"st": 88, "ed": 90, "text": "approach called"}, {"st": 91, "ed": 93, "text": "proximal gradient"}, {"st": 105, "ed": 107, "text": "convex loss"}, {"st": 112, "ed": 114, "text": "structured sparsity"}, {"st": 130, "ed": 132, "text": "convergence rate"}, {"st": 154, "ed": 156, "text": "numerical results"}, {"st": 161, "ed": 164, "text": "efficiency and scalability"}]
[{"st": 21, "ed": 24, "text": "theoretical and empirical"}, {"st": 40, "ed": 42, "text": "theoretical guarantees"}, {"st": 45, "ed": 47, "text": "rademacher complexity"}, {"st": 58, "ed": 60, "text": "learning algorithm"}, {"st": 83, "ed": 85, "text": "kernel based"}, {"st": 99, "ed": 101, "text": "empirical results"}]
[{"st": 27, "ed": 29, "text": "clinical trials"}, {"st": 43, "ed": 45, "text": "active learning"}, {"st": 84, "ed": 86, "text": "optimization criteria"}, {"st": 100, "ed": 102, "text": "active learning"}, {"st": 105, "ed": 107, "text": "simulated data"}, {"st": 112, "ed": 114, "text": "clinical trial"}, {"st": 125, "ed": 127, "text": "active learning"}]
[{"st": 7, "ed": 9, "text": "probabilistic model"}, {"st": 20, "ed": 22, "text": "boosting algorithm"}, {"st": 48, "ed": 50, "text": "learning rules"}, {"st": 54, "ed": 56, "text": "boosting algorithm"}, {"st": 99, "ed": 101, "text": "generalization performance"}]
[{"st": 10, "ed": 12, "text": "domain knowledge"}, {"st": 34, "ed": 36, "text": "pac bayesian"}, {"st": 58, "ed": 60, "text": "pac bayesian"}, {"st": 64, "ed": 66, "text": "reinforcement learning"}, {"st": 84, "ed": 86, "text": "transfer learning"}, {"st": 88, "ed": 90, "text": "empirical results"}, {"st": 92, "ed": 94, "text": "pac bayesian"}, {"st": 94, "ed": 96, "text": "policy evaluation"}, {"st": 100, "ed": 102, "text": "prior distributions"}]
[{"st": 10, "ed": 12, "text": "feature selection"}, {"st": 67, "ed": 69, "text": "feature selection"}, {"st": 72, "ed": 74, "text": "mixed integer"}, {"st": 83, "ed": 85, "text": "linear programming"}, {"st": 91, "ed": 93, "text": "cutting plane"}, {"st": 100, "ed": 103, "text": "multiple kernel learning"}, {"st": 109, "ed": 111, "text": "ridge regression"}, {"st": 117, "ed": 120, "text": "benchmark data sets"}, {"st": 123, "ed": 125, "text": "proposed method"}, {"st": 137, "ed": 139, "text": "feature selection"}]
[{"st": 3, "ed": 6, "text": "semi supervised learning"}, {"st": 15, "ed": 17, "text": "previously proposed"}, {"st": 17, "ed": 19, "text": "error bound"}, {"st": 20, "ed": 22, "text": "active learning"}, {"st": 42, "ed": 44, "text": "submodular functions"}, {"st": 52, "ed": 54, "text": "submodular functions"}, {"st": 59, "ed": 61, "text": "error bound"}, {"st": 85, "ed": 87, "text": "error bound"}, {"st": 96, "ed": 98, "text": "submodular function"}, {"st": 101, "ed": 104, "text": "semi supervised learning"}, {"st": 116, "ed": 118, "text": "error bound"}, {"st": 137, "ed": 139, "text": "theoretical results"}]
[{"st": 4, "ed": 6, "text": "bregman divergence"}, {"st": 13, "ed": 15, "text": "statistical models"}, {"st": 18, "ed": 21, "text": "discrete random variables"}, {"st": 52, "ed": 54, "text": "proposed framework"}]
[{"st": 39, "ed": 41, "text": "gaussian process"}, {"st": 72, "ed": 74, "text": "kalman filter"}, {"st": 82, "ed": 84, "text": "recently proposed"}, {"st": 100, "ed": 102, "text": "probabilistic model"}]
[{"st": 61, "ed": 63, "text": "maximum likelihood"}, {"st": 65, "ed": 67, "text": "moment matching"}, {"st": 112, "ed": 114, "text": "fixed points"}]
[{"st": 4, "ed": 6, "text": "log likelihood"}, {"st": 8, "ed": 11, "text": "probabilistic graphical models"}, {"st": 12, "ed": 14, "text": "lipschitz continuous"}, {"st": 37, "ed": 40, "text": "kullback leibler divergence"}, {"st": 62, "ed": 64, "text": "log likelihood"}, {"st": 76, "ed": 78, "text": "generalization ability"}, {"st": 97, "ed": 99, "text": "error rate"}, {"st": 115, "ed": 117, "text": "metric spaces"}, {"st": 119, "ed": 121, "text": "dimensionality reduction"}, {"st": 139, "ed": 141, "text": "preliminary results"}, {"st": 142, "ed": 144, "text": "activity recognition"}]
[{"st": 10, "ed": 12, "text": "observed variables"}, {"st": 23, "ed": 25, "text": "causal models"}, {"st": 62, "ed": 64, "text": "causal models"}, {"st": 65, "ed": 67, "text": "latent variables"}, {"st": 97, "ed": 99, "text": "cause effect"}, {"st": 113, "ed": 115, "text": "causal models"}, {"st": 116, "ed": 118, "text": "conditional probability"}, {"st": 146, "ed": 148, "text": "learning algorithms"}]
[{"st": 4, "ed": 6, "text": "observed variables"}, {"st": 29, "ed": 31, "text": "causal structure"}, {"st": 40, "ed": 42, "text": "observed data"}, {"st": 59, "ed": 61, "text": "causal model"}, {"st": 62, "ed": 64, "text": "binary data"}, {"st": 73, "ed": 75, "text": "causal structure"}, {"st": 86, "ed": 88, "text": "experimental evaluation"}, {"st": 89, "ed": 91, "text": "excellent performance"}]
[{"st": 11, "ed": 13, "text": "observed variables"}, {"st": 37, "ed": 39, "text": "low complexity"}, {"st": 91, "ed": 93, "text": "conditional distributions"}, {"st": 106, "ed": 108, "text": "encouraging results"}]
[{"st": 1, "ed": 3, "text": "maximum likelihood"}, {"st": 25, "ed": 27, "text": "recent research"}, {"st": 65, "ed": 67, "text": "recently proposed"}, {"st": 101, "ed": 103, "text": "statistical efficiency"}, {"st": 108, "ed": 110, "text": "recently proposed"}]
[{"st": 79, "ed": 81, "text": "large data"}]
[{"st": 1, "ed": 4, "text": "restricted boltzmann machines"}, {"st": 7, "ed": 9, "text": "probabilistic models"}, {"st": 19, "ed": 21, "text": "problems including"}, {"st": 21, "ed": 23, "text": "collaborative filtering"}, {"st": 26, "ed": 28, "text": "motion capture"}, {"st": 64, "ed": 66, "text": "structured output"}, {"st": 72, "ed": 74, "text": "contrastive divergence"}, {"st": 90, "ed": 92, "text": "structured output"}, {"st": 92, "ed": 94, "text": "prediction problems"}, {"st": 98, "ed": 100, "text": "learning algorithm"}, {"st": 110, "ed": 112, "text": "output space"}, {"st": 138, "ed": 140, "text": "output space"}, {"st": 146, "ed": 148, "text": "output space"}, {"st": 155, "ed": 157, "text": "image denoising"}, {"st": 165, "ed": 167, "text": "learning algorithms"}, {"st": 172, "ed": 174, "text": "contrastive divergence"}]
[{"st": 0, "ed": 2, "text": "reinforcement learning"}, {"st": 22, "ed": 24, "text": "bandit problems"}, {"st": 42, "ed": 44, "text": "learning algorithm"}, {"st": 45, "ed": 47, "text": "bandit problems"}, {"st": 92, "ed": 94, "text": "sample complexity"}]
[{"st": 4, "ed": 8, "text": "markov chain monte carlo"}, {"st": 11, "ed": 13, "text": "posterior probabilities"}, {"st": 25, "ed": 27, "text": "posterior distribution"}, {"st": 39, "ed": 41, "text": "conditional probabilities"}, {"st": 51, "ed": 53, "text": "empirical results"}, {"st": 63, "ed": 65, "text": "previous methods"}]
[{"st": 12, "ed": 14, "text": "generating process"}, {"st": 18, "ed": 20, "text": "causal graph"}, {"st": 22, "ed": 24, "text": "joint distribution"}, {"st": 28, "ed": 30, "text": "conditional independence"}, {"st": 31, "ed": 33, "text": "causal discovery"}, {"st": 53, "ed": 55, "text": "causal graph"}, {"st": 97, "ed": 99, "text": "generating process"}, {"st": 175, "ed": 177, "text": "causal graph"}, {"st": 183, "ed": 185, "text": "simulated data"}]
[{"st": 0, "ed": 2, "text": "low dimensional"}, {"st": 8, "ed": 10, "text": "anomaly detection"}, {"st": 20, "ed": 22, "text": "existing methods"}, {"st": 32, "ed": 34, "text": "finite dimensional"}, {"st": 83, "ed": 85, "text": "low dimensional"}, {"st": 89, "ed": 91, "text": "anomaly detection"}, {"st": 104, "ed": 106, "text": "machine learning"}, {"st": 111, "ed": 113, "text": "empirical results"}, {"st": 114, "ed": 116, "text": "synthetic data"}]
[{"st": 0, "ed": 2, "text": "kernel methods"}, {"st": 7, "ed": 9, "text": "machine learning"}, {"st": 17, "ed": 19, "text": "feature maps"}, {"st": 31, "ed": 33, "text": "kernel matrix"}, {"st": 83, "ed": 85, "text": "inner product"}, {"st": 85, "ed": 87, "text": "kernel functions"}, {"st": 88, "ed": 90, "text": "radial basis"}, {"st": 121, "ed": 123, "text": "case study"}]
[{"st": 0, "ed": 2, "text": "probabilistic inference"}, {"st": 25, "ed": 27, "text": "inference algorithms"}, {"st": 32, "ed": 34, "text": "belief propagation"}, {"st": 59, "ed": 61, "text": "latent variable"}, {"st": 61, "ed": 63, "text": "causal models"}]
[{"st": 15, "ed": 17, "text": "sample size"}]
[{"st": 72, "ed": 74, "text": "relational model"}, {"st": 77, "ed": 79, "text": "sparse matrix"}, {"st": 80, "ed": 82, "text": "gaussian process"}, {"st": 90, "ed": 92, "text": "generative models"}, {"st": 99, "ed": 101, "text": "matrix variate"}, {"st": 101, "ed": 103, "text": "gaussian process"}, {"st": 111, "ed": 113, "text": "prior distributions"}, {"st": 130, "ed": 132, "text": "latent variables"}, {"st": 140, "ed": 142, "text": "expectation maximization"}, {"st": 157, "ed": 161, "text": "synthetic and real world"}]
[{"st": 14, "ed": 16, "text": "multi class"}, {"st": 17, "ed": 19, "text": "recent research"}, {"st": 23, "ed": 25, "text": "structure learning"}, {"st": 28, "ed": 30, "text": "multi class"}, {"st": 45, "ed": 48, "text": "multi class classification"}, {"st": 82, "ed": 84, "text": "proposed method"}, {"st": 103, "ed": 106, "text": "accuracy and efficiency"}, {"st": 108, "ed": 110, "text": "proposed method"}, {"st": 112, "ed": 115, "text": "multi class classification"}, {"st": 117, "ed": 119, "text": "real world"}, {"st": 119, "ed": 121, "text": "large scale"}, {"st": 127, "ed": 129, "text": "proposed method"}, {"st": 140, "ed": 142, "text": "performs comparably"}, {"st": 145, "ed": 147, "text": "structure learning"}]
[{"st": 17, "ed": 20, "text": "markov random field"}, {"st": 95, "ed": 97, "text": "outperforms existing"}]
[{"st": 34, "ed": 36, "text": "random variables"}, {"st": 85, "ed": 87, "text": "convergence rate"}]
[{"st": 0, "ed": 2, "text": "conditional independence"}, {"st": 18, "ed": 21, "text": "curse of dimensionality"}, {"st": 23, "ed": 25, "text": "conditional independence"}, {"st": 26, "ed": 28, "text": "continuous variables"}, {"st": 34, "ed": 36, "text": "kernel based"}, {"st": 36, "ed": 38, "text": "conditional independence"}, {"st": 54, "ed": 56, "text": "null hypothesis"}, {"st": 60, "ed": 62, "text": "proposed method"}, {"st": 63, "ed": 65, "text": "computationally efficient"}, {"st": 86, "ed": 88, "text": "sample size"}]
[{"st": 1, "ed": 3, "text": "support vector"}, {"st": 6, "ed": 8, "text": "performance measures"}, {"st": 9, "ed": 11, "text": "recently introduced"}, {"st": 17, "ed": 19, "text": "optimization problem"}, {"st": 23, "ed": 25, "text": "cutting plane"}, {"st": 75, "ed": 77, "text": "precision recall"}, {"st": 95, "ed": 97, "text": "optimization algorithm"}, {"st": 117, "ed": 119, "text": "per iteration"}, {"st": 132, "ed": 134, "text": "empirical evaluation"}, {"st": 138, "ed": 141, "text": "publicly available datasets"}, {"st": 146, "ed": 148, "text": "significantly faster"}, {"st": 149, "ed": 151, "text": "cutting plane"}, {"st": 152, "ed": 154, "text": "without sacrificing"}]
[{"st": 11, "ed": 13, "text": "topic models"}, {"st": 15, "ed": 17, "text": "latent representations"}, {"st": 23, "ed": 26, "text": "probabilistic topic models"}, {"st": 59, "ed": 61, "text": "sparsity inducing"}, {"st": 69, "ed": 71, "text": "error function"}, {"st": 73, "ed": 75, "text": "hinge loss"}, {"st": 76, "ed": 78, "text": "supervised learning"}, {"st": 108, "ed": 110, "text": "classification accuracy"}]
[{"st": 49, "ed": 51, "text": "observed variables"}, {"st": 94, "ed": 96, "text": "theoretical analysis"}, {"st": 128, "ed": 130, "text": "method yields"}, {"st": 130, "ed": 132, "text": "promising results"}]
[{"st": 4, "ed": 6, "text": "sparse coding"}, {"st": 14, "ed": 16, "text": "sparse linear"}, {"st": 39, "ed": 41, "text": "sparse coding"}, {"st": 45, "ed": 47, "text": "impressive performance"}, {"st": 51, "ed": 53, "text": "supervised tasks"}, {"st": 55, "ed": 57, "text": "generalization properties"}, {"st": 65, "ed": 67, "text": "generalization error"}, {"st": 70, "ed": 72, "text": "sparse coding"}, {"st": 95, "ed": 97, "text": "infinite dimensional"}, {"st": 142, "ed": 144, "text": "sparse codes"}, {"st": 158, "ed": 160, "text": "estimation error"}, {"st": 164, "ed": 166, "text": "tilde o"}, {"st": 180, "ed": 182, "text": "infinite dimensional"}, {"st": 191, "ed": 193, "text": "tilde o"}, {"st": 222, "ed": 224, "text": "training data"}]
[{"st": 5, "ed": 7, "text": "binary classification"}, {"st": 36, "ed": 38, "text": "logistic regression"}, {"st": 60, "ed": 62, "text": "classification problem"}, {"st": 118, "ed": 120, "text": "local patterns"}, {"st": 133, "ed": 135, "text": "supervised learning"}, {"st": 135, "ed": 138, "text": "point of view"}, {"st": 142, "ed": 144, "text": "weak classifiers"}]
[{"st": 8, "ed": 10, "text": "convex surrogate"}, {"st": 27, "ed": 29, "text": "nuclear norm"}, {"st": 40, "ed": 42, "text": "convex relaxations"}]
[{"st": 1, "ed": 5, "text": "restricted boltzmann machine rbm"}, {"st": 11, "ed": 13, "text": "complex data"}, {"st": 17, "ed": 19, "text": "significant computational"}, {"st": 30, "ed": 32, "text": "natural language"}, {"st": 109, "ed": 113, "text": "markov chain monte carlo"}, {"st": 121, "ed": 123, "text": "computational complexity"}, {"st": 142, "ed": 144, "text": "n grams"}, {"st": 153, "ed": 155, "text": "learned features"}, {"st": 156, "ed": 158, "text": "improve performance"}, {"st": 161, "ed": 163, "text": "sentiment classification"}]
[{"st": 6, "ed": 8, "text": "sampling algorithm"}, {"st": 64, "ed": 66, "text": "empirical evaluation"}, {"st": 93, "ed": 95, "text": "special case"}, {"st": 104, "ed": 106, "text": "sampling algorithm"}, {"st": 169, "ed": 171, "text": "sampling algorithm"}]
[{"st": 8, "ed": 10, "text": "labeled data"}, {"st": 27, "ed": 29, "text": "approximation error"}, {"st": 41, "ed": 43, "text": "real world"}, {"st": 57, "ed": 59, "text": "sampling based"}, {"st": 100, "ed": 102, "text": "active learning"}]
[{"st": 6, "ed": 8, "text": "pac bayes"}, {"st": 19, "ed": 21, "text": "multi class"}, {"st": 56, "ed": 58, "text": "performance measure"}, {"st": 100, "ed": 102, "text": "empirical risk"}, {"st": 110, "ed": 112, "text": "training examples"}, {"st": 125, "ed": 127, "text": "pac bayes"}]
[{"st": 4, "ed": 6, "text": "kernel based"}, {"st": 6, "ed": 8, "text": "discriminative learning"}, {"st": 20, "ed": 22, "text": "training examples"}, {"st": 29, "ed": 31, "text": "probability distributions"}, {"st": 43, "ed": 45, "text": "probability distributions"}, {"st": 50, "ed": 54, "text": "reproducing kernel hilbert space"}, {"st": 62, "ed": 64, "text": "kernel based"}, {"st": 78, "ed": 81, "text": "support vector machine"}, {"st": 116, "ed": 118, "text": "kernel functions"}, {"st": 126, "ed": 130, "text": "synthetic and real world"}]
[{"st": 13, "ed": 15, "text": "basis functions"}, {"st": 17, "ed": 19, "text": "nonparametric regression"}, {"st": 36, "ed": 38, "text": "linear combination"}, {"st": 51, "ed": 53, "text": "low rank"}, {"st": 58, "ed": 60, "text": "estimation problem"}, {"st": 67, "ed": 69, "text": "trace norm"}, {"st": 71, "ed": 73, "text": "ell 1"}, {"st": 90, "ed": 96, "text": "alternating direction method of multipliers admm"}, {"st": 100, "ed": 102, "text": "efficient algorithms"}, {"st": 116, "ed": 118, "text": "theoretical analysis"}, {"st": 131, "ed": 133, "text": "optimal solution"}, {"st": 143, "ed": 145, "text": "basis functions"}, {"st": 165, "ed": 168, "text": "effectiveness and efficiency"}]
[{"st": 1, "ed": 3, "text": "recent years"}, {"st": 27, "ed": 29, "text": "l1 regularized"}, {"st": 46, "ed": 48, "text": "cross validation"}, {"st": 58, "ed": 60, "text": "predictive performance"}, {"st": 69, "ed": 71, "text": "regularization parameter"}, {"st": 82, "ed": 84, "text": "fully bayesian"}, {"st": 88, "ed": 91, "text": "spike and slab"}, {"st": 110, "ed": 112, "text": "langevin dynamics"}, {"st": 136, "ed": 138, "text": "parameter values"}, {"st": 150, "ed": 152, "text": "predictive performance"}, {"st": 158, "ed": 160, "text": "based methods"}, {"st": 161, "ed": 163, "text": "hyper parameter"}]
[{"st": 3, "ed": 6, "text": "stochastic gradient descent"}, {"st": 11, "ed": 13, "text": "learning rates"}, {"st": 27, "ed": 29, "text": "learning rates"}, {"st": 34, "ed": 36, "text": "expected error"}, {"st": 52, "ed": 54, "text": "learning rates"}, {"st": 64, "ed": 66, "text": "non stationary"}, {"st": 106, "ed": 108, "text": "learning rate"}]
[{"st": 109, "ed": 111, "text": "existing approaches"}, {"st": 143, "ed": 145, "text": "greedy algorithms"}, {"st": 165, "ed": 167, "text": "group sparse"}]
[{"st": 9, "ed": 11, "text": "convex relaxations"}, {"st": 17, "ed": 19, "text": "nuclear norm"}, {"st": 21, "ed": 23, "text": "ell 1"}, {"st": 76, "ed": 78, "text": "learning problems"}]
[{"st": 18, "ed": 20, "text": "cross validation"}, {"st": 24, "ed": 26, "text": "time consuming"}, {"st": 34, "ed": 36, "text": "cross validation"}, {"st": 43, "ed": 45, "text": "sequential analysis"}, {"st": 90, "ed": 92, "text": "statistical power"}, {"st": 96, "ed": 98, "text": "experimental evaluation"}, {"st": 117, "ed": 119, "text": "cross validation"}]
[{"st": 0, "ed": 2, "text": "machine learning"}, {"st": 10, "ed": 12, "text": "regularization terms"}, {"st": 54, "ed": 56, "text": "learning algorithm"}, {"st": 79, "ed": 81, "text": "learning algorithm"}, {"st": 82, "ed": 84, "text": "generalization performance"}, {"st": 91, "ed": 93, "text": "gaussian process"}, {"st": 97, "ed": 99, "text": "posterior distribution"}, {"st": 131, "ed": 133, "text": "gaussian process"}, {"st": 137, "ed": 139, "text": "inference procedure"}, {"st": 168, "ed": 170, "text": "machine learning"}, {"st": 227, "ed": 230, "text": "latent dirichlet allocation"}]
[{"st": 3, "ed": 5, "text": "statistical properties"}, {"st": 6, "ed": 8, "text": "linear classifiers"}, {"st": 25, "ed": 27, "text": "finite dimensional"}, {"st": 32, "ed": 34, "text": "feature maps"}, {"st": 44, "ed": 46, "text": "logistic regression"}, {"st": 68, "ed": 70, "text": "finite dimensional"}, {"st": 78, "ed": 80, "text": "decision boundaries"}, {"st": 90, "ed": 92, "text": "sample size"}]
[{"st": 3, "ed": 5, "text": "unsupervised learning"}, {"st": 61, "ed": 63, "text": "local optima"}, {"st": 67, "ed": 69, "text": "spectral methods"}, {"st": 71, "ed": 73, "text": "directly applied"}, {"st": 78, "ed": 80, "text": "parse tree"}]
[{"st": 0, "ed": 2, "text": "continuous state"}, {"st": 44, "ed": 46, "text": "sample complexity"}, {"st": 69, "ed": 71, "text": "value iteration"}, {"st": 108, "ed": 110, "text": "real world"}, {"st": 119, "ed": 121, "text": "efficiently solve"}]
[{"st": 7, "ed": 10, "text": "block coordinate descent"}, {"st": 16, "ed": 18, "text": "gaussian process"}, {"st": 21, "ed": 23, "text": "large scale"}, {"st": 27, "ed": 29, "text": "large scale"}, {"st": 50, "ed": 52, "text": "active set"}, {"st": 64, "ed": 66, "text": "existing methods"}, {"st": 69, "ed": 71, "text": "active set"}, {"st": 76, "ed": 79, "text": "constrained optimization problem"}, {"st": 92, "ed": 94, "text": "objective function"}, {"st": 123, "ed": 125, "text": "conjugate gradient"}, {"st": 153, "ed": 155, "text": "100 000"}]
[{"st": 71, "ed": 74, "text": "bethe free energy"}, {"st": 92, "ed": 94, "text": "empirical results"}]
[{"st": 1, "ed": 3, "text": "multi view"}, {"st": 40, "ed": 42, "text": "multi view"}, {"st": 64, "ed": 66, "text": "multi view"}, {"st": 70, "ed": 72, "text": "successfully applied"}, {"st": 81, "ed": 83, "text": "audio visual"}, {"st": 99, "ed": 101, "text": "multi view"}]
[{"st": 10, "ed": 12, "text": "probabilistic models"}, {"st": 14, "ed": 16, "text": "mean field"}, {"st": 33, "ed": 36, "text": "bethe free energy"}, {"st": 63, "ed": 65, "text": "bethe free"}, {"st": 93, "ed": 96, "text": "a sufficient condition"}, {"st": 101, "ed": 103, "text": "message passing"}, {"st": 120, "ed": 123, "text": "bethe free energy"}, {"st": 132, "ed": 134, "text": "local minimum"}]
[{"st": 4, "ed": 6, "text": "maximum likelihood"}, {"st": 11, "ed": 13, "text": "probabilistic inference"}, {"st": 24, "ed": 26, "text": "inference process"}, {"st": 35, "ed": 37, "text": "inference process"}, {"st": 46, "ed": 48, "text": "convex function"}, {"st": 50, "ed": 52, "text": "free energy"}, {"st": 65, "ed": 67, "text": "inference process"}]
[{"st": 1, "ed": 4, "text": "markov random fields"}, {"st": 27, "ed": 29, "text": "high dimensional"}, {"st": 34, "ed": 36, "text": "l1 norm"}, {"st": 41, "ed": 43, "text": "inverse covariance"}, {"st": 55, "ed": 57, "text": "previous methods"}, {"st": 74, "ed": 76, "text": "l1 regularized"}, {"st": 86, "ed": 88, "text": "inverse covariance"}, {"st": 109, "ed": 111, "text": "generalization performance"}, {"st": 116, "ed": 118, "text": "network analysis"}]
[{"st": 22, "ed": 24, "text": "transfer learning"}, {"st": 30, "ed": 32, "text": "fully bayesian"}, {"st": 103, "ed": 105, "text": "similarity measures"}, {"st": 124, "ed": 126, "text": "convex objective"}, {"st": 128, "ed": 130, "text": "learning problems"}, {"st": 155, "ed": 157, "text": "joint distributions"}, {"st": 178, "ed": 180, "text": "real life"}]
[{"st": 2, "ed": 4, "text": "machine learning"}, {"st": 5, "ed": 8, "text": "labeled training data"}, {"st": 11, "ed": 13, "text": "unlabeled data"}, {"st": 26, "ed": 28, "text": "multiple views"}, {"st": 49, "ed": 51, "text": "multi view"}]
[{"st": 0, "ed": 2, "text": "parameter estimation"}, {"st": 3, "ed": 7, "text": "markov random fields mrfs"}, {"st": 9, "ed": 11, "text": "difficult task"}, {"st": 29, "ed": 31, "text": "exact inference"}, {"st": 36, "ed": 39, "text": "loopy belief propagation"}, {"st": 65, "ed": 67, "text": "maximum likelihood"}, {"st": 73, "ed": 75, "text": "moment matching"}, {"st": 90, "ed": 92, "text": "unlike previous"}, {"st": 102, "ed": 104, "text": "parameter sharing"}, {"st": 109, "ed": 111, "text": "log linear"}, {"st": 129, "ed": 131, "text": "special case"}, {"st": 158, "ed": 160, "text": "real world"}, {"st": 167, "ed": 169, "text": "significantly outperform"}]
[{"st": 6, "ed": 8, "text": "graphical model"}, {"st": 47, "ed": 49, "text": "conditional independence"}]
[{"st": 0, "ed": 2, "text": "inference problems"}, {"st": 10, "ed": 12, "text": "constrained optimization"}, {"st": 14, "ed": 16, "text": "free energy"}, {"st": 23, "ed": 26, "text": "bethe free energy"}, {"st": 32, "ed": 34, "text": "belief propagation"}, {"st": 39, "ed": 41, "text": "local minima"}, {"st": 57, "ed": 60, "text": "bethe free energy"}, {"st": 74, "ed": 76, "text": "efficient algorithms"}, {"st": 78, "ed": 80, "text": "local minima"}, {"st": 82, "ed": 84, "text": "free energy"}, {"st": 105, "ed": 108, "text": "guaranteed to converge"}, {"st": 110, "ed": 112, "text": "global minimum"}, {"st": 140, "ed": 142, "text": "free energy"}]
[{"st": 5, "ed": 7, "text": "directed graph"}, {"st": 65, "ed": 67, "text": "closed form"}, {"st": 93, "ed": 97, "text": "unsupervised and semi supervised"}]
[{"st": 4, "ed": 6, "text": "natural extension"}, {"st": 46, "ed": 48, "text": "mean shift"}]
[{"st": 4, "ed": 6, "text": "generalization error"}, {"st": 10, "ed": 12, "text": "training samples"}, {"st": 24, "ed": 26, "text": "generalization error"}, {"st": 59, "ed": 61, "text": "generalization error"}, {"st": 79, "ed": 81, "text": "poor performance"}, {"st": 92, "ed": 94, "text": "generalization error"}, {"st": 106, "ed": 108, "text": "normal distribution"}, {"st": 120, "ed": 122, "text": "generalization error"}, {"st": 184, "ed": 186, "text": "simulated data"}]
[{"st": 0, "ed": 2, "text": "nonparametric bayesian"}, {"st": 22, "ed": 25, "text": "bag of words"}, {"st": 69, "ed": 71, "text": "latent feature"}]
[{"st": 11, "ed": 13, "text": "gaussian process"}, {"st": 43, "ed": 45, "text": "additive models"}, {"st": 125, "ed": 127, "text": "gaussian process"}, {"st": 165, "ed": 167, "text": "computational complexity"}, {"st": 180, "ed": 182, "text": "additive models"}, {"st": 193, "ed": 195, "text": "real data"}]
[{"st": 1, "ed": 3, "text": "based clustering"}, {"st": 18, "ed": 22, "text": "synthetic and real world"}, {"st": 40, "ed": 42, "text": "pairwise similarity"}, {"st": 51, "ed": 53, "text": "underlying structure"}, {"st": 54, "ed": 56, "text": "clustering problems"}, {"st": 57, "ed": 59, "text": "similarity measures"}, {"st": 81, "ed": 84, "text": "number of clusters"}, {"st": 117, "ed": 119, "text": "recently introduced"}, {"st": 119, "ed": 121, "text": "affinity propagation"}, {"st": 127, "ed": 129, "text": "belief propagation"}]
[{"st": 0, "ed": 2, "text": "variational bayesian"}, {"st": 5, "ed": 7, "text": "gibbs sampling"}, {"st": 13, "ed": 15, "text": "inference algorithms"}, {"st": 21, "ed": 24, "text": "advantages and disadvantages"}, {"st": 25, "ed": 27, "text": "gibbs sampling"}, {"st": 50, "ed": 52, "text": "variational bayesian"}, {"st": 54, "ed": 57, "text": "efficient and accurate"}, {"st": 97, "ed": 99, "text": "significantly improve"}, {"st": 103, "ed": 105, "text": "variational inference"}]
[{"st": 12, "ed": 15, "text": "structure and parameters"}, {"st": 72, "ed": 74, "text": "structure learning"}, {"st": 90, "ed": 92, "text": "experiments demonstrate"}]
[{"st": 8, "ed": 10, "text": "intrinsic dimensionality"}, {"st": 39, "ed": 41, "text": "intrinsic dimensionality"}, {"st": 59, "ed": 61, "text": "higher dimensional"}, {"st": 70, "ed": 72, "text": "intrinsic dimensionality"}, {"st": 77, "ed": 79, "text": "complementary information"}, {"st": 84, "ed": 86, "text": "nearest neighbor"}, {"st": 103, "ed": 105, "text": "kullback leibler"}, {"st": 114, "ed": 117, "text": "synthetic and real"}, {"st": 126, "ed": 128, "text": "proposed algorithm"}]
[{"st": 1, "ed": 4, "text": "principal component analysis"}, {"st": 7, "ed": 9, "text": "low dimensional"}, {"st": 23, "ed": 25, "text": "maximum likelihood"}, {"st": 65, "ed": 67, "text": "temporal correlations"}, {"st": 88, "ed": 90, "text": "component analysis"}, {"st": 116, "ed": 118, "text": "low rank"}, {"st": 138, "ed": 140, "text": "time series"}, {"st": 147, "ed": 149, "text": "human skeleton"}, {"st": 150, "ed": 152, "text": "motion capture"}]
[{"st": 1, "ed": 3, "text": "wide variety"}, {"st": 4, "ed": 6, "text": "machine learning"}, {"st": 9, "ed": 12, "text": "support vector machine"}, {"st": 45, "ed": 47, "text": "robust optimization"}, {"st": 77, "ed": 79, "text": "provide theoretical"}, {"st": 105, "ed": 108, "text": "non convex optimization"}]
[{"st": 19, "ed": 21, "text": "supervised classification"}, {"st": 34, "ed": 36, "text": "class distributions"}, {"st": 42, "ed": 44, "text": "class distributions"}, {"st": 70, "ed": 72, "text": "data model"}, {"st": 74, "ed": 76, "text": "monte carlo"}]
[{"st": 1, "ed": 3, "text": "multitask learning"}, {"st": 15, "ed": 17, "text": "negative transfer"}, {"st": 87, "ed": 89, "text": "proposed method"}, {"st": 112, "ed": 114, "text": "strongly convex"}, {"st": 117, "ed": 119, "text": "efficiently solved"}, {"st": 130, "ed": 134, "text": "synthetic and real world"}, {"st": 146, "ed": 148, "text": "proposed method"}, {"st": 156, "ed": 158, "text": "specific task"}]
[{"st": 18, "ed": 20, "text": "space complexity"}, {"st": 25, "ed": 27, "text": "linear complexity"}, {"st": 35, "ed": 37, "text": "feature spaces"}, {"st": 41, "ed": 43, "text": "low dimensional"}, {"st": 68, "ed": 70, "text": "similar performance"}, {"st": 72, "ed": 74, "text": "natural language"}]
[{"st": 4, "ed": 7, "text": "multi task learning"}, {"st": 14, "ed": 16, "text": "relevant features"}, {"st": 26, "ed": 28, "text": "existing methods"}, {"st": 42, "ed": 44, "text": "feature space"}, {"st": 49, "ed": 51, "text": "real world"}, {"st": 62, "ed": 64, "text": "related tasks"}, {"st": 77, "ed": 79, "text": "exponentially large"}, {"st": 93, "ed": 95, "text": "main contribution"}, {"st": 111, "ed": 113, "text": "related tasks"}, {"st": 122, "ed": 124, "text": "feature space"}, {"st": 151, "ed": 153, "text": "feature space"}, {"st": 171, "ed": 173, "text": "feature space"}, {"st": 180, "ed": 182, "text": "active set"}, {"st": 194, "ed": 196, "text": "exponentially large"}, {"st": 221, "ed": 223, "text": "related tasks"}, {"st": 224, "ed": 226, "text": "empirical results"}, {"st": 227, "ed": 229, "text": "benchmark datasets"}, {"st": 243, "ed": 246, "text": "multi task learning"}]
[{"st": 6, "ed": 8, "text": "approach called"}, {"st": 9, "ed": 11, "text": "semi supervised"}, {"st": 11, "ed": 13, "text": "metric learning"}, {"st": 18, "ed": 20, "text": "metric learning"}, {"st": 34, "ed": 36, "text": "mahalanobis distance"}, {"st": 44, "ed": 46, "text": "labeled data"}, {"st": 50, "ed": 52, "text": "unlabeled data"}, {"st": 58, "ed": 61, "text": "supervised and unsupervised"}, {"st": 78, "ed": 80, "text": "low rank"}, {"st": 90, "ed": 92, "text": "solved efficiently"}, {"st": 108, "ed": 110, "text": "experiments demonstrate"}, {"st": 112, "ed": 115, "text": "compares favorably with"}, {"st": 118, "ed": 121, "text": "global and local"}, {"st": 121, "ed": 123, "text": "metric learning"}]
[{"st": 19, "ed": 21, "text": "active learning"}, {"st": 30, "ed": 32, "text": "hash codes"}, {"st": 65, "ed": 67, "text": "key idea"}, {"st": 74, "ed": 76, "text": "hash functions"}, {"st": 86, "ed": 88, "text": "hash functions"}, {"st": 129, "ed": 131, "text": "random projection"}, {"st": 133, "ed": 135, "text": "large scale"}, {"st": 135, "ed": 137, "text": "active learning"}]
[{"st": 2, "ed": 5, "text": "classification and regression"}, {"st": 8, "ed": 10, "text": "machine learning"}, {"st": 16, "ed": 18, "text": "parametric models"}, {"st": 23, "ed": 25, "text": "computationally efficient"}, {"st": 33, "ed": 35, "text": "decision trees"}, {"st": 58, "ed": 60, "text": "scoring function"}, {"st": 65, "ed": 68, "text": "classification and regression"}, {"st": 93, "ed": 95, "text": "differential entropy"}, {"st": 107, "ed": 109, "text": "predictive performance"}, {"st": 116, "ed": 118, "text": "decision tree"}]
[]
[{"st": 6, "ed": 8, "text": "structure learning"}, {"st": 25, "ed": 27, "text": "local learning"}, {"st": 36, "ed": 39, "text": "low rank matrix"}, {"st": 50, "ed": 52, "text": "clustering method"}, {"st": 55, "ed": 57, "text": "structure learning"}, {"st": 60, "ed": 62, "text": "clustering method"}, {"st": 91, "ed": 93, "text": "synthetic data"}, {"st": 93, "ed": 95, "text": "handwritten digit"}, {"st": 97, "ed": 99, "text": "motion capture"}, {"st": 109, "ed": 111, "text": "proposed approach"}, {"st": 114, "ed": 116, "text": "clustering accuracy"}, {"st": 118, "ed": 120, "text": "promising results"}, {"st": 121, "ed": 123, "text": "challenging tasks"}, {"st": 124, "ed": 126, "text": "human motion"}]
[{"st": 3, "ed": 5, "text": "convergence rate"}, {"st": 10, "ed": 12, "text": "np hard"}, {"st": 33, "ed": 36, "text": "structure and parameters"}, {"st": 43, "ed": 45, "text": "convergence rate"}, {"st": 85, "ed": 87, "text": "proximal gradient"}, {"st": 140, "ed": 143, "text": "guaranteed to converge"}]
[{"st": 2, "ed": 5, "text": "principal component analysis"}, {"st": 36, "ed": 38, "text": "robust pca"}, {"st": 42, "ed": 44, "text": "theoretical properties"}, {"st": 73, "ed": 75, "text": "proposed method"}, {"st": 78, "ed": 80, "text": "computational efficiency"}, {"st": 85, "ed": 87, "text": "large scale"}]
[{"st": 0, "ed": 2, "text": "kernel based"}, {"st": 2, "ed": 4, "text": "online learning"}, {"st": 14, "ed": 16, "text": "online learning"}, {"st": 30, "ed": 32, "text": "support vectors"}, {"st": 41, "ed": 43, "text": "large scale"}, {"st": 53, "ed": 55, "text": "kernel based"}, {"st": 55, "ed": 57, "text": "online learning"}, {"st": 64, "ed": 66, "text": "support vectors"}, {"st": 81, "ed": 83, "text": "computationally efficient"}, {"st": 110, "ed": 112, "text": "kernel based"}, {"st": 112, "ed": 114, "text": "online learning"}, {"st": 124, "ed": 126, "text": "efficient algorithms"}, {"st": 134, "ed": 136, "text": "kernel based"}, {"st": 136, "ed": 138, "text": "online learning"}, {"st": 142, "ed": 144, "text": "support vectors"}, {"st": 145, "ed": 147, "text": "uniform sampling"}, {"st": 152, "ed": 154, "text": "support vectors"}, {"st": 160, "ed": 162, "text": "theoretical analysis"}, {"st": 163, "ed": 165, "text": "regret bound"}, {"st": 171, "ed": 173, "text": "empirical performance"}, {"st": 190, "ed": 192, "text": "kernel based"}, {"st": 192, "ed": 194, "text": "online learning"}, {"st": 195, "ed": 197, "text": "large scale"}]
[{"st": 13, "ed": 15, "text": "latent variables"}, {"st": 41, "ed": 43, "text": "posterior distributions"}, {"st": 49, "ed": 51, "text": "training data"}, {"st": 62, "ed": 65, "text": "restricted boltzmann machines"}, {"st": 67, "ed": 70, "text": "undirected graphical models"}, {"st": 72, "ed": 75, "text": "single hidden layer"}, {"st": 99, "ed": 101, "text": "layer wise"}, {"st": 101, "ed": 103, "text": "learning algorithm"}, {"st": 133, "ed": 136, "text": "learning and inference"}, {"st": 156, "ed": 158, "text": "higher level"}, {"st": 162, "ed": 164, "text": "demonstrate empirically"}, {"st": 177, "ed": 180, "text": "restricted boltzmann machine"}, {"st": 182, "ed": 184, "text": "wide variety"}]
[{"st": 9, "ed": 12, "text": "received increasing attention"}, {"st": 20, "ed": 22, "text": "based methods"}, {"st": 24, "ed": 26, "text": "widely applied"}, {"st": 36, "ed": 38, "text": "building block"}, {"st": 51, "ed": 53, "text": "ell 1"}, {"st": 54, "ed": 56, "text": "ell 1"}, {"st": 152, "ed": 154, "text": "empirical study"}, {"st": 157, "ed": 159, "text": "method achieves"}]
[{"st": 2, "ed": 4, "text": "subspace clustering"}, {"st": 17, "ed": 19, "text": "spectral clustering"}, {"st": 61, "ed": 63, "text": "subspace clustering"}, {"st": 89, "ed": 91, "text": "affinity matrix"}, {"st": 93, "ed": 95, "text": "spectral clustering"}, {"st": 116, "ed": 121, "text": "synthetic and real world datasets"}]
[{"st": 8, "ed": 10, "text": "learning algorithms"}, {"st": 10, "ed": 12, "text": "achieve high"}, {"st": 13, "ed": 15, "text": "low dimensional"}, {"st": 38, "ed": 40, "text": "existing algorithms"}, {"st": 45, "ed": 47, "text": "spectral methods"}, {"st": 62, "ed": 65, "text": "global and local"}, {"st": 75, "ed": 78, "text": "orders of magnitude"}]
[{"st": 2, "ed": 4, "text": "supervised learning"}, {"st": 11, "ed": 14, "text": "training and test"}, {"st": 17, "ed": 19, "text": "probability distributions"}, {"st": 32, "ed": 34, "text": "covariate shift"}, {"st": 39, "ed": 41, "text": "confidence bounds"}, {"st": 49, "ed": 51, "text": "convergence rate"}]
[{"st": 4, "ed": 6, "text": "topic model"}, {"st": 40, "ed": 42, "text": "labeled data"}, {"st": 59, "ed": 62, "text": "number of topics"}, {"st": 103, "ed": 105, "text": "labeled images"}, {"st": 106, "ed": 108, "text": "image segmentation"}]
[{"st": 3, "ed": 5, "text": "max margin"}, {"st": 6, "ed": 8, "text": "latent feature"}, {"st": 14, "ed": 16, "text": "max margin"}, {"st": 23, "ed": 25, "text": "latent features"}, {"st": 26, "ed": 28, "text": "link prediction"}, {"st": 39, "ed": 41, "text": "hinge loss"}, {"st": 49, "ed": 51, "text": "posterior inference"}, {"st": 56, "ed": 58, "text": "highly nonlinear"}, {"st": 59, "ed": 61, "text": "likelihood function"}, {"st": 64, "ed": 66, "text": "fully bayesian"}, {"st": 76, "ed": 78, "text": "real datasets"}, {"st": 85, "ed": 87, "text": "max margin"}, {"st": 89, "ed": 91, "text": "fully bayesian"}]
[{"st": 2, "ed": 4, "text": "real world"}, {"st": 6, "ed": 8, "text": "machine learning"}, {"st": 37, "ed": 39, "text": "ranking loss"}, {"st": 60, "ed": 62, "text": "probability distributions"}, {"st": 81, "ed": 83, "text": "real world"}]
[{"st": 3, "ed": 5, "text": "loss functions"}, {"st": 16, "ed": 18, "text": "probability distributions"}, {"st": 28, "ed": 30, "text": "strong convexity"}]
[{"st": 25, "ed": 27, "text": "recently proposed"}]
[{"st": 0, "ed": 2, "text": "variational methods"}, {"st": 6, "ed": 8, "text": "approximate posterior"}, {"st": 33, "ed": 35, "text": "variational approximations"}, {"st": 52, "ed": 54, "text": "variational parameters"}, {"st": 64, "ed": 66, "text": "marginal likelihood"}, {"st": 98, "ed": 100, "text": "logistic regression"}, {"st": 104, "ed": 106, "text": "matrix factorization"}, {"st": 109, "ed": 111, "text": "predictive performance"}, {"st": 119, "ed": 121, "text": "variational methods"}, {"st": 122, "ed": 124, "text": "sample based"}, {"st": 139, "ed": 141, "text": "variational methods"}]
[{"st": 0, "ed": 4, "text": "canonical correlation analysis cca"}, {"st": 19, "ed": 21, "text": "recent years"}, {"st": 24, "ed": 26, "text": "widely applied"}, {"st": 73, "ed": 75, "text": "linear models"}, {"st": 100, "ed": 102, "text": "recent developments"}, {"st": 117, "ed": 119, "text": "theoretical properties"}, {"st": 136, "ed": 138, "text": "extensive simulations"}]
[{"st": 41, "ed": 43, "text": "power law"}, {"st": 60, "ed": 62, "text": "recently developed"}]
[{"st": 57, "ed": 59, "text": "large datasets"}, {"st": 66, "ed": 68, "text": "hierarchical clustering"}, {"st": 75, "ed": 77, "text": "clustering algorithm"}, {"st": 88, "ed": 90, "text": "performance measurement"}, {"st": 101, "ed": 103, "text": "spectral clustering"}]
[{"st": 10, "ed": 12, "text": "additive models"}, {"st": 14, "ed": 16, "text": "prior knowledge"}, {"st": 33, "ed": 35, "text": "previous works"}, {"st": 38, "ed": 40, "text": "group sparsity"}, {"st": 45, "ed": 47, "text": "group lasso"}, {"st": 59, "ed": 61, "text": "structural information"}, {"st": 63, "ed": 65, "text": "additive models"}, {"st": 73, "ed": 75, "text": "method called"}, {"st": 75, "ed": 77, "text": "group sparse"}, {"st": 77, "ed": 79, "text": "additive models"}, {"st": 83, "ed": 85, "text": "group sparsity"}, {"st": 99, "ed": 101, "text": "sparsity inducing"}, {"st": 118, "ed": 120, "text": "group level"}, {"st": 124, "ed": 127, "text": "block coordinate descent"}, {"st": 141, "ed": 143, "text": "competing methods"}, {"st": 149, "ed": 151, "text": "prediction accuracy"}, {"st": 152, "ed": 154, "text": "additive models"}, {"st": 163, "ed": 165, "text": "breast cancer"}]
[{"st": 1, "ed": 3, "text": "real world"}, {"st": 3, "ed": 5, "text": "classification problems"}, {"st": 59, "ed": 61, "text": "labeled data"}, {"st": 83, "ed": 85, "text": "probability distributions"}, {"st": 86, "ed": 89, "text": "training and test"}, {"st": 97, "ed": 99, "text": "proposed approach"}]
[{"st": 7, "ed": 9, "text": "linear regression"}, {"st": 13, "ed": 16, "text": "support vector regression"}, {"st": 40, "ed": 43, "text": "simple and efficient"}, {"st": 50, "ed": 52, "text": "ridge regression"}, {"st": 56, "ed": 58, "text": "total number"}, {"st": 74, "ed": 77, "text": "support vector regression"}, {"st": 94, "ed": 96, "text": "open problem"}, {"st": 108, "ed": 110, "text": "theoretical bounds"}]
[{"st": 9, "ed": 12, "text": "hidden markov models"}, {"st": 24, "ed": 26, "text": "recently developed"}, {"st": 31, "ed": 33, "text": "hidden variables"}, {"st": 34, "ed": 36, "text": "mixture models"}, {"st": 45, "ed": 47, "text": "mixture models"}, {"st": 71, "ed": 73, "text": "mixture models"}, {"st": 74, "ed": 76, "text": "desirable properties"}, {"st": 87, "ed": 89, "text": "log likelihood"}, {"st": 93, "ed": 95, "text": "hidden state"}, {"st": 114, "ed": 116, "text": "hyper parameter"}, {"st": 119, "ed": 121, "text": "model selection"}, {"st": 136, "ed": 138, "text": "variational bayesian"}, {"st": 149, "ed": 152, "text": "accuracy and computational"}]
[{"st": 0, "ed": 2, "text": "map inference"}, {"st": 18, "ed": 20, "text": "linear programming"}, {"st": 29, "ed": 31, "text": "quadratic programming"}, {"st": 42, "ed": 45, "text": "kullback leibler divergence"}, {"st": 65, "ed": 67, "text": "efficient algorithms"}, {"st": 78, "ed": 80, "text": "convex objective"}, {"st": 81, "ed": 83, "text": "belief propagation"}, {"st": 89, "ed": 95, "text": "experiments on synthetic and real world"}, {"st": 104, "ed": 106, "text": "substantially improve"}]
[{"st": 11, "ed": 13, "text": "learning representations"}, {"st": 14, "ed": 16, "text": "supervised learning"}, {"st": 19, "ed": 21, "text": "original input"}, {"st": 59, "ed": 61, "text": "feature vectors"}, {"st": 80, "ed": 82, "text": "probabilistic model"}, {"st": 90, "ed": 92, "text": "hidden variables"}, {"st": 99, "ed": 101, "text": "conventional approaches"}]
[{"st": 9, "ed": 11, "text": "exponential families"}, {"st": 29, "ed": 31, "text": "sufficient statistics"}, {"st": 53, "ed": 55, "text": "kernel density"}, {"st": 59, "ed": 62, "text": "under mild conditions"}, {"st": 68, "ed": 70, "text": "sufficient statistics"}, {"st": 77, "ed": 79, "text": "exponential family"}, {"st": 92, "ed": 94, "text": "proposed approach"}, {"st": 98, "ed": 100, "text": "random graph"}]
[{"st": 8, "ed": 10, "text": "variational inference"}, {"st": 13, "ed": 15, "text": "probabilistic models"}, {"st": 24, "ed": 26, "text": "existing approaches"}, {"st": 32, "ed": 34, "text": "variational inference"}, {"st": 56, "ed": 58, "text": "optimization methods"}, {"st": 79, "ed": 81, "text": "mean field"}, {"st": 90, "ed": 92, "text": "speed ups"}, {"st": 93, "ed": 95, "text": "probabilistic models"}]
[{"st": 5, "ed": 7, "text": "rule based"}, {"st": 7, "ed": 10, "text": "semi supervised learning"}, {"st": 14, "ed": 16, "text": "successfully applied"}, {"st": 71, "ed": 73, "text": "objective function"}, {"st": 79, "ed": 81, "text": "cross entropy"}, {"st": 102, "ed": 104, "text": "rule based"}, {"st": 104, "ed": 107, "text": "semi supervised learning"}]
[{"st": 0, "ed": 2, "text": "sparse coding"}, {"st": 4, "ed": 6, "text": "unsupervised learning"}, {"st": 19, "ed": 21, "text": "unlabeled data"}, {"st": 27, "ed": 29, "text": "sparse linear"}, {"st": 42, "ed": 44, "text": "visual cortex"}, {"st": 44, "ed": 46, "text": "sparse coding"}, {"st": 65, "ed": 67, "text": "supervised classification"}, {"st": 72, "ed": 74, "text": "unlabeled data"}, {"st": 82, "ed": 84, "text": "supervised learning"}, {"st": 87, "ed": 89, "text": "sparse coding"}, {"st": 94, "ed": 96, "text": "sparse coding"}, {"st": 100, "ed": 102, "text": "time series"}, {"st": 107, "ed": 109, "text": "basis functions"}, {"st": 134, "ed": 136, "text": "convex optimization"}, {"st": 146, "ed": 148, "text": "l1 regularized"}, {"st": 149, "ed": 151, "text": "least squares"}, {"st": 154, "ed": 157, "text": "hundreds of thousands"}, {"st": 159, "ed": 161, "text": "existing methods"}, {"st": 181, "ed": 183, "text": "efficiently compute"}, {"st": 196, "ed": 198, "text": "least squares"}, {"st": 202, "ed": 204, "text": "complex valued"}, {"st": 241, "ed": 243, "text": "classification tasks"}, {"st": 254, "ed": 256, "text": "learned features"}]
[{"st": 0, "ed": 2, "text": "inference problems"}, {"st": 12, "ed": 14, "text": "constrained optimization"}, {"st": 15, "ed": 17, "text": "message passing"}, {"st": 20, "ed": 22, "text": "belief propagation"}, {"st": 37, "ed": 39, "text": "convergence guarantees"}, {"st": 65, "ed": 68, "text": "guaranteed to converge"}, {"st": 70, "ed": 72, "text": "global optimum"}]
[{"st": 55, "ed": 57, "text": "dynamic programming"}, {"st": 132, "ed": 134, "text": "proposal distribution"}, {"st": 157, "ed": 159, "text": "structure learning"}]
[{"st": 17, "ed": 19, "text": "observed behavior"}, {"st": 30, "ed": 32, "text": "reward function"}, {"st": 45, "ed": 47, "text": "reward function"}, {"st": 51, "ed": 53, "text": "optimal policy"}, {"st": 99, "ed": 101, "text": "proposed method"}]
[{"st": 4, "ed": 6, "text": "active learning"}, {"st": 11, "ed": 13, "text": "non stationary"}, {"st": 15, "ed": 17, "text": "real world"}, {"st": 24, "ed": 26, "text": "predictive model"}, {"st": 61, "ed": 63, "text": "labeled data"}, {"st": 65, "ed": 67, "text": "active learning"}, {"st": 88, "ed": 90, "text": "predictive performance"}, {"st": 96, "ed": 101, "text": "simulated and real world data"}]
[{"st": 4, "ed": 6, "text": "bayesian network"}, {"st": 9, "ed": 12, "text": "a posteriori map"}, {"st": 41, "ed": 43, "text": "efficient learning"}, {"st": 87, "ed": 89, "text": "temporal data"}, {"st": 91, "ed": 94, "text": "dynamic bayesian networks"}, {"st": 121, "ed": 123, "text": "bayes net"}, {"st": 129, "ed": 131, "text": "naive bayes"}, {"st": 138, "ed": 140, "text": "feature selection"}]
[{"st": 0, "ed": 3, "text": "distance metric learning"}, {"st": 12, "ed": 14, "text": "statistical classification"}, {"st": 19, "ed": 21, "text": "existing approaches"}, {"st": 23, "ed": 25, "text": "distance metrics"}, {"st": 26, "ed": 28, "text": "pairwise constraints"}, {"st": 43, "ed": 45, "text": "distance metric"}, {"st": 54, "ed": 56, "text": "training examples"}, {"st": 65, "ed": 67, "text": "training examples"}, {"st": 85, "ed": 88, "text": "distance metric learning"}, {"st": 91, "ed": 93, "text": "posterior distribution"}, {"st": 95, "ed": 97, "text": "distance metric"}, {"st": 125, "ed": 128, "text": "distance metric learning"}, {"st": 147, "ed": 149, "text": "proposed framework"}, {"st": 149, "ed": 151, "text": "achieves higher"}, {"st": 151, "ed": 153, "text": "classification accuracy"}, {"st": 157, "ed": 159, "text": "training examples"}, {"st": 169, "ed": 172, "text": "distance metric learning"}]
[{"st": 1, "ed": 3, "text": "marginal likelihood"}, {"st": 13, "ed": 15, "text": "bayesian network"}, {"st": 29, "ed": 31, "text": "network structures"}, {"st": 50, "ed": 52, "text": "sample size"}, {"st": 86, "ed": 88, "text": "network structure"}, {"st": 88, "ed": 90, "text": "optimization problem"}]
[{"st": 7, "ed": 10, "text": "multiple kernel learning"}, {"st": 84, "ed": 86, "text": "prior methods"}, {"st": 88, "ed": 90, "text": "empirical evaluation"}, {"st": 97, "ed": 99, "text": "significantly faster"}, {"st": 101, "ed": 103, "text": "compare favorably"}]
[{"st": 4, "ed": 6, "text": "computationally efficient"}, {"st": 34, "ed": 36, "text": "parameter estimates"}, {"st": 37, "ed": 39, "text": "low order"}, {"st": 49, "ed": 51, "text": "computationally efficient"}, {"st": 63, "ed": 65, "text": "mixture models"}, {"st": 69, "ed": 71, "text": "mixture components"}]
[{"st": 6, "ed": 8, "text": "principled approach"}, {"st": 12, "ed": 14, "text": "classification problem"}, {"st": 42, "ed": 44, "text": "existing methods"}, {"st": 84, "ed": 86, "text": "benchmark datasets"}, {"st": 90, "ed": 92, "text": "real world"}]
[{"st": 8, "ed": 10, "text": "gaussian process"}, {"st": 29, "ed": 32, "text": "efficient and effective"}, {"st": 56, "ed": 58, "text": "parameter estimation"}, {"st": 69, "ed": 71, "text": "sampling based"}, {"st": 73, "ed": 75, "text": "selection method"}, {"st": 79, "ed": 81, "text": "generalization performance"}, {"st": 98, "ed": 100, "text": "parameter estimation"}, {"st": 137, "ed": 139, "text": "generalization performance"}, {"st": 144, "ed": 146, "text": "selection method"}, {"st": 148, "ed": 150, "text": "benchmark datasets"}]
[{"st": 8, "ed": 10, "text": "gaussian process"}, {"st": 20, "ed": 22, "text": "cross validation"}, {"st": 23, "ed": 25, "text": "based optimization"}, {"st": 33, "ed": 35, "text": "predictive distributions"}, {"st": 61, "ed": 63, "text": "f measure"}, {"st": 65, "ed": 67, "text": "error rate"}, {"st": 80, "ed": 82, "text": "predictive distributions"}, {"st": 92, "ed": 94, "text": "predictive distributions"}, {"st": 96, "ed": 98, "text": "expectation propagation"}, {"st": 101, "ed": 103, "text": "conduct experiments"}, {"st": 105, "ed": 107, "text": "real world"}, {"st": 127, "ed": 129, "text": "generalization performance"}, {"st": 139, "ed": 141, "text": "f measure"}, {"st": 145, "ed": 147, "text": "f measure"}, {"st": 147, "ed": 149, "text": "generalization performance"}]
[{"st": 40, "ed": 42, "text": "random variables"}]
[{"st": 5, "ed": 8, "text": "k nearest neighbor"}, {"st": 33, "ed": 35, "text": "shortest path"}, {"st": 41, "ed": 43, "text": "sample size"}, {"st": 59, "ed": 61, "text": "distance function"}, {"st": 79, "ed": 81, "text": "shortest path"}]
[{"st": 12, "ed": 14, "text": "sparse models"}, {"st": 20, "ed": 22, "text": "observed data"}, {"st": 26, "ed": 28, "text": "linear combination"}, {"st": 37, "ed": 39, "text": "precision matrix"}, {"st": 73, "ed": 75, "text": "sufficient conditions"}, {"st": 92, "ed": 94, "text": "ell 1"}, {"st": 95, "ed": 98, "text": "maximum likelihood estimator"}, {"st": 98, "ed": 100, "text": "ell 1"}, {"st": 166, "ed": 168, "text": "ell 1"}]
[{"st": 12, "ed": 14, "text": "class probabilities"}, {"st": 29, "ed": 31, "text": "theoretical framework"}, {"st": 78, "ed": 80, "text": "empirical results"}]
[{"st": 11, "ed": 13, "text": "optimization problems"}, {"st": 14, "ed": 16, "text": "nuclear norm"}, {"st": 17, "ed": 19, "text": "unlike existing"}, {"st": 28, "ed": 30, "text": "low rank"}, {"st": 45, "ed": 47, "text": "linear algebra"}, {"st": 57, "ed": 59, "text": "low rank"}, {"st": 76, "ed": 78, "text": "matrix completion"}, {"st": 86, "ed": 88, "text": "highly competitive"}, {"st": 90, "ed": 92, "text": "recently proposed"}]
[{"st": 19, "ed": 21, "text": "base classifiers"}, {"st": 55, "ed": 57, "text": "base classifier"}, {"st": 76, "ed": 78, "text": "base classifiers"}, {"st": 115, "ed": 117, "text": "object detection"}, {"st": 140, "ed": 142, "text": "multi class"}, {"st": 145, "ed": 147, "text": "multi class"}, {"st": 161, "ed": 163, "text": "significantly improve"}]
[{"st": 9, "ed": 11, "text": "latent variable"}, {"st": 45, "ed": 48, "text": "non convex optimization"}, {"st": 60, "ed": 62, "text": "convex relaxation"}, {"st": 75, "ed": 77, "text": "regularization parameter"}]
[{"st": 4, "ed": 6, "text": "convex functions"}, {"st": 7, "ed": 9, "text": "noisy observations"}, {"st": 40, "ed": 42, "text": "gaussian process"}, {"st": 71, "ed": 73, "text": "strong performance"}, {"st": 79, "ed": 81, "text": "sample complexity"}, {"st": 89, "ed": 91, "text": "cumulative regret"}, {"st": 95, "ed": 97, "text": "empirical evidence"}]
[{"st": 6, "ed": 9, "text": "linear dimensionality reduction"}, {"st": 16, "ed": 18, "text": "projection matrix"}, {"st": 23, "ed": 25, "text": "mutual information"}, {"st": 31, "ed": 33, "text": "class label"}, {"st": 36, "ed": 38, "text": "shannon entropy"}, {"st": 43, "ed": 45, "text": "recent theoretical"}, {"st": 50, "ed": 52, "text": "mutual information"}, {"st": 54, "ed": 56, "text": "optimization problem"}, {"st": 70, "ed": 72, "text": "theoretical analysis"}, {"st": 73, "ed": 75, "text": "empirical comparison"}, {"st": 79, "ed": 81, "text": "proposed method"}, {"st": 83, "ed": 85, "text": "closely related"}, {"st": 86, "ed": 89, "text": "linear discriminant analysis"}, {"st": 110, "ed": 112, "text": "mutual information"}, {"st": 133, "ed": 135, "text": "proposed method"}, {"st": 136, "ed": 138, "text": "promising results"}]
[{"st": 17, "ed": 19, "text": "reinforcement learning"}, {"st": 79, "ed": 81, "text": "linear regression"}, {"st": 104, "ed": 106, "text": "robotic arm"}]
[{"st": 0, "ed": 3, "text": "online learning algorithms"}, {"st": 23, "ed": 25, "text": "online algorithm"}, {"st": 26, "ed": 29, "text": "ability to learn"}, {"st": 40, "ed": 42, "text": "standard definition"}, {"st": 55, "ed": 57, "text": "online algorithm"}, {"st": 78, "ed": 80, "text": "online algorithm"}, {"st": 95, "ed": 97, "text": "bandit algorithm"}, {"st": 129, "ed": 131, "text": "bandit algorithm"}, {"st": 134, "ed": 136, "text": "regret bound"}]
[{"st": 6, "ed": 8, "text": "loss minimization"}, {"st": 21, "ed": 23, "text": "convex surrogate"}, {"st": 107, "ed": 109, "text": "regret bounds"}, {"st": 116, "ed": 119, "text": "efficient and scalable"}]
[{"st": 4, "ed": 6, "text": "exploration exploitation"}, {"st": 31, "ed": 33, "text": "simultaneously learn"}, {"st": 53, "ed": 57, "text": "multi armed bandit problem"}, {"st": 66, "ed": 68, "text": "gaussian process"}, {"st": 101, "ed": 103, "text": "ucb algorithm"}, {"st": 119, "ed": 121, "text": "cumulative regret"}, {"st": 123, "ed": 125, "text": "parallel algorithm"}, {"st": 129, "ed": 131, "text": "constant factor"}, {"st": 134, "ed": 136, "text": "batch size"}, {"st": 159, "ed": 161, "text": "real world"}]
[{"st": 4, "ed": 6, "text": "binary classification"}, {"st": 52, "ed": 54, "text": "real world"}, {"st": 71, "ed": 73, "text": "generalization error"}, {"st": 84, "ed": 86, "text": "decision theory"}, {"st": 123, "ed": 125, "text": "theoretical result"}, {"st": 131, "ed": 133, "text": "optimal policy"}, {"st": 159, "ed": 161, "text": "search space"}, {"st": 168, "ed": 170, "text": "optimal policy"}]
[{"st": 5, "ed": 7, "text": "object recognition"}, {"st": 21, "ed": 23, "text": "labeled examples"}, {"st": 31, "ed": 33, "text": "feature learning"}, {"st": 43, "ed": 46, "text": "spike and slab"}, {"st": 46, "ed": 48, "text": "sparse coding"}, {"st": 79, "ed": 81, "text": "inference procedure"}, {"st": 95, "ed": 97, "text": "training set"}, {"st": 102, "ed": 104, "text": "latent factors"}, {"st": 118, "ed": 120, "text": "supervised learning"}, {"st": 123, "ed": 125, "text": "sparse coding"}, {"st": 127, "ed": 130, "text": "spike and slab"}, {"st": 130, "ed": 133, "text": "restricted boltzmann machine"}, {"st": 136, "ed": 138, "text": "cifar 10"}, {"st": 142, "ed": 144, "text": "cifar 100"}, {"st": 175, "ed": 177, "text": "hierarchical models"}, {"st": 177, "ed": 179, "text": "transfer learning"}]
[{"st": 30, "ed": 32, "text": "map inference"}]
[{"st": 9, "ed": 11, "text": "semi supervised"}, {"st": 13, "ed": 15, "text": "key idea"}, {"st": 27, "ed": 30, "text": "labeled and unlabeled"}, {"st": 33, "ed": 35, "text": "basis functions"}, {"st": 64, "ed": 66, "text": "error bound"}, {"st": 80, "ed": 82, "text": "proposed algorithm"}]
[{"st": 5, "ed": 7, "text": "multi class"}, {"st": 9, "ed": 11, "text": "weakly supervised"}, {"st": 28, "ed": 31, "text": "block coordinate descent"}, {"st": 34, "ed": 37, "text": "expectation maximization em"}, {"st": 50, "ed": 52, "text": "cost function"}, {"st": 55, "ed": 57, "text": "convex relaxation"}, {"st": 67, "ed": 69, "text": "specifically designed"}, {"st": 70, "ed": 72, "text": "efficiently solve"}, {"st": 80, "ed": 83, "text": "method compares favorably"}, {"st": 90, "ed": 93, "text": "multiple instance learning"}, {"st": 94, "ed": 97, "text": "semi supervised learning"}]
[{"st": 18, "ed": 20, "text": "large datasets"}, {"st": 57, "ed": 59, "text": "computationally efficient"}, {"st": 72, "ed": 74, "text": "distributed computing"}, {"st": 80, "ed": 82, "text": "statistical efficiency"}, {"st": 84, "ed": 86, "text": "theoretical properties"}, {"st": 95, "ed": 97, "text": "extensive empirical"}, {"st": 112, "ed": 114, "text": "large scale"}]
[{"st": 0, "ed": 2, "text": "latent variable"}, {"st": 68, "ed": 70, "text": "hierarchical bayesian"}, {"st": 91, "ed": 93, "text": "latent feature"}, {"st": 116, "ed": 118, "text": "significantly improved"}, {"st": 118, "ed": 120, "text": "predictive performance"}, {"st": 124, "ed": 126, "text": "link prediction"}]
[{"st": 4, "ed": 7, "text": "multi task learning"}, {"st": 10, "ed": 12, "text": "prediction tasks"}, {"st": 13, "ed": 15, "text": "learned jointly"}, {"st": 25, "ed": 27, "text": "multi task"}, {"st": 136, "ed": 138, "text": "approach outperforms"}]
[{"st": 1, "ed": 3, "text": "multiple tasks"}, {"st": 8, "ed": 10, "text": "challenging problem"}, {"st": 12, "ed": 14, "text": "feature space"}, {"st": 27, "ed": 29, "text": "multiple tasks"}, {"st": 48, "ed": 50, "text": "jointly learn"}, {"st": 90, "ed": 92, "text": "theoretical bounds"}, {"st": 94, "ed": 96, "text": "estimation error"}, {"st": 108, "ed": 111, "text": "expectation maximization algorithm"}]
[{"st": 0, "ed": 2, "text": "structured learning"}, {"st": 6, "ed": 8, "text": "structured outputs"}, {"st": 19, "ed": 21, "text": "training set"}, {"st": 32, "ed": 34, "text": "ground truth"}, {"st": 43, "ed": 45, "text": "main contribution"}, {"st": 47, "ed": 49, "text": "large margin"}, {"st": 52, "ed": 54, "text": "structured learning"}, {"st": 57, "ed": 59, "text": "annotated data"}, {"st": 62, "ed": 64, "text": "optimization problem"}, {"st": 70, "ed": 72, "text": "efficiently solve"}, {"st": 90, "ed": 92, "text": "assignment problem"}, {"st": 139, "ed": 141, "text": "structured learning"}, {"st": 144, "ed": 146, "text": "empirical comparison"}]
[{"st": 9, "ed": 11, "text": "weak learners"}, {"st": 22, "ed": 24, "text": "theoretical foundation"}, {"st": 66, "ed": 68, "text": "boosting algorithm"}, {"st": 71, "ed": 73, "text": "theoretical guarantee"}, {"st": 95, "ed": 97, "text": "weak learners"}, {"st": 99, "ed": 101, "text": "theoretical results"}, {"st": 112, "ed": 116, "text": "real world data sets"}, {"st": 119, "ed": 121, "text": "proposed algorithm"}, {"st": 121, "ed": 124, "text": "compares favorably with"}]
[{"st": 7, "ed": 9, "text": "topic models"}, {"st": 15, "ed": 17, "text": "gibbs sampling"}, {"st": 49, "ed": 51, "text": "variational inference"}]
[{"st": 6, "ed": 10, "text": "expectation maximization em algorithm"}, {"st": 11, "ed": 14, "text": "gaussian mixture model"}, {"st": 49, "ed": 51, "text": "mixture components"}, {"st": 62, "ed": 64, "text": "dynamic range"}, {"st": 77, "ed": 79, "text": "significantly improves"}, {"st": 93, "ed": 95, "text": "proposed algorithm"}, {"st": 100, "ed": 102, "text": "optimization techniques"}, {"st": 104, "ed": 106, "text": "conjugate gradient"}, {"st": 125, "ed": 127, "text": "mixture model"}, {"st": 134, "ed": 136, "text": "variational bayesian"}]
[{"st": 4, "ed": 6, "text": "kernel methods"}, {"st": 22, "ed": 25, "text": "multiple kernel learning"}, {"st": 34, "ed": 36, "text": "base kernels"}, {"st": 56, "ed": 59, "text": "multiple kernel learning"}, {"st": 65, "ed": 67, "text": "binary classification"}, {"st": 137, "ed": 140, "text": "compares favorably with"}]
[{"st": 8, "ed": 10, "text": "globally optimal"}, {"st": 10, "ed": 12, "text": "bayesian network"}, {"st": 43, "ed": 45, "text": "network structures"}, {"st": 48, "ed": 50, "text": "soft margin"}, {"st": 51, "ed": 53, "text": "successfully applied"}, {"st": 60, "ed": 63, "text": "branch and bound"}, {"st": 66, "ed": 68, "text": "linear programming"}, {"st": 77, "ed": 79, "text": "worst case"}, {"st": 92, "ed": 94, "text": "network structure"}, {"st": 120, "ed": 122, "text": "network structures"}]
[{"st": 2, "ed": 4, "text": "auto encoder"}, {"st": 9, "ed": 11, "text": "input data"}, {"st": 18, "ed": 21, "text": "each data point"}, {"st": 38, "ed": 40, "text": "singular values"}, {"st": 113, "ed": 116, "text": "restricted boltzmann machines"}, {"st": 138, "ed": 140, "text": "lower level"}]
[{"st": 8, "ed": 10, "text": "variational bayes"}, {"st": 15, "ed": 17, "text": "taylor expansion"}, {"st": 22, "ed": 25, "text": "latent dirichlet allocation"}]
[{"st": 6, "ed": 8, "text": "unified framework"}, {"st": 9, "ed": 11, "text": "structured prediction"}, {"st": 12, "ed": 14, "text": "latent variables"}, {"st": 17, "ed": 20, "text": "conditional random fields"}, {"st": 23, "ed": 26, "text": "support vector machines"}, {"st": 45, "ed": 47, "text": "message passing"}, {"st": 61, "ed": 63, "text": "image segmentation"}, {"st": 68, "ed": 70, "text": "scene understanding"}, {"st": 82, "ed": 85, "text": "support vector machines"}]
[{"st": 5, "ed": 8, "text": "unsupervised domain adaptation"}, {"st": 17, "ed": 19, "text": "source domain"}, {"st": 25, "ed": 27, "text": "existing approaches"}, {"st": 29, "ed": 31, "text": "domain invariant"}, {"st": 44, "ed": 46, "text": "jointly learn"}, {"st": 54, "ed": 56, "text": "feature space"}, {"st": 72, "ed": 74, "text": "feature space"}, {"st": 104, "ed": 106, "text": "gradient based"}, {"st": 117, "ed": 119, "text": "labeled data"}, {"st": 123, "ed": 125, "text": "empirical studies"}, {"st": 126, "ed": 128, "text": "benchmark tasks"}, {"st": 129, "ed": 131, "text": "object recognition"}, {"st": 132, "ed": 134, "text": "sentiment analysis"}, {"st": 140, "ed": 142, "text": "significant improvement"}]
[{"st": 0, "ed": 2, "text": "traditional approaches"}, {"st": 5, "ed": 7, "text": "web search"}, {"st": 62, "ed": 64, "text": "behavioral economics"}, {"st": 144, "ed": 146, "text": "a weighting"}, {"st": 164, "ed": 167, "text": "empirical risk minimization"}, {"st": 171, "ed": 173, "text": "efficient learning"}, {"st": 185, "ed": 187, "text": "based approaches"}]
[{"st": 6, "ed": 8, "text": "convex surrogate"}, {"st": 8, "ed": 10, "text": "loss functions"}, {"st": 15, "ed": 17, "text": "error rate"}, {"st": 33, "ed": 35, "text": "convex surrogate"}, {"st": 37, "ed": 39, "text": "hinge loss"}, {"st": 47, "ed": 50, "text": "convex loss functions"}, {"st": 53, "ed": 55, "text": "error rate"}]
[{"st": 6, "ed": 8, "text": "reinforcement learning"}, {"st": 12, "ed": 14, "text": "inverse problems"}, {"st": 59, "ed": 61, "text": "objective functions"}, {"st": 115, "ed": 117, "text": "error bounds"}]
[{"st": 28, "ed": 30, "text": "exponential families"}, {"st": 33, "ed": 35, "text": "bregman divergences"}]
[{"st": 0, "ed": 2, "text": "online optimization"}, {"st": 5, "ed": 7, "text": "powerful tool"}, {"st": 8, "ed": 10, "text": "large scale"}, {"st": 16, "ed": 18, "text": "efficient online"}, {"st": 37, "ed": 39, "text": "batch setting"}, {"st": 45, "ed": 47, "text": "convergence rate"}, {"st": 66, "ed": 68, "text": "online setting"}, {"st": 87, "ed": 89, "text": "regret bounds"}, {"st": 92, "ed": 94, "text": "objective function"}, {"st": 102, "ed": 104, "text": "strongly convex"}, {"st": 105, "ed": 107, "text": "preliminary results"}]
[{"st": 1, "ed": 3, "text": "reinforcement learning"}, {"st": 5, "ed": 7, "text": "prior knowledge"}, {"st": 29, "ed": 31, "text": "monte carlo"}, {"st": 47, "ed": 49, "text": "finite set"}, {"st": 54, "ed": 56, "text": "parameter values"}, {"st": 60, "ed": 64, "text": "partially observable markov decision"}, {"st": 71, "ed": 73, "text": "cross product"}, {"st": 79, "ed": 81, "text": "reinforcement learning"}, {"st": 109, "ed": 111, "text": "point based"}, {"st": 120, "ed": 122, "text": "partially observable"}]
[{"st": 5, "ed": 7, "text": "multivariate regression"}, {"st": 33, "ed": 35, "text": "low rank"}, {"st": 43, "ed": 45, "text": "sparse linear"}, {"st": 66, "ed": 68, "text": "sparse coding"}, {"st": 108, "ed": 110, "text": "sparse coding"}, {"st": 116, "ed": 118, "text": "computer vision"}, {"st": 127, "ed": 129, "text": "sparse coding"}, {"st": 131, "ed": 133, "text": "theoretical properties"}, {"st": 136, "ed": 138, "text": "predictive accuracy"}]
[{"st": 1, "ed": 3, "text": "machine learning"}, {"st": 45, "ed": 47, "text": "feature set"}, {"st": 62, "ed": 64, "text": "feature extraction"}, {"st": 92, "ed": 94, "text": "multi class"}, {"st": 103, "ed": 105, "text": "cost effective"}]
[{"st": 6, "ed": 10, "text": "canonical correlation analysis cca"}, {"st": 29, "ed": 31, "text": "efficient algorithms"}, {"st": 47, "ed": 49, "text": "change detection"}]
[{"st": 0, "ed": 2, "text": "contextual bandit"}, {"st": 5, "ed": 7, "text": "increasingly popular"}, {"st": 10, "ed": 12, "text": "recommender systems"}, {"st": 39, "ed": 42, "text": "coarse to fine"}, {"st": 46, "ed": 48, "text": "prior knowledge"}, {"st": 57, "ed": 59, "text": "user preferences"}, {"st": 66, "ed": 68, "text": "low dimensional"}, {"st": 68, "ed": 70, "text": "feature space"}, {"st": 79, "ed": 81, "text": "high dimensional"}, {"st": 88, "ed": 90, "text": "bandit algorithm"}, {"st": 94, "ed": 97, "text": "coarse to fine"}, {"st": 100, "ed": 102, "text": "performance guarantees"}, {"st": 117, "ed": 119, "text": "substantial improvement"}, {"st": 121, "ed": 123, "text": "bandit algorithms"}, {"st": 131, "ed": 133, "text": "user study"}]
[{"st": 7, "ed": 9, "text": "dimensionality reduction"}, {"st": 18, "ed": 20, "text": "loss minimization"}, {"st": 60, "ed": 62, "text": "key contribution"}]
[{"st": 6, "ed": 8, "text": "gaussian process"}, {"st": 17, "ed": 20, "text": "branch and bound"}, {"st": 26, "ed": 28, "text": "ucb algorithm"}, {"st": 30, "ed": 32, "text": "et al"}, {"st": 47, "ed": 49, "text": "et al"}, {"st": 61, "ed": 63, "text": "sqrt t"}, {"st": 119, "ed": 121, "text": "search space"}, {"st": 133, "ed": 135, "text": "objective function"}]
[{"st": 4, "ed": 6, "text": "active learning"}, {"st": 61, "ed": 63, "text": "active learning"}, {"st": 68, "ed": 70, "text": "k 1"}, {"st": 79, "ed": 81, "text": "active learning"}, {"st": 92, "ed": 94, "text": "active learning"}, {"st": 111, "ed": 114, "text": "monte carlo simulation"}, {"st": 119, "ed": 121, "text": "unlabeled examples"}, {"st": 149, "ed": 151, "text": "combinatorial optimization"}, {"st": 165, "ed": 167, "text": "np hard"}, {"st": 188, "ed": 190, "text": "benchmark datasets"}, {"st": 193, "ed": 195, "text": "proposed approach"}, {"st": 196, "ed": 198, "text": "highly effective"}]
[{"st": 15, "ed": 18, "text": "markov decision processes"}, {"st": 28, "ed": 30, "text": "sample complexity"}, {"st": 33, "ed": 35, "text": "value iteration"}, {"st": 41, "ed": 43, "text": "generative model"}, {"st": 112, "ed": 114, "text": "sample complexity"}, {"st": 139, "ed": 141, "text": "sample complexity"}, {"st": 179, "ed": 181, "text": "significantly improve"}]
[{"st": 55, "ed": 57, "text": "similarity learning"}, {"st": 67, "ed": 69, "text": "experiment results"}, {"st": 74, "ed": 76, "text": "significantly improve"}, {"st": 77, "ed": 80, "text": "classification and clustering"}]
[{"st": 108, "ed": 110, "text": "accurate estimates"}, {"st": 114, "ed": 116, "text": "neural networks"}]
[{"st": 0, "ed": 3, "text": "multiple kernel learning"}, {"st": 15, "ed": 17, "text": "similarity measure"}, {"st": 20, "ed": 22, "text": "feature representations"}, {"st": 30, "ed": 32, "text": "previous research"}, {"st": 39, "ed": 41, "text": "computational efficiency"}, {"st": 73, "ed": 75, "text": "variational approximation"}, {"st": 92, "ed": 94, "text": "proposed method"}, {"st": 111, "ed": 114, "text": "benchmark data sets"}, {"st": 132, "ed": 134, "text": "image recognition"}, {"st": 137, "ed": 139, "text": "method outperforms"}, {"st": 139, "ed": 141, "text": "previously reported"}]
[{"st": 1, "ed": 3, "text": "classification problems"}, {"st": 20, "ed": 22, "text": "collective classification"}, {"st": 35, "ed": 37, "text": "labeled training"}, {"st": 46, "ed": 49, "text": "semi supervised learning"}, {"st": 103, "ed": 105, "text": "unlabeled data"}, {"st": 119, "ed": 122, "text": "easy to implement"}]
[{"st": 46, "ed": 48, "text": "feature vector"}, {"st": 59, "ed": 61, "text": "feature vectors"}, {"st": 77, "ed": 79, "text": "multivariate gaussian"}, {"st": 81, "ed": 83, "text": "low rank"}, {"st": 88, "ed": 90, "text": "random variables"}, {"st": 114, "ed": 116, "text": "low dimensional"}]
[{"st": 13, "ed": 15, "text": "causal model"}, {"st": 26, "ed": 28, "text": "covariate shift"}, {"st": 28, "ed": 30, "text": "concept drift"}, {"st": 30, "ed": 32, "text": "transfer learning"}, {"st": 61, "ed": 64, "text": "semi supervised learning"}]
[{"st": 12, "ed": 14, "text": "sparse linear"}, {"st": 37, "ed": 39, "text": "multi class"}, {"st": 59, "ed": 61, "text": "group lasso"}, {"st": 72, "ed": 74, "text": "experiments demonstrate"}, {"st": 81, "ed": 83, "text": "without compromising"}, {"st": 96, "ed": 98, "text": "low dimensional"}, {"st": 104, "ed": 106, "text": "highly efficient"}]
[{"st": 1, "ed": 3, "text": "evaluation measures"}, {"st": 38, "ed": 40, "text": "bipartite graph"}, {"st": 104, "ed": 106, "text": "feature vectors"}, {"st": 125, "ed": 127, "text": "demonstrate empirically"}]
[{"st": 1, "ed": 3, "text": "recent years"}, {"st": 9, "ed": 11, "text": "machine learning"}, {"st": 50, "ed": 52, "text": "k nn"}, {"st": 101, "ed": 103, "text": "feature space"}, {"st": 124, "ed": 126, "text": "generalization bound"}, {"st": 149, "ed": 151, "text": "provide evidence"}]
[{"st": 5, "ed": 7, "text": "learning paradigm"}, {"st": 10, "ed": 12, "text": "automatically identify"}, {"st": 36, "ed": 38, "text": "feature selection"}, {"st": 40, "ed": 42, "text": "recently developed"}, {"st": 42, "ed": 44, "text": "cutting plane"}, {"st": 49, "ed": 51, "text": "proposed algorithm"}, {"st": 77, "ed": 79, "text": "significant improvements"}, {"st": 80, "ed": 82, "text": "prediction performance"}, {"st": 88, "ed": 90, "text": "feature selection"}, {"st": 98, "ed": 100, "text": "learning process"}, {"st": 136, "ed": 138, "text": "extensive empirical"}, {"st": 146, "ed": 148, "text": "real world"}]
[{"st": 8, "ed": 10, "text": "multi label"}, {"st": 13, "ed": 15, "text": "multi label"}, {"st": 41, "ed": 43, "text": "coding theory"}, {"st": 79, "ed": 81, "text": "max margin"}, {"st": 94, "ed": 96, "text": "metric learning"}, {"st": 100, "ed": 102, "text": "exponentially large"}, {"st": 109, "ed": 111, "text": "structured prediction"}, {"st": 128, "ed": 130, "text": "cutting plane"}, {"st": 135, "ed": 137, "text": "empirical study"}, {"st": 147, "ed": 149, "text": "multi label"}, {"st": 149, "ed": 151, "text": "prediction methods"}, {"st": 152, "ed": 154, "text": "image text"}]
[{"st": 40, "ed": 42, "text": "low dimensional"}, {"st": 93, "ed": 96, "text": "multi label classification"}, {"st": 97, "ed": 99, "text": "multivariate regression"}, {"st": 118, "ed": 120, "text": "multiple output"}]
[{"st": 32, "ed": 34, "text": "l1 regularization"}, {"st": 38, "ed": 40, "text": "feature selection"}, {"st": 65, "ed": 67, "text": "fixed point"}, {"st": 71, "ed": 73, "text": "l1 regularization"}, {"st": 113, "ed": 115, "text": "proposed algorithm"}]
[{"st": 15, "ed": 17, "text": "recently proposed"}, {"st": 106, "ed": 108, "text": "classification task"}, {"st": 109, "ed": 111, "text": "real world"}]
[{"st": 0, "ed": 2, "text": "feature selection"}, {"st": 5, "ed": 7, "text": "becoming increasingly"}, {"st": 14, "ed": 16, "text": "reinforcement learning"}, {"st": 32, "ed": 34, "text": "feature selection"}, {"st": 38, "ed": 40, "text": "sparsity inducing"}, {"st": 58, "ed": 60, "text": "supervised learning"}, {"st": 74, "ed": 76, "text": "supervised learning"}, {"st": 101, "ed": 103, "text": "regularization methods"}, {"st": 113, "ed": 115, "text": "theoretical guarantees"}, {"st": 135, "ed": 137, "text": "matching pursuit"}, {"st": 171, "ed": 173, "text": "sparse recovery"}, {"st": 181, "ed": 183, "text": "theoretical guarantees"}, {"st": 196, "ed": 198, "text": "prior methods"}, {"st": 201, "ed": 204, "text": "accuracy and efficiency"}]
[{"st": 0, "ed": 2, "text": "multitask learning"}, {"st": 12, "ed": 14, "text": "latent structure"}, {"st": 37, "ed": 39, "text": "multitask learning"}, {"st": 58, "ed": 60, "text": "nonparametric bayesian"}, {"st": 83, "ed": 85, "text": "existing models"}, {"st": 95, "ed": 97, "text": "low rank"}, {"st": 100, "ed": 102, "text": "linear subspace"}, {"st": 125, "ed": 127, "text": "variational inference"}, {"st": 134, "ed": 139, "text": "synthetic and real world datasets"}, {"st": 143, "ed": 145, "text": "classification problems"}]
[{"st": 50, "ed": 52, "text": "i d"}]
[{"st": 1, "ed": 3, "text": "linear models"}, {"st": 21, "ed": 23, "text": "linear classifiers"}, {"st": 25, "ed": 28, "text": "support vector machines"}, {"st": 89, "ed": 91, "text": "class conditional"}, {"st": 96, "ed": 98, "text": "log linear"}, {"st": 111, "ed": 113, "text": "supervised learning"}, {"st": 133, "ed": 135, "text": "linear function"}, {"st": 146, "ed": 148, "text": "linear functions"}, {"st": 152, "ed": 154, "text": "probabilistic models"}, {"st": 155, "ed": 157, "text": "log linear"}, {"st": 167, "ed": 169, "text": "linear subspaces"}, {"st": 207, "ed": 209, "text": "closely related"}, {"st": 235, "ed": 237, "text": "optimization problems"}, {"st": 253, "ed": 255, "text": "real world"}]
[{"st": 8, "ed": 11, "text": "structure and parameters"}, {"st": 21, "ed": 23, "text": "exponential family"}, {"st": 35, "ed": 38, "text": "structure and parameters"}, {"st": 78, "ed": 80, "text": "convex relaxation"}, {"st": 115, "ed": 117, "text": "continuous relaxation"}, {"st": 144, "ed": 146, "text": "local minima"}, {"st": 152, "ed": 154, "text": "convex relaxations"}]
[{"st": 55, "ed": 57, "text": "real world"}]
[{"st": 4, "ed": 6, "text": "multi class"}, {"st": 6, "ed": 9, "text": "support vector machine"}, {"st": 21, "ed": 23, "text": "map estimation"}, {"st": 43, "ed": 45, "text": "hierarchical bayesian"}, {"st": 49, "ed": 51, "text": "fully bayesian"}, {"st": 51, "ed": 53, "text": "inference procedure"}, {"st": 54, "ed": 56, "text": "multi class"}, {"st": 63, "ed": 65, "text": "empirical results"}]
[{"st": 3, "ed": 5, "text": "maximum likelihood"}, {"st": 10, "ed": 13, "text": "undirected graphical model"}, {"st": 17, "ed": 19, "text": "posterior distribution"}, {"st": 26, "ed": 28, "text": "undirected models"}, {"st": 31, "ed": 33, "text": "computer vision"}, {"st": 37, "ed": 40, "text": "conditional random fields"}, {"st": 57, "ed": 59, "text": "undirected models"}, {"st": 118, "ed": 120, "text": "random fields"}, {"st": 168, "ed": 171, "text": "real world data"}]
[{"st": 4, "ed": 6, "text": "random variables"}, {"st": 28, "ed": 30, "text": "p value"}, {"st": 32, "ed": 34, "text": "mutual information"}, {"st": 55, "ed": 58, "text": "simple yet effective"}, {"st": 83, "ed": 85, "text": "parameter estimates"}, {"st": 102, "ed": 104, "text": "sample size"}]
[{"st": 4, "ed": 6, "text": "gaussian process"}, {"st": 15, "ed": 17, "text": "gp regression"}, {"st": 85, "ed": 87, "text": "input space"}, {"st": 89, "ed": 91, "text": "low dimensional"}, {"st": 96, "ed": 98, "text": "supervised manner"}, {"st": 122, "ed": 124, "text": "input dependent"}, {"st": 158, "ed": 160, "text": "input dependent"}, {"st": 172, "ed": 174, "text": "complex data"}, {"st": 188, "ed": 192, "text": "synthetic and real world"}]
[{"st": 9, "ed": 11, "text": "step size"}, {"st": 14, "ed": 16, "text": "step size"}, {"st": 64, "ed": 66, "text": "linear functions"}, {"st": 84, "ed": 86, "text": "step size"}, {"st": 93, "ed": 95, "text": "step size"}]
[{"st": 5, "ed": 7, "text": "cost sensitive"}, {"st": 15, "ed": 17, "text": "hinge loss"}, {"st": 21, "ed": 23, "text": "cost sensitive"}, {"st": 41, "ed": 43, "text": "hinge loss"}, {"st": 58, "ed": 60, "text": "cost sensitive"}, {"st": 69, "ed": 71, "text": "cost sensitive"}, {"st": 82, "ed": 84, "text": "decision rules"}, {"st": 87, "ed": 89, "text": "hinge loss"}, {"st": 91, "ed": 93, "text": "bayes optimal"}, {"st": 93, "ed": 95, "text": "cost sensitive"}, {"st": 101, "ed": 103, "text": "hinge loss"}, {"st": 113, "ed": 115, "text": "optimization problem"}, {"st": 137, "ed": 139, "text": "sensitivity analysis"}, {"st": 147, "ed": 149, "text": "proposed algorithm"}, {"st": 153, "ed": 155, "text": "cost sensitive"}, {"st": 162, "ed": 164, "text": "cost sensitive"}, {"st": 169, "ed": 171, "text": "performance measure"}, {"st": 187, "ed": 189, "text": "previous approaches"}, {"st": 190, "ed": 192, "text": "cost sensitive"}, {"st": 207, "ed": 209, "text": "cost sensitive"}]
[{"st": 30, "ed": 32, "text": "prior knowledge"}, {"st": 70, "ed": 72, "text": "neural network"}, {"st": 77, "ed": 79, "text": "monte carlo"}, {"st": 103, "ed": 105, "text": "optimization methods"}, {"st": 120, "ed": 122, "text": "current approaches"}, {"st": 124, "ed": 126, "text": "complexity theory"}]
[{"st": 0, "ed": 2, "text": "trace norm"}, {"st": 12, "ed": 14, "text": "excess risk"}, {"st": 44, "ed": 46, "text": "input space"}, {"st": 74, "ed": 76, "text": "positive semidefinite"}]
[{"st": 19, "ed": 21, "text": "marginal probabilities"}, {"st": 26, "ed": 28, "text": "random fields"}, {"st": 56, "ed": 58, "text": "marginal probabilities"}, {"st": 65, "ed": 67, "text": "random fields"}, {"st": 84, "ed": 86, "text": "large scale"}, {"st": 86, "ed": 88, "text": "random fields"}, {"st": 96, "ed": 98, "text": "approximation algorithms"}]
[{"st": 1, "ed": 3, "text": "machine learning"}, {"st": 3, "ed": 5, "text": "domain adaptation"}, {"st": 37, "ed": 39, "text": "strong assumptions"}, {"st": 42, "ed": 44, "text": "covariate shift"}, {"st": 46, "ed": 49, "text": "source and target"}, {"st": 109, "ed": 111, "text": "pac bayesian"}, {"st": 138, "ed": 140, "text": "pac bayesian"}, {"st": 149, "ed": 151, "text": "pac bayesian"}, {"st": 168, "ed": 170, "text": "majority vote"}, {"st": 171, "ed": 174, "text": "takes into account"}, {"st": 189, "ed": 191, "text": "pac bayesian"}, {"st": 197, "ed": 199, "text": "majority vote"}, {"st": 202, "ed": 205, "text": "kullback leibler divergence"}, {"st": 208, "ed": 210, "text": "empirical risk"}, {"st": 230, "ed": 232, "text": "majority vote"}, {"st": 239, "ed": 242, "text": "source and target"}]
[{"st": 2, "ed": 5, "text": "mixture of experts"}, {"st": 30, "ed": 32, "text": "maximum likelihood"}, {"st": 41, "ed": 43, "text": "maximum likelihood"}, {"st": 57, "ed": 59, "text": "previous attempts"}, {"st": 72, "ed": 74, "text": "ad hoc"}, {"st": 85, "ed": 87, "text": "joint distribution"}, {"st": 89, "ed": 92, "text": "input and output"}, {"st": 99, "ed": 101, "text": "fully bayesian"}, {"st": 112, "ed": 115, "text": "local and global"}, {"st": 115, "ed": 117, "text": "variational methods"}, {"st": 139, "ed": 141, "text": "training phase"}]
[{"st": 2, "ed": 4, "text": "hidden variables"}, {"st": 9, "ed": 12, "text": "probabilistic graphical models"}, {"st": 18, "ed": 20, "text": "real life"}, {"st": 78, "ed": 80, "text": "hidden variables"}, {"st": 93, "ed": 95, "text": "hidden variables"}, {"st": 124, "ed": 126, "text": "expectation maximization"}, {"st": 127, "ed": 129, "text": "em algorithm"}]
[{"st": 11, "ed": 13, "text": "marginal distribution"}, {"st": 17, "ed": 19, "text": "unlabeled data"}, {"st": 116, "ed": 118, "text": "classification task"}, {"st": 119, "ed": 121, "text": "unlabeled data"}, {"st": 125, "ed": 127, "text": "sample complexity"}, {"st": 140, "ed": 142, "text": "conditional distributions"}, {"st": 144, "ed": 146, "text": "logistic regression"}, {"st": 154, "ed": 156, "text": "finite set"}]
[{"st": 2, "ed": 4, "text": "training data"}, {"st": 74, "ed": 76, "text": "naive bayes"}, {"st": 78, "ed": 80, "text": "active learning"}, {"st": 104, "ed": 106, "text": "decision making"}]
[{"st": 19, "ed": 21, "text": "metric learning"}, {"st": 54, "ed": 56, "text": "text classification"}]
[{"st": 28, "ed": 30, "text": "co occurrence"}, {"st": 38, "ed": 40, "text": "compact representation"}, {"st": 52, "ed": 54, "text": "recently introduced"}, {"st": 55, "ed": 57, "text": "dimensionality reduction"}, {"st": 74, "ed": 76, "text": "mutual information"}, {"st": 104, "ed": 106, "text": "face recognition"}, {"st": 157, "ed": 159, "text": "irrelevant features"}, {"st": 194, "ed": 196, "text": "side information"}, {"st": 254, "ed": 257, "text": "constrained optimization problem"}, {"st": 282, "ed": 284, "text": "maximum entropy"}, {"st": 290, "ed": 292, "text": "synthetic data"}, {"st": 297, "ed": 299, "text": "face recognition"}]
[{"st": 4, "ed": 7, "text": "naive bayes classifier"}, {"st": 9, "ed": 11, "text": "machine learning"}, {"st": 31, "ed": 33, "text": "naive bayes"}, {"st": 52, "ed": 54, "text": "naive bayes"}, {"st": 61, "ed": 63, "text": "local models"}, {"st": 72, "ed": 74, "text": "naive bayes"}, {"st": 80, "ed": 82, "text": "naive bayes"}, {"st": 101, "ed": 103, "text": "naive bayes"}]
[{"st": 26, "ed": 28, "text": "greedy algorithms"}, {"st": 53, "ed": 56, "text": "branch and bound"}, {"st": 69, "ed": 71, "text": "naive bayesian"}, {"st": 78, "ed": 80, "text": "distance measures"}, {"st": 86, "ed": 88, "text": "classification error"}, {"st": 108, "ed": 110, "text": "synthetic data"}]
[{"st": 5, "ed": 7, "text": "em algorithm"}, {"st": 34, "ed": 36, "text": "optimization algorithms"}, {"st": 37, "ed": 39, "text": "expectation maximization"}, {"st": 48, "ed": 50, "text": "optimization algorithms"}, {"st": 52, "ed": 54, "text": "gradient based"}, {"st": 69, "ed": 71, "text": "optimization methods"}, {"st": 86, "ed": 88, "text": "optimization algorithms"}, {"st": 89, "ed": 91, "text": "quasi newton"}, {"st": 115, "ed": 117, "text": "convergence properties"}, {"st": 129, "ed": 131, "text": "faster convergence"}, {"st": 134, "ed": 136, "text": "empirical results"}, {"st": 149, "ed": 151, "text": "improved performance"}]
[{"st": 12, "ed": 14, "text": "marginal likelihood"}, {"st": 42, "ed": 44, "text": "hidden variables"}, {"st": 63, "ed": 65, "text": "marginal likelihood"}, {"st": 105, "ed": 107, "text": "marginal likelihood"}]
[{"st": 11, "ed": 13, "text": "inference algorithms"}, {"st": 52, "ed": 54, "text": "probabilistic model"}, {"st": 58, "ed": 60, "text": "spectral clustering"}, {"st": 89, "ed": 91, "text": "distance measure"}, {"st": 94, "ed": 96, "text": "clustering problem"}, {"st": 113, "ed": 115, "text": "clustering algorithms"}, {"st": 116, "ed": 118, "text": "spectral clustering"}]
[{"st": 37, "ed": 39, "text": "finite state"}, {"st": 44, "ed": 46, "text": "transition model"}, {"st": 57, "ed": 59, "text": "learning parameters"}, {"st": 71, "ed": 73, "text": "conjugate prior"}, {"st": 85, "ed": 87, "text": "parameter estimation"}, {"st": 111, "ed": 113, "text": "structure learning"}, {"st": 124, "ed": 126, "text": "structure learning"}, {"st": 127, "ed": 130, "text": "dynamic bayesian networks"}, {"st": 140, "ed": 142, "text": "dependency structure"}]
[{"st": 0, "ed": 3, "text": "conditional random fields"}, {"st": 5, "ed": 8, "text": "undirected graphical models"}, {"st": 9, "ed": 11, "text": "special case"}, {"st": 17, "ed": 19, "text": "finite state"}, {"st": 83, "ed": 85, "text": "log likelihood"}, {"st": 94, "ed": 96, "text": "et al"}, {"st": 122, "ed": 124, "text": "traditional approaches"}, {"st": 129, "ed": 131, "text": "improved accuracy"}, {"st": 148, "ed": 150, "text": "higher order"}, {"st": 201, "ed": 203, "text": "named entity"}]
[{"st": 3, "ed": 5, "text": "random walks"}, {"st": 7, "ed": 9, "text": "discrete data"}, {"st": 23, "ed": 25, "text": "transition probabilities"}, {"st": 50, "ed": 52, "text": "path integral"}, {"st": 65, "ed": 67, "text": "continuous data"}]
[{"st": 15, "ed": 17, "text": "data mining"}, {"st": 30, "ed": 32, "text": "statistical properties"}, {"st": 75, "ed": 77, "text": "fisher information"}, {"st": 94, "ed": 96, "text": "recent years"}, {"st": 133, "ed": 135, "text": "latent variables"}, {"st": 165, "ed": 167, "text": "generalization error"}]
[{"st": 1, "ed": 3, "text": "mean field"}, {"st": 8, "ed": 10, "text": "probability distributions"}, {"st": 57, "ed": 59, "text": "approximate inference"}, {"st": 67, "ed": 69, "text": "mean field"}, {"st": 71, "ed": 73, "text": "variational approximation"}, {"st": 88, "ed": 90, "text": "constrained optimization"}, {"st": 99, "ed": 101, "text": "mean field"}, {"st": 104, "ed": 106, "text": "approximate inference"}, {"st": 108, "ed": 110, "text": "exponential family"}, {"st": 171, "ed": 173, "text": "dependency structure"}, {"st": 211, "ed": 213, "text": "higher order"}]
[{"st": 3, "ed": 5, "text": "low dimensional"}, {"st": 48, "ed": 50, "text": "probabilistic model"}, {"st": 55, "ed": 57, "text": "learning rules"}, {"st": 62, "ed": 64, "text": "learning rules"}, {"st": 76, "ed": 78, "text": "learning algorithm"}, {"st": 93, "ed": 95, "text": "random field"}]
[{"st": 4, "ed": 6, "text": "statistical learning"}, {"st": 21, "ed": 23, "text": "maximum entropy"}, {"st": 32, "ed": 34, "text": "maximum entropy"}, {"st": 38, "ed": 40, "text": "maximum likelihood"}, {"st": 52, "ed": 54, "text": "parameter estimation"}, {"st": 64, "ed": 66, "text": "em algorithm"}, {"st": 81, "ed": 83, "text": "maximum likelihood"}, {"st": 87, "ed": 89, "text": "hidden units"}]
[{"st": 23, "ed": 25, "text": "causal relationships"}, {"st": 46, "ed": 48, "text": "observed variables"}, {"st": 62, "ed": 64, "text": "search algorithms"}, {"st": 100, "ed": 102, "text": "bayes net"}, {"st": 102, "ed": 104, "text": "search algorithms"}, {"st": 106, "ed": 108, "text": "prior knowledge"}, {"st": 112, "ed": 114, "text": "latent variables"}, {"st": 137, "ed": 139, "text": "simulated data"}]
[{"st": 11, "ed": 13, "text": "prior methods"}]
[{"st": 9, "ed": 11, "text": "linear classifier"}, {"st": 74, "ed": 76, "text": "efficient online"}, {"st": 76, "ed": 78, "text": "learning algorithm"}, {"st": 90, "ed": 93, "text": "empirical risk minimization"}, {"st": 106, "ed": 108, "text": "uniform convergence"}]
[{"st": 11, "ed": 14, "text": "latent semantic analysis"}, {"st": 20, "ed": 22, "text": "learning algorithms"}]
[{"st": 3, "ed": 5, "text": "generative model"}, {"st": 21, "ed": 23, "text": "variational inference"}, {"st": 23, "ed": 25, "text": "update rule"}]
[{"st": 47, "ed": 49, "text": "spanning tree"}, {"st": 73, "ed": 75, "text": "randomized algorithm"}, {"st": 93, "ed": 95, "text": "spanning tree"}, {"st": 117, "ed": 119, "text": "real world"}]
[{"st": 0, "ed": 3, "text": "gaussian process gp"}, {"st": 5, "ed": 7, "text": "typically assume"}, {"st": 22, "ed": 24, "text": "input dependent"}, {"st": 50, "ed": 52, "text": "gp regression"}, {"st": 55, "ed": 57, "text": "latent variable"}, {"st": 85, "ed": 87, "text": "partial derivative"}, {"st": 107, "ed": 109, "text": "input dependent"}, {"st": 116, "ed": 118, "text": "input dependent"}, {"st": 131, "ed": 133, "text": "synthetic datasets"}, {"st": 160, "ed": 162, "text": "gp model"}, {"st": 169, "ed": 173, "text": "markov chain monte carlo"}, {"st": 195, "ed": 198, "text": "mean squared error"}, {"st": 199, "ed": 201, "text": "negative log"}, {"st": 201, "ed": 203, "text": "probability density"}]
[{"st": 8, "ed": 10, "text": "real world"}, {"st": 36, "ed": 39, "text": "self organizing map"}]
[{"st": 9, "ed": 12, "text": "multiple kernel learning"}, {"st": 18, "ed": 20, "text": "efficiently learn"}, {"st": 54, "ed": 57, "text": "coordinate descent algorithm"}, {"st": 64, "ed": 66, "text": "convergence rate"}, {"st": 70, "ed": 72, "text": "convergence rate"}, {"st": 84, "ed": 86, "text": "ell 2"}, {"st": 123, "ed": 125, "text": "generalization error"}]
[{"st": 2, "ed": 4, "text": "generalization bounds"}, {"st": 21, "ed": 23, "text": "kernel learning"}]
[{"st": 7, "ed": 9, "text": "parameter estimation"}, {"st": 13, "ed": 15, "text": "missing values"}, {"st": 16, "ed": 18, "text": "hidden variables"}, {"st": 35, "ed": 37, "text": "unified framework"}, {"st": 38, "ed": 40, "text": "parameter estimation"}, {"st": 78, "ed": 80, "text": "model selection"}, {"st": 95, "ed": 97, "text": "em algorithm"}, {"st": 124, "ed": 126, "text": "theoretical results"}, {"st": 131, "ed": 133, "text": "faster convergence"}, {"st": 135, "ed": 137, "text": "maximum likelihood"}]
[{"st": 10, "ed": 12, "text": "conditional probability"}, {"st": 25, "ed": 27, "text": "input variables"}, {"st": 111, "ed": 113, "text": "prior knowledge"}, {"st": 171, "ed": 173, "text": "sufficient conditions"}]
[{"st": 10, "ed": 12, "text": "unsupervised learning"}, {"st": 19, "ed": 21, "text": "k means"}, {"st": 22, "ed": 25, "text": "expectation maximization em"}, {"st": 42, "ed": 44, "text": "k means"}, {"st": 56, "ed": 58, "text": "k means"}, {"st": 109, "ed": 111, "text": "k means"}, {"st": 178, "ed": 180, "text": "k means"}, {"st": 197, "ed": 199, "text": "k means"}]
[{"st": 11, "ed": 13, "text": "proposed approach"}, {"st": 15, "ed": 17, "text": "matrix variate"}, {"st": 17, "ed": 19, "text": "gaussian process"}, {"st": 21, "ed": 23, "text": "generative model"}, {"st": 36, "ed": 38, "text": "variational inference"}, {"st": 41, "ed": 43, "text": "low rank"}, {"st": 47, "ed": 49, "text": "matrix variate"}, {"st": 59, "ed": 61, "text": "closed form"}, {"st": 71, "ed": 73, "text": "matrix variate"}, {"st": 78, "ed": 80, "text": "elastic net"}, {"st": 85, "ed": 87, "text": "variational inference"}, {"st": 91, "ed": 93, "text": "matrix variate"}, {"st": 93, "ed": 95, "text": "gaussian process"}, {"st": 97, "ed": 99, "text": "maximum likelihood"}, {"st": 99, "ed": 101, "text": "parameter estimation"}, {"st": 168, "ed": 170, "text": "real world"}, {"st": 177, "ed": 179, "text": "low rank"}, {"st": 185, "ed": 188, "text": "training and testing"}]
[{"st": 13, "ed": 15, "text": "high dimensional"}, {"st": 27, "ed": 29, "text": "self organizing"}]
[{"st": 1, "ed": 3, "text": "recent years"}, {"st": 11, "ed": 13, "text": "bayesian networks"}, {"st": 29, "ed": 32, "text": "minimum description length"}, {"st": 77, "ed": 79, "text": "sample complexity"}, {"st": 129, "ed": 131, "text": "sample complexity"}, {"st": 133, "ed": 135, "text": "low order"}, {"st": 178, "ed": 180, "text": "sample complexity"}]
[{"st": 0, "ed": 3, "text": "hidden markov models"}, {"st": 16, "ed": 18, "text": "speech recognition"}, {"st": 20, "ed": 22, "text": "natural language"}, {"st": 27, "ed": 29, "text": "discriminative models"}, {"st": 31, "ed": 33, "text": "classification performance"}, {"st": 75, "ed": 77, "text": "kernel based"}, {"st": 112, "ed": 114, "text": "training data"}]
[{"st": 15, "ed": 17, "text": "proposed method"}, {"st": 20, "ed": 22, "text": "clustering methods"}, {"st": 29, "ed": 31, "text": "model based"}, {"st": 39, "ed": 41, "text": "clustering methods"}, {"st": 55, "ed": 57, "text": "clustering algorithm"}, {"st": 102, "ed": 104, "text": "previous methods"}]
[{"st": 3, "ed": 6, "text": "dictionary learning algorithms"}, {"st": 13, "ed": 15, "text": "machine learning"}, {"st": 137, "ed": 139, "text": "clustering algorithm"}]
[{"st": 35, "ed": 37, "text": "selection algorithms"}, {"st": 68, "ed": 70, "text": "linear regression"}, {"st": 73, "ed": 75, "text": "learning task"}]
[{"st": 15, "ed": 19, "text": "prediction with expert advice"}, {"st": 94, "ed": 96, "text": "bandit feedback"}, {"st": 112, "ed": 114, "text": "sqrt t"}]
[{"st": 10, "ed": 12, "text": "recently introduced"}, {"st": 15, "ed": 17, "text": "technique called"}, {"st": 62, "ed": 64, "text": "fast approximate"}, {"st": 90, "ed": 92, "text": "classification performance"}, {"st": 94, "ed": 96, "text": "benchmark datasets"}, {"st": 96, "ed": 99, "text": "mnist cifar 10"}, {"st": 99, "ed": 101, "text": "cifar 100"}]
[{"st": 36, "ed": 38, "text": "sufficiently large"}, {"st": 71, "ed": 73, "text": "trace norm"}, {"st": 94, "ed": 96, "text": "mild assumptions"}, {"st": 111, "ed": 113, "text": "iterative algorithm"}, {"st": 164, "ed": 166, "text": "active learning"}, {"st": 201, "ed": 203, "text": "structured data"}, {"st": 215, "ed": 217, "text": "convex relaxation"}]
[{"st": 3, "ed": 5, "text": "matrix factorization"}, {"st": 8, "ed": 10, "text": "rank minimization"}, {"st": 22, "ed": 24, "text": "matrix completion"}, {"st": 25, "ed": 27, "text": "extremely large"}, {"st": 39, "ed": 41, "text": "matrix completion"}, {"st": 80, "ed": 82, "text": "prior knowledge"}, {"st": 140, "ed": 142, "text": "matrix completion"}, {"st": 167, "ed": 169, "text": "matrix completion"}, {"st": 190, "ed": 192, "text": "collaborative filtering"}, {"st": 223, "ed": 225, "text": "large scale"}, {"st": 229, "ed": 231, "text": "real data"}]
[{"st": 3, "ed": 5, "text": "fundamental problems"}, {"st": 6, "ed": 8, "text": "machine learning"}, {"st": 39, "ed": 41, "text": "lower dimensional"}, {"st": 85, "ed": 87, "text": "deep learning"}, {"st": 94, "ed": 96, "text": "representation space"}, {"st": 120, "ed": 122, "text": "latent distribution"}]
[{"st": 0, "ed": 3, "text": "support vector machine"}, {"st": 7, "ed": 9, "text": "active research"}, {"st": 17, "ed": 19, "text": "recent years"}, {"st": 74, "ed": 76, "text": "computational complexity"}, {"st": 103, "ed": 105, "text": "working set"}, {"st": 118, "ed": 120, "text": "working set"}, {"st": 137, "ed": 139, "text": "working set"}]
[{"st": 8, "ed": 10, "text": "least squares"}, {"st": 13, "ed": 15, "text": "sparse signal"}, {"st": 28, "ed": 30, "text": "l1 norm"}, {"st": 49, "ed": 51, "text": "penalty functions"}, {"st": 62, "ed": 64, "text": "cost function"}, {"st": 74, "ed": 76, "text": "penalty functions"}, {"st": 97, "ed": 99, "text": "semidefinite programming"}, {"st": 106, "ed": 108, "text": "approach yields"}, {"st": 111, "ed": 113, "text": "sparsity inducing"}, {"st": 113, "ed": 115, "text": "penalty functions"}, {"st": 120, "ed": 122, "text": "cost function"}, {"st": 142, "ed": 144, "text": "sparsity inducing"}, {"st": 146, "ed": 148, "text": "l1 norm"}]
[{"st": 3, "ed": 5, "text": "conformal prediction"}, {"st": 45, "ed": 47, "text": "finite sample"}, {"st": 73, "ed": 75, "text": "conformal prediction"}, {"st": 76, "ed": 79, "text": "high computational cost"}, {"st": 80, "ed": 82, "text": "functional data"}, {"st": 106, "ed": 108, "text": "real data"}]
[{"st": 0, "ed": 2, "text": "time series"}, {"st": 16, "ed": 18, "text": "active research"}, {"st": 43, "ed": 45, "text": "time series"}, {"st": 61, "ed": 63, "text": "time series"}, {"st": 82, "ed": 84, "text": "time series"}, {"st": 88, "ed": 90, "text": "neural networks"}, {"st": 112, "ed": 114, "text": "time series"}, {"st": 125, "ed": 127, "text": "time series"}, {"st": 140, "ed": 142, "text": "time series"}, {"st": 175, "ed": 177, "text": "time series"}, {"st": 182, "ed": 184, "text": "performance measures"}, {"st": 229, "ed": 231, "text": "time series"}]
[{"st": 9, "ed": 11, "text": "naive bayesian"}, {"st": 31, "ed": 33, "text": "naive bayesian"}, {"st": 43, "ed": 45, "text": "greedy search"}, {"st": 83, "ed": 85, "text": "domains including"}, {"st": 87, "ed": 89, "text": "decision tree"}, {"st": 102, "ed": 104, "text": "naive bayesian"}, {"st": 108, "ed": 111, "text": "directions for future"}]
[{"st": 4, "ed": 6, "text": "neural activity"}, {"st": 8, "ed": 10, "text": "human brain"}, {"st": 33, "ed": 35, "text": "latent variables"}, {"st": 70, "ed": 72, "text": "parallel algorithm"}, {"st": 82, "ed": 84, "text": "low rank"}, {"st": 99, "ed": 102, "text": "times faster than"}, {"st": 125, "ed": 127, "text": "missing data"}, {"st": 166, "ed": 168, "text": "latent variables"}]
[{"st": 10, "ed": 12, "text": "multiple sources"}, {"st": 23, "ed": 25, "text": "statistical model"}, {"st": 45, "ed": 47, "text": "consensus clustering"}, {"st": 66, "ed": 68, "text": "consensus clustering"}, {"st": 120, "ed": 122, "text": "breast cancer"}]
[{"st": 4, "ed": 6, "text": "large scale"}, {"st": 12, "ed": 15, "text": "each data point"}, {"st": 27, "ed": 29, "text": "ground truth"}, {"st": 90, "ed": 92, "text": "variational bayesian"}, {"st": 92, "ed": 94, "text": "inference algorithm"}, {"st": 124, "ed": 126, "text": "real world"}, {"st": 185, "ed": 187, "text": "majority vote"}]
[{"st": 1, "ed": 4, "text": "k nearest neighbour"}, {"st": 15, "ed": 18, "text": "k nearest neighbour"}, {"st": 19, "ed": 21, "text": "classification algorithm"}, {"st": 65, "ed": 67, "text": "decision making"}, {"st": 94, "ed": 96, "text": "statistical model"}, {"st": 110, "ed": 112, "text": "machine learning"}, {"st": 119, "ed": 121, "text": "approximation algorithm"}, {"st": 134, "ed": 136, "text": "time consuming"}, {"st": 136, "ed": 138, "text": "monte carlo"}, {"st": 144, "ed": 146, "text": "cross validation"}]
[{"st": 2, "ed": 4, "text": "reinforcement learning"}, {"st": 28, "ed": 30, "text": "reinforcement learning"}, {"st": 52, "ed": 54, "text": "reinforcement learning"}, {"st": 68, "ed": 70, "text": "o sqrt"}, {"st": 84, "ed": 86, "text": "computational complexity"}]
[{"st": 1, "ed": 3, "text": "mean shift"}, {"st": 9, "ed": 11, "text": "computer vision"}, {"st": 33, "ed": 35, "text": "mean shift"}, {"st": 42, "ed": 44, "text": "mean shift"}, {"st": 55, "ed": 57, "text": "mean shift"}, {"st": 116, "ed": 118, "text": "mean shift"}, {"st": 125, "ed": 127, "text": "mean shift"}]
[{"st": 26, "ed": 28, "text": "multivariate gaussian"}, {"st": 28, "ed": 30, "text": "piecewise linear"}, {"st": 53, "ed": 55, "text": "high dimensional"}, {"st": 61, "ed": 63, "text": "thompson sampling"}, {"st": 64, "ed": 67, "text": "approximate dynamic programming"}, {"st": 88, "ed": 90, "text": "reinforcement learning"}, {"st": 92, "ed": 94, "text": "continuous state"}, {"st": 103, "ed": 105, "text": "least squares"}]
[{"st": 0, "ed": 3, "text": "modern machine learning"}, {"st": 10, "ed": 12, "text": "large scale"}, {"st": 13, "ed": 15, "text": "learning systems"}, {"st": 25, "ed": 27, "text": "recently developed"}, {"st": 28, "ed": 30, "text": "factor analysis"}, {"st": 35, "ed": 37, "text": "statistical model"}, {"st": 40, "ed": 42, "text": "machine learning"}, {"st": 103, "ed": 105, "text": "post processing"}, {"st": 108, "ed": 110, "text": "user defined"}, {"st": 127, "ed": 129, "text": "user defined"}, {"st": 181, "ed": 183, "text": "post processing"}, {"st": 187, "ed": 189, "text": "prediction performance"}, {"st": 210, "ed": 212, "text": "proposed approach"}]
[{"st": 4, "ed": 6, "text": "multivariate regression"}, {"st": 6, "ed": 8, "text": "method named"}, {"st": 13, "ed": 15, "text": "multivariate regression"}, {"st": 18, "ed": 20, "text": "existing methods"}, {"st": 31, "ed": 33, "text": "noise level"}, {"st": 39, "ed": 41, "text": "finite sample"}, {"st": 47, "ed": 50, "text": "provide sufficient conditions"}, {"st": 56, "ed": 59, "text": "rate of convergence"}, {"st": 68, "ed": 70, "text": "proximal gradient"}, {"st": 73, "ed": 75, "text": "worst case"}, {"st": 76, "ed": 79, "text": "rate of convergence"}, {"st": 91, "ed": 93, "text": "objective function"}, {"st": 97, "ed": 99, "text": "numerical simulations"}, {"st": 103, "ed": 105, "text": "consistently outperforms"}, {"st": 108, "ed": 110, "text": "multivariate regression"}, {"st": 144, "ed": 146, "text": "proposed method"}]
[{"st": 6, "ed": 8, "text": "generalization properties"}, {"st": 9, "ed": 11, "text": "online learning"}, {"st": 15, "ed": 17, "text": "supervised learning"}, {"st": 20, "ed": 22, "text": "loss function"}, {"st": 28, "ed": 30, "text": "training sample"}, {"st": 31, "ed": 33, "text": "metric learning"}, {"st": 46, "ed": 48, "text": "rademacher complexity"}, {"st": 49, "ed": 51, "text": "generalization error"}, {"st": 63, "ed": 65, "text": "et al"}, {"st": 81, "ed": 83, "text": "fast convergence"}, {"st": 85, "ed": 87, "text": "strongly convex"}, {"st": 100, "ed": 102, "text": "efficient online"}, {"st": 102, "ed": 104, "text": "learning algorithms"}, {"st": 106, "ed": 108, "text": "learning problems"}, {"st": 116, "ed": 118, "text": "training samples"}, {"st": 131, "ed": 133, "text": "generalization bounds"}, {"st": 138, "ed": 140, "text": "efficient online"}, {"st": 140, "ed": 142, "text": "learning algorithm"}, {"st": 143, "ed": 145, "text": "higher order"}, {"st": 145, "ed": 147, "text": "learning problems"}]
[{"st": 11, "ed": 13, "text": "document summarization"}, {"st": 28, "ed": 30, "text": "reward functions"}, {"st": 43, "ed": 45, "text": "near optimal"}, {"st": 49, "ed": 51, "text": "prediction problems"}, {"st": 64, "ed": 66, "text": "submodular optimization"}, {"st": 107, "ed": 109, "text": "performance guarantees"}, {"st": 128, "ed": 130, "text": "problems including"}]
[{"st": 12, "ed": 14, "text": "loss minimization"}, {"st": 27, "ed": 29, "text": "mini batch"}, {"st": 37, "ed": 39, "text": "main contribution"}, {"st": 52, "ed": 54, "text": "fast convergence"}, {"st": 67, "ed": 69, "text": "parallel computing"}]
[{"st": 5, "ed": 7, "text": "generalization bounds"}, {"st": 17, "ed": 19, "text": "exponential loss"}, {"st": 88, "ed": 90, "text": "learning rate"}, {"st": 105, "ed": 107, "text": "convex surrogate"}]
[{"st": 2, "ed": 4, "text": "online learning"}, {"st": 27, "ed": 29, "text": "maximum likelihood"}, {"st": 76, "ed": 78, "text": "open problem"}, {"st": 93, "ed": 95, "text": "exponential family"}, {"st": 103, "ed": 105, "text": "exponential family"}, {"st": 112, "ed": 114, "text": "exponential family"}, {"st": 114, "ed": 116, "text": "online learning"}]
[{"st": 0, "ed": 2, "text": "collective classification"}, {"st": 12, "ed": 14, "text": "important applications"}, {"st": 22, "ed": 24, "text": "collective classification"}, {"st": 35, "ed": 37, "text": "class labels"}, {"st": 53, "ed": 55, "text": "collective classification"}, {"st": 89, "ed": 91, "text": "existing approaches"}, {"st": 117, "ed": 119, "text": "collective classification"}, {"st": 184, "ed": 186, "text": "collective classification"}, {"st": 198, "ed": 200, "text": "network size"}, {"st": 209, "ed": 211, "text": "collective classification"}, {"st": 247, "ed": 249, "text": "empirical studies"}, {"st": 250, "ed": 252, "text": "real world"}, {"st": 262, "ed": 264, "text": "collective classification"}]
[{"st": 4, "ed": 6, "text": "generative model"}, {"st": 7, "ed": 9, "text": "latent dirichlet"}, {"st": 9, "ed": 12, "text": "hidden markov models"}, {"st": 47, "ed": 49, "text": "transition matrix"}, {"st": 65, "ed": 67, "text": "latent dirichlet"}, {"st": 67, "ed": 69, "text": "random variables"}, {"st": 97, "ed": 99, "text": "hyper parameters"}, {"st": 110, "ed": 112, "text": "hyper parameters"}, {"st": 122, "ed": 124, "text": "iterative algorithm"}, {"st": 157, "ed": 159, "text": "empirical results"}, {"st": 167, "ed": 171, "text": "real world data sets"}, {"st": 188, "ed": 190, "text": "generalization performance"}, {"st": 193, "ed": 195, "text": "log likelihood"}, {"st": 197, "ed": 199, "text": "competitive results"}]
[{"st": 2, "ed": 5, "text": "online learning algorithms"}, {"st": 12, "ed": 14, "text": "regret bounds"}, {"st": 47, "ed": 49, "text": "space complexity"}]
[{"st": 12, "ed": 14, "text": "mixture model"}, {"st": 18, "ed": 20, "text": "sequential data"}, {"st": 33, "ed": 35, "text": "low variance"}, {"st": 35, "ed": 37, "text": "asymptotic analysis"}, {"st": 39, "ed": 41, "text": "gibbs sampling"}, {"st": 51, "ed": 53, "text": "convergence guarantees"}, {"st": 58, "ed": 60, "text": "k means"}, {"st": 61, "ed": 63, "text": "empirical results"}, {"st": 86, "ed": 89, "text": "orders of magnitude"}, {"st": 97, "ed": 99, "text": "clustering algorithms"}, {"st": 101, "ed": 103, "text": "higher accuracy"}]
[{"st": 9, "ed": 11, "text": "breast cancer"}, {"st": 76, "ed": 78, "text": "classification algorithms"}, {"st": 78, "ed": 80, "text": "decision tree"}, {"st": 81, "ed": 84, "text": "artificial neural network"}, {"st": 86, "ed": 89, "text": "support vector machine"}, {"st": 154, "ed": 156, "text": "classification algorithms"}, {"st": 183, "ed": 185, "text": "analysis shows"}, {"st": 190, "ed": 192, "text": "classification models"}, {"st": 196, "ed": 198, "text": "breast cancer"}, {"st": 200, "ed": 202, "text": "error rate"}]
[{"st": 1, "ed": 3, "text": "machine learning"}, {"st": 7, "ed": 9, "text": "input samples"}, {"st": 21, "ed": 23, "text": "input space"}, {"st": 28, "ed": 30, "text": "unsupervised learning"}, {"st": 32, "ed": 35, "text": "input and output"}, {"st": 43, "ed": 47, "text": "supervised and semi supervised"}, {"st": 68, "ed": 70, "text": "additional information"}, {"st": 71, "ed": 73, "text": "machine learning"}, {"st": 141, "ed": 143, "text": "supervised learning"}, {"st": 190, "ed": 192, "text": "dot product"}, {"st": 207, "ed": 209, "text": "wide variety"}, {"st": 210, "ed": 212, "text": "clustering techniques"}, {"st": 235, "ed": 237, "text": "digit recognition"}, {"st": 242, "ed": 244, "text": "real world"}]
[{"st": 8, "ed": 11, "text": "empirical risk minimization"}, {"st": 21, "ed": 23, "text": "squared loss"}, {"st": 64, "ed": 66, "text": "heavy tailed"}, {"st": 69, "ed": 71, "text": "heavy tailed"}, {"st": 79, "ed": 81, "text": "noise level"}]
[{"st": 11, "ed": 13, "text": "theoretical properties"}, {"st": 15, "ed": 17, "text": "learning process"}, {"st": 22, "ed": 24, "text": "domain adaptation"}, {"st": 28, "ed": 30, "text": "multiple sources"}, {"st": 37, "ed": 39, "text": "domain adaptation"}, {"st": 85, "ed": 87, "text": "multiple domains"}, {"st": 113, "ed": 115, "text": "generalization bounds"}, {"st": 130, "ed": 132, "text": "generalization bounds"}, {"st": 145, "ed": 148, "text": "rate of convergence"}, {"st": 150, "ed": 152, "text": "learning process"}, {"st": 167, "ed": 169, "text": "learning process"}, {"st": 171, "ed": 173, "text": "numerical experiments"}, {"st": 175, "ed": 177, "text": "theoretical findings"}, {"st": 186, "ed": 188, "text": "existing results"}, {"st": 189, "ed": 191, "text": "domain adaptation"}]
[{"st": 10, "ed": 12, "text": "hierarchical clustering"}, {"st": 14, "ed": 16, "text": "proposed method"}, {"st": 83, "ed": 87, "text": "real and synthetic data"}]
[{"st": 11, "ed": 13, "text": "group level"}, {"st": 13, "ed": 15, "text": "context information"}, {"st": 18, "ed": 20, "text": "low dimensional"}, {"st": 36, "ed": 38, "text": "building block"}, {"st": 72, "ed": 75, "text": "dirichlet process mixture"}, {"st": 121, "ed": 123, "text": "extensive experiments"}, {"st": 124, "ed": 126, "text": "real world"}, {"st": 132, "ed": 134, "text": "context information"}, {"st": 139, "ed": 142, "text": "text and image"}]
[{"st": 9, "ed": 11, "text": "ridge regression"}, {"st": 17, "ed": 19, "text": "loss function"}, {"st": 60, "ed": 62, "text": "outer product"}, {"st": 89, "ed": 91, "text": "doesn t"}, {"st": 105, "ed": 108, "text": "positive and negative"}, {"st": 119, "ed": 121, "text": "least square"}]
[{"st": 1, "ed": 3, "text": "time series"}, {"st": 29, "ed": 32, "text": "support vector regression"}, {"st": 37, "ed": 39, "text": "multi step"}, {"st": 40, "ed": 42, "text": "time series"}, {"st": 58, "ed": 60, "text": "step ahead"}, {"st": 60, "ed": 62, "text": "time series"}, {"st": 66, "ed": 68, "text": "multiple output"}, {"st": 68, "ed": 71, "text": "support vector regression"}, {"st": 76, "ed": 78, "text": "multiple output"}, {"st": 106, "ed": 108, "text": "multi step"}, {"st": 117, "ed": 119, "text": "proposed approach"}, {"st": 123, "ed": 126, "text": "simulated and real"}, {"st": 139, "ed": 141, "text": "prediction accuracy"}, {"st": 182, "ed": 184, "text": "computational cost"}, {"st": 198, "ed": 200, "text": "prediction accuracy"}]
[{"st": 0, "ed": 2, "text": "uniform sampling"}, {"st": 3, "ed": 5, "text": "training data"}, {"st": 11, "ed": 13, "text": "stochastic optimization"}, {"st": 17, "ed": 20, "text": "stochastic gradient descent"}, {"st": 32, "ed": 34, "text": "uniform sampling"}, {"st": 77, "ed": 79, "text": "importance sampling"}, {"st": 82, "ed": 84, "text": "convergence rate"}, {"st": 99, "ed": 101, "text": "importance sampling"}, {"st": 113, "ed": 115, "text": "uniform sampling"}, {"st": 117, "ed": 119, "text": "training process"}, {"st": 120, "ed": 122, "text": "proposed algorithm"}, {"st": 123, "ed": 125, "text": "importance sampling"}, {"st": 138, "ed": 140, "text": "importance sampling"}, {"st": 157, "ed": 159, "text": "theoretical analysis"}, {"st": 163, "ed": 165, "text": "convergence rates"}, {"st": 168, "ed": 170, "text": "importance sampling"}, {"st": 173, "ed": 175, "text": "significantly improved"}]
[{"st": 6, "ed": 8, "text": "well calibrated"}, {"st": 29, "ed": 31, "text": "well calibrated"}, {"st": 36, "ed": 38, "text": "machine learning"}, {"st": 56, "ed": 58, "text": "binary classification"}, {"st": 64, "ed": 66, "text": "bayes optimal"}, {"st": 93, "ed": 95, "text": "predictive model"}, {"st": 102, "ed": 104, "text": "post processing"}, {"st": 116, "ed": 118, "text": "wide variety"}, {"st": 119, "ed": 121, "text": "machine learning"}]
[{"st": 4, "ed": 6, "text": "predictive models"}, {"st": 14, "ed": 16, "text": "decision making"}, {"st": 36, "ed": 38, "text": "probabilistic models"}, {"st": 40, "ed": 42, "text": "well calibrated"}, {"st": 51, "ed": 53, "text": "post processing"}, {"st": 63, "ed": 65, "text": "well calibrated"}, {"st": 79, "ed": 81, "text": "post processing"}, {"st": 99, "ed": 101, "text": "machine learning"}, {"st": 129, "ed": 131, "text": "post processing"}, {"st": 140, "ed": 142, "text": "well calibrated"}, {"st": 184, "ed": 187, "text": "synthetic and real"}]
[{"st": 0, "ed": 2, "text": "coordinate descent"}, {"st": 15, "ed": 17, "text": "optimization problems"}, {"st": 26, "ed": 28, "text": "linear models"}, {"st": 30, "ed": 33, "text": "support vector machine"}, {"st": 107, "ed": 109, "text": "optimization problems"}, {"st": 111, "ed": 113, "text": "machine learning"}, {"st": 118, "ed": 120, "text": "speed ups"}]
[{"st": 9, "ed": 11, "text": "excess risk"}, {"st": 17, "ed": 19, "text": "loss functions"}, {"st": 27, "ed": 29, "text": "loss functions"}, {"st": 31, "ed": 33, "text": "fundamental problems"}, {"st": 34, "ed": 36, "text": "machine learning"}, {"st": 38, "ed": 40, "text": "squared loss"}, {"st": 41, "ed": 43, "text": "linear regression"}, {"st": 43, "ed": 45, "text": "logistic loss"}, {"st": 55, "ed": 57, "text": "batch setting"}, {"st": 65, "ed": 68, "text": "empirical risk minimization"}, {"st": 90, "ed": 92, "text": "online setting"}, {"st": 97, "ed": 99, "text": "training examples"}, {"st": 106, "ed": 109, "text": "online learning algorithm"}, {"st": 113, "ed": 115, "text": "modified version"}, {"st": 131, "ed": 133, "text": "loss function"}, {"st": 145, "ed": 147, "text": "excess risk"}]
[{"st": 4, "ed": 6, "text": "observed variables"}, {"st": 29, "ed": 31, "text": "causal model"}, {"st": 40, "ed": 42, "text": "observed data"}, {"st": 59, "ed": 61, "text": "causal model"}, {"st": 62, "ed": 64, "text": "binary data"}, {"st": 74, "ed": 76, "text": "causal model"}, {"st": 79, "ed": 81, "text": "binary data"}, {"st": 89, "ed": 91, "text": "experimental evaluation"}, {"st": 92, "ed": 94, "text": "excellent performance"}]
[{"st": 13, "ed": 17, "text": "reproducing kernel hilbert space"}, {"st": 20, "ed": 22, "text": "gaussian kernel"}, {"st": 41, "ed": 43, "text": "important issue"}, {"st": 47, "ed": 49, "text": "small sample"}, {"st": 51, "ed": 53, "text": "previous research"}, {"st": 65, "ed": 67, "text": "rule based"}, {"st": 94, "ed": 96, "text": "optimization strategy"}, {"st": 124, "ed": 127, "text": "mean square error"}, {"st": 129, "ed": 131, "text": "theoretical results"}, {"st": 137, "ed": 139, "text": "excellent performance"}, {"st": 155, "ed": 157, "text": "time series"}]
[{"st": 5, "ed": 7, "text": "computational biology"}, {"st": 9, "ed": 12, "text": "low rank matrix"}, {"st": 39, "ed": 41, "text": "matrix factorization"}, {"st": 86, "ed": 90, "text": "non negative matrix factorization"}]
[{"st": 5, "ed": 7, "text": "real valued"}, {"st": 50, "ed": 52, "text": "hierarchical structure"}, {"st": 56, "ed": 58, "text": "decision tree"}, {"st": 82, "ed": 84, "text": "linear model"}, {"st": 89, "ed": 91, "text": "conventional approaches"}, {"st": 144, "ed": 146, "text": "computational complexity"}, {"st": 172, "ed": 174, "text": "significant gains"}, {"st": 179, "ed": 181, "text": "real data"}]
[{"st": 44, "ed": 46, "text": "latent feature"}, {"st": 50, "ed": 52, "text": "latent structure"}, {"st": 132, "ed": 134, "text": "generative model"}, {"st": 154, "ed": 156, "text": "gibbs sampler"}, {"st": 179, "ed": 181, "text": "variational inference"}, {"st": 195, "ed": 197, "text": "computational complexity"}, {"st": 200, "ed": 202, "text": "gibbs sampler"}]
[{"st": 74, "ed": 77, "text": "learning to rank"}]
[{"st": 14, "ed": 16, "text": "exploration exploitation"}, {"st": 47, "ed": 49, "text": "real world"}, {"st": 57, "ed": 59, "text": "prediction performance"}]
[{"st": 3, "ed": 5, "text": "latent variable"}, {"st": 9, "ed": 11, "text": "belief networks"}, {"st": 12, "ed": 15, "text": "difficult to train"}, {"st": 16, "ed": 18, "text": "large datasets"}, {"st": 19, "ed": 21, "text": "exact inference"}, {"st": 29, "ed": 31, "text": "approximate inference"}, {"st": 46, "ed": 48, "text": "approximate inference"}, {"st": 67, "ed": 69, "text": "inference network"}, {"st": 70, "ed": 72, "text": "trained jointly"}, {"st": 75, "ed": 78, "text": "variational lower bound"}, {"st": 108, "ed": 110, "text": "variance reduction"}, {"st": 117, "ed": 119, "text": "belief networks"}]
[{"st": 1, "ed": 3, "text": "feature selection"}, {"st": 16, "ed": 18, "text": "observational data"}, {"st": 87, "ed": 89, "text": "kernel based"}, {"st": 105, "ed": 108, "text": "easy to implement"}, {"st": 109, "ed": 112, "text": "compares favorably to"}, {"st": 115, "ed": 118, "text": "synthetic and real"}]
[{"st": 4, "ed": 7, "text": "principal component analysis"}, {"st": 9, "ed": 13, "text": "canonical correlation analysis cca"}, {"st": 52, "ed": 54, "text": "recent research"}, {"st": 77, "ed": 79, "text": "random features"}, {"st": 114, "ed": 116, "text": "multivariate analysis"}, {"st": 119, "ed": 121, "text": "spectral clustering"}, {"st": 130, "ed": 133, "text": "real world data"}]
[{"st": 9, "ed": 11, "text": "vladimir vapnik"}, {"st": 12, "ed": 14, "text": "equivalence classes"}, {"st": 28, "ed": 30, "text": "successfully applied"}, {"st": 57, "ed": 59, "text": "multi class"}, {"st": 59, "ed": 61, "text": "multi label"}, {"st": 72, "ed": 75, "text": "non convex optimization"}, {"st": 77, "ed": 79, "text": "globally optimal"}, {"st": 102, "ed": 104, "text": "proposed method"}]
[{"st": 3, "ed": 5, "text": "data science"}, {"st": 12, "ed": 15, "text": "coronary artery disease"}, {"st": 22, "ed": 24, "text": "base pair"}, {"st": 26, "ed": 29, "text": "single nucleotide polymorphisms"}, {"st": 46, "ed": 48, "text": "supervised learning"}, {"st": 50, "ed": 53, "text": "k nearest neighbour"}, {"st": 53, "ed": 55, "text": "k nn"}, {"st": 56, "ed": 58, "text": "random forest"}, {"st": 66, "ed": 68, "text": "k nn"}, {"st": 74, "ed": 76, "text": "finite dimensional"}, {"st": 84, "ed": 86, "text": "dimensionality reduction"}, {"st": 87, "ed": 89, "text": "random projections"}, {"st": 91, "ed": 93, "text": "feature extraction"}, {"st": 109, "ed": 111, "text": "feature selection"}, {"st": 121, "ed": 123, "text": "random projections"}, {"st": 125, "ed": 127, "text": "k nn"}, {"st": 130, "ed": 132, "text": "feature selection"}, {"st": 133, "ed": 135, "text": "random forest"}, {"st": 143, "ed": 145, "text": "f measure"}, {"st": 160, "ed": 162, "text": "feature selection"}, {"st": 163, "ed": 165, "text": "random forest"}, {"st": 169, "ed": 171, "text": "random projections"}, {"st": 175, "ed": 177, "text": "random forest"}, {"st": 207, "ed": 209, "text": "feature selection"}]
[{"st": 0, "ed": 2, "text": "hierarchical bayesian"}, {"st": 4, "ed": 6, "text": "neural networks"}, {"st": 8, "ed": 10, "text": "hidden units"}, {"st": 62, "ed": 64, "text": "posterior inference"}, {"st": 95, "ed": 97, "text": "monte carlo"}, {"st": 100, "ed": 102, "text": "marginal likelihood"}, {"st": 109, "ed": 111, "text": "theoretical results"}]
[{"st": 7, "ed": 9, "text": "contextual bandit"}, {"st": 44, "ed": 46, "text": "fully supervised"}, {"st": 46, "ed": 48, "text": "cost sensitive"}, {"st": 48, "ed": 50, "text": "classification problems"}, {"st": 54, "ed": 56, "text": "optimal regret"}, {"st": 59, "ed": 61, "text": "tilde o"}, {"st": 93, "ed": 95, "text": "contextual bandit"}, {"st": 95, "ed": 97, "text": "learning algorithm"}, {"st": 109, "ed": 112, "text": "proof of concept"}, {"st": 119, "ed": 121, "text": "prediction performance"}]
[{"st": 0, "ed": 2, "text": "convex optimization"}, {"st": 7, "ed": 9, "text": "machine learning"}, {"st": 18, "ed": 20, "text": "minimization problems"}, {"st": 35, "ed": 37, "text": "convex problems"}, {"st": 61, "ed": 63, "text": "objective function"}, {"st": 93, "ed": 95, "text": "convex optimization"}, {"st": 102, "ed": 104, "text": "mathbb r"}, {"st": 128, "ed": 130, "text": "primal dual"}]
[{"st": 10, "ed": 12, "text": "machine learning"}, {"st": 19, "ed": 21, "text": "recent progress"}, {"st": 35, "ed": 37, "text": "model based"}, {"st": 47, "ed": 49, "text": "cross validation"}, {"st": 52, "ed": 54, "text": "learning algorithm"}, {"st": 68, "ed": 70, "text": "learned models"}, {"st": 71, "ed": 73, "text": "consistently outperform"}, {"st": 101, "ed": 103, "text": "recently proposed"}, {"st": 127, "ed": 129, "text": "proposed approach"}]
[{"st": 3, "ed": 5, "text": "central role"}, {"st": 25, "ed": 27, "text": "network analysis"}, {"st": 53, "ed": 55, "text": "financial instruments"}, {"st": 69, "ed": 71, "text": "noisy observations"}, {"st": 87, "ed": 89, "text": "probabilistic model"}, {"st": 93, "ed": 95, "text": "point processes"}, {"st": 96, "ed": 98, "text": "random graph"}, {"st": 114, "ed": 116, "text": "fully bayesian"}]
[{"st": 35, "ed": 37, "text": "hidden units"}, {"st": 47, "ed": 49, "text": "theoretical results"}, {"st": 86, "ed": 88, "text": "deep models"}, {"st": 189, "ed": 191, "text": "efficient online"}]
[{"st": 7, "ed": 9, "text": "highly effective"}, {"st": 12, "ed": 14, "text": "global optimization"}, {"st": 37, "ed": 39, "text": "gaussian processes"}, {"st": 73, "ed": 75, "text": "non stationary"}, {"st": 82, "ed": 84, "text": "machine learning"}, {"st": 131, "ed": 133, "text": "input space"}, {"st": 146, "ed": 148, "text": "multi task"}, {"st": 152, "ed": 154, "text": "multiple tasks"}]
[{"st": 1, "ed": 4, "text": "multiple instance learning"}, {"st": 9, "ed": 11, "text": "feature vectors"}, {"st": 58, "ed": 60, "text": "low dimensional"}, {"st": 76, "ed": 78, "text": "high dimensional"}, {"st": 82, "ed": 84, "text": "total number"}, {"st": 150, "ed": 153, "text": "multiple instance learning"}]
[{"st": 0, "ed": 3, "text": "gaussian processes gps"}, {"st": 5, "ed": 7, "text": "powerful tool"}, {"st": 8, "ed": 10, "text": "probabilistic inference"}, {"st": 21, "ed": 24, "text": "linear dimensionality reduction"}, {"st": 26, "ed": 28, "text": "desirable properties"}, {"st": 30, "ed": 32, "text": "uncertainty estimates"}, {"st": 65, "ed": 67, "text": "variational inference"}, {"st": 69, "ed": 71, "text": "gp regression"}, {"st": 72, "ed": 74, "text": "latent variable"}, {"st": 100, "ed": 103, "text": "evidence lower bound"}, {"st": 118, "ed": 120, "text": "computational resources"}, {"st": 138, "ed": 140, "text": "gaussian processes"}, {"st": 164, "ed": 166, "text": "latent variable"}]
[{"st": 1, "ed": 3, "text": "statistical learning"}, {"st": 53, "ed": 55, "text": "convergence rate"}, {"st": 111, "ed": 113, "text": "surrogate loss"}, {"st": 116, "ed": 118, "text": "loss function"}, {"st": 138, "ed": 140, "text": "generalization bound"}, {"st": 146, "ed": 148, "text": "excess risk"}, {"st": 151, "ed": 153, "text": "excess risk"}, {"st": 174, "ed": 176, "text": "convex loss"}, {"st": 181, "ed": 183, "text": "excess risk"}]
[{"st": 42, "ed": 44, "text": "structured sparsity"}]
[{"st": 5, "ed": 8, "text": "generative and discriminative"}, {"st": 10, "ed": 12, "text": "log likelihood"}, {"st": 39, "ed": 41, "text": "training objective"}, {"st": 78, "ed": 80, "text": "maximum likelihood"}, {"st": 120, "ed": 122, "text": "objective function"}]
[{"st": 8, "ed": 10, "text": "convex optimization"}, {"st": 17, "ed": 19, "text": "mathcal o"}, {"st": 33, "ed": 35, "text": "mathbb r"}, {"st": 37, "ed": 39, "text": "mathbb r"}, {"st": 46, "ed": 48, "text": "random walk"}]
[{"st": 13, "ed": 16, "text": "multiple kernel learning"}, {"st": 36, "ed": 40, "text": "sum product networks spns"}]
[{"st": 13, "ed": 15, "text": "unlabeled instances"}, {"st": 49, "ed": 51, "text": "training data"}, {"st": 76, "ed": 78, "text": "semi supervised"}, {"st": 108, "ed": 110, "text": "fully supervised"}, {"st": 127, "ed": 129, "text": "existing methods"}, {"st": 135, "ed": 137, "text": "significant improvement"}]
[{"st": 65, "ed": 67, "text": "random forest"}, {"st": 78, "ed": 80, "text": "consistently outperform"}, {"st": 83, "ed": 85, "text": "problems involving"}, {"st": 85, "ed": 87, "text": "real world"}, {"st": 108, "ed": 110, "text": "big data"}, {"st": 119, "ed": 121, "text": "gaussian processes"}, {"st": 121, "ed": 124, "text": "support vector machines"}]
[{"st": 17, "ed": 19, "text": "open ended"}, {"st": 21, "ed": 23, "text": "statistical models"}, {"st": 41, "ed": 43, "text": "natural language"}, {"st": 52, "ed": 54, "text": "gaussian processes"}, {"st": 60, "ed": 62, "text": "gaussian processes"}, {"st": 81, "ed": 83, "text": "compositional structure"}, {"st": 103, "ed": 105, "text": "nonparametric models"}, {"st": 114, "ed": 116, "text": "open ended"}, {"st": 130, "ed": 132, "text": "time series"}]
[{"st": 27, "ed": 30, "text": "statistical relational learning"}, {"st": 39, "ed": 41, "text": "constraint satisfaction"}, {"st": 66, "ed": 69, "text": "first order logic"}, {"st": 169, "ed": 171, "text": "structured output"}, {"st": 190, "ed": 192, "text": "max margin"}]
[{"st": 10, "ed": 12, "text": "central role"}, {"st": 16, "ed": 18, "text": "machine learning"}, {"st": 63, "ed": 65, "text": "feature selection"}, {"st": 99, "ed": 101, "text": "disjoint sets"}, {"st": 111, "ed": 113, "text": "group lasso"}, {"st": 126, "ed": 128, "text": "group lasso"}, {"st": 134, "ed": 136, "text": "group lasso"}, {"st": 156, "ed": 158, "text": "main contribution"}, {"st": 172, "ed": 174, "text": "convex optimization"}, {"st": 189, "ed": 191, "text": "error bounds"}, {"st": 193, "ed": 195, "text": "classification problems"}, {"st": 203, "ed": 205, "text": "error bounds"}, {"st": 230, "ed": 232, "text": "group lasso"}, {"st": 274, "ed": 278, "text": "real and synthetic data"}]
[{"st": 3, "ed": 5, "text": "subspace learning"}, {"st": 10, "ed": 12, "text": "dimensional subspace"}, {"st": 13, "ed": 15, "text": "mathbb r"}, {"st": 38, "ed": 40, "text": "subspace learning"}, {"st": 42, "ed": 44, "text": "partial information"}, {"st": 63, "ed": 65, "text": "efficient algorithms"}, {"st": 71, "ed": 73, "text": "sample complexity"}]
[{"st": 0, "ed": 3, "text": "determinantal point processes"}, {"st": 72, "ed": 74, "text": "large scale"}, {"st": 102, "ed": 104, "text": "diabetic neuropathy"}, {"st": 114, "ed": 116, "text": "human perception"}]
[{"st": 0, "ed": 2, "text": "approximate inference"}, {"st": 6, "ed": 8, "text": "probabilistic models"}, {"st": 13, "ed": 15, "text": "computational statistics"}, {"st": 23, "ed": 25, "text": "variational inference"}, {"st": 34, "ed": 36, "text": "monte carlo"}, {"st": 51, "ed": 53, "text": "variational approximations"}, {"st": 64, "ed": 66, "text": "monte carlo"}, {"st": 82, "ed": 84, "text": "mean field"}, {"st": 112, "ed": 114, "text": "monte carlo"}, {"st": 164, "ed": 167, "text": "markov random fields"}, {"st": 167, "ed": 169, "text": "nonparametric bayesian"}, {"st": 187, "ed": 189, "text": "real world"}]
[{"st": 7, "ed": 9, "text": "deep networks"}, {"st": 39, "ed": 41, "text": "gaussian process"}, {"st": 65, "ed": 68, "text": "degrees of freedom"}, {"st": 88, "ed": 90, "text": "network architecture"}]
[{"st": 24, "ed": 26, "text": "conditional distribution"}, {"st": 28, "ed": 30, "text": "response variable"}, {"st": 57, "ed": 59, "text": "least squares"}, {"st": 74, "ed": 76, "text": "prediction errors"}, {"st": 99, "ed": 102, "text": "hyper parameter tuning"}, {"st": 127, "ed": 129, "text": "prediction methods"}]
[{"st": 3, "ed": 5, "text": "gaussian process"}, {"st": 55, "ed": 57, "text": "feature space"}, {"st": 85, "ed": 87, "text": "gaussian processes"}, {"st": 92, "ed": 94, "text": "jointly learns"}, {"st": 101, "ed": 103, "text": "feature space"}, {"st": 105, "ed": 107, "text": "gp regression"}, {"st": 109, "ed": 111, "text": "feature space"}, {"st": 137, "ed": 140, "text": "proof of concept"}, {"st": 152, "ed": 154, "text": "perform poorly"}]
[{"st": 10, "ed": 12, "text": "provable guarantees"}, {"st": 18, "ed": 20, "text": "parameter learning"}, {"st": 21, "ed": 23, "text": "latent variable"}, {"st": 53, "ed": 55, "text": "topic models"}, {"st": 116, "ed": 118, "text": "empirical results"}, {"st": 133, "ed": 135, "text": "synthetic data"}]
[{"st": 67, "ed": 69, "text": "total variation"}, {"st": 80, "ed": 82, "text": "spatial regions"}, {"st": 93, "ed": 98, "text": "direction method of multipliers admm"}, {"st": 106, "ed": 108, "text": "iterative optimization"}, {"st": 116, "ed": 118, "text": "proposed method"}, {"st": 121, "ed": 124, "text": "divide and conquer"}, {"st": 124, "ed": 126, "text": "learning algorithm"}]
[{"st": 0, "ed": 3, "text": "hidden markov models"}, {"st": 12, "ed": 14, "text": "time series"}, {"st": 50, "ed": 52, "text": "variational bayesian"}, {"st": 55, "ed": 57, "text": "probabilistic models"}, {"st": 62, "ed": 64, "text": "probabilistic models"}, {"st": 65, "ed": 67, "text": "random variables"}, {"st": 87, "ed": 89, "text": "special case"}, {"st": 104, "ed": 106, "text": "multivariate gaussian"}, {"st": 124, "ed": 126, "text": "case studies"}]
[{"st": 7, "ed": 9, "text": "pac bayesian"}, {"st": 20, "ed": 22, "text": "negative log"}, {"st": 23, "ed": 25, "text": "loss function"}, {"st": 31, "ed": 33, "text": "pac bayesian"}, {"st": 68, "ed": 70, "text": "negative log"}, {"st": 74, "ed": 76, "text": "loss function"}, {"st": 81, "ed": 83, "text": "pac bayesian"}, {"st": 102, "ed": 104, "text": "linear regression"}]
[{"st": 6, "ed": 8, "text": "pure exploration"}, {"st": 9, "ed": 11, "text": "bandit problem"}, {"st": 44, "ed": 46, "text": "parameter free"}, {"st": 78, "ed": 80, "text": "non trivial"}, {"st": 80, "ed": 82, "text": "pure exploration"}, {"st": 85, "ed": 87, "text": "fixed budget"}]
[{"st": 8, "ed": 10, "text": "active learning"}, {"st": 12, "ed": 14, "text": "learning algorithm"}, {"st": 24, "ed": 26, "text": "fisher information"}, {"st": 90, "ed": 92, "text": "active learning"}, {"st": 113, "ed": 115, "text": "log likelihood"}, {"st": 152, "ed": 154, "text": "active learning"}]
[{"st": 20, "ed": 22, "text": "unlabeled examples"}, {"st": 50, "ed": 52, "text": "significantly higher"}, {"st": 54, "ed": 56, "text": "boosted trees"}, {"st": 56, "ed": 58, "text": "random forests"}, {"st": 59, "ed": 61, "text": "logistic regression"}, {"st": 62, "ed": 64, "text": "unlabeled examples"}]
[{"st": 6, "ed": 9, "text": "best arm identification"}, {"st": 12, "ed": 14, "text": "fixed budget"}, {"st": 40, "ed": 42, "text": "bandit problem"}, {"st": 147, "ed": 149, "text": "fixed budget"}, {"st": 149, "ed": 152, "text": "best arm identification"}]
[{"st": 20, "ed": 22, "text": "machine learning"}, {"st": 42, "ed": 44, "text": "feature maps"}, {"st": 44, "ed": 46, "text": "convex optimization"}, {"st": 104, "ed": 106, "text": "random vectors"}, {"st": 110, "ed": 112, "text": "theoretical guarantees"}, {"st": 140, "ed": 142, "text": "angular distance"}, {"st": 148, "ed": 150, "text": "theoretical results"}, {"st": 172, "ed": 174, "text": "special cases"}, {"st": 181, "ed": 183, "text": "recently introduced"}, {"st": 189, "ed": 192, "text": "accuracy and efficiency"}]
[{"st": 0, "ed": 2, "text": "inverse classification"}, {"st": 93, "ed": 95, "text": "logistic regression"}, {"st": 96, "ed": 98, "text": "gaussian kernel"}, {"st": 102, "ed": 104, "text": "inverse classification"}, {"st": 179, "ed": 181, "text": "sensitivity analysis"}, {"st": 194, "ed": 196, "text": "machine learning"}, {"st": 199, "ed": 201, "text": "real world"}, {"st": 201, "ed": 203, "text": "cardiovascular disease"}]
[{"st": 6, "ed": 8, "text": "spectral methods"}, {"st": 14, "ed": 16, "text": "topic models"}, {"st": 20, "ed": 23, "text": "latent dirichlet allocation"}, {"st": 78, "ed": 80, "text": "special case"}, {"st": 94, "ed": 96, "text": "topic model"}, {"st": 101, "ed": 103, "text": "spectral methods"}, {"st": 112, "ed": 114, "text": "low order"}, {"st": 173, "ed": 175, "text": "topic model"}, {"st": 178, "ed": 180, "text": "real datasets"}, {"st": 184, "ed": 186, "text": "new york"}]
[{"st": 11, "ed": 13, "text": "random forest"}, {"st": 27, "ed": 30, "text": "classification and regression"}, {"st": 38, "ed": 40, "text": "low variance"}, {"st": 79, "ed": 81, "text": "feature space"}]
[{"st": 18, "ed": 20, "text": "powerful tool"}, {"st": 21, "ed": 23, "text": "machine learning"}, {"st": 36, "ed": 39, "text": "a reproducing kernel"}, {"st": 48, "ed": 50, "text": "kernel methods"}, {"st": 66, "ed": 68, "text": "feature map"}, {"st": 70, "ed": 73, "text": "support vector machines"}, {"st": 94, "ed": 96, "text": "kernel machines"}, {"st": 97, "ed": 99, "text": "probabilistic modeling"}, {"st": 100, "ed": 102, "text": "statistical inference"}, {"st": 102, "ed": 104, "text": "causal discovery"}, {"st": 122, "ed": 124, "text": "recent advances"}, {"st": 136, "ed": 138, "text": "open problems"}, {"st": 156, "ed": 158, "text": "positive definite"}, {"st": 177, "ed": 179, "text": "marginal distributions"}, {"st": 179, "ed": 181, "text": "theoretical guarantees"}, {"st": 228, "ed": 230, "text": "conditional distributions"}, {"st": 231, "ed": 233, "text": "theoretical insights"}, {"st": 245, "ed": 247, "text": "sum product"}, {"st": 254, "ed": 256, "text": "graphical model"}, {"st": 256, "ed": 258, "text": "probabilistic inference"}, {"st": 259, "ed": 261, "text": "reinforcement learning"}]
[{"st": 2, "ed": 4, "text": "large scale"}, {"st": 4, "ed": 6, "text": "classification problems"}, {"st": 34, "ed": 36, "text": "entire dataset"}, {"st": 50, "ed": 52, "text": "computational cost"}, {"st": 86, "ed": 88, "text": "proposed method"}, {"st": 110, "ed": 112, "text": "numerical experiments"}, {"st": 114, "ed": 116, "text": "proposed method"}, {"st": 122, "ed": 124, "text": "computational costs"}, {"st": 136, "ed": 138, "text": "large scale"}]
[{"st": 8, "ed": 10, "text": "clustering algorithm"}, {"st": 13, "ed": 15, "text": "hierarchical clustering"}, {"st": 25, "ed": 27, "text": "clustering algorithm"}, {"st": 32, "ed": 35, "text": "number of clusters"}]
[{"st": 9, "ed": 11, "text": "jointly learns"}, {"st": 16, "ed": 18, "text": "inference network"}, {"st": 29, "ed": 31, "text": "latent variables"}, {"st": 37, "ed": 39, "text": "inference network"}, {"st": 40, "ed": 42, "text": "training examples"}, {"st": 76, "ed": 78, "text": "generative network"}, {"st": 114, "ed": 116, "text": "learned representations"}, {"st": 128, "ed": 130, "text": "semi supervised"}]
[{"st": 4, "ed": 6, "text": "machine learning"}, {"st": 27, "ed": 29, "text": "differential privacy"}, {"st": 34, "ed": 37, "text": "gaussian processes gps"}, {"st": 46, "ed": 48, "text": "differentially private"}, {"st": 66, "ed": 68, "text": "training data"}, {"st": 85, "ed": 87, "text": "method achieves"}, {"st": 102, "ed": 104, "text": "multi dimensional"}, {"st": 114, "ed": 116, "text": "differential privacy"}]
[{"st": 25, "ed": 27, "text": "feature extraction"}, {"st": 61, "ed": 63, "text": "optimization criteria"}, {"st": 95, "ed": 97, "text": "multiple modalities"}, {"st": 161, "ed": 163, "text": "dimensionality reduction"}]
[{"st": 92, "ed": 95, "text": "cold start problem"}, {"st": 133, "ed": 135, "text": "low rank"}, {"st": 157, "ed": 159, "text": "large scale"}]
[{"st": 0, "ed": 3, "text": "k nearest neighbors"}, {"st": 3, "ed": 5, "text": "k nn"}, {"st": 12, "ed": 14, "text": "supervised learning"}, {"st": 43, "ed": 45, "text": "labeled samples"}, {"st": 56, "ed": 58, "text": "k nn"}, {"st": 82, "ed": 84, "text": "random walk"}, {"st": 90, "ed": 92, "text": "nearest neighbor"}, {"st": 110, "ed": 112, "text": "nearest neighbors"}, {"st": 121, "ed": 123, "text": "class label"}, {"st": 158, "ed": 160, "text": "local neighborhood"}, {"st": 167, "ed": 169, "text": "synthetic data"}, {"st": 171, "ed": 174, "text": "real world data"}, {"st": 183, "ed": 185, "text": "k nn"}, {"st": 193, "ed": 195, "text": "k nn"}, {"st": 204, "ed": 206, "text": "real world"}, {"st": 213, "ed": 215, "text": "k nn"}, {"st": 220, "ed": 222, "text": "k nn"}]
[{"st": 3, "ed": 5, "text": "kernel methods"}, {"st": 11, "ed": 13, "text": "positive semidefinite"}, {"st": 84, "ed": 86, "text": "base kernels"}, {"st": 92, "ed": 94, "text": "positive semidefinite"}, {"st": 98, "ed": 100, "text": "base kernels"}, {"st": 135, "ed": 137, "text": "classification accuracy"}, {"st": 140, "ed": 143, "text": "benchmark data sets"}]
[{"st": 4, "ed": 6, "text": "clustering approaches"}, {"st": 10, "ed": 12, "text": "recent years"}, {"st": 67, "ed": 69, "text": "clustering approach"}, {"st": 136, "ed": 138, "text": "random walk"}, {"st": 162, "ed": 164, "text": "similarity measure"}, {"st": 190, "ed": 192, "text": "real world"}, {"st": 195, "ed": 198, "text": "effectiveness and efficiency"}]
[{"st": 17, "ed": 19, "text": "feature maps"}, {"st": 22, "ed": 25, "text": "multi task learning"}, {"st": 45, "ed": 47, "text": "hidden layer"}, {"st": 59, "ed": 62, "text": "multi task learning"}]
[{"st": 2, "ed": 6, "text": "generative adversarial networks gans"}, {"st": 8, "ed": 10, "text": "semi supervised"}, {"st": 23, "ed": 25, "text": "generative model"}, {"st": 92, "ed": 94, "text": "higher quality"}]
[{"st": 37, "ed": 39, "text": "performance metrics"}, {"st": 46, "ed": 48, "text": "training data"}, {"st": 57, "ed": 59, "text": "generic framework"}, {"st": 62, "ed": 64, "text": "performance metric"}, {"st": 69, "ed": 71, "text": "objective function"}, {"st": 101, "ed": 103, "text": "generalization error"}, {"st": 112, "ed": 114, "text": "estimation error"}, {"st": 122, "ed": 124, "text": "regret bounds"}, {"st": 129, "ed": 131, "text": "collaborative filtering"}, {"st": 137, "ed": 139, "text": "positive unlabeled"}, {"st": 152, "ed": 154, "text": "regret bound"}, {"st": 167, "ed": 169, "text": "empirical results"}, {"st": 172, "ed": 174, "text": "benchmark datasets"}, {"st": 177, "ed": 179, "text": "explicitly modeling"}, {"st": 186, "ed": 188, "text": "performance metric"}]
[{"st": 60, "ed": 62, "text": "variational autoencoder"}, {"st": 88, "ed": 90, "text": "generative model"}, {"st": 97, "ed": 99, "text": "efficient learning"}, {"st": 104, "ed": 107, "text": "unsupervised and supervised"}, {"st": 125, "ed": 127, "text": "generative models"}, {"st": 137, "ed": 139, "text": "previously unseen"}, {"st": 155, "ed": 157, "text": "neural network"}]
[{"st": 5, "ed": 7, "text": "semi supervised"}, {"st": 39, "ed": 41, "text": "computational complexity"}, {"st": 57, "ed": 59, "text": "based clustering"}, {"st": 73, "ed": 75, "text": "computational complexity"}, {"st": 85, "ed": 88, "text": "k means clustering"}, {"st": 97, "ed": 99, "text": "k means"}, {"st": 111, "ed": 113, "text": "np hard"}, {"st": 157, "ed": 160, "text": "number of clusters"}, {"st": 206, "ed": 208, "text": "computationally efficient"}, {"st": 208, "ed": 210, "text": "clustering algorithm"}]
[{"st": 0, "ed": 2, "text": "feature selection"}, {"st": 7, "ed": 9, "text": "fundamental problems"}, {"st": 20, "ed": 22, "text": "feature selection"}, {"st": 28, "ed": 30, "text": "mutual information"}, {"st": 59, "ed": 61, "text": "existing methods"}, {"st": 100, "ed": 102, "text": "feature selection"}, {"st": 119, "ed": 121, "text": "experiments demonstrate"}, {"st": 123, "ed": 125, "text": "proposed method"}, {"st": 126, "ed": 128, "text": "outperforms existing"}, {"st": 130, "ed": 132, "text": "feature selection"}]
[{"st": 0, "ed": 2, "text": "learning parameters"}, {"st": 19, "ed": 21, "text": "learning framework"}, {"st": 59, "ed": 61, "text": "training set"}, {"st": 71, "ed": 73, "text": "proposed framework"}, {"st": 76, "ed": 78, "text": "compressive sensing"}, {"st": 104, "ed": 106, "text": "iterative algorithm"}, {"st": 128, "ed": 131, "text": "gaussian mixture model"}, {"st": 142, "ed": 144, "text": "theoretical guarantees"}, {"st": 150, "ed": 152, "text": "synthetic data"}, {"st": 154, "ed": 156, "text": "proposed algorithm"}, {"st": 162, "ed": 165, "text": "expectation maximization em"}, {"st": 192, "ed": 194, "text": "large scale"}, {"st": 198, "ed": 200, "text": "training samples"}, {"st": 215, "ed": 217, "text": "proposed framework"}, {"st": 223, "ed": 225, "text": "probability distributions"}, {"st": 257, "ed": 259, "text": "theoretical framework"}, {"st": 268, "ed": 270, "text": "infinite dimensional"}]
[{"st": 6, "ed": 8, "text": "causal models"}, {"st": 31, "ed": 33, "text": "causal inference"}, {"st": 39, "ed": 41, "text": "bandit feedback"}, {"st": 64, "ed": 66, "text": "simple regret"}]
[{"st": 3, "ed": 5, "text": "probabilistic models"}, {"st": 12, "ed": 14, "text": "monte carlo"}, {"st": 34, "ed": 38, "text": "markov chain monte carlo"}, {"st": 94, "ed": 96, "text": "generative model"}, {"st": 103, "ed": 105, "text": "sampling distribution"}, {"st": 116, "ed": 119, "text": "generative adversarial networks"}, {"st": 120, "ed": 122, "text": "proposed framework"}]
[{"st": 10, "ed": 13, "text": "generative adversarial network"}, {"st": 18, "ed": 20, "text": "disentangled representations"}, {"st": 28, "ed": 31, "text": "generative adversarial network"}, {"st": 35, "ed": 37, "text": "mutual information"}, {"st": 43, "ed": 45, "text": "latent variables"}, {"st": 55, "ed": 57, "text": "mutual information"}, {"st": 67, "ed": 69, "text": "training procedure"}, {"st": 91, "ed": 93, "text": "mnist dataset"}, {"st": 114, "ed": 116, "text": "visual concepts"}, {"st": 146, "ed": 148, "text": "fully supervised"}]
[{"st": 10, "ed": 12, "text": "iterative method"}, {"st": 18, "ed": 20, "text": "sparse signal"}, {"st": 22, "ed": 24, "text": "linear models"}, {"st": 27, "ed": 29, "text": "sparse recovery"}, {"st": 31, "ed": 33, "text": "compressed sensing"}, {"st": 34, "ed": 36, "text": "machine learning"}, {"st": 67, "ed": 69, "text": "sparse signal"}, {"st": 81, "ed": 83, "text": "missing data"}, {"st": 89, "ed": 91, "text": "big data"}, {"st": 92, "ed": 94, "text": "machine learning"}, {"st": 133, "ed": 135, "text": "numerical results"}]
[{"st": 7, "ed": 9, "text": "least squares"}, {"st": 36, "ed": 38, "text": "finite dimensional"}, {"st": 41, "ed": 43, "text": "random features"}, {"st": 54, "ed": 56, "text": "inner product"}, {"st": 80, "ed": 82, "text": "existing algorithms"}]
[{"st": 4, "ed": 6, "text": "real world"}, {"st": 55, "ed": 57, "text": "classification methods"}, {"st": 86, "ed": 88, "text": "closed set"}, {"st": 116, "ed": 118, "text": "open set"}, {"st": 152, "ed": 155, "text": "support vector machines"}, {"st": 161, "ed": 164, "text": "support vector machines"}, {"st": 171, "ed": 173, "text": "open set"}, {"st": 177, "ed": 179, "text": "empirical risk"}, {"st": 192, "ed": 194, "text": "feature space"}, {"st": 233, "ed": 236, "text": "radial basis function"}, {"st": 253, "ed": 255, "text": "open set"}, {"st": 260, "ed": 262, "text": "sufficient conditions"}, {"st": 273, "ed": 277, "text": "extensive set of experiments"}, {"st": 279, "ed": 281, "text": "proposed method"}, {"st": 282, "ed": 284, "text": "existing solutions"}, {"st": 288, "ed": 290, "text": "open set"}]
[{"st": 7, "ed": 9, "text": "key challenge"}, {"st": 13, "ed": 15, "text": "recent advances"}, {"st": 20, "ed": 23, "text": "vision and language"}, {"st": 26, "ed": 28, "text": "deep learning"}, {"st": 50, "ed": 52, "text": "metric learning"}, {"st": 54, "ed": 56, "text": "deep neural"}, {"st": 59, "ed": 61, "text": "recent advances"}, {"st": 63, "ed": 65, "text": "neural networks"}, {"st": 91, "ed": 93, "text": "fine tuning"}, {"st": 103, "ed": 105, "text": "shot learning"}]
[{"st": 33, "ed": 35, "text": "noise free"}, {"st": 38, "ed": 40, "text": "training examples"}, {"st": 57, "ed": 59, "text": "joint distributions"}, {"st": 66, "ed": 68, "text": "sufficient statistic"}, {"st": 81, "ed": 83, "text": "supervised learning"}, {"st": 98, "ed": 100, "text": "causal inference"}]
[{"st": 51, "ed": 53, "text": "low dimensional"}, {"st": 55, "ed": 57, "text": "noisy data"}]
[{"st": 13, "ed": 15, "text": "multimodal data"}, {"st": 30, "ed": 34, "text": "canonical correlation analysis cca"}, {"st": 39, "ed": 41, "text": "kernel based"}]
[{"st": 1, "ed": 3, "text": "machine learning"}, {"st": 8, "ed": 10, "text": "null hypothesis"}, {"st": 62, "ed": 64, "text": "machine learning"}]
[{"st": 18, "ed": 20, "text": "time series"}, {"st": 42, "ed": 44, "text": "standard classification"}, {"st": 73, "ed": 75, "text": "gaussian process"}, {"st": 81, "ed": 83, "text": "time series"}, {"st": 121, "ed": 123, "text": "gaussian process"}, {"st": 131, "ed": 134, "text": "end to end"}]
[{"st": 7, "ed": 9, "text": "recently proposed"}, {"st": 10, "ed": 13, "text": "stochastic gradient descent"}, {"st": 16, "ed": 19, "text": "recurrent neural network"}, {"st": 42, "ed": 45, "text": "stochastic gradient descent"}, {"st": 71, "ed": 73, "text": "learning problems"}, {"st": 75, "ed": 77, "text": "real world"}, {"st": 77, "ed": 79, "text": "mnist handwritten"}, {"st": 79, "ed": 81, "text": "digit recognition"}, {"st": 91, "ed": 93, "text": "highly competitive"}]
[{"st": 28, "ed": 30, "text": "geometric structure"}, {"st": 41, "ed": 43, "text": "compressive sensing"}, {"st": 115, "ed": 118, "text": "local and global"}, {"st": 118, "ed": 120, "text": "geometric structure"}, {"st": 145, "ed": 147, "text": "computationally expensive"}, {"st": 156, "ed": 158, "text": "numerical experiments"}]
[{"st": 3, "ed": 5, "text": "normalizing flows"}, {"st": 11, "ed": 13, "text": "variational inference"}, {"st": 70, "ed": 72, "text": "significantly improves"}, {"st": 86, "ed": 88, "text": "variational autoencoder"}, {"st": 95, "ed": 97, "text": "autoregressive models"}, {"st": 101, "ed": 103, "text": "log likelihood"}, {"st": 104, "ed": 106, "text": "natural images"}, {"st": 108, "ed": 110, "text": "significantly faster"}]
[{"st": 27, "ed": 29, "text": "previous approaches"}, {"st": 81, "ed": 83, "text": "predictive power"}]
[{"st": 64, "ed": 66, "text": "efficient algorithms"}, {"st": 73, "ed": 75, "text": "limited data"}, {"st": 76, "ed": 79, "text": "provide theoretical guarantees"}, {"st": 84, "ed": 86, "text": "gaussian process"}, {"st": 119, "ed": 121, "text": "practical applications"}]
[{"st": 5, "ed": 7, "text": "random forest"}, {"st": 9, "ed": 11, "text": "resource constrained"}, {"st": 77, "ed": 79, "text": "primal dual"}, {"st": 97, "ed": 99, "text": "conventional methods"}, {"st": 118, "ed": 120, "text": "algorithm outperforms"}, {"st": 125, "ed": 127, "text": "resource constrained"}]
[{"st": 5, "ed": 7, "text": "decision rules"}, {"st": 23, "ed": 25, "text": "decision tree"}, {"st": 50, "ed": 52, "text": "majority class"}, {"st": 57, "ed": 59, "text": "decision rules"}, {"st": 64, "ed": 67, "text": "the minority class"}, {"st": 77, "ed": 79, "text": "class imbalanced"}, {"st": 91, "ed": 93, "text": "performance metrics"}]
[{"st": 2, "ed": 4, "text": "machine learning"}, {"st": 23, "ed": 25, "text": "feature engineering"}, {"st": 48, "ed": 50, "text": "machine learning"}, {"st": 56, "ed": 58, "text": "interpretable models"}, {"st": 100, "ed": 102, "text": "machine learning"}, {"st": 103, "ed": 105, "text": "interpretable models"}, {"st": 117, "ed": 119, "text": "machine learning"}, {"st": 127, "ed": 129, "text": "machine learning"}, {"st": 163, "ed": 165, "text": "main challenges"}, {"st": 171, "ed": 173, "text": "recently introduced"}]
[{"st": 45, "ed": 47, "text": "feature learning"}, {"st": 48, "ed": 50, "text": "n gram"}]
[{"st": 17, "ed": 20, "text": "number of clusters"}, {"st": 28, "ed": 31, "text": "number of clusters"}, {"st": 42, "ed": 44, "text": "functional form"}, {"st": 101, "ed": 103, "text": "ground truth"}, {"st": 118, "ed": 120, "text": "ground truth"}, {"st": 168, "ed": 170, "text": "ground truth"}]
[{"st": 10, "ed": 12, "text": "machine learning"}, {"st": 20, "ed": 22, "text": "input output"}, {"st": 27, "ed": 29, "text": "visual analytics"}, {"st": 43, "ed": 45, "text": "without sacrificing"}]
[{"st": 3, "ed": 5, "text": "bandit problem"}, {"st": 53, "ed": 55, "text": "expected loss"}, {"st": 61, "ed": 63, "text": "expected loss"}, {"st": 84, "ed": 86, "text": "group sparse"}, {"st": 86, "ed": 88, "text": "low rank"}, {"st": 151, "ed": 153, "text": "regret bounds"}, {"st": 158, "ed": 160, "text": "existing literature"}]
[{"st": 5, "ed": 7, "text": "machine learning"}, {"st": 12, "ed": 14, "text": "optimization framework"}, {"st": 28, "ed": 31, "text": "conjunctive normal form"}, {"st": 35, "ed": 38, "text": "disjunctive normal form"}, {"st": 44, "ed": 46, "text": "objective function"}, {"st": 50, "ed": 52, "text": "classification accuracy"}, {"st": 77, "ed": 79, "text": "linear programming"}, {"st": 81, "ed": 84, "text": "block coordinate descent"}]
[{"st": 16, "ed": 18, "text": "historical data"}, {"st": 36, "ed": 38, "text": "multi class"}, {"st": 67, "ed": 70, "text": "multi class classification"}]
[{"st": 24, "ed": 26, "text": "domain specific"}, {"st": 126, "ed": 128, "text": "prior distribution"}]
[{"st": 4, "ed": 6, "text": "variational autoencoders"}, {"st": 17, "ed": 19, "text": "unsupervised learning"}, {"st": 35, "ed": 37, "text": "neural networks"}, {"st": 58, "ed": 60, "text": "handwritten digits"}, {"st": 95, "ed": 97, "text": "prior knowledge"}, {"st": 98, "ed": 100, "text": "variational bayesian"}]
[{"st": 16, "ed": 18, "text": "differential privacy"}, {"st": 85, "ed": 87, "text": "existing results"}]
[{"st": 30, "ed": 32, "text": "monte carlo"}, {"st": 88, "ed": 91, "text": "nonnegative matrix factorization"}, {"st": 92, "ed": 94, "text": "empirically evaluate"}, {"st": 131, "ed": 133, "text": "sampling methods"}, {"st": 184, "ed": 186, "text": "sampling methods"}]
[{"st": 8, "ed": 10, "text": "feature selection"}, {"st": 18, "ed": 20, "text": "feature selection"}, {"st": 31, "ed": 33, "text": "feature selection"}, {"st": 41, "ed": 43, "text": "real valued"}, {"st": 83, "ed": 86, "text": "real life data"}, {"st": 94, "ed": 96, "text": "feature selection"}, {"st": 116, "ed": 118, "text": "potential applications"}]
[{"st": 5, "ed": 7, "text": "kernel based"}, {"st": 28, "ed": 30, "text": "selection algorithm"}, {"st": 35, "ed": 37, "text": "classification performance"}, {"st": 47, "ed": 49, "text": "character recognition"}]
[{"st": 13, "ed": 15, "text": "low dimensional"}, {"st": 90, "ed": 92, "text": "prediction error"}, {"st": 107, "ed": 109, "text": "distance matrix"}, {"st": 112, "ed": 114, "text": "mathbb r"}, {"st": 137, "ed": 139, "text": "special case"}, {"st": 160, "ed": 162, "text": "prediction errors"}, {"st": 169, "ed": 171, "text": "non trivial"}, {"st": 176, "ed": 178, "text": "linear map"}]
[{"st": 0, "ed": 3, "text": "deep reinforcement learning"}, {"st": 11, "ed": 13, "text": "great promise"}, {"st": 15, "ed": 17, "text": "challenging problems"}, {"st": 65, "ed": 68, "text": "markov decision process"}, {"st": 72, "ed": 75, "text": "deep q network"}, {"st": 83, "ed": 85, "text": "visualization method"}, {"st": 92, "ed": 94, "text": "directed graph"}, {"st": 99, "ed": 101, "text": "t sne"}, {"st": 128, "ed": 130, "text": "fully automatic"}, {"st": 134, "ed": 136, "text": "domain specific"}, {"st": 143, "ed": 145, "text": "likelihood based"}]
[{"st": 2, "ed": 4, "text": "recommender systems"}, {"st": 63, "ed": 66, "text": "restricted boltzmann machines"}, {"st": 84, "ed": 86, "text": "collaborative filtering"}]
[{"st": 32, "ed": 34, "text": "ballpoint pen"}, {"st": 40, "ed": 43, "text": "spatial and temporal"}, {"st": 82, "ed": 84, "text": "machine learning"}, {"st": 146, "ed": 148, "text": "machine learning"}]
[{"st": 4, "ed": 6, "text": "algorithmic composition"}, {"st": 23, "ed": 25, "text": "temporal dependencies"}, {"st": 32, "ed": 35, "text": "artificial neural networks"}, {"st": 66, "ed": 69, "text": "gated recurrent unit"}]
[{"st": 31, "ed": 33, "text": "expert knowledge"}, {"st": 53, "ed": 55, "text": "active set"}, {"st": 76, "ed": 78, "text": "linear classifier"}, {"st": 108, "ed": 110, "text": "active set"}, {"st": 113, "ed": 115, "text": "hyperspectral image"}, {"st": 115, "ed": 117, "text": "classification problems"}]
[{"st": 16, "ed": 18, "text": "image classification"}, {"st": 19, "ed": 21, "text": "sparse linear"}]
[{"st": 52, "ed": 54, "text": "convex objectives"}, {"st": 69, "ed": 71, "text": "convex objectives"}, {"st": 82, "ed": 84, "text": "globally optimal"}]
[{"st": 27, "ed": 29, "text": "machine learning"}, {"st": 34, "ed": 36, "text": "random forests"}, {"st": 87, "ed": 90, "text": "point of view"}]
[{"st": 4, "ed": 7, "text": "theoretically and empirically"}, {"st": 10, "ed": 12, "text": "optimization criteria"}, {"st": 13, "ed": 15, "text": "reinforcement learning"}, {"st": 38, "ed": 40, "text": "policy search"}, {"st": 68, "ed": 70, "text": "theoretical analysis"}, {"st": 77, "ed": 79, "text": "policy optimization"}, {"st": 95, "ed": 97, "text": "randomly generated"}, {"st": 98, "ed": 101, "text": "markov decision processes"}, {"st": 101, "ed": 103, "text": "specifically designed"}, {"st": 124, "ed": 126, "text": "policy optimization"}, {"st": 170, "ed": 172, "text": "reinforcement learning"}]
[{"st": 10, "ed": 12, "text": "weight vector"}, {"st": 23, "ed": 25, "text": "partial information"}, {"st": 30, "ed": 32, "text": "linear combinations"}, {"st": 36, "ed": 38, "text": "low rank"}, {"st": 51, "ed": 53, "text": "matrix completion"}, {"st": 62, "ed": 64, "text": "compressed sensing"}, {"st": 66, "ed": 68, "text": "big data"}, {"st": 77, "ed": 79, "text": "matrix completion"}, {"st": 91, "ed": 93, "text": "computational cost"}, {"st": 132, "ed": 134, "text": "matrix completion"}, {"st": 150, "ed": 152, "text": "approach achieves"}, {"st": 158, "ed": 160, "text": "methods outperform"}]
[{"st": 3, "ed": 5, "text": "classification algorithm"}, {"st": 7, "ed": 9, "text": "posterior distributions"}, {"st": 10, "ed": 12, "text": "positive unlabeled"}, {"st": 29, "ed": 31, "text": "recent years"}, {"st": 39, "ed": 41, "text": "positive unlabeled"}, {"st": 72, "ed": 74, "text": "classification algorithms"}, {"st": 75, "ed": 77, "text": "explicitly model"}, {"st": 109, "ed": 112, "text": "kernel density estimation"}, {"st": 137, "ed": 139, "text": "classification algorithms"}, {"st": 140, "ed": 142, "text": "positive unlabeled"}]
[{"st": 3, "ed": 6, "text": "unsupervised representation learning"}, {"st": 22, "ed": 24, "text": "existing methods"}, {"st": 99, "ed": 101, "text": "proposed approach"}, {"st": 108, "ed": 110, "text": "latent representations"}, {"st": 118, "ed": 120, "text": "existing approaches"}]
[{"st": 1, "ed": 3, "text": "input data"}, {"st": 15, "ed": 17, "text": "intrinsic dimension"}, {"st": 30, "ed": 32, "text": "low dimensional"}, {"st": 36, "ed": 38, "text": "low dimensional"}, {"st": 42, "ed": 44, "text": "distance metrics"}, {"st": 60, "ed": 62, "text": "low dimensional"}, {"st": 72, "ed": 74, "text": "distance metric"}, {"st": 81, "ed": 83, "text": "low dimensional"}, {"st": 127, "ed": 129, "text": "multiple views"}]
[{"st": 20, "ed": 23, "text": "mixture of gaussians"}, {"st": 25, "ed": 27, "text": "semi definite"}, {"st": 51, "ed": 53, "text": "pairwise distances"}, {"st": 77, "ed": 79, "text": "monte carlo"}]
[{"st": 4, "ed": 6, "text": "supervised classification"}, {"st": 11, "ed": 13, "text": "log loss"}]
[{"st": 8, "ed": 12, "text": "convolutional neural networks cnns"}, {"st": 13, "ed": 15, "text": "low dimensional"}, {"st": 31, "ed": 33, "text": "social networks"}, {"st": 51, "ed": 53, "text": "spectral graph"}, {"st": 68, "ed": 70, "text": "convolutional filters"}, {"st": 80, "ed": 82, "text": "computational complexity"}, {"st": 107, "ed": 109, "text": "deep learning"}]
[{"st": 6, "ed": 10, "text": "multi armed bandit problem"}, {"st": 62, "ed": 64, "text": "thompson sampling"}, {"st": 77, "ed": 79, "text": "exponential families"}, {"st": 89, "ed": 91, "text": "asymptotically optimal"}]
[{"st": 5, "ed": 7, "text": "ensemble learning"}, {"st": 53, "ed": 55, "text": "class label"}, {"st": 55, "ed": 57, "text": "noise levels"}, {"st": 60, "ed": 62, "text": "base learners"}, {"st": 86, "ed": 88, "text": "noise level"}, {"st": 105, "ed": 107, "text": "class label"}, {"st": 133, "ed": 135, "text": "extensive empirical"}, {"st": 138, "ed": 140, "text": "beta distribution"}]
[{"st": 45, "ed": 47, "text": "credit card"}]
[{"st": 24, "ed": 26, "text": "unlabeled instances"}, {"st": 74, "ed": 76, "text": "real world"}, {"st": 89, "ed": 91, "text": "labeled examples"}, {"st": 113, "ed": 115, "text": "sentiment analysis"}]
[{"st": 0, "ed": 2, "text": "unsupervised learning"}, {"st": 3, "ed": 5, "text": "imbalanced data"}, {"st": 10, "ed": 12, "text": "imbalanced data"}, {"st": 33, "ed": 35, "text": "latent variable"}, {"st": 40, "ed": 42, "text": "imbalanced data"}, {"st": 45, "ed": 47, "text": "latent space"}, {"st": 49, "ed": 51, "text": "shared space"}, {"st": 57, "ed": 59, "text": "gaussian process"}, {"st": 59, "ed": 61, "text": "latent variable"}, {"st": 73, "ed": 75, "text": "latent space"}, {"st": 79, "ed": 81, "text": "variational inference"}, {"st": 92, "ed": 94, "text": "medical image"}]
[{"st": 11, "ed": 13, "text": "predictive models"}, {"st": 14, "ed": 16, "text": "continuous data"}, {"st": 75, "ed": 77, "text": "significantly reduce"}, {"st": 86, "ed": 89, "text": "learning and inference"}, {"st": 93, "ed": 95, "text": "prediction tasks"}, {"st": 132, "ed": 134, "text": "boosting algorithm"}, {"st": 153, "ed": 155, "text": "relational model"}, {"st": 173, "ed": 175, "text": "proposed algorithm"}]
[{"st": 6, "ed": 8, "text": "time series"}, {"st": 15, "ed": 17, "text": "time series"}, {"st": 49, "ed": 51, "text": "time series"}, {"st": 71, "ed": 73, "text": "recent advances"}, {"st": 85, "ed": 87, "text": "existing techniques"}, {"st": 95, "ed": 97, "text": "time series"}, {"st": 151, "ed": 153, "text": "additive noise"}, {"st": 161, "ed": 163, "text": "synthetic datasets"}]
[{"st": 0, "ed": 2, "text": "bregman divergences"}, {"st": 4, "ed": 6, "text": "central role"}, {"st": 15, "ed": 17, "text": "machine learning"}, {"st": 24, "ed": 26, "text": "bregman divergences"}, {"st": 43, "ed": 45, "text": "bregman divergences"}, {"st": 70, "ed": 72, "text": "bregman divergence"}, {"st": 109, "ed": 111, "text": "bregman divergences"}, {"st": 133, "ed": 135, "text": "multi class"}, {"st": 193, "ed": 195, "text": "bregman divergence"}]
[{"st": 16, "ed": 19, "text": "deep neural network"}, {"st": 26, "ed": 28, "text": "hidden layer"}, {"st": 34, "ed": 36, "text": "input features"}, {"st": 37, "ed": 39, "text": "feature selection"}, {"st": 71, "ed": 73, "text": "group lasso"}, {"st": 77, "ed": 79, "text": "linear regression"}, {"st": 84, "ed": 86, "text": "group level"}, {"st": 146, "ed": 148, "text": "extensive experimental"}, {"st": 153, "ed": 155, "text": "weight decay"}, {"st": 166, "ed": 168, "text": "group lasso"}, {"st": 172, "ed": 174, "text": "achieve competitive"}, {"st": 200, "ed": 202, "text": "handwritten digit"}, {"st": 207, "ed": 209, "text": "large scale"}]
[{"st": 2, "ed": 4, "text": "rademacher complexity"}, {"st": 13, "ed": 15, "text": "semi supervised"}, {"st": 25, "ed": 27, "text": "labeled data"}, {"st": 38, "ed": 40, "text": "labeled training"}, {"st": 67, "ed": 69, "text": "empirical loss"}, {"st": 71, "ed": 73, "text": "labeled training"}, {"st": 98, "ed": 100, "text": "generalization error"}, {"st": 112, "ed": 114, "text": "clustering technique"}, {"st": 120, "ed": 122, "text": "rademacher complexity"}, {"st": 130, "ed": 132, "text": "theoretical result"}, {"st": 133, "ed": 135, "text": "convergence rates"}, {"st": 151, "ed": 153, "text": "classification problems"}, {"st": 154, "ed": 156, "text": "empirical evidence"}]
[{"st": 11, "ed": 13, "text": "internal representations"}, {"st": 15, "ed": 17, "text": "learning systems"}, {"st": 59, "ed": 61, "text": "classification accuracy"}, {"st": 64, "ed": 66, "text": "floating point"}, {"st": 99, "ed": 101, "text": "prior works"}, {"st": 125, "ed": 128, "text": "stochastic gradient descent"}, {"st": 131, "ed": 133, "text": "learning algorithm"}, {"st": 139, "ed": 142, "text": "support vector machine"}, {"st": 158, "ed": 160, "text": "classification accuracy"}, {"st": 193, "ed": 195, "text": "breast cancer"}, {"st": 208, "ed": 210, "text": "fixed point"}]
[{"st": 2, "ed": 4, "text": "labeled data"}, {"st": 15, "ed": 17, "text": "precision recall"}, {"st": 29, "ed": 31, "text": "anomaly detection"}]
[{"st": 1, "ed": 3, "text": "machine learning"}, {"st": 19, "ed": 21, "text": "complex data"}, {"st": 25, "ed": 27, "text": "task oriented"}, {"st": 27, "ed": 29, "text": "feature learning"}, {"st": 36, "ed": 38, "text": "deep learning"}, {"st": 41, "ed": 43, "text": "feature learning"}, {"st": 56, "ed": 58, "text": "discriminative features"}, {"st": 60, "ed": 63, "text": "deep neural nets"}, {"st": 81, "ed": 83, "text": "discriminative features"}, {"st": 88, "ed": 90, "text": "pattern classification"}, {"st": 93, "ed": 97, "text": "number of training samples"}]
[{"st": 5, "ed": 7, "text": "iterative algorithm"}, {"st": 10, "ed": 12, "text": "optimization problems"}, {"st": 13, "ed": 15, "text": "machine learning"}, {"st": 47, "ed": 49, "text": "machine learning"}, {"st": 58, "ed": 61, "text": "support vector machines"}, {"st": 62, "ed": 64, "text": "semi supervised"}]
[{"st": 2, "ed": 4, "text": "linear regression"}, {"st": 6, "ed": 8, "text": "clustering problem"}, {"st": 68, "ed": 70, "text": "based approach"}, {"st": 88, "ed": 90, "text": "genetic algorithm"}, {"st": 96, "ed": 99, "text": "k means clustering"}, {"st": 130, "ed": 132, "text": "clustering problem"}, {"st": 142, "ed": 144, "text": "real world"}, {"st": 154, "ed": 156, "text": "clustering problem"}, {"st": 204, "ed": 206, "text": "real world"}]
[{"st": 86, "ed": 88, "text": "prior information"}, {"st": 120, "ed": 122, "text": "contextual bandits"}, {"st": 172, "ed": 174, "text": "real world"}, {"st": 213, "ed": 215, "text": "significantly improve"}]
[{"st": 66, "ed": 68, "text": "signal processing"}, {"st": 69, "ed": 71, "text": "data mining"}, {"st": 82, "ed": 84, "text": "starting point"}, {"st": 186, "ed": 188, "text": "alternating optimization"}, {"st": 198, "ed": 200, "text": "source separation"}, {"st": 201, "ed": 203, "text": "collaborative filtering"}, {"st": 205, "ed": 207, "text": "topic modeling"}]
[{"st": 13, "ed": 15, "text": "iterative optimization"}, {"st": 18, "ed": 20, "text": "special cases"}]
[{"st": 13, "ed": 15, "text": "spectral clustering"}, {"st": 38, "ed": 40, "text": "spectral clustering"}, {"st": 48, "ed": 50, "text": "proposed approach"}, {"st": 76, "ed": 78, "text": "extensive experimental"}, {"st": 105, "ed": 107, "text": "spectral clustering"}]
[{"st": 13, "ed": 15, "text": "machine learning"}, {"st": 21, "ed": 23, "text": "new york"}]
[{"st": 0, "ed": 2, "text": "naive bayes"}, {"st": 2, "ed": 4, "text": "nearest neighbour"}, {"st": 7, "ed": 10, "text": "simple and effective"}, {"st": 18, "ed": 21, "text": "k nearest neighbour"}, {"st": 26, "ed": 28, "text": "competitive results"}, {"st": 30, "ed": 32, "text": "computer vision"}, {"st": 137, "ed": 139, "text": "class conditional"}, {"st": 139, "ed": 141, "text": "metric learning"}, {"st": 153, "ed": 155, "text": "metric learning"}, {"st": 165, "ed": 167, "text": "proposed method"}, {"st": 182, "ed": 184, "text": "empirical evaluation"}, {"st": 187, "ed": 189, "text": "retrieval tasks"}, {"st": 192, "ed": 194, "text": "proposed method"}, {"st": 195, "ed": 197, "text": "outperforms existing"}, {"st": 198, "ed": 200, "text": "distance metrics"}]
[{"st": 0, "ed": 2, "text": "approximate inference"}, {"st": 7, "ed": 9, "text": "recently introduced"}, {"st": 16, "ed": 18, "text": "probabilistic inference"}, {"st": 29, "ed": 31, "text": "efficient algorithms"}, {"st": 32, "ed": 34, "text": "approximate inference"}, {"st": 68, "ed": 70, "text": "submodular optimization"}, {"st": 76, "ed": 78, "text": "recent advances"}, {"st": 79, "ed": 81, "text": "submodular optimization"}, {"st": 85, "ed": 87, "text": "greedy algorithm"}, {"st": 95, "ed": 97, "text": "probabilistic models"}, {"st": 112, "ed": 114, "text": "group sparse"}, {"st": 115, "ed": 117, "text": "group sparse"}, {"st": 117, "ed": 120, "text": "principal components analysis"}, {"st": 122, "ed": 125, "text": "canonical correlation analysis"}, {"st": 128, "ed": 130, "text": "empirical results"}, {"st": 131, "ed": 133, "text": "simulated data"}]
[{"st": 36, "ed": 38, "text": "graph based"}, {"st": 63, "ed": 65, "text": "recently introduced"}, {"st": 90, "ed": 92, "text": "method yields"}, {"st": 92, "ed": 94, "text": "similar accuracy"}, {"st": 99, "ed": 102, "text": "mean squared error"}]
[{"st": 2, "ed": 4, "text": "big data"}, {"st": 11, "ed": 13, "text": "image recognition"}, {"st": 32, "ed": 35, "text": "curse of dimensionality"}, {"st": 37, "ed": 39, "text": "dimensionality reduction"}, {"st": 56, "ed": 59, "text": "principal components analysis"}, {"st": 76, "ed": 78, "text": "statistical properties"}, {"st": 111, "ed": 113, "text": "anomaly detection"}, {"st": 115, "ed": 117, "text": "machine learning"}, {"st": 135, "ed": 137, "text": "dimensionality reduction"}, {"st": 151, "ed": 153, "text": "low dimensional"}, {"st": 168, "ed": 170, "text": "user defined"}, {"st": 196, "ed": 198, "text": "natural extension"}, {"st": 200, "ed": 202, "text": "low dimensional"}, {"st": 226, "ed": 231, "text": "synthetic and real world datasets"}, {"st": 237, "ed": 239, "text": "anomaly detection"}]
[{"st": 3, "ed": 5, "text": "min max"}, {"st": 7, "ed": 9, "text": "recently proposed"}, {"st": 22, "ed": 24, "text": "machine learning"}, {"st": 33, "ed": 35, "text": "large scale"}, {"st": 65, "ed": 67, "text": "machine learning"}, {"st": 85, "ed": 88, "text": "random fourier features"}, {"st": 92, "ed": 95, "text": "radial basis function"}, {"st": 110, "ed": 112, "text": "typically requires"}, {"st": 121, "ed": 123, "text": "achieve comparable"}, {"st": 166, "ed": 168, "text": "extensive experiments"}, {"st": 173, "ed": 175, "text": "large datasets"}]
[{"st": 8, "ed": 10, "text": "recent advances"}, {"st": 17, "ed": 19, "text": "smart phone"}, {"st": 44, "ed": 46, "text": "collected data"}, {"st": 48, "ed": 50, "text": "off line"}]
[{"st": 22, "ed": 24, "text": "clinical practice"}, {"st": 33, "ed": 35, "text": "features extracted"}, {"st": 84, "ed": 86, "text": "feature selection"}, {"st": 103, "ed": 105, "text": "feature vectors"}, {"st": 124, "ed": 126, "text": "patient specific"}, {"st": 133, "ed": 135, "text": "training data"}]
[{"st": 7, "ed": 10, "text": "recurrent neural networks"}, {"st": 16, "ed": 18, "text": "exploding gradient"}, {"st": 49, "ed": 51, "text": "lie algebra"}, {"st": 57, "ed": 59, "text": "lie group"}, {"st": 141, "ed": 143, "text": "recently proposed"}, {"st": 154, "ed": 156, "text": "recently proposed"}, {"st": 157, "ed": 160, "text": "recurrent neural network"}]
[{"st": 33, "ed": 35, "text": "closed form"}, {"st": 50, "ed": 52, "text": "riemannian geometry"}, {"st": 53, "ed": 55, "text": "positive definite"}, {"st": 65, "ed": 68, "text": "orders of magnitude"}, {"st": 79, "ed": 82, "text": "standard benchmark datasets"}, {"st": 83, "ed": 85, "text": "closed form"}]
[{"st": 3, "ed": 5, "text": "off policy"}, {"st": 11, "ed": 13, "text": "optimal policy"}, {"st": 15, "ed": 17, "text": "training set"}]
[{"st": 23, "ed": 25, "text": "decision making"}, {"st": 28, "ed": 32, "text": "multi armed bandit problems"}, {"st": 43, "ed": 45, "text": "exponential family"}]
[{"st": 0, "ed": 2, "text": "predictive modeling"}, {"st": 13, "ed": 15, "text": "fine grained"}, {"st": 87, "ed": 89, "text": "predictive models"}]
[{"st": 42, "ed": 44, "text": "hierarchical clustering"}]
[{"st": 89, "ed": 91, "text": "social network"}, {"st": 109, "ed": 111, "text": "moment matching"}, {"st": 125, "ed": 127, "text": "numerical experiments"}]
[{"st": 3, "ed": 5, "text": "hierarchical clustering"}, {"st": 62, "ed": 64, "text": "convex combination"}, {"st": 113, "ed": 115, "text": "convex combination"}, {"st": 131, "ed": 133, "text": "clustering methods"}, {"st": 150, "ed": 152, "text": "united states"}]
[{"st": 13, "ed": 15, "text": "training data"}, {"st": 32, "ed": 34, "text": "computational requirements"}, {"st": 63, "ed": 65, "text": "neural network"}, {"st": 104, "ed": 106, "text": "neural networks"}, {"st": 109, "ed": 111, "text": "output layer"}, {"st": 154, "ed": 156, "text": "semi supervised"}, {"st": 182, "ed": 185, "text": "support vector machines"}, {"st": 203, "ed": 205, "text": "time series"}, {"st": 217, "ed": 219, "text": "echo state"}, {"st": 245, "ed": 247, "text": "practical applications"}]
[{"st": 5, "ed": 8, "text": "deep neural networks"}, {"st": 27, "ed": 29, "text": "recently introduced"}, {"st": 29, "ed": 31, "text": "technique called"}, {"st": 31, "ed": 33, "text": "batch normalization"}, {"st": 73, "ed": 75, "text": "significantly reduces"}, {"st": 87, "ed": 89, "text": "batch normalization"}, {"st": 93, "ed": 95, "text": "mini batch"}, {"st": 114, "ed": 116, "text": "batch normalization"}, {"st": 146, "ed": 148, "text": "batch normalization"}, {"st": 171, "ed": 173, "text": "batch normalization"}, {"st": 181, "ed": 184, "text": "training and test"}, {"st": 192, "ed": 195, "text": "recurrent neural networks"}, {"st": 213, "ed": 215, "text": "hidden state"}, {"st": 233, "ed": 235, "text": "previously published"}]
[{"st": 9, "ed": 11, "text": "sparse filtering"}, {"st": 14, "ed": 16, "text": "covariate shift"}, {"st": 20, "ed": 22, "text": "theoretical analysis"}, {"st": 23, "ed": 25, "text": "sparse filtering"}, {"st": 32, "ed": 34, "text": "covariate shift"}, {"st": 38, "ed": 40, "text": "sparse filtering"}, {"st": 46, "ed": 48, "text": "conditional distribution"}, {"st": 70, "ed": 72, "text": "sparse filtering"}, {"st": 77, "ed": 79, "text": "theoretical analysis"}, {"st": 80, "ed": 82, "text": "covariate shift"}, {"st": 87, "ed": 89, "text": "sparse filtering"}, {"st": 101, "ed": 103, "text": "conditional distribution"}, {"st": 125, "ed": 127, "text": "theoretical results"}, {"st": 134, "ed": 136, "text": "sparse filtering"}, {"st": 137, "ed": 140, "text": "real world data"}, {"st": 147, "ed": 150, "text": "computationally efficient algorithm"}, {"st": 153, "ed": 155, "text": "achieve competitive"}]
[{"st": 0, "ed": 2, "text": "interactive learning"}, {"st": 8, "ed": 10, "text": "machine learning"}, {"st": 21, "ed": 23, "text": "randomly chosen"}, {"st": 38, "ed": 40, "text": "interactive learning"}, {"st": 42, "ed": 44, "text": "noisy labels"}, {"st": 73, "ed": 75, "text": "latent variable"}, {"st": 91, "ed": 93, "text": "theoretically analyze"}, {"st": 99, "ed": 101, "text": "interactive learning"}]
[{"st": 1, "ed": 4, "text": "support vector machine"}, {"st": 12, "ed": 14, "text": "simultaneously learn"}, {"st": 34, "ed": 36, "text": "real world"}, {"st": 39, "ed": 42, "text": "large scale problems"}, {"st": 49, "ed": 51, "text": "extremely high"}, {"st": 119, "ed": 121, "text": "training phase"}, {"st": 131, "ed": 133, "text": "computational cost"}, {"st": 133, "ed": 135, "text": "without sacrificing"}, {"st": 143, "ed": 145, "text": "proposed method"}, {"st": 162, "ed": 165, "text": "synthetic and real"}, {"st": 182, "ed": 185, "text": "approach significantly outperforms"}]
[{"st": 5, "ed": 7, "text": "supervised learning"}, {"st": 38, "ed": 40, "text": "higher order"}, {"st": 52, "ed": 54, "text": "efficient algorithms"}, {"st": 89, "ed": 91, "text": "link prediction"}]
[{"st": 0, "ed": 2, "text": "feature engineering"}, {"st": 39, "ed": 42, "text": "end to end"}, {"st": 42, "ed": 44, "text": "deep learning"}, {"st": 83, "ed": 85, "text": "convolutional neural"}, {"st": 135, "ed": 137, "text": "underlying structure"}]
[{"st": 5, "ed": 7, "text": "machine learning"}, {"st": 21, "ed": 23, "text": "clinical trial"}, {"st": 54, "ed": 56, "text": "predictive models"}, {"st": 57, "ed": 60, "text": "support vector machine"}, {"st": 68, "ed": 70, "text": "logistic regression"}, {"st": 80, "ed": 82, "text": "decision rules"}, {"st": 95, "ed": 97, "text": "pre processing"}, {"st": 120, "ed": 122, "text": "experiments demonstrate"}, {"st": 122, "ed": 124, "text": "significant improvement"}]
[{"st": 0, "ed": 4, "text": "convolutional neural networks cnn"}, {"st": 22, "ed": 24, "text": "theoretical understanding"}, {"st": 26, "ed": 28, "text": "forward pass"}, {"st": 46, "ed": 48, "text": "sparse coding"}, {"st": 50, "ed": 52, "text": "gained increasing"}, {"st": 85, "ed": 87, "text": "multi layer"}, {"st": 117, "ed": 119, "text": "forward pass"}, {"st": 187, "ed": 189, "text": "forward pass"}, {"st": 196, "ed": 198, "text": "residual networks"}]
[{"st": 0, "ed": 2, "text": "human trafficking"}, {"st": 41, "ed": 43, "text": "human trafficking"}, {"st": 59, "ed": 61, "text": "ground truth"}, {"st": 68, "ed": 70, "text": "human trafficking"}, {"st": 91, "ed": 94, "text": "semi supervised learning"}, {"st": 101, "ed": 104, "text": "labeled and unlabeled"}, {"st": 108, "ed": 110, "text": "unseen data"}]
[{"st": 7, "ed": 9, "text": "recently proposed"}, {"st": 17, "ed": 20, "text": "classification and regression"}, {"st": 57, "ed": 59, "text": "parameter learning"}, {"st": 61, "ed": 63, "text": "low rank"}, {"st": 65, "ed": 67, "text": "estimation problem"}]
[{"st": 4, "ed": 6, "text": "supervised learning"}, {"st": 52, "ed": 54, "text": "entire dataset"}, {"st": 70, "ed": 73, "text": "global and local"}, {"st": 91, "ed": 93, "text": "multitask learning"}, {"st": 107, "ed": 109, "text": "optimization algorithms"}, {"st": 131, "ed": 133, "text": "tuning parameters"}, {"st": 135, "ed": 137, "text": "empirical results"}, {"st": 138, "ed": 140, "text": "synthetic data"}]
[{"st": 23, "ed": 25, "text": "resource constrained"}, {"st": 25, "ed": 27, "text": "low power"}, {"st": 88, "ed": 90, "text": "jointly learning"}, {"st": 124, "ed": 126, "text": "joint learning"}, {"st": 183, "ed": 185, "text": "smart phone"}, {"st": 204, "ed": 206, "text": "activity recognition"}]
[{"st": 10, "ed": 12, "text": "l2 regularization"}, {"st": 12, "ed": 14, "text": "parameter estimation"}, {"st": 16, "ed": 18, "text": "domain adaptation"}, {"st": 31, "ed": 33, "text": "training data"}, {"st": 33, "ed": 35, "text": "source domain"}, {"st": 44, "ed": 46, "text": "cross validation"}, {"st": 73, "ed": 75, "text": "regularization parameter"}, {"st": 105, "ed": 107, "text": "empirical analysis"}]
[{"st": 17, "ed": 19, "text": "support vector"}, {"st": 37, "ed": 39, "text": "support vector"}, {"st": 112, "ed": 114, "text": "step size"}, {"st": 118, "ed": 120, "text": "training samples"}]
[{"st": 13, "ed": 16, "text": "kernel ridge regression"}, {"st": 35, "ed": 37, "text": "training data"}, {"st": 81, "ed": 83, "text": "batch processing"}, {"st": 85, "ed": 87, "text": "large scale"}, {"st": 98, "ed": 100, "text": "without sacrificing"}, {"st": 175, "ed": 177, "text": "significantly reduced"}, {"st": 194, "ed": 196, "text": "proposed method"}, {"st": 209, "ed": 211, "text": "without sacrificing"}, {"st": 218, "ed": 220, "text": "proposed method"}, {"st": 224, "ed": 226, "text": "streaming data"}]
[{"st": 5, "ed": 7, "text": "parameter estimation"}, {"st": 9, "ed": 12, "text": "probabilistic graphical models"}, {"st": 38, "ed": 40, "text": "noisy labels"}, {"st": 47, "ed": 49, "text": "likelihood based"}]
[{"st": 26, "ed": 29, "text": "electronic health records"}, {"st": 33, "ed": 35, "text": "latent factor"}, {"st": 38, "ed": 43, "text": "non negative matrix factorization nmf"}, {"st": 46, "ed": 48, "text": "domain specific"}, {"st": 52, "ed": 54, "text": "latent factors"}, {"st": 79, "ed": 81, "text": "latent factors"}, {"st": 89, "ed": 91, "text": "empirical results"}, {"st": 111, "ed": 113, "text": "proposed method"}]
[{"st": 0, "ed": 2, "text": "word embeddings"}, {"st": 20, "ed": 22, "text": "exponential family"}, {"st": 32, "ed": 34, "text": "word embeddings"}, {"st": 48, "ed": 50, "text": "real valued"}, {"st": 67, "ed": 69, "text": "main idea"}, {"st": 135, "ed": 137, "text": "embedding model"}, {"st": 141, "ed": 143, "text": "exponential family"}, {"st": 144, "ed": 146, "text": "conditional distributions"}, {"st": 149, "ed": 151, "text": "latent embedding"}, {"st": 173, "ed": 175, "text": "neural activity"}, {"st": 185, "ed": 187, "text": "exponential family"}, {"st": 187, "ed": 189, "text": "embedding models"}]
[{"st": 10, "ed": 13, "text": "high computational cost"}, {"st": 14, "ed": 16, "text": "large scale"}, {"st": 38, "ed": 40, "text": "nystr om"}, {"st": 43, "ed": 45, "text": "low rank"}, {"st": 65, "ed": 67, "text": "kernel matrix"}, {"st": 72, "ed": 74, "text": "low rank"}, {"st": 79, "ed": 81, "text": "linear algebra"}, {"st": 103, "ed": 106, "text": "o n 2"}, {"st": 124, "ed": 126, "text": "training examples"}, {"st": 146, "ed": 148, "text": "empirical results"}]
[{"st": 4, "ed": 6, "text": "artificial intelligence"}, {"st": 51, "ed": 53, "text": "variational autoencoders"}, {"st": 57, "ed": 59, "text": "cost functions"}, {"st": 60, "ed": 63, "text": "generative adversarial networks"}, {"st": 71, "ed": 73, "text": "transition model"}, {"st": 99, "ed": 101, "text": "transition model"}, {"st": 105, "ed": 107, "text": "cost function"}]
[{"st": 6, "ed": 8, "text": "nonparametric regression"}, {"st": 14, "ed": 17, "text": "k nearest neighbor"}, {"st": 22, "ed": 24, "text": "gaussian process"}, {"st": 56, "ed": 59, "text": "k nearest neighbor"}, {"st": 91, "ed": 94, "text": "real world data"}]
[{"st": 3, "ed": 5, "text": "framework named"}, {"st": 10, "ed": 12, "text": "dissimilarity measure"}, {"st": 16, "ed": 19, "text": "hidden markov models"}, {"st": 21, "ed": 23, "text": "conditional distributions"}, {"st": 29, "ed": 31, "text": "marginal distribution"}, {"st": 37, "ed": 39, "text": "gaussian mixture"}, {"st": 59, "ed": 62, "text": "gaussian mixture model"}, {"st": 77, "ed": 79, "text": "optimal transport"}, {"st": 80, "ed": 83, "text": "the wasserstein metric"}, {"st": 97, "ed": 99, "text": "optimal transport"}, {"st": 106, "ed": 109, "text": "the wasserstein metric"}, {"st": 116, "ed": 118, "text": "optimization problem"}, {"st": 123, "ed": 126, "text": "the wasserstein metric"}, {"st": 132, "ed": 134, "text": "wasserstein distance"}, {"st": 144, "ed": 146, "text": "monte carlo"}, {"st": 201, "ed": 203, "text": "synthetic data"}, {"st": 204, "ed": 206, "text": "real data"}]
[{"st": 6, "ed": 9, "text": "divide and conquer"}, {"st": 11, "ed": 14, "text": "kernel ridge regression"}, {"st": 34, "ed": 36, "text": "input space"}, {"st": 73, "ed": 75, "text": "generalization bounds"}, {"st": 90, "ed": 92, "text": "approximation error"}, {"st": 95, "ed": 97, "text": "generalization error"}]
[{"st": 19, "ed": 21, "text": "posterior sampling"}, {"st": 22, "ed": 24, "text": "reinforcement learning"}, {"st": 45, "ed": 47, "text": "finite horizon"}, {"st": 49, "ed": 51, "text": "et al"}, {"st": 64, "ed": 66, "text": "et al"}, {"st": 75, "ed": 77, "text": "infinite horizon"}, {"st": 79, "ed": 81, "text": "et al"}, {"st": 165, "ed": 167, "text": "infinite horizon"}, {"st": 173, "ed": 175, "text": "additional assumptions"}, {"st": 182, "ed": 184, "text": "o sqrt"}, {"st": 197, "ed": 199, "text": "existing results"}, {"st": 203, "ed": 205, "text": "reinforcement learning"}]
[{"st": 29, "ed": 31, "text": "reinforcement learning"}, {"st": 44, "ed": 46, "text": "et al"}, {"st": 137, "ed": 139, "text": "existing results"}, {"st": 143, "ed": 145, "text": "reinforcement learning"}]
[{"st": 5, "ed": 7, "text": "supervised classification"}, {"st": 22, "ed": 24, "text": "kernel density"}, {"st": 45, "ed": 48, "text": "each data point"}, {"st": 81, "ed": 84, "text": "k nearest neighbors"}, {"st": 84, "ed": 86, "text": "k nn"}, {"st": 148, "ed": 150, "text": "simulated data"}, {"st": 160, "ed": 163, "text": "compares favorably to"}, {"st": 164, "ed": 166, "text": "classification methods"}, {"st": 171, "ed": 173, "text": "kernel density"}, {"st": 174, "ed": 176, "text": "k nn"}]
[{"st": 9, "ed": 11, "text": "online learning"}, {"st": 17, "ed": 19, "text": "learning agent"}, {"st": 23, "ed": 26, "text": "row and column"}, {"st": 50, "ed": 53, "text": "row and column"}, {"st": 68, "ed": 71, "text": "computationally efficient algorithm"}, {"st": 116, "ed": 119, "text": "row and column"}, {"st": 126, "ed": 129, "text": "row and column"}, {"st": 145, "ed": 147, "text": "bandit algorithm"}, {"st": 199, "ed": 201, "text": "near optimal"}]
[{"st": 1, "ed": 3, "text": "structured prediction"}, {"st": 13, "ed": 15, "text": "marginal likelihood"}, {"st": 61, "ed": 63, "text": "sufficient statistics"}, {"st": 79, "ed": 81, "text": "statistical properties"}, {"st": 102, "ed": 104, "text": "low cost"}]
[{"st": 14, "ed": 16, "text": "main contribution"}, {"st": 28, "ed": 30, "text": "capture complex"}, {"st": 32, "ed": 34, "text": "activity patterns"}, {"st": 53, "ed": 55, "text": "matrix factorization"}, {"st": 72, "ed": 74, "text": "rnn based"}, {"st": 92, "ed": 94, "text": "strong performance"}]
[{"st": 6, "ed": 8, "text": "least squares"}, {"st": 8, "ed": 10, "text": "regularization scheme"}, {"st": 11, "ed": 14, "text": "a reproducing kernel"}, {"st": 20, "ed": 23, "text": "divide and conquer"}, {"st": 36, "ed": 38, "text": "least squares"}, {"st": 38, "ed": 40, "text": "regularization scheme"}, {"st": 69, "ed": 71, "text": "error bounds"}, {"st": 76, "ed": 78, "text": "l 2"}, {"st": 107, "ed": 109, "text": "error bounds"}, {"st": 143, "ed": 145, "text": "least squares"}, {"st": 145, "ed": 147, "text": "regularization scheme"}, {"st": 159, "ed": 161, "text": "learning rate"}]
[{"st": 15, "ed": 17, "text": "text mining"}, {"st": 45, "ed": 47, "text": "distance measure"}, {"st": 61, "ed": 63, "text": "feature representations"}, {"st": 65, "ed": 67, "text": "n grams"}, {"st": 111, "ed": 113, "text": "finite dimensional"}, {"st": 151, "ed": 153, "text": "significantly higher"}, {"st": 171, "ed": 173, "text": "real world"}]
[{"st": 1, "ed": 4, "text": "k nearest neighbor"}, {"st": 6, "ed": 8, "text": "k nnc"}, {"st": 18, "ed": 20, "text": "k nn"}, {"st": 23, "ed": 25, "text": "k nnc"}, {"st": 29, "ed": 31, "text": "k nnc"}, {"st": 40, "ed": 42, "text": "k nnc"}, {"st": 44, "ed": 46, "text": "k nn"}, {"st": 49, "ed": 51, "text": "k nnc"}, {"st": 64, "ed": 66, "text": "k nnc"}, {"st": 68, "ed": 70, "text": "k nnc"}, {"st": 79, "ed": 81, "text": "k nnc"}, {"st": 88, "ed": 90, "text": "k nn"}, {"st": 92, "ed": 94, "text": "k nn"}, {"st": 104, "ed": 106, "text": "k nn"}, {"st": 121, "ed": 123, "text": "k nn"}, {"st": 128, "ed": 130, "text": "gaussian process"}, {"st": 140, "ed": 142, "text": "k nn"}, {"st": 144, "ed": 146, "text": "k nn"}, {"st": 153, "ed": 155, "text": "class labels"}, {"st": 170, "ed": 172, "text": "k nnc"}, {"st": 173, "ed": 175, "text": "k nnc"}, {"st": 177, "ed": 179, "text": "k nnc"}, {"st": 189, "ed": 191, "text": "cross validation"}]
[{"st": 0, "ed": 3, "text": "determinantal point processes"}, {"st": 33, "ed": 36, "text": "machine learning tasks"}, {"st": 45, "ed": 47, "text": "semi definite"}, {"st": 56, "ed": 58, "text": "low rank"}, {"st": 73, "ed": 75, "text": "large scale"}, {"st": 101, "ed": 103, "text": "low rank"}, {"st": 104, "ed": 106, "text": "mixture model"}, {"st": 112, "ed": 114, "text": "latent structure"}, {"st": 126, "ed": 128, "text": "low rank"}, {"st": 144, "ed": 146, "text": "mixture model"}, {"st": 156, "ed": 158, "text": "low rank"}, {"st": 163, "ed": 166, "text": "efficient and scalable"}, {"st": 166, "ed": 170, "text": "markov chain monte carlo"}, {"st": 171, "ed": 173, "text": "learning algorithm"}, {"st": 178, "ed": 180, "text": "gibbs sampling"}, {"st": 184, "ed": 186, "text": "monte carlo"}, {"st": 193, "ed": 195, "text": "real world"}, {"st": 202, "ed": 204, "text": "low rank"}, {"st": 205, "ed": 207, "text": "mixture model"}, {"st": 210, "ed": 212, "text": "predictive performance"}, {"st": 218, "ed": 220, "text": "low rank"}]
[{"st": 3, "ed": 5, "text": "clustering schemes"}, {"st": 28, "ed": 30, "text": "result shows"}, {"st": 55, "ed": 57, "text": "metric spaces"}]
[{"st": 2, "ed": 4, "text": "convex optimization"}, {"st": 10, "ed": 12, "text": "convex function"}, {"st": 26, "ed": 28, "text": "convex set"}, {"st": 31, "ed": 33, "text": "mathbb r"}, {"st": 38, "ed": 40, "text": "unknown distribution"}, {"st": 80, "ed": 83, "text": "empirical risk minimization"}, {"st": 90, "ed": 92, "text": "frac 1"}, {"st": 118, "ed": 120, "text": "closely related"}, {"st": 122, "ed": 124, "text": "uniform convergence"}, {"st": 157, "ed": 159, "text": "sample size"}, {"st": 160, "ed": 162, "text": "scales linearly"}, {"st": 182, "ed": 184, "text": "ell 2"}, {"st": 184, "ed": 186, "text": "ell 2"}, {"st": 209, "ed": 211, "text": "ell 2"}, {"st": 211, "ed": 213, "text": "ell 2"}, {"st": 219, "ed": 221, "text": "ell 1"}, {"st": 253, "ed": 256, "text": "ell 1 regularization"}]
[{"st": 5, "ed": 7, "text": "variational inference"}, {"st": 44, "ed": 46, "text": "empirical studies"}, {"st": 50, "ed": 52, "text": "real world"}, {"st": 78, "ed": 80, "text": "theoretical result"}, {"st": 96, "ed": 98, "text": "recently proposed"}]
[{"st": 7, "ed": 9, "text": "learning framework"}, {"st": 13, "ed": 15, "text": "domain transfer"}, {"st": 26, "ed": 28, "text": "common space"}, {"st": 56, "ed": 58, "text": "common space"}, {"st": 79, "ed": 81, "text": "source domain"}, {"st": 99, "ed": 101, "text": "transfer learning"}, {"st": 107, "ed": 109, "text": "cross domain"}, {"st": 120, "ed": 122, "text": "transfer learning"}]
[{"st": 68, "ed": 72, "text": "canonical correlation analysis cca"}]
[{"st": 9, "ed": 11, "text": "machine learning"}, {"st": 42, "ed": 44, "text": "ranking based"}, {"st": 44, "ed": 46, "text": "performance metric"}, {"st": 52, "ed": 54, "text": "precision recall"}, {"st": 86, "ed": 88, "text": "existing approaches"}, {"st": 92, "ed": 94, "text": "large scale"}, {"st": 101, "ed": 103, "text": "classification accuracy"}, {"st": 124, "ed": 126, "text": "unified framework"}, {"st": 129, "ed": 131, "text": "building block"}, {"st": 134, "ed": 136, "text": "highly scalable"}, {"st": 142, "ed": 144, "text": "ranking based"}, {"st": 154, "ed": 156, "text": "real life"}, {"st": 170, "ed": 172, "text": "substantial improvement"}]
[{"st": 14, "ed": 16, "text": "outlier detection"}, {"st": 30, "ed": 33, "text": "real world data"}, {"st": 43, "ed": 46, "text": "discrete and continuous"}, {"st": 67, "ed": 69, "text": "outlier detection"}, {"st": 78, "ed": 81, "text": "restricted boltzmann machine"}, {"st": 88, "ed": 90, "text": "probabilistic method"}, {"st": 99, "ed": 101, "text": "free energy"}, {"st": 143, "ed": 145, "text": "negative log"}, {"st": 154, "ed": 156, "text": "proposed method"}, {"st": 157, "ed": 162, "text": "synthetic and real world datasets"}, {"st": 178, "ed": 180, "text": "free energy"}, {"st": 192, "ed": 194, "text": "highly competitive"}]
[{"st": 15, "ed": 17, "text": "time series"}, {"st": 50, "ed": 52, "text": "individual level"}, {"st": 88, "ed": 90, "text": "observational data"}, {"st": 128, "ed": 130, "text": "functional data"}, {"st": 132, "ed": 134, "text": "posterior inference"}, {"st": 151, "ed": 153, "text": "time series"}, {"st": 187, "ed": 189, "text": "observational data"}]
[{"st": 8, "ed": 10, "text": "active learning"}]
[{"st": 10, "ed": 12, "text": "probabilistic models"}, {"st": 13, "ed": 15, "text": "negative log"}, {"st": 40, "ed": 42, "text": "parameter estimation"}, {"st": 70, "ed": 72, "text": "submodular function"}, {"st": 94, "ed": 97, "text": "log partition function"}, {"st": 139, "ed": 141, "text": "binary image"}, {"st": 149, "ed": 151, "text": "probabilistic model"}]
[{"st": 1, "ed": 3, "text": "likelihood function"}, {"st": 5, "ed": 7, "text": "finite mixture"}, {"st": 11, "ed": 13, "text": "convex function"}, {"st": 20, "ed": 22, "text": "iterative algorithms"}, {"st": 60, "ed": 62, "text": "finite mixture"}, {"st": 143, "ed": 145, "text": "convex optimization"}, {"st": 199, "ed": 201, "text": "finite mixture"}, {"st": 219, "ed": 221, "text": "global optimum"}]
[{"st": 0, "ed": 2, "text": "multi view"}, {"st": 2, "ed": 4, "text": "spectral clustering"}, {"st": 30, "ed": 32, "text": "existing methods"}, {"st": 32, "ed": 34, "text": "low rank"}, {"st": 36, "ed": 38, "text": "based method"}, {"st": 60, "ed": 62, "text": "low dimensional"}, {"st": 64, "ed": 67, "text": "multi view data"}, {"st": 83, "ed": 85, "text": "spectral clustering"}, {"st": 88, "ed": 90, "text": "low rank"}, {"st": 145, "ed": 147, "text": "low rank"}, {"st": 157, "ed": 159, "text": "low rank"}, {"st": 201, "ed": 203, "text": "extensive experiments"}, {"st": 204, "ed": 206, "text": "real world"}, {"st": 206, "ed": 208, "text": "multi view"}]
[{"st": 38, "ed": 40, "text": "dimension reduction"}, {"st": 46, "ed": 48, "text": "recently introduced"}, {"st": 51, "ed": 53, "text": "intrinsic dimension"}, {"st": 78, "ed": 80, "text": "proposed algorithm"}, {"st": 84, "ed": 88, "text": "simulated and real world"}, {"st": 93, "ed": 95, "text": "sample size"}, {"st": 113, "ed": 115, "text": "random forests"}, {"st": 121, "ed": 123, "text": "significantly reduced"}, {"st": 136, "ed": 138, "text": "feature selection"}, {"st": 141, "ed": 143, "text": "promising performance"}]
[{"st": 4, "ed": 6, "text": "pac bayesian"}, {"st": 23, "ed": 25, "text": "posterior distribution"}, {"st": 34, "ed": 36, "text": "empirical performance"}, {"st": 38, "ed": 40, "text": "posterior distribution"}, {"st": 49, "ed": 52, "text": "kullback leibler divergence"}, {"st": 83, "ed": 86, "text": "provide sufficient conditions"}, {"st": 101, "ed": 103, "text": "alternating minimization"}, {"st": 104, "ed": 107, "text": "guaranteed to converge"}, {"st": 109, "ed": 111, "text": "global minimum"}, {"st": 128, "ed": 130, "text": "cross validation"}, {"st": 154, "ed": 156, "text": "sufficient conditions"}]
[{"st": 0, "ed": 2, "text": "recent advances"}, {"st": 3, "ed": 6, "text": "semi supervised learning"}, {"st": 7, "ed": 10, "text": "deep generative models"}, {"st": 17, "ed": 19, "text": "labeled datasets"}, {"st": 19, "ed": 21, "text": "mathbf x"}, {"st": 27, "ed": 29, "text": "mathbf x"}, {"st": 54, "ed": 56, "text": "semi supervised"}, {"st": 56, "ed": 58, "text": "generative model"}, {"st": 67, "ed": 69, "text": "empirical results"}, {"st": 70, "ed": 72, "text": "improved performance"}, {"st": 74, "ed": 76, "text": "latent variable"}, {"st": 87, "ed": 89, "text": "handwritten digit"}]
[{"st": 27, "ed": 29, "text": "d dimensional"}, {"st": 63, "ed": 65, "text": "training instances"}, {"st": 110, "ed": 112, "text": "special cases"}, {"st": 131, "ed": 133, "text": "matrix completion"}]
[{"st": 0, "ed": 2, "text": "random projection"}, {"st": 6, "ed": 8, "text": "widely applied"}, {"st": 19, "ed": 21, "text": "low dimensional"}, {"st": 42, "ed": 44, "text": "dimensionality reduction"}, {"st": 63, "ed": 65, "text": "classification accuracy"}, {"st": 76, "ed": 79, "text": "component analysis pca"}, {"st": 79, "ed": 82, "text": "linear discriminant analysis"}, {"st": 84, "ed": 86, "text": "feature selection"}, {"st": 90, "ed": 92, "text": "classification accuracy"}, {"st": 116, "ed": 118, "text": "classification accuracy"}, {"st": 149, "ed": 151, "text": "classification accuracy"}, {"st": 164, "ed": 166, "text": "classification accuracy"}]
[{"st": 32, "ed": 34, "text": "existing techniques"}, {"st": 85, "ed": 87, "text": "learning process"}, {"st": 101, "ed": 103, "text": "irrelevant features"}, {"st": 123, "ed": 125, "text": "computationally intensive"}, {"st": 136, "ed": 138, "text": "without compromising"}, {"st": 149, "ed": 151, "text": "empirical studies"}]
[{"st": 0, "ed": 2, "text": "non invasive"}, {"st": 31, "ed": 33, "text": "domain adaptation"}, {"st": 43, "ed": 45, "text": "significantly reduce"}, {"st": 49, "ed": 51, "text": "training data"}, {"st": 74, "ed": 76, "text": "prior information"}, {"st": 91, "ed": 93, "text": "domain adaptation"}]
[{"st": 8, "ed": 10, "text": "time series"}, {"st": 34, "ed": 36, "text": "special case"}, {"st": 65, "ed": 67, "text": "random process"}, {"st": 70, "ed": 72, "text": "latent state"}, {"st": 86, "ed": 89, "text": "hidden markov models"}, {"st": 153, "ed": 156, "text": "significant performance gains"}]
[{"st": 0, "ed": 3, "text": "deep neural networks"}, {"st": 36, "ed": 38, "text": "deep networks"}, {"st": 54, "ed": 56, "text": "adversarial examples"}, {"st": 67, "ed": 69, "text": "linear classifiers"}, {"st": 81, "ed": 83, "text": "adversarial examples"}, {"st": 102, "ed": 104, "text": "adversarial examples"}, {"st": 172, "ed": 174, "text": "classification performance"}, {"st": 217, "ed": 219, "text": "adversarial examples"}]
[{"st": 18, "ed": 20, "text": "random forest"}, {"st": 31, "ed": 33, "text": "machine learning"}, {"st": 50, "ed": 52, "text": "random forest"}, {"st": 62, "ed": 64, "text": "random forest"}, {"st": 70, "ed": 72, "text": "decision trees"}, {"st": 74, "ed": 76, "text": "nearest neighbors"}, {"st": 83, "ed": 85, "text": "feature space"}, {"st": 98, "ed": 100, "text": "rank aggregation"}, {"st": 109, "ed": 111, "text": "random forest"}, {"st": 118, "ed": 120, "text": "existing methods"}, {"st": 122, "ed": 124, "text": "random forest"}, {"st": 146, "ed": 149, "text": "extensive experimental results"}, {"st": 154, "ed": 156, "text": "method achieves"}, {"st": 158, "ed": 160, "text": "predictive accuracy"}]
[{"st": 14, "ed": 16, "text": "potts model"}, {"st": 34, "ed": 36, "text": "regularization parameter"}, {"st": 54, "ed": 56, "text": "fully bayesian"}, {"st": 59, "ed": 61, "text": "regularization parameter"}, {"st": 63, "ed": 65, "text": "hierarchical models"}, {"st": 80, "ed": 82, "text": "hierarchical bayesian"}, {"st": 83, "ed": 85, "text": "potts model"}, {"st": 94, "ed": 96, "text": "regularization parameter"}, {"st": 126, "ed": 128, "text": "regularization parameter"}, {"st": 139, "ed": 141, "text": "compare favorably"}, {"st": 145, "ed": 147, "text": "fully bayesian"}]
[{"st": 0, "ed": 3, "text": "recurrent neural network"}, {"st": 27, "ed": 29, "text": "discrete optimization"}, {"st": 38, "ed": 40, "text": "objective function"}, {"st": 47, "ed": 49, "text": "greedy algorithm"}, {"st": 106, "ed": 108, "text": "rnn architectures"}]
[{"st": 0, "ed": 2, "text": "feature selection"}, {"st": 11, "ed": 13, "text": "pattern recognition"}, {"st": 14, "ed": 16, "text": "machine learning"}, {"st": 17, "ed": 19, "text": "data mining"}, {"st": 21, "ed": 23, "text": "feature selection"}, {"st": 54, "ed": 56, "text": "feature selection"}, {"st": 57, "ed": 59, "text": "supervised learning"}, {"st": 72, "ed": 74, "text": "unified framework"}, {"st": 75, "ed": 77, "text": "feature selection"}, {"st": 87, "ed": 89, "text": "proposed approach"}, {"st": 91, "ed": 93, "text": "structure learning"}, {"st": 105, "ed": 107, "text": "predictive modeling"}, {"st": 113, "ed": 115, "text": "likelihood based"}, {"st": 129, "ed": 131, "text": "proposed method"}, {"st": 160, "ed": 162, "text": "empirical studies"}, {"st": 163, "ed": 165, "text": "computational complexity"}, {"st": 168, "ed": 170, "text": "proposed approach"}, {"st": 176, "ed": 178, "text": "benchmark datasets"}, {"st": 185, "ed": 187, "text": "extensive experiments"}, {"st": 189, "ed": 191, "text": "significant improvement"}, {"st": 193, "ed": 195, "text": "proposed approach"}]
[{"st": 9, "ed": 11, "text": "dimensionality reduction"}, {"st": 27, "ed": 29, "text": "low dimensional"}, {"st": 55, "ed": 57, "text": "multi label"}, {"st": 81, "ed": 83, "text": "previously proposed"}, {"st": 89, "ed": 91, "text": "convex relaxations"}, {"st": 107, "ed": 109, "text": "convex relaxation"}, {"st": 119, "ed": 121, "text": "natural extension"}, {"st": 123, "ed": 125, "text": "multi label"}, {"st": 131, "ed": 133, "text": "theoretical analysis"}, {"st": 142, "ed": 144, "text": "probabilistic model"}, {"st": 151, "ed": 153, "text": "o sqrt"}, {"st": 186, "ed": 188, "text": "iterative algorithm"}]
[{"st": 21, "ed": 23, "text": "linear map"}, {"st": 25, "ed": 27, "text": "lower dimensional"}, {"st": 33, "ed": 36, "text": "linear discriminant analysis"}, {"st": 40, "ed": 42, "text": "projection matrix"}, {"st": 98, "ed": 100, "text": "optimal transport"}, {"st": 141, "ed": 143, "text": "automatic differentiation"}, {"st": 146, "ed": 148, "text": "numerical experiments"}, {"st": 149, "ed": 151, "text": "promising results"}, {"st": 162, "ed": 164, "text": "real life"}, {"st": 170, "ed": 172, "text": "deep features"}]
[{"st": 0, "ed": 4, "text": "sum product networks spns"}, {"st": 5, "ed": 7, "text": "recently introduced"}, {"st": 9, "ed": 11, "text": "probabilistic models"}, {"st": 97, "ed": 99, "text": "probabilistic models"}, {"st": 108, "ed": 110, "text": "empirically evaluate"}, {"st": 122, "ed": 124, "text": "representation learning"}, {"st": 135, "ed": 138, "text": "deep neural networks"}, {"st": 144, "ed": 146, "text": "visualization techniques"}, {"st": 151, "ed": 153, "text": "network outputs"}, {"st": 164, "ed": 166, "text": "feature extractors"}, {"st": 174, "ed": 176, "text": "unsupervised fashion"}, {"st": 177, "ed": 179, "text": "image datasets"}, {"st": 180, "ed": 182, "text": "supervised classification"}, {"st": 211, "ed": 213, "text": "empirical comparison"}, {"st": 224, "ed": 226, "text": "feature extractors"}, {"st": 246, "ed": 248, "text": "probabilistic models"}]
[{"st": 1, "ed": 3, "text": "recent years"}, {"st": 16, "ed": 18, "text": "large scale"}, {"st": 22, "ed": 24, "text": "traditional methods"}, {"st": 30, "ed": 32, "text": "computational burden"}, {"st": 59, "ed": 61, "text": "incremental learning"}, {"st": 68, "ed": 70, "text": "random forest"}, {"st": 91, "ed": 93, "text": "random forest"}, {"st": 100, "ed": 102, "text": "comparable performance"}, {"st": 122, "ed": 124, "text": "decision trees"}]
[{"st": 35, "ed": 37, "text": "training data"}, {"st": 62, "ed": 64, "text": "training data"}, {"st": 93, "ed": 95, "text": "learning task"}, {"st": 130, "ed": 132, "text": "globally optimal"}, {"st": 152, "ed": 154, "text": "observational data"}, {"st": 167, "ed": 169, "text": "personalized medicine"}]
[{"st": 14, "ed": 16, "text": "class imbalance"}, {"st": 25, "ed": 27, "text": "theoretical understanding"}, {"st": 106, "ed": 108, "text": "class imbalance"}, {"st": 111, "ed": 113, "text": "learning task"}, {"st": 122, "ed": 124, "text": "loss function"}]
[{"st": 44, "ed": 46, "text": "sequence prediction"}, {"st": 87, "ed": 89, "text": "node classification"}, {"st": 122, "ed": 124, "text": "approximation algorithm"}, {"st": 141, "ed": 143, "text": "prediction methods"}, {"st": 170, "ed": 172, "text": "probabilistic model"}]
[{"st": 0, "ed": 2, "text": "deep learning"}, {"st": 3, "ed": 5, "text": "architectures including"}, {"st": 5, "ed": 8, "text": "depth and width"}, {"st": 73, "ed": 75, "text": "experiments demonstrate"}, {"st": 107, "ed": 109, "text": "neural network"}]
[{"st": 3, "ed": 5, "text": "active learning"}, {"st": 8, "ed": 10, "text": "expected error"}, {"st": 79, "ed": 82, "text": "exploration and exploitation"}, {"st": 87, "ed": 89, "text": "computational complexity"}, {"st": 101, "ed": 104, "text": "exploration and exploitation"}, {"st": 108, "ed": 110, "text": "real world"}, {"st": 113, "ed": 115, "text": "method outperforms"}]
[{"st": 97, "ed": 99, "text": "main result"}, {"st": 101, "ed": 104, "text": "under mild conditions"}, {"st": 147, "ed": 149, "text": "worst case"}]
[{"st": 0, "ed": 3, "text": "artificial neural networks"}, {"st": 9, "ed": 11, "text": "back propagation"}, {"st": 24, "ed": 27, "text": "layer by layer"}, {"st": 29, "ed": 31, "text": "output layer"}, {"st": 38, "ed": 40, "text": "method called"}, {"st": 52, "ed": 54, "text": "don t"}, {"st": 96, "ed": 98, "text": "hidden layers"}, {"st": 124, "ed": 126, "text": "output layer"}, {"st": 138, "ed": 140, "text": "training error"}, {"st": 142, "ed": 144, "text": "convolutional networks"}, {"st": 146, "ed": 148, "text": "deep networks"}, {"st": 159, "ed": 161, "text": "biologically plausible"}, {"st": 161, "ed": 163, "text": "machine learning"}, {"st": 185, "ed": 188, "text": "mnist and cifar"}, {"st": 196, "ed": 198, "text": "back propagation"}, {"st": 199, "ed": 201, "text": "fully connected"}, {"st": 207, "ed": 209, "text": "method achieves"}]
[{"st": 3, "ed": 5, "text": "expectation maximization"}, {"st": 17, "ed": 19, "text": "monte carlo"}, {"st": 68, "ed": 70, "text": "contrastive divergence"}, {"st": 71, "ed": 74, "text": "restricted boltzmann machine"}, {"st": 98, "ed": 100, "text": "generative models"}, {"st": 104, "ed": 106, "text": "computational complexity"}]
[{"st": 9, "ed": 11, "text": "excess risk"}, {"st": 14, "ed": 17, "text": "empirical risk minimization"}, {"st": 38, "ed": 40, "text": "loss functions"}, {"st": 42, "ed": 44, "text": "squared loss"}, {"st": 69, "ed": 71, "text": "detailed analysis"}, {"st": 79, "ed": 81, "text": "least squares"}, {"st": 91, "ed": 93, "text": "sample complexity"}]
[{"st": 8, "ed": 10, "text": "least squares"}, {"st": 10, "ed": 13, "text": "support vector machines"}, {"st": 18, "ed": 20, "text": "random matrix"}, {"st": 45, "ed": 48, "text": "gaussian mixture model"}, {"st": 50, "ed": 52, "text": "input data"}]
[{"st": 0, "ed": 2, "text": "probabilistic models"}, {"st": 4, "ed": 6, "text": "latent variables"}, {"st": 16, "ed": 19, "text": "difficult to train"}, {"st": 23, "ed": 25, "text": "discrete variables"}, {"st": 39, "ed": 41, "text": "probabilistic models"}, {"st": 43, "ed": 45, "text": "latent variables"}, {"st": 47, "ed": 49, "text": "variational autoencoder"}, {"st": 61, "ed": 63, "text": "probabilistic models"}]
[{"st": 13, "ed": 15, "text": "based clustering"}, {"st": 47, "ed": 49, "text": "based clustering"}, {"st": 67, "ed": 69, "text": "clustering methods"}, {"st": 75, "ed": 77, "text": "metric spaces"}, {"st": 98, "ed": 100, "text": "clustering schemes"}, {"st": 196, "ed": 198, "text": "clustering methods"}, {"st": 208, "ed": 210, "text": "preliminary results"}]
[{"st": 1, "ed": 4, "text": "multi label classification"}, {"st": 7, "ed": 9, "text": "multi label"}, {"st": 11, "ed": 14, "text": "hundreds of thousands"}, {"st": 26, "ed": 28, "text": "power law"}, {"st": 52, "ed": 55, "text": "multi label classification"}, {"st": 68, "ed": 70, "text": "low dimensional"}, {"st": 78, "ed": 80, "text": "power law"}, {"st": 81, "ed": 83, "text": "extremely large"}, {"st": 91, "ed": 93, "text": "low rank"}, {"st": 106, "ed": 108, "text": "large scale"}, {"st": 115, "ed": 117, "text": "linear classifiers"}, {"st": 138, "ed": 140, "text": "low rank"}, {"st": 157, "ed": 160, "text": "hundreds of thousands"}, {"st": 185, "ed": 187, "text": "conduct extensive"}, {"st": 187, "ed": 189, "text": "empirical evaluation"}, {"st": 192, "ed": 194, "text": "real world"}, {"st": 227, "ed": 229, "text": "tree based"}, {"st": 231, "ed": 233, "text": "ranking based"}]
[{"st": 3, "ed": 6, "text": "best arm identification"}, {"st": 8, "ed": 11, "text": "multi armed bandits"}, {"st": 21, "ed": 23, "text": "fixed budget"}, {"st": 26, "ed": 28, "text": "finite set"}, {"st": 79, "ed": 81, "text": "analysis reveals"}, {"st": 83, "ed": 85, "text": "performance measure"}, {"st": 115, "ed": 117, "text": "nonlinear function"}, {"st": 124, "ed": 127, "text": "provide theoretical guarantees"}, {"st": 147, "ed": 149, "text": "theoretical results"}, {"st": 184, "ed": 186, "text": "theoretical guarantees"}, {"st": 188, "ed": 190, "text": "pure exploration"}]
[{"st": 20, "ed": 22, "text": "machine learning"}, {"st": 30, "ed": 32, "text": "random forests"}, {"st": 48, "ed": 50, "text": "unsupervised fashion"}, {"st": 61, "ed": 63, "text": "significantly reduces"}]
[{"st": 6, "ed": 9, "text": "semi supervised learning"}, {"st": 10, "ed": 13, "text": "graph structured data"}, {"st": 21, "ed": 24, "text": "convolutional neural networks"}, {"st": 35, "ed": 37, "text": "convolutional architecture"}, {"st": 44, "ed": 46, "text": "spectral graph"}, {"st": 49, "ed": 51, "text": "scales linearly"}, {"st": 59, "ed": 61, "text": "hidden layer"}, {"st": 83, "ed": 85, "text": "knowledge graph"}, {"st": 90, "ed": 92, "text": "approach outperforms"}, {"st": 92, "ed": 94, "text": "related methods"}]
[{"st": 0, "ed": 3, "text": "principal component analysis"}, {"st": 33, "ed": 35, "text": "l1 norm"}]
[{"st": 5, "ed": 8, "text": "generative adversarial network"}, {"st": 78, "ed": 80, "text": "wide variety"}, {"st": 107, "ed": 109, "text": "auto encoder"}]
[{"st": 0, "ed": 3, "text": "locality sensitive hashing"}, {"st": 6, "ed": 8, "text": "powerful tool"}, {"st": 11, "ed": 14, "text": "approximate nearest neighbor"}, {"st": 29, "ed": 31, "text": "hash codes"}, {"st": 59, "ed": 61, "text": "hash codes"}, {"st": 70, "ed": 72, "text": "inner product"}, {"st": 96, "ed": 98, "text": "pre defined"}, {"st": 146, "ed": 148, "text": "multi modal"}, {"st": 159, "ed": 162, "text": "theoretically and empirically"}]
[{"st": 3, "ed": 5, "text": "online learning"}, {"st": 15, "ed": 17, "text": "machine learning"}, {"st": 22, "ed": 24, "text": "online learning"}, {"st": 29, "ed": 31, "text": "deep learning"}, {"st": 41, "ed": 43, "text": "theoretical guarantees"}, {"st": 46, "ed": 48, "text": "empirical performance"}, {"st": 104, "ed": 106, "text": "computational complexity"}, {"st": 122, "ed": 124, "text": "low dimensional"}, {"st": 147, "ed": 149, "text": "ell 2"}, {"st": 152, "ed": 154, "text": "ell 1"}, {"st": 159, "ed": 161, "text": "per iteration"}, {"st": 182, "ed": 184, "text": "preliminary results"}]
[{"st": 5, "ed": 7, "text": "large scale"}, {"st": 7, "ed": 9, "text": "streaming data"}, {"st": 45, "ed": 47, "text": "large scale"}, {"st": 67, "ed": 69, "text": "proposed approach"}, {"st": 72, "ed": 74, "text": "anomaly detection"}, {"st": 78, "ed": 80, "text": "low rank"}, {"st": 97, "ed": 99, "text": "low rank"}, {"st": 113, "ed": 115, "text": "low rank"}, {"st": 118, "ed": 121, "text": "curse of dimensionality"}, {"st": 123, "ed": 125, "text": "anomaly detection"}, {"st": 130, "ed": 132, "text": "recent advances"}, {"st": 133, "ed": 135, "text": "subspace clustering"}, {"st": 140, "ed": 142, "text": "proposed method"}, {"st": 149, "ed": 151, "text": "proposed method"}, {"st": 156, "ed": 158, "text": "missing data"}, {"st": 163, "ed": 165, "text": "online optimization"}]
[{"st": 2, "ed": 5, "text": "deep neural networks"}, {"st": 22, "ed": 24, "text": "clustering algorithms"}, {"st": 63, "ed": 68, "text": "cifar 10 and cifar 100"}, {"st": 89, "ed": 91, "text": "clustering algorithm"}, {"st": 91, "ed": 93, "text": "consistently outperforms"}]
[{"st": 8, "ed": 11, "text": "deep neural networks"}, {"st": 12, "ed": 14, "text": "recurrent networks"}, {"st": 79, "ed": 81, "text": "multi class"}, {"st": 86, "ed": 89, "text": "end to end"}, {"st": 90, "ed": 92, "text": "extensive experiments"}, {"st": 95, "ed": 99, "text": "cifar 10 cifar 100"}, {"st": 101, "ed": 103, "text": "large scale"}, {"st": 117, "ed": 119, "text": "batch normalization"}, {"st": 119, "ed": 121, "text": "word embedding"}, {"st": 142, "ed": 144, "text": "non linearity"}]
[{"st": 6, "ed": 8, "text": "model selection"}, {"st": 12, "ed": 14, "text": "conditional independence"}, {"st": 21, "ed": 23, "text": "random process"}, {"st": 23, "ed": 25, "text": "time series"}, {"st": 47, "ed": 49, "text": "selection method"}, {"st": 73, "ed": 75, "text": "sample size"}, {"st": 87, "ed": 89, "text": "sample size"}, {"st": 93, "ed": 95, "text": "selection method"}]
[{"st": 51, "ed": 53, "text": "max margin"}, {"st": 56, "ed": 58, "text": "randomly selected"}, {"st": 98, "ed": 100, "text": "output layer"}, {"st": 106, "ed": 108, "text": "output layer"}, {"st": 134, "ed": 136, "text": "benchmark datasets"}, {"st": 146, "ed": 150, "text": "multi layer perceptron mlp"}, {"st": 159, "ed": 161, "text": "parameter space"}]
[{"st": 11, "ed": 13, "text": "random projections"}, {"st": 25, "ed": 27, "text": "computational costs"}, {"st": 61, "ed": 63, "text": "clustering schemes"}, {"st": 101, "ed": 103, "text": "unlike existing"}, {"st": 114, "ed": 116, "text": "empirically validate"}, {"st": 127, "ed": 129, "text": "clustering schemes"}, {"st": 166, "ed": 169, "text": "fast and accurate"}, {"st": 173, "ed": 175, "text": "large datasets"}, {"st": 178, "ed": 180, "text": "theoretical analysis"}, {"st": 183, "ed": 185, "text": "extensive experiments"}]
[{"st": 14, "ed": 18, "text": "robust principal component analysis"}, {"st": 25, "ed": 27, "text": "low dimensional"}, {"st": 55, "ed": 57, "text": "low dimensional"}, {"st": 148, "ed": 150, "text": "matrix multiplication"}, {"st": 152, "ed": 154, "text": "significantly faster"}, {"st": 160, "ed": 162, "text": "robust pca"}, {"st": 166, "ed": 168, "text": "performance guarantees"}, {"st": 182, "ed": 184, "text": "noise free"}, {"st": 191, "ed": 193, "text": "robust pca"}]
[{"st": 18, "ed": 20, "text": "previous approaches"}, {"st": 30, "ed": 32, "text": "domain specific"}, {"st": 38, "ed": 40, "text": "deep learning"}, {"st": 58, "ed": 62, "text": "convolutional neural network cnn"}, {"st": 83, "ed": 85, "text": "multi channel"}, {"st": 129, "ed": 131, "text": "trained cnn"}, {"st": 154, "ed": 156, "text": "error rate"}]
[{"st": 1, "ed": 3, "text": "important applications"}, {"st": 6, "ed": 8, "text": "parameter estimation"}, {"st": 9, "ed": 11, "text": "signal processing"}, {"st": 18, "ed": 20, "text": "low dimensional"}, {"st": 28, "ed": 30, "text": "pairwise distances"}, {"st": 38, "ed": 40, "text": "geometric structure"}, {"st": 74, "ed": 77, "text": "scale to large"}, {"st": 80, "ed": 82, "text": "main contribution"}, {"st": 89, "ed": 92, "text": "linear dimensionality reduction"}, {"st": 94, "ed": 96, "text": "significantly outperforms"}, {"st": 120, "ed": 122, "text": "worst case"}, {"st": 122, "ed": 124, "text": "theoretical guarantees"}, {"st": 146, "ed": 148, "text": "real world"}, {"st": 162, "ed": 165, "text": "times faster than"}, {"st": 199, "ed": 201, "text": "computational efficiency"}, {"st": 209, "ed": 212, "text": "approximate nearest neighbor"}]
[{"st": 11, "ed": 13, "text": "machine learning"}, {"st": 82, "ed": 84, "text": "feature selection"}, {"st": 92, "ed": 94, "text": "gaussian distributions"}, {"st": 112, "ed": 114, "text": "proposed algorithm"}, {"st": 135, "ed": 137, "text": "type ii"}, {"st": 137, "ed": 139, "text": "maximum likelihood"}, {"st": 143, "ed": 145, "text": "synthetic data"}, {"st": 146, "ed": 149, "text": "benchmark data sets"}, {"st": 174, "ed": 176, "text": "generalization performance"}, {"st": 181, "ed": 183, "text": "generalization error"}]
[{"st": 91, "ed": 93, "text": "proposed algorithm"}]
[{"st": 0, "ed": 2, "text": "ensemble methods"}, {"st": 3, "ed": 6, "text": "classification and clustering"}, {"st": 13, "ed": 15, "text": "ensemble learning"}, {"st": 30, "ed": 32, "text": "ensemble approach"}, {"st": 36, "ed": 38, "text": "multi dimensional"}, {"st": 42, "ed": 44, "text": "improved accuracy"}, {"st": 71, "ed": 73, "text": "bias variance"}, {"st": 83, "ed": 85, "text": "binary classification"}, {"st": 92, "ed": 94, "text": "bias variance"}, {"st": 104, "ed": 106, "text": "ensemble approach"}, {"st": 126, "ed": 128, "text": "unlike existing"}, {"st": 155, "ed": 157, "text": "building blocks"}, {"st": 177, "ed": 179, "text": "data model"}, {"st": 200, "ed": 202, "text": "extensive experiments"}, {"st": 204, "ed": 206, "text": "real world"}, {"st": 211, "ed": 213, "text": "machine learning"}, {"st": 247, "ed": 249, "text": "significant improvement"}]
[{"st": 5, "ed": 7, "text": "online learning"}, {"st": 8, "ed": 10, "text": "optimal control"}, {"st": 78, "ed": 81, "text": "taking into account"}]
[{"st": 8, "ed": 11, "text": "coordinate descent algorithm"}, {"st": 24, "ed": 27, "text": "coordinate descent algorithm"}, {"st": 68, "ed": 70, "text": "sufficient conditions"}, {"st": 87, "ed": 89, "text": "theoretical results"}, {"st": 104, "ed": 106, "text": "linear regression"}]
[{"st": 6, "ed": 10, "text": "non negative matrix factorization"}, {"st": 43, "ed": 45, "text": "topic models"}, {"st": 62, "ed": 65, "text": "the paper presents"}, {"st": 67, "ed": 69, "text": "sufficient conditions"}, {"st": 71, "ed": 73, "text": "observed data"}, {"st": 91, "ed": 93, "text": "observed data"}, {"st": 97, "ed": 99, "text": "least squares"}, {"st": 106, "ed": 108, "text": "topic model"}]
[{"st": 36, "ed": 39, "text": "mean square error"}, {"st": 69, "ed": 71, "text": "sampling strategy"}, {"st": 92, "ed": 94, "text": "numerical results"}, {"st": 96, "ed": 98, "text": "theoretical findings"}, {"st": 104, "ed": 106, "text": "proposed method"}]
[{"st": 23, "ed": 25, "text": "powerful tool"}, {"st": 63, "ed": 65, "text": "approximation error"}, {"st": 119, "ed": 121, "text": "surrogate loss"}, {"st": 164, "ed": 166, "text": "surrogate loss"}, {"st": 175, "ed": 177, "text": "case studies"}, {"st": 210, "ed": 212, "text": "logistic regression"}]
[{"st": 10, "ed": 12, "text": "spectral methods"}, {"st": 14, "ed": 16, "text": "latent variable"}, {"st": 31, "ed": 33, "text": "latent variables"}, {"st": 53, "ed": 56, "text": "hidden markov model"}, {"st": 73, "ed": 75, "text": "recent advances"}, {"st": 77, "ed": 79, "text": "linear algebra"}, {"st": 80, "ed": 82, "text": "numerical analysis"}, {"st": 85, "ed": 87, "text": "computationally efficient"}, {"st": 117, "ed": 119, "text": "sample complexity"}, {"st": 151, "ed": 154, "text": "synthetic and real"}]
[{"st": 5, "ed": 7, "text": "hierarchical clustering"}, {"st": 19, "ed": 21, "text": "dissimilarity measure"}, {"st": 30, "ed": 32, "text": "dissimilarity measure"}, {"st": 49, "ed": 51, "text": "dissimilarity measure"}, {"st": 58, "ed": 60, "text": "distance based"}, {"st": 110, "ed": 112, "text": "heavy tailed"}, {"st": 130, "ed": 132, "text": "shannon entropy"}, {"st": 137, "ed": 140, "text": "kullback leibler divergence"}, {"st": 159, "ed": 161, "text": "dissimilarity measure"}, {"st": 164, "ed": 166, "text": "kullback leibler"}, {"st": 188, "ed": 190, "text": "simulated data"}]
[{"st": 0, "ed": 2, "text": "feature selection"}, {"st": 30, "ed": 32, "text": "theoretical framework"}, {"st": 44, "ed": 46, "text": "feature selection"}, {"st": 49, "ed": 51, "text": "mutual information"}, {"st": 57, "ed": 59, "text": "mutual information"}, {"st": 75, "ed": 77, "text": "theoretical framework"}, {"st": 96, "ed": 98, "text": "objective function"}, {"st": 119, "ed": 121, "text": "random variables"}]
[{"st": 21, "ed": 23, "text": "point process"}, {"st": 27, "ed": 29, "text": "application domains"}, {"st": 63, "ed": 67, "text": "markov chain monte carlo"}, {"st": 74, "ed": 76, "text": "scales linearly"}, {"st": 93, "ed": 95, "text": "inference procedure"}, {"st": 110, "ed": 112, "text": "expectation maximization"}, {"st": 124, "ed": 127, "text": "geometric brownian motion"}]
[{"st": 33, "ed": 35, "text": "data mining"}, {"st": 52, "ed": 54, "text": "predictive models"}]
[{"st": 3, "ed": 5, "text": "convex optimization"}, {"st": 6, "ed": 8, "text": "online learning"}, {"st": 45, "ed": 47, "text": "bandit feedback"}, {"st": 48, "ed": 51, "text": "bias and variance"}, {"st": 85, "ed": 87, "text": "previous works"}]
[{"st": 4, "ed": 6, "text": "real world"}, {"st": 17, "ed": 19, "text": "fixed length"}, {"st": 31, "ed": 35, "text": "multiple instance learning mil"}, {"st": 56, "ed": 58, "text": "feature vectors"}, {"st": 60, "ed": 62, "text": "fixed length"}, {"st": 67, "ed": 69, "text": "class label"}, {"st": 107, "ed": 109, "text": "neural network"}, {"st": 145, "ed": 147, "text": "back propagation"}, {"st": 163, "ed": 165, "text": "prior art"}, {"st": 172, "ed": 174, "text": "benchmark datasets"}]
[{"st": 0, "ed": 3, "text": "semi supervised clustering"}, {"st": 28, "ed": 30, "text": "existing methods"}, {"st": 45, "ed": 47, "text": "similarity metric"}, {"st": 76, "ed": 78, "text": "unsupervised clustering"}, {"st": 92, "ed": 94, "text": "outperforms existing"}, {"st": 94, "ed": 97, "text": "semi supervised clustering"}]
[{"st": 49, "ed": 51, "text": "d dimensional"}, {"st": 164, "ed": 166, "text": "maximum likelihood"}]
[{"st": 2, "ed": 5, "text": "simple and effective"}, {"st": 8, "ed": 11, "text": "deep neural networks"}, {"st": 23, "ed": 25, "text": "recent theoretical"}, {"st": 32, "ed": 35, "text": "training and inference"}, {"st": 59, "ed": 61, "text": "latent variable"}, {"st": 68, "ed": 70, "text": "parameter sharing"}, {"st": 82, "ed": 84, "text": "neural networks"}, {"st": 112, "ed": 114, "text": "training objective"}, {"st": 127, "ed": 130, "text": "simple and efficient"}, {"st": 141, "ed": 144, "text": "loss in accuracy"}, {"st": 151, "ed": 153, "text": "input distributions"}, {"st": 160, "ed": 162, "text": "image classification"}, {"st": 162, "ed": 164, "text": "benchmark datasets"}]
[{"st": 1, "ed": 3, "text": "machine learning"}, {"st": 9, "ed": 12, "text": "empirical risk minimization"}, {"st": 17, "ed": 19, "text": "optimization algorithms"}, {"st": 24, "ed": 27, "text": "stochastic gradient descent"}, {"st": 38, "ed": 40, "text": "optimization algorithms"}, {"st": 43, "ed": 45, "text": "convergence rates"}, {"st": 47, "ed": 49, "text": "training process"}, {"st": 53, "ed": 55, "text": "machine learning"}, {"st": 61, "ed": 63, "text": "generalization performance"}, {"st": 90, "ed": 92, "text": "generalization error"}, {"st": 114, "ed": 116, "text": "generalization error"}, {"st": 121, "ed": 123, "text": "convergence rate"}, {"st": 125, "ed": 127, "text": "optimization algorithm"}, {"st": 142, "ed": 144, "text": "mathcal o"}, {"st": 161, "ed": 164, "text": "number of iterations"}, {"st": 172, "ed": 174, "text": "mathcal o"}, {"st": 174, "ed": 176, "text": "left frac"}, {"st": 200, "ed": 202, "text": "generalization error"}, {"st": 211, "ed": 213, "text": "training process"}, {"st": 214, "ed": 216, "text": "generalization error"}, {"st": 221, "ed": 223, "text": "optimization algorithms"}, {"st": 232, "ed": 234, "text": "generalization ability"}, {"st": 240, "ed": 242, "text": "conducted experiments"}, {"st": 247, "ed": 249, "text": "convex problems"}]
[{"st": 2, "ed": 4, "text": "variational autoencoder"}, {"st": 17, "ed": 19, "text": "deep generative"}, {"st": 29, "ed": 31, "text": "latent image"}, {"st": 34, "ed": 39, "text": "deep convolutional neural network cnn"}, {"st": 60, "ed": 62, "text": "latent code"}, {"st": 66, "ed": 68, "text": "generative models"}, {"st": 71, "ed": 74, "text": "support vector machine"}, {"st": 76, "ed": 79, "text": "recurrent neural network"}, {"st": 102, "ed": 104, "text": "computationally efficient"}, {"st": 132, "ed": 134, "text": "semi supervised"}]
[{"st": 12, "ed": 14, "text": "statistical significance"}, {"st": 31, "ed": 33, "text": "statistical methods"}, {"st": 66, "ed": 68, "text": "null hypothesis"}, {"st": 78, "ed": 80, "text": "multi class"}, {"st": 85, "ed": 87, "text": "cross validation"}, {"st": 108, "ed": 110, "text": "deep architectures"}]
[{"st": 3, "ed": 5, "text": "learning rates"}, {"st": 45, "ed": 47, "text": "loss function"}, {"st": 65, "ed": 67, "text": "multi scale"}, {"st": 80, "ed": 82, "text": "learning rate"}, {"st": 97, "ed": 99, "text": "multi scale"}, {"st": 119, "ed": 121, "text": "learning rates"}, {"st": 128, "ed": 131, "text": "k means clustering"}, {"st": 132, "ed": 134, "text": "heavy tailed"}, {"st": 142, "ed": 144, "text": "learning rates"}, {"st": 148, "ed": 150, "text": "existing results"}]
[{"st": 37, "ed": 39, "text": "random sampling"}, {"st": 43, "ed": 46, "text": "numerical linear algebra"}, {"st": 149, "ed": 151, "text": "practical problems"}]
[{"st": 85, "ed": 87, "text": "shortest path"}, {"st": 118, "ed": 120, "text": "theoretical analysis"}, {"st": 124, "ed": 126, "text": "extensive experimental"}]
[{"st": 12, "ed": 14, "text": "classification accuracy"}, {"st": 28, "ed": 30, "text": "cost function"}, {"st": 57, "ed": 59, "text": "non negativity"}, {"st": 68, "ed": 70, "text": "cost function"}, {"st": 79, "ed": 81, "text": "convex relaxation"}, {"st": 94, "ed": 96, "text": "proposed method"}, {"st": 99, "ed": 101, "text": "similar performance"}, {"st": 108, "ed": 110, "text": "ensemble methods"}]
[{"st": 15, "ed": 17, "text": "artificial intelligence"}, {"st": 46, "ed": 48, "text": "underlying structure"}, {"st": 136, "ed": 138, "text": "high dimensional"}, {"st": 147, "ed": 149, "text": "fine tuning"}, {"st": 150, "ed": 153, "text": "deep convolutional network"}, {"st": 156, "ed": 158, "text": "pre trained"}]
[{"st": 15, "ed": 17, "text": "mixture model"}, {"st": 25, "ed": 27, "text": "side information"}, {"st": 48, "ed": 50, "text": "sample complexity"}, {"st": 65, "ed": 67, "text": "main contributions"}, {"st": 80, "ed": 82, "text": "side information"}, {"st": 109, "ed": 112, "text": "gaussian mixture models"}, {"st": 113, "ed": 115, "text": "topic models"}, {"st": 115, "ed": 117, "text": "subspace clustering"}, {"st": 134, "ed": 136, "text": "side information"}, {"st": 140, "ed": 142, "text": "parameter estimates"}, {"st": 154, "ed": 156, "text": "mixture model"}, {"st": 171, "ed": 173, "text": "side information"}, {"st": 180, "ed": 182, "text": "real data"}, {"st": 195, "ed": 197, "text": "significant improvement"}]
[{"st": 15, "ed": 17, "text": "error rate"}]
[{"st": 52, "ed": 54, "text": "cost function"}, {"st": 57, "ed": 59, "text": "sensitive information"}, {"st": 86, "ed": 88, "text": "recent advances"}, {"st": 92, "ed": 94, "text": "topic models"}, {"st": 109, "ed": 112, "text": "latent dirichlet allocation"}, {"st": 120, "ed": 122, "text": "synthetic data"}, {"st": 129, "ed": 131, "text": "similar performance"}]
[{"st": 3, "ed": 5, "text": "machine learning"}, {"st": 8, "ed": 10, "text": "fine grained"}, {"st": 15, "ed": 17, "text": "molecular dynamics"}, {"st": 21, "ed": 23, "text": "dynamical systems"}, {"st": 52, "ed": 54, "text": "em algorithm"}, {"st": 89, "ed": 91, "text": "frank wolfe"}, {"st": 103, "ed": 105, "text": "em algorithm"}, {"st": 115, "ed": 117, "text": "opioid peptide"}, {"st": 126, "ed": 128, "text": "learned models"}, {"st": 129, "ed": 131, "text": "significant improvements"}, {"st": 132, "ed": 134, "text": "temporal coherence"}, {"st": 151, "ed": 153, "text": "drug design"}]
[{"st": 0, "ed": 2, "text": "neural network"}, {"st": 61, "ed": 63, "text": "linear classifiers"}, {"st": 74, "ed": 76, "text": "hidden units"}, {"st": 90, "ed": 92, "text": "training phase"}]
[{"st": 0, "ed": 2, "text": "inverse classification"}, {"st": 60, "ed": 62, "text": "inverse classification"}, {"st": 152, "ed": 154, "text": "real valued"}, {"st": 155, "ed": 157, "text": "based methods"}, {"st": 159, "ed": 161, "text": "sensitivity analysis"}, {"st": 173, "ed": 175, "text": "real world"}]
[{"st": 2, "ed": 6, "text": "convolutional neural networks cnns"}, {"st": 19, "ed": 21, "text": "task specific"}, {"st": 52, "ed": 54, "text": "random sampling"}, {"st": 56, "ed": 59, "text": "stochastic gradient descent"}, {"st": 131, "ed": 136, "text": "american academy of sleep medicine"}, {"st": 162, "ed": 164, "text": "hand engineered"}, {"st": 171, "ed": 173, "text": "domain knowledge"}, {"st": 176, "ed": 178, "text": "automatically learn"}]
[{"st": 2, "ed": 4, "text": "esophageal cancer"}, {"st": 31, "ed": 33, "text": "medical history"}, {"st": 46, "ed": 48, "text": "user preferences"}, {"st": 64, "ed": 67, "text": "electronic health records"}, {"st": 73, "ed": 75, "text": "key idea"}, {"st": 93, "ed": 95, "text": "naive bayes"}, {"st": 97, "ed": 99, "text": "random forests"}, {"st": 100, "ed": 103, "text": "support vector machines"}, {"st": 105, "ed": 107, "text": "logistic regression"}, {"st": 111, "ed": 113, "text": "logistic regression"}, {"st": 140, "ed": 142, "text": "proposed algorithm"}, {"st": 155, "ed": 157, "text": "classification algorithm"}, {"st": 225, "ed": 227, "text": "selection algorithm"}, {"st": 229, "ed": 231, "text": "case studies"}]
[{"st": 1, "ed": 4, "text": "generative adversarial networks"}, {"st": 6, "ed": 8, "text": "unsupervised learning"}, {"st": 13, "ed": 15, "text": "reinforcement learning"}, {"st": 137, "ed": 139, "text": "deep networks"}]
[{"st": 1, "ed": 3, "text": "neural networks"}, {"st": 4, "ed": 7, "text": "multiple instance learning"}, {"st": 12, "ed": 14, "text": "artificial intelligence"}, {"st": 17, "ed": 20, "text": "deep neural networks"}, {"st": 21, "ed": 24, "text": "achieved great success"}, {"st": 25, "ed": 27, "text": "supervised learning"}, {"st": 29, "ed": 32, "text": "multiple instance learning"}, {"st": 35, "ed": 37, "text": "weakly supervised"}, {"st": 45, "ed": 47, "text": "computer vision"}, {"st": 61, "ed": 64, "text": "multiple instance learning"}, {"st": 68, "ed": 70, "text": "neural networks"}, {"st": 74, "ed": 77, "text": "multiple instance learning"}, {"st": 79, "ed": 81, "text": "multiple instance"}, {"st": 81, "ed": 83, "text": "neural networks"}, {"st": 84, "ed": 87, "text": "multiple instance learning"}, {"st": 89, "ed": 92, "text": "end to end"}, {"st": 115, "ed": 117, "text": "multiple instance"}, {"st": 130, "ed": 132, "text": "multiple instance"}, {"st": 132, "ed": 134, "text": "neural network"}, {"st": 144, "ed": 146, "text": "multiple instance"}, {"st": 146, "ed": 148, "text": "neural networks"}, {"st": 160, "ed": 162, "text": "deep learning"}, {"st": 166, "ed": 168, "text": "multiple instance"}, {"st": 185, "ed": 187, "text": "multiple instance"}, {"st": 194, "ed": 196, "text": "competitive performance"}]
[{"st": 50, "ed": 52, "text": "pre processing"}, {"st": 57, "ed": 59, "text": "machine learning"}, {"st": 86, "ed": 88, "text": "machine learning"}, {"st": 94, "ed": 96, "text": "weak learners"}, {"st": 105, "ed": 107, "text": "post processing"}]
[{"st": 6, "ed": 8, "text": "compressed sensing"}, {"st": 14, "ed": 17, "text": "high dimensional sparse"}, {"st": 41, "ed": 43, "text": "proposed method"}, {"st": 51, "ed": 53, "text": "sparse signal"}, {"st": 55, "ed": 57, "text": "automatically identify"}, {"st": 79, "ed": 81, "text": "random variables"}, {"st": 107, "ed": 109, "text": "sparse signal"}, {"st": 121, "ed": 123, "text": "variational bayesian"}, {"st": 140, "ed": 142, "text": "proposed method"}, {"st": 145, "ed": 147, "text": "performance improvement"}, {"st": 150, "ed": 152, "text": "compressed sensing"}]
[{"st": 3, "ed": 5, "text": "maximum entropy"}, {"st": 60, "ed": 63, "text": "k nearest neighbour"}, {"st": 94, "ed": 96, "text": "don t"}, {"st": 118, "ed": 120, "text": "maximum entropy"}, {"st": 187, "ed": 189, "text": "maximum entropy"}]
[{"st": 3, "ed": 6, "text": "semi supervised learning"}, {"st": 28, "ed": 31, "text": "semi supervised learning"}, {"st": 34, "ed": 36, "text": "least squares"}, {"st": 56, "ed": 59, "text": "block coordinate descent"}, {"st": 80, "ed": 82, "text": "missing data"}, {"st": 102, "ed": 104, "text": "benchmark datasets"}, {"st": 117, "ed": 119, "text": "local minima"}]
[{"st": 17, "ed": 19, "text": "empirical evidence"}, {"st": 21, "ed": 23, "text": "predictive power"}, {"st": 24, "ed": 27, "text": "support vector machines"}, {"st": 49, "ed": 52, "text": "support vector machines"}, {"st": 62, "ed": 64, "text": "trained model"}, {"st": 87, "ed": 89, "text": "special cases"}, {"st": 90, "ed": 93, "text": "sheds light on"}, {"st": 112, "ed": 115, "text": "support vector machines"}]
[{"st": 0, "ed": 2, "text": "similarity learning"}, {"st": 37, "ed": 39, "text": "distance function"}, {"st": 51, "ed": 53, "text": "feature space"}, {"st": 74, "ed": 76, "text": "rademacher complexity"}]
[{"st": 4, "ed": 6, "text": "information society"}, {"st": 13, "ed": 15, "text": "social networks"}, {"st": 27, "ed": 29, "text": "data mining"}, {"st": 30, "ed": 32, "text": "machine learning"}, {"st": 63, "ed": 65, "text": "active learning"}, {"st": 71, "ed": 73, "text": "training process"}, {"st": 123, "ed": 125, "text": "labeled data"}, {"st": 127, "ed": 129, "text": "input space"}, {"st": 161, "ed": 163, "text": "article presents"}, {"st": 175, "ed": 178, "text": "semi supervised learning"}, {"st": 180, "ed": 183, "text": "support vector machines"}, {"st": 194, "ed": 196, "text": "probabilistic models"}, {"st": 208, "ed": 210, "text": "probabilistic models"}, {"st": 231, "ed": 233, "text": "kernel function"}, {"st": 242, "ed": 245, "text": "generative and discriminative"}, {"st": 249, "ed": 252, "text": "benchmark data sets"}]
[{"st": 0, "ed": 2, "text": "domain adaptation"}, {"st": 10, "ed": 12, "text": "machine learning"}, {"st": 22, "ed": 24, "text": "source domain"}, {"st": 34, "ed": 36, "text": "theoretical results"}, {"st": 52, "ed": 55, "text": "source and target"}, {"st": 84, "ed": 87, "text": "the wasserstein metric"}, {"st": 115, "ed": 118, "text": "source and target"}, {"st": 118, "ed": 120, "text": "labeled data"}, {"st": 121, "ed": 123, "text": "multiple source"}]
[{"st": 1, "ed": 3, "text": "linear bandits"}, {"st": 11, "ed": 13, "text": "armed bandits"}, {"st": 17, "ed": 19, "text": "current approaches"}, {"st": 22, "ed": 24, "text": "existing techniques"}, {"st": 26, "ed": 28, "text": "armed bandits"}, {"st": 43, "ed": 45, "text": "worst case"}, {"st": 75, "ed": 77, "text": "thompson sampling"}, {"st": 118, "ed": 120, "text": "linear bandits"}]
[{"st": 21, "ed": 24, "text": "linear discriminant analysis"}, {"st": 68, "ed": 70, "text": "objective function"}, {"st": 92, "ed": 94, "text": "multi label"}, {"st": 116, "ed": 118, "text": "multi label"}, {"st": 133, "ed": 135, "text": "multi label"}]
[{"st": 5, "ed": 7, "text": "parameter free"}, {"st": 7, "ed": 10, "text": "online learning algorithm"}, {"st": 29, "ed": 31, "text": "regret bound"}, {"st": 48, "ed": 50, "text": "empirical results"}, {"st": 62, "ed": 65, "text": "with expert advice"}, {"st": 66, "ed": 68, "text": "metric learning"}]
[{"st": 2, "ed": 4, "text": "computationally efficient"}, {"st": 26, "ed": 28, "text": "joint distribution"}, {"st": 37, "ed": 39, "text": "finite set"}, {"st": 76, "ed": 78, "text": "sample size"}, {"st": 91, "ed": 93, "text": "null hypothesis"}, {"st": 101, "ed": 103, "text": "joint distribution"}, {"st": 125, "ed": 127, "text": "real world"}]
[{"st": 5, "ed": 7, "text": "dimensionality reduction"}, {"st": 13, "ed": 15, "text": "generative model"}, {"st": 43, "ed": 45, "text": "low dimensional"}, {"st": 67, "ed": 69, "text": "structure learning"}, {"st": 105, "ed": 107, "text": "spanning tree"}, {"st": 117, "ed": 119, "text": "feature representation"}, {"st": 122, "ed": 124, "text": "unlike traditional"}, {"st": 124, "ed": 126, "text": "clustering methods"}, {"st": 148, "ed": 150, "text": "cluster centers"}, {"st": 165, "ed": 167, "text": "extensive experiments"}, {"st": 173, "ed": 175, "text": "proposed framework"}, {"st": 180, "ed": 182, "text": "feature representations"}, {"st": 190, "ed": 192, "text": "real world"}]
[{"st": 2, "ed": 4, "text": "metric learning"}, {"st": 8, "ed": 12, "text": "dynamic time warping dtw"}, {"st": 17, "ed": 19, "text": "dissimilarity measure"}, {"st": 26, "ed": 28, "text": "motion capture"}, {"st": 30, "ed": 32, "text": "metric learning"}, {"st": 62, "ed": 64, "text": "large margin"}, {"st": 88, "ed": 90, "text": "classification accuracy"}]
[{"st": 6, "ed": 8, "text": "contextual bandits"}, {"st": 85, "ed": 87, "text": "pre defined"}, {"st": 108, "ed": 110, "text": "convex loss"}, {"st": 117, "ed": 119, "text": "meta algorithm"}, {"st": 132, "ed": 134, "text": "contextual bandit"}, {"st": 145, "ed": 147, "text": "near optimal"}]
[{"st": 3, "ed": 5, "text": "least squares"}, {"st": 25, "ed": 27, "text": "training set"}, {"st": 31, "ed": 33, "text": "error rate"}, {"st": 63, "ed": 65, "text": "semi supervised"}, {"st": 83, "ed": 85, "text": "learning curve"}, {"st": 108, "ed": 110, "text": "learning curve"}]
[{"st": 7, "ed": 9, "text": "probability distributions"}, {"st": 14, "ed": 16, "text": "importance sampling"}, {"st": 41, "ed": 45, "text": "markov chain monte carlo"}, {"st": 56, "ed": 59, "text": "markov random fields"}, {"st": 78, "ed": 80, "text": "importance sampling"}, {"st": 119, "ed": 122, "text": "theoretical and empirical"}]
[{"st": 6, "ed": 9, "text": "determinantal point processes"}, {"st": 17, "ed": 19, "text": "parameter learning"}, {"st": 34, "ed": 36, "text": "low rank"}, {"st": 64, "ed": 66, "text": "text documents"}, {"st": 76, "ed": 78, "text": "maximum likelihood"}, {"st": 100, "ed": 102, "text": "document summarization"}]
[{"st": 0, "ed": 2, "text": "meta learning"}, {"st": 11, "ed": 14, "text": "short term memory"}, {"st": 66, "ed": 68, "text": "learning algorithm"}, {"st": 71, "ed": 73, "text": "hidden layer"}, {"st": 73, "ed": 77, "text": "multi layer perceptron mlp"}, {"st": 79, "ed": 81, "text": "linearly separable"}]
[{"st": 21, "ed": 23, "text": "machine learning"}, {"st": 26, "ed": 28, "text": "transfer learning"}, {"st": 66, "ed": 69, "text": "source and target"}, {"st": 87, "ed": 89, "text": "transfer learning"}, {"st": 100, "ed": 102, "text": "mutual information"}, {"st": 110, "ed": 112, "text": "computer vision"}]
[{"st": 4, "ed": 6, "text": "unified framework"}, {"st": 12, "ed": 14, "text": "optimal transport"}, {"st": 20, "ed": 22, "text": "optimal transport"}, {"st": 43, "ed": 45, "text": "previously proposed"}, {"st": 50, "ed": 52, "text": "shannon entropy"}, {"st": 55, "ed": 58, "text": "kullback leibler divergence"}, {"st": 69, "ed": 71, "text": "optimal transport"}, {"st": 172, "ed": 174, "text": "proposed framework"}, {"st": 188, "ed": 190, "text": "machine learning"}, {"st": 205, "ed": 207, "text": "synthetic data"}, {"st": 222, "ed": 225, "text": "real world data"}, {"st": 227, "ed": 229, "text": "pattern recognition"}]
[{"st": 3, "ed": 5, "text": "sg mcmc"}, {"st": 8, "ed": 10, "text": "important role"}, {"st": 11, "ed": 13, "text": "large scale"}, {"st": 25, "ed": 27, "text": "sg mcmc"}, {"st": 29, "ed": 31, "text": "becoming increasingly"}, {"st": 34, "ed": 36, "text": "distributed systems"}, {"st": 60, "ed": 62, "text": "sg mcmc"}, {"st": 65, "ed": 67, "text": "convergence properties"}, {"st": 88, "ed": 90, "text": "sg mcmc"}, {"st": 124, "ed": 126, "text": "sg mcmc"}, {"st": 156, "ed": 158, "text": "synthetic data"}, {"st": 159, "ed": 162, "text": "deep neural networks"}, {"st": 171, "ed": 173, "text": "sg mcmc"}]
[{"st": 6, "ed": 8, "text": "anomaly detection"}, {"st": 19, "ed": 22, "text": "kullback leibler divergence"}, {"st": 36, "ed": 38, "text": "empirical analysis"}]
[{"st": 12, "ed": 14, "text": "large margin"}, {"st": 38, "ed": 40, "text": "generalization error"}, {"st": 50, "ed": 52, "text": "false alarm"}, {"st": 76, "ed": 78, "text": "maximum entropy"}, {"st": 96, "ed": 98, "text": "simulated data"}, {"st": 101, "ed": 103, "text": "multimodal data"}, {"st": 110, "ed": 112, "text": "improved performance"}, {"st": 115, "ed": 117, "text": "classification methods"}, {"st": 121, "ed": 123, "text": "classification accuracy"}, {"st": 124, "ed": 126, "text": "anomaly detection"}]
[{"st": 3, "ed": 5, "text": "kernel pca"}, {"st": 12, "ed": 14, "text": "convex optimization"}, {"st": 24, "ed": 26, "text": "principal component"}, {"st": 33, "ed": 35, "text": "principal component"}, {"st": 75, "ed": 77, "text": "convex optimization"}, {"st": 79, "ed": 81, "text": "semi supervised"}, {"st": 101, "ed": 103, "text": "least squares"}, {"st": 107, "ed": 109, "text": "regularization parameter"}, {"st": 117, "ed": 119, "text": "variational principle"}, {"st": 127, "ed": 130, "text": "semi supervised learning"}, {"st": 138, "ed": 140, "text": "numerical experiments"}, {"st": 142, "ed": 144, "text": "classification tasks"}]
[{"st": 8, "ed": 10, "text": "large datasets"}, {"st": 36, "ed": 38, "text": "constant factor"}, {"st": 60, "ed": 62, "text": "batch sizes"}, {"st": 66, "ed": 68, "text": "step size"}, {"st": 97, "ed": 99, "text": "experiments demonstrate"}]
[{"st": 15, "ed": 17, "text": "performance metrics"}, {"st": 20, "ed": 22, "text": "f measure"}, {"st": 27, "ed": 29, "text": "proposed framework"}, {"st": 36, "ed": 39, "text": "batch and online"}, {"st": 51, "ed": 53, "text": "recent results"}, {"st": 56, "ed": 58, "text": "bayes optimal"}, {"st": 70, "ed": 72, "text": "conditional probability"}, {"st": 120, "ed": 122, "text": "finite sample"}, {"st": 150, "ed": 152, "text": "conditional probability"}, {"st": 158, "ed": 160, "text": "online algorithm"}, {"st": 168, "ed": 170, "text": "conditional probability"}, {"st": 176, "ed": 178, "text": "special case"}, {"st": 180, "ed": 182, "text": "conditional probability"}, {"st": 184, "ed": 186, "text": "logistic regression"}, {"st": 190, "ed": 192, "text": "frac 1"}, {"st": 194, "ed": 196, "text": "sample complexity"}, {"st": 198, "ed": 201, "text": "batch and online"}, {"st": 202, "ed": 204, "text": "empirical evaluation"}, {"st": 218, "ed": 220, "text": "prediction performance"}]
[{"st": 3, "ed": 5, "text": "deep learning"}, {"st": 41, "ed": 43, "text": "inner product"}, {"st": 58, "ed": 60, "text": "benchmark datasets"}, {"st": 79, "ed": 81, "text": "benchmark dataset"}]
[{"st": 13, "ed": 15, "text": "neural networks"}, {"st": 21, "ed": 23, "text": "training data"}, {"st": 61, "ed": 63, "text": "social cost"}, {"st": 99, "ed": 101, "text": "recently proposed"}, {"st": 103, "ed": 106, "text": "non convex optimization"}, {"st": 139, "ed": 141, "text": "squared loss"}, {"st": 141, "ed": 143, "text": "cross entropy"}, {"st": 147, "ed": 149, "text": "ell 2"}, {"st": 150, "ed": 152, "text": "sparsity inducing"}, {"st": 193, "ed": 195, "text": "multi core"}]
[{"st": 0, "ed": 2, "text": "subspace clustering"}, {"st": 7, "ed": 9, "text": "unlabeled data"}, {"st": 12, "ed": 15, "text": "number of clusters"}, {"st": 26, "ed": 28, "text": "low dimensional"}, {"st": 32, "ed": 34, "text": "practical scenarios"}, {"st": 58, "ed": 60, "text": "theoretical properties"}, {"st": 63, "ed": 65, "text": "subspace clustering"}, {"st": 67, "ed": 70, "text": "sparse subspace clustering"}, {"st": 111, "ed": 113, "text": "dimensionality reduction"}, {"st": 116, "ed": 118, "text": "random projection"}, {"st": 143, "ed": 145, "text": "differentially private"}]
[{"st": 3, "ed": 5, "text": "anomaly detection"}, {"st": 24, "ed": 26, "text": "ensemble learning"}, {"st": 31, "ed": 34, "text": "classification and clustering"}, {"st": 48, "ed": 50, "text": "existing methods"}, {"st": 74, "ed": 76, "text": "theoretical foundation"}, {"st": 87, "ed": 89, "text": "posterior distributions"}, {"st": 98, "ed": 100, "text": "bias variance"}, {"st": 102, "ed": 104, "text": "error rates"}, {"st": 112, "ed": 114, "text": "real world"}, {"st": 122, "ed": 124, "text": "time series"}]
[{"st": 0, "ed": 2, "text": "cross validation"}, {"st": 11, "ed": 13, "text": "statistical models"}, {"st": 13, "ed": 15, "text": "learning systems"}, {"st": 23, "ed": 25, "text": "wide applicability"}, {"st": 28, "ed": 30, "text": "computational cost"}, {"st": 48, "ed": 50, "text": "linear regression"}, {"st": 90, "ed": 92, "text": "real world"}]
[{"st": 0, "ed": 2, "text": "predictive modeling"}, {"st": 83, "ed": 85, "text": "training data"}, {"st": 114, "ed": 116, "text": "predictive models"}, {"st": 128, "ed": 130, "text": "unlike previous"}, {"st": 156, "ed": 158, "text": "criminal justice"}, {"st": 171, "ed": 173, "text": "proposed method"}, {"st": 229, "ed": 231, "text": "proposed method"}]
[{"st": 4, "ed": 6, "text": "discriminative models"}, {"st": 7, "ed": 9, "text": "neural networks"}, {"st": 15, "ed": 17, "text": "recent approaches"}, {"st": 18, "ed": 20, "text": "generative models"}, {"st": 22, "ed": 24, "text": "weak supervision"}, {"st": 26, "ed": 28, "text": "user defined"}, {"st": 47, "ed": 49, "text": "ground truth"}, {"st": 82, "ed": 84, "text": "training data"}, {"st": 106, "ed": 108, "text": "discriminative model"}, {"st": 109, "ed": 111, "text": "automatically identify"}, {"st": 119, "ed": 121, "text": "generative model"}, {"st": 128, "ed": 130, "text": "ground truth"}, {"st": 133, "ed": 135, "text": "generative model"}, {"st": 143, "ed": 145, "text": "relation extraction"}, {"st": 153, "ed": 155, "text": "weak supervision"}]
[{"st": 3, "ed": 5, "text": "decision making"}, {"st": 132, "ed": 134, "text": "decision boundary"}, {"st": 154, "ed": 156, "text": "real world"}]
[{"st": 0, "ed": 3, "text": "spike and slab"}, {"st": 11, "ed": 13, "text": "signal processing"}, {"st": 33, "ed": 35, "text": "sparse recovery"}, {"st": 60, "ed": 62, "text": "mixed integer"}, {"st": 65, "ed": 67, "text": "existing solutions"}, {"st": 69, "ed": 71, "text": "optimization problem"}, {"st": 87, "ed": 89, "text": "matching pursuit"}, {"st": 137, "ed": 139, "text": "cholesky decomposition"}, {"st": 147, "ed": 149, "text": "simulated data"}, {"st": 153, "ed": 155, "text": "real world"}]
[{"st": 5, "ed": 7, "text": "structure learning"}, {"st": 62, "ed": 64, "text": "structure learning"}, {"st": 94, "ed": 96, "text": "sample sizes"}, {"st": 107, "ed": 109, "text": "sample sizes"}]
[{"st": 5, "ed": 7, "text": "transfer learning"}, {"st": 26, "ed": 28, "text": "lifelong learning"}, {"st": 61, "ed": 63, "text": "regret bound"}, {"st": 77, "ed": 79, "text": "loss function"}, {"st": 92, "ed": 94, "text": "finite set"}]
[{"st": 3, "ed": 5, "text": "transfer learning"}, {"st": 17, "ed": 19, "text": "feature mapping"}, {"st": 45, "ed": 47, "text": "feature mapping"}, {"st": 62, "ed": 64, "text": "transfer learning"}, {"st": 69, "ed": 71, "text": "sparse coding"}, {"st": 78, "ed": 80, "text": "learning algorithms"}, {"st": 82, "ed": 84, "text": "unlabeled data"}, {"st": 87, "ed": 89, "text": "empirical performance"}, {"st": 90, "ed": 92, "text": "theoretical analysis"}]
[{"st": 30, "ed": 32, "text": "k means"}, {"st": 35, "ed": 37, "text": "cluster centers"}, {"st": 51, "ed": 53, "text": "demonstrate empirically"}, {"st": 88, "ed": 90, "text": "computational complexity"}, {"st": 134, "ed": 137, "text": "orders of magnitude"}, {"st": 146, "ed": 148, "text": "clustering performance"}, {"st": 158, "ed": 160, "text": "handwritten digits"}]
[{"st": 1, "ed": 3, "text": "dimensionality reduction"}, {"st": 89, "ed": 91, "text": "multi class"}, {"st": 91, "ed": 93, "text": "supervised learning"}, {"st": 124, "ed": 126, "text": "optimization problem"}, {"st": 177, "ed": 181, "text": "reproducing kernel hilbert spaces"}, {"st": 203, "ed": 205, "text": "multi class"}, {"st": 211, "ed": 214, "text": "principal component analysis"}]
[{"st": 0, "ed": 2, "text": "positive definite"}, {"st": 2, "ed": 4, "text": "kernel functions"}, {"st": 8, "ed": 10, "text": "kernel methods"}, {"st": 28, "ed": 30, "text": "positive definite"}, {"st": 77, "ed": 79, "text": "large scale"}, {"st": 79, "ed": 81, "text": "kernel machines"}, {"st": 88, "ed": 90, "text": "feature map"}, {"st": 90, "ed": 92, "text": "random fourier"}, {"st": 95, "ed": 97, "text": "kernel function"}, {"st": 113, "ed": 115, "text": "feature map"}, {"st": 145, "ed": 147, "text": "inner product"}, {"st": 154, "ed": 156, "text": "random fourier"}]
[{"st": 5, "ed": 7, "text": "recurrent networks"}, {"st": 23, "ed": 25, "text": "step ahead"}, {"st": 28, "ed": 30, "text": "multi step"}, {"st": 40, "ed": 42, "text": "domain adaptation"}, {"st": 48, "ed": 50, "text": "recurrent network"}, {"st": 99, "ed": 101, "text": "character level"}]
[{"st": 7, "ed": 10, "text": "random fourier features"}, {"st": 11, "ed": 13, "text": "gaussian kernel"}, {"st": 24, "ed": 26, "text": "orthogonal matrix"}, {"st": 28, "ed": 30, "text": "kernel approximation"}, {"st": 36, "ed": 38, "text": "random features"}, {"st": 40, "ed": 42, "text": "provide theoretical"}, {"st": 57, "ed": 59, "text": "random features"}, {"st": 81, "ed": 83, "text": "mathcal o"}, {"st": 86, "ed": 88, "text": "mathcal o"}, {"st": 102, "ed": 104, "text": "kernel approximation"}]
[{"st": 10, "ed": 12, "text": "supervised learning"}, {"st": 30, "ed": 32, "text": "missing data"}, {"st": 38, "ed": 40, "text": "social science"}, {"st": 45, "ed": 47, "text": "machine learning"}, {"st": 47, "ed": 49, "text": "benchmark datasets"}, {"st": 70, "ed": 72, "text": "missing data"}, {"st": 80, "ed": 82, "text": "predictive accuracy"}, {"st": 86, "ed": 88, "text": "missing data"}, {"st": 96, "ed": 98, "text": "missing data"}, {"st": 101, "ed": 103, "text": "prediction accuracy"}]
[{"st": 3, "ed": 5, "text": "open source"}, {"st": 8, "ed": 11, "text": "online learning algorithms"}, {"st": 30, "ed": 33, "text": "online learning algorithms"}, {"st": 34, "ed": 36, "text": "large scale"}, {"st": 38, "ed": 41, "text": "multi class classification"}, {"st": 63, "ed": 65, "text": "command line"}, {"st": 92, "ed": 94, "text": "machine learning"}, {"st": 102, "ed": 104, "text": "online learning"}, {"st": 105, "ed": 107, "text": "experiments demonstrate"}, {"st": 110, "ed": 112, "text": "highly efficient"}, {"st": 115, "ed": 117, "text": "large scale"}, {"st": 117, "ed": 119, "text": "machine learning"}]
[{"st": 0, "ed": 2, "text": "large scale"}, {"st": 35, "ed": 37, "text": "ell 1"}, {"st": 38, "ed": 40, "text": "linear models"}, {"st": 59, "ed": 61, "text": "regularization parameter"}, {"st": 70, "ed": 72, "text": "non stationary"}, {"st": 77, "ed": 79, "text": "regularization parameter"}, {"st": 100, "ed": 102, "text": "ell 1"}, {"st": 123, "ed": 125, "text": "regularization parameter"}, {"st": 133, "ed": 135, "text": "proposed method"}, {"st": 138, "ed": 140, "text": "linear regression"}, {"st": 152, "ed": 155, "text": "simulated and real"}]
[{"st": 4, "ed": 6, "text": "observed data"}, {"st": 6, "ed": 9, "text": "principal components analysis"}, {"st": 45, "ed": 47, "text": "theoretical guarantees"}]
[{"st": 18, "ed": 20, "text": "empirical performance"}, {"st": 22, "ed": 25, "text": "stochastic gradient descent"}, {"st": 42, "ed": 44, "text": "global optimization"}, {"st": 52, "ed": 54, "text": "objective function"}, {"st": 81, "ed": 83, "text": "challenging problem"}, {"st": 88, "ed": 90, "text": "global convergence"}, {"st": 132, "ed": 134, "text": "phase transition"}, {"st": 150, "ed": 152, "text": "local search"}]
[{"st": 0, "ed": 2, "text": "low rank"}, {"st": 7, "ed": 9, "text": "wide variety"}, {"st": 10, "ed": 12, "text": "applications including"}, {"st": 12, "ed": 14, "text": "recommendation systems"}, {"st": 14, "ed": 16, "text": "topic models"}, {"st": 17, "ed": 19, "text": "source separation"}, {"st": 38, "ed": 40, "text": "temporal information"}, {"st": 49, "ed": 51, "text": "significant improvements"}, {"st": 60, "ed": 62, "text": "empirical performance"}, {"st": 73, "ed": 75, "text": "theoretical justification"}, {"st": 93, "ed": 96, "text": "problem of recovering"}, {"st": 99, "ed": 101, "text": "low rank"}, {"st": 126, "ed": 128, "text": "error bounds"}, {"st": 138, "ed": 140, "text": "matrix completion"}, {"st": 168, "ed": 172, "text": "synthetic and real world"}]
[{"st": 6, "ed": 8, "text": "reinforcement learning"}, {"st": 21, "ed": 23, "text": "decision processes"}, {"st": 47, "ed": 49, "text": "near optimal"}, {"st": 61, "ed": 63, "text": "reinforcement learning"}, {"st": 70, "ed": 72, "text": "reinforcement learning"}, {"st": 81, "ed": 83, "text": "decision processes"}, {"st": 91, "ed": 93, "text": "near optimal"}, {"st": 128, "ed": 130, "text": "efficient exploration"}, {"st": 131, "ed": 133, "text": "reinforcement learning"}]
[{"st": 2, "ed": 4, "text": "active learning"}]
[{"st": 1, "ed": 3, "text": "complex valued"}, {"st": 12, "ed": 14, "text": "real valued"}, {"st": 46, "ed": 48, "text": "complex valued"}, {"st": 71, "ed": 73, "text": "complex valued"}]
[{"st": 17, "ed": 19, "text": "machine learning"}, {"st": 36, "ed": 38, "text": "machine learning"}, {"st": 39, "ed": 41, "text": "predictive modeling"}, {"st": 42, "ed": 44, "text": "optimization methods"}, {"st": 53, "ed": 55, "text": "previous research"}, {"st": 129, "ed": 131, "text": "coordinate descent"}, {"st": 135, "ed": 137, "text": "numerical experiments"}, {"st": 140, "ed": 142, "text": "real world"}, {"st": 143, "ed": 145, "text": "simulated data"}, {"st": 155, "ed": 157, "text": "empirical evaluation"}]
[{"st": 0, "ed": 2, "text": "support vector"}, {"st": 14, "ed": 16, "text": "multivariate data"}, {"st": 18, "ed": 20, "text": "class classification"}, {"st": 27, "ed": 29, "text": "gaussian kernel"}, {"st": 65, "ed": 67, "text": "gaussian kernel"}, {"st": 110, "ed": 112, "text": "kernel bandwidth"}, {"st": 123, "ed": 126, "text": "ability to capture"}, {"st": 143, "ed": 145, "text": "kernel bandwidth"}, {"st": 161, "ed": 163, "text": "gaussian kernel"}, {"st": 188, "ed": 190, "text": "proposed method"}, {"st": 205, "ed": 207, "text": "gaussian kernel"}, {"st": 225, "ed": 227, "text": "proposed method"}, {"st": 229, "ed": 231, "text": "empirical comparison"}]
[{"st": 5, "ed": 7, "text": "large scale"}, {"st": 26, "ed": 28, "text": "differentiable function"}, {"st": 33, "ed": 35, "text": "random variable"}, {"st": 48, "ed": 50, "text": "chain rule"}, {"st": 54, "ed": 56, "text": "low variance"}, {"st": 68, "ed": 70, "text": "random variables"}, {"st": 73, "ed": 76, "text": "discrete random variables"}, {"st": 93, "ed": 95, "text": "random variables"}, {"st": 111, "ed": 113, "text": "closed form"}, {"st": 125, "ed": 127, "text": "computation graph"}, {"st": 147, "ed": 149, "text": "automatic differentiation"}, {"st": 151, "ed": 153, "text": "low variance"}, {"st": 185, "ed": 187, "text": "structured prediction"}]
[{"st": 20, "ed": 22, "text": "hierarchical clustering"}, {"st": 37, "ed": 39, "text": "method generates"}, {"st": 44, "ed": 46, "text": "sufficiently large"}, {"st": 102, "ed": 104, "text": "method called"}, {"st": 179, "ed": 181, "text": "hierarchical clustering"}]
[{"st": 15, "ed": 17, "text": "neural networks"}, {"st": 20, "ed": 22, "text": "latent variables"}, {"st": 47, "ed": 49, "text": "categorical distribution"}, {"st": 56, "ed": 58, "text": "gumbel softmax"}, {"st": 79, "ed": 81, "text": "gumbel softmax"}, {"st": 90, "ed": 92, "text": "structured output"}, {"st": 95, "ed": 97, "text": "generative modeling"}, {"st": 100, "ed": 102, "text": "latent variables"}, {"st": 107, "ed": 109, "text": "semi supervised"}]
[{"st": 6, "ed": 8, "text": "neural networks"}, {"st": 16, "ed": 18, "text": "mean field"}, {"st": 105, "ed": 107, "text": "deep networks"}, {"st": 127, "ed": 129, "text": "critical point"}, {"st": 144, "ed": 146, "text": "mean field"}, {"st": 162, "ed": 165, "text": "vanishing and exploding"}]
[{"st": 0, "ed": 2, "text": "low variance"}, {"st": 13, "ed": 15, "text": "neural networks"}, {"st": 31, "ed": 33, "text": "low variance"}, {"st": 42, "ed": 44, "text": "discrete variables"}, {"st": 116, "ed": 118, "text": "likelihood ratio"}, {"st": 122, "ed": 124, "text": "input dependent"}, {"st": 127, "ed": 129, "text": "empirical results"}]
[{"st": 4, "ed": 7, "text": "deep neural networks"}, {"st": 15, "ed": 17, "text": "machine learning"}, {"st": 31, "ed": 33, "text": "recently gained"}, {"st": 34, "ed": 36, "text": "spin glass"}, {"st": 38, "ed": 40, "text": "mean field"}, {"st": 97, "ed": 99, "text": "deep linear"}, {"st": 202, "ed": 204, "text": "large scale"}, {"st": 206, "ed": 208, "text": "empirical results"}]
[{"st": 33, "ed": 35, "text": "sample size"}, {"st": 45, "ed": 47, "text": "computationally feasible"}, {"st": 61, "ed": 63, "text": "proposed method"}, {"st": 77, "ed": 79, "text": "feature set"}]
[{"st": 19, "ed": 21, "text": "labeled dataset"}, {"st": 40, "ed": 42, "text": "unlabeled data"}, {"st": 49, "ed": 51, "text": "labeled dataset"}, {"st": 68, "ed": 70, "text": "positive class"}, {"st": 82, "ed": 84, "text": "key idea"}, {"st": 118, "ed": 121, "text": "computationally efficient algorithm"}, {"st": 129, "ed": 131, "text": "estimation error"}, {"st": 136, "ed": 138, "text": "experimentally demonstrate"}]
[{"st": 7, "ed": 9, "text": "reinforcement learning"}, {"st": 19, "ed": 21, "text": "constrained optimization"}, {"st": 34, "ed": 37, "text": "deep reinforcement learning"}, {"st": 64, "ed": 66, "text": "significant improvements"}]
[{"st": 8, "ed": 10, "text": "neural networks"}, {"st": 28, "ed": 30, "text": "neural network"}, {"st": 52, "ed": 54, "text": "method works"}, {"st": 62, "ed": 64, "text": "density function"}]
[{"st": 5, "ed": 7, "text": "optimization algorithm"}, {"st": 12, "ed": 15, "text": "deep neural networks"}, {"st": 20, "ed": 22, "text": "local geometry"}, {"st": 30, "ed": 32, "text": "generalization error"}, {"st": 46, "ed": 49, "text": "positive or negative"}, {"st": 61, "ed": 63, "text": "objective function"}, {"st": 99, "ed": 101, "text": "langevin dynamics"}, {"st": 145, "ed": 148, "text": "convolutional and recurrent"}, {"st": 153, "ed": 156, "text": "compares favorably to"}, {"st": 164, "ed": 166, "text": "generalization error"}]
[{"st": 2, "ed": 5, "text": "deep generative models"}, {"st": 8, "ed": 10, "text": "multiple modalities"}, {"st": 25, "ed": 27, "text": "multiple modalities"}, {"st": 28, "ed": 31, "text": "deep generative models"}, {"st": 33, "ed": 35, "text": "variational autoencoders"}, {"st": 40, "ed": 42, "text": "typically assume"}, {"st": 68, "ed": 70, "text": "joint representation"}, {"st": 95, "ed": 97, "text": "variational autoencoder"}, {"st": 114, "ed": 116, "text": "joint distribution"}, {"st": 160, "ed": 162, "text": "proposed method"}, {"st": 165, "ed": 167, "text": "joint representation"}, {"st": 168, "ed": 170, "text": "multiple modalities"}, {"st": 190, "ed": 192, "text": "multiple modalities"}]
[{"st": 10, "ed": 12, "text": "optimization algorithms"}, {"st": 13, "ed": 15, "text": "large scale"}, {"st": 15, "ed": 17, "text": "machine learning"}, {"st": 28, "ed": 30, "text": "objective function"}, {"st": 49, "ed": 51, "text": "group lasso"}, {"st": 51, "ed": 53, "text": "logistic regression"}, {"st": 57, "ed": 59, "text": "models including"}, {"st": 87, "ed": 90, "text": "linear convergence rate"}, {"st": 91, "ed": 93, "text": "strong convexity"}, {"st": 103, "ed": 105, "text": "strong convexity"}, {"st": 131, "ed": 133, "text": "optimal solution"}]
[{"st": 0, "ed": 2, "text": "random forests"}, {"st": 5, "ed": 7, "text": "machine learning"}, {"st": 74, "ed": 76, "text": "anomaly detection"}]
[{"st": 2, "ed": 4, "text": "supervised learning"}, {"st": 10, "ed": 12, "text": "machine learning"}, {"st": 14, "ed": 16, "text": "machine learning"}, {"st": 20, "ed": 22, "text": "real world"}, {"st": 44, "ed": 46, "text": "worst case"}, {"st": 59, "ed": 61, "text": "theoretically analyze"}, {"st": 85, "ed": 87, "text": "training distribution"}]
[{"st": 6, "ed": 10, "text": "generative adversarial networks gans"}, {"st": 74, "ed": 76, "text": "mode collapse"}]
[{"st": 34, "ed": 36, "text": "large scale"}, {"st": 38, "ed": 40, "text": "social networks"}, {"st": 70, "ed": 72, "text": "unlike previous"}, {"st": 112, "ed": 114, "text": "transition probabilities"}, {"st": 118, "ed": 120, "text": "variational inference"}, {"st": 146, "ed": 149, "text": "wireless sensor network"}, {"st": 161, "ed": 163, "text": "proposed algorithm"}]
[{"st": 2, "ed": 4, "text": "semi supervised"}, {"st": 39, "ed": 42, "text": "labeled and unlabeled"}, {"st": 47, "ed": 50, "text": "k nearest neighbor"}, {"st": 74, "ed": 76, "text": "finite sample"}, {"st": 80, "ed": 83, "text": "mean squared error"}, {"st": 115, "ed": 118, "text": "labeled and unlabeled"}]
[{"st": 7, "ed": 9, "text": "low quality"}, {"st": 13, "ed": 15, "text": "time series"}, {"st": 32, "ed": 34, "text": "time series"}, {"st": 50, "ed": 52, "text": "positive class"}, {"st": 83, "ed": 85, "text": "learning framework"}, {"st": 92, "ed": 94, "text": "base classifiers"}, {"st": 108, "ed": 110, "text": "proposed framework"}, {"st": 110, "ed": 112, "text": "significantly outperforms"}, {"st": 124, "ed": 126, "text": "noise free"}, {"st": 131, "ed": 134, "text": "multiple instance learning"}]
[{"st": 3, "ed": 5, "text": "prediction methods"}, {"st": 8, "ed": 11, "text": "multivariate time series"}, {"st": 33, "ed": 35, "text": "time series"}, {"st": 39, "ed": 42, "text": "theoretical and empirical"}, {"st": 81, "ed": 83, "text": "time series"}, {"st": 114, "ed": 116, "text": "regret bounds"}, {"st": 123, "ed": 125, "text": "theoretical guarantees"}, {"st": 149, "ed": 151, "text": "least squares"}, {"st": 171, "ed": 174, "text": "simulated and real"}]
[{"st": 11, "ed": 13, "text": "input output"}, {"st": 31, "ed": 33, "text": "input output"}, {"st": 45, "ed": 48, "text": "divide and conquer"}, {"st": 63, "ed": 65, "text": "inductive bias"}, {"st": 81, "ed": 83, "text": "scale invariant"}, {"st": 113, "ed": 115, "text": "weakly supervised"}, {"st": 120, "ed": 122, "text": "input output"}, {"st": 147, "ed": 149, "text": "computational complexity"}, {"st": 151, "ed": 153, "text": "regularization term"}, {"st": 167, "ed": 170, "text": "divide and conquer"}, {"st": 194, "ed": 196, "text": "significant improvements"}, {"st": 199, "ed": 201, "text": "generalization error"}, {"st": 202, "ed": 204, "text": "computational complexity"}]
[{"st": 0, "ed": 2, "text": "representation learning"}, {"st": 8, "ed": 10, "text": "observed data"}, {"st": 18, "ed": 20, "text": "downstream tasks"}, {"st": 61, "ed": 63, "text": "variational autoencoder"}, {"st": 66, "ed": 68, "text": "autoregressive models"}, {"st": 88, "ed": 90, "text": "latent code"}, {"st": 103, "ed": 105, "text": "latent code"}, {"st": 130, "ed": 132, "text": "autoregressive models"}, {"st": 134, "ed": 136, "text": "prior distribution"}, {"st": 148, "ed": 150, "text": "generative modeling"}]
[{"st": 0, "ed": 2, "text": "neural networks"}, {"st": 39, "ed": 42, "text": "provide theoretical guarantees"}, {"st": 62, "ed": 65, "text": "convex loss function"}, {"st": 68, "ed": 70, "text": "local minima"}, {"st": 74, "ed": 76, "text": "neural networks"}, {"st": 79, "ed": 81, "text": "stationary points"}, {"st": 83, "ed": 85, "text": "loss function"}, {"st": 113, "ed": 115, "text": "hidden layer"}, {"st": 115, "ed": 117, "text": "neural networks"}, {"st": 127, "ed": 129, "text": "neural networks"}, {"st": 181, "ed": 183, "text": "kernel methods"}, {"st": 201, "ed": 203, "text": "kernel function"}, {"st": 206, "ed": 208, "text": "activation function"}]
[{"st": 0, "ed": 2, "text": "recent advances"}, {"st": 3, "ed": 5, "text": "machine learning"}, {"st": 12, "ed": 15, "text": "deep neural networks"}, {"st": 25, "ed": 27, "text": "predictive power"}, {"st": 34, "ed": 36, "text": "small molecule"}, {"st": 68, "ed": 70, "text": "significantly lower"}, {"st": 97, "ed": 100, "text": "convolutional neural networks"}, {"st": 100, "ed": 102, "text": "significantly improves"}, {"st": 103, "ed": 106, "text": "ability to learn"}, {"st": 107, "ed": 109, "text": "distance metrics"}, {"st": 113, "ed": 115, "text": "open source"}, {"st": 126, "ed": 128, "text": "open source"}, {"st": 130, "ed": 132, "text": "deep learning"}]
[{"st": 2, "ed": 4, "text": "policy search"}, {"st": 39, "ed": 41, "text": "real world"}, {"st": 47, "ed": 49, "text": "dimensionality reduction"}, {"st": 56, "ed": 59, "text": "principal component analysis"}, {"st": 75, "ed": 77, "text": "policy search"}, {"st": 115, "ed": 118, "text": "linear dimensionality reduction"}, {"st": 123, "ed": 125, "text": "nuclear norm"}, {"st": 132, "ed": 134, "text": "proposed method"}, {"st": 136, "ed": 138, "text": "dimensionality reduction"}, {"st": 139, "ed": 142, "text": "principal component analysis"}, {"st": 149, "ed": 151, "text": "policy search"}]
[{"st": 3, "ed": 5, "text": "conditional generative"}, {"st": 12, "ed": 15, "text": "factors of variation"}, {"st": 32, "ed": 35, "text": "factors of variation"}, {"st": 97, "ed": 99, "text": "intra class"}, {"st": 106, "ed": 109, "text": "factors of variation"}, {"st": 207, "ed": 209, "text": "deep convolutional"}, {"st": 217, "ed": 220, "text": "factors of variation"}, {"st": 229, "ed": 231, "text": "embedding space"}, {"st": 243, "ed": 246, "text": "synthetic and real"}, {"st": 250, "ed": 252, "text": "proposed method"}, {"st": 257, "ed": 259, "text": "unseen classes"}, {"st": 260, "ed": 262, "text": "intra class"}]
[{"st": 6, "ed": 9, "text": "multiple kernel learning"}, {"st": 49, "ed": 51, "text": "based approach"}, {"st": 53, "ed": 55, "text": "multi task"}, {"st": 73, "ed": 75, "text": "latent feature"}, {"st": 77, "ed": 79, "text": "task specific"}, {"st": 92, "ed": 94, "text": "prior knowledge"}, {"st": 103, "ed": 105, "text": "alternating minimization"}, {"st": 121, "ed": 123, "text": "large scale"}, {"st": 133, "ed": 136, "text": "online learning algorithm"}, {"st": 140, "ed": 142, "text": "significantly reduces"}, {"st": 154, "ed": 156, "text": "joint learning"}, {"st": 160, "ed": 162, "text": "benchmark datasets"}, {"st": 173, "ed": 175, "text": "multitask learning"}]
[{"st": 34, "ed": 36, "text": "large scale"}, {"st": 146, "ed": 151, "text": "alternating direction method of multipliers"}, {"st": 174, "ed": 176, "text": "highly effective"}, {"st": 182, "ed": 184, "text": "large scale"}]
[{"st": 1, "ed": 3, "text": "deep learning"}, {"st": 16, "ed": 18, "text": "large datasets"}, {"st": 21, "ed": 23, "text": "highly complex"}, {"st": 68, "ed": 70, "text": "machine learning"}, {"st": 73, "ed": 75, "text": "machine translation"}, {"st": 75, "ed": 77, "text": "computer vision"}, {"st": 166, "ed": 168, "text": "deep learning"}]
[{"st": 2, "ed": 5, "text": "recurrent neural network"}, {"st": 45, "ed": 47, "text": "gaussian process"}, {"st": 51, "ed": 53, "text": "global optimization"}, {"st": 55, "ed": 58, "text": "hyper parameter tuning"}, {"st": 71, "ed": 74, "text": "exploration and exploitation"}]
[{"st": 0, "ed": 4, "text": "restricted boltzmann machine rbm"}, {"st": 7, "ed": 9, "text": "graphical model"}, {"st": 14, "ed": 16, "text": "building block"}, {"st": 24, "ed": 26, "text": "numerical stability"}, {"st": 45, "ed": 47, "text": "exponential family"}, {"st": 50, "ed": 53, "text": "rectified linear units"}, {"st": 62, "ed": 64, "text": "marginal distributions"}, {"st": 119, "ed": 121, "text": "linear unit"}, {"st": 159, "ed": 161, "text": "importance sampling"}, {"st": 169, "ed": 171, "text": "sampling algorithm"}, {"st": 176, "ed": 178, "text": "contrastive divergence"}]
[{"st": 3, "ed": 6, "text": "supervised machine learning"}, {"st": 23, "ed": 25, "text": "time series"}, {"st": 32, "ed": 34, "text": "low latency"}, {"st": 34, "ed": 36, "text": "anomaly detection"}, {"st": 45, "ed": 47, "text": "anomaly detection"}, {"st": 55, "ed": 57, "text": "challenging problem"}, {"st": 89, "ed": 91, "text": "conditional probability"}]
[{"st": 0, "ed": 2, "text": "semi supervised"}, {"st": 16, "ed": 18, "text": "previous works"}, {"st": 29, "ed": 31, "text": "semi supervised"}, {"st": 56, "ed": 58, "text": "semi supervised"}, {"st": 78, "ed": 80, "text": "false positives"}, {"st": 81, "ed": 83, "text": "false negatives"}, {"st": 114, "ed": 116, "text": "base learner"}, {"st": 118, "ed": 120, "text": "labeled data"}, {"st": 125, "ed": 127, "text": "labeled data"}]
[{"st": 0, "ed": 3, "text": "generative adversarial networks"}, {"st": 50, "ed": 52, "text": "gumbel softmax"}, {"st": 60, "ed": 62, "text": "multinomial distribution"}, {"st": 80, "ed": 83, "text": "recurrent neural networks"}, {"st": 84, "ed": 86, "text": "gumbel softmax"}]
[{"st": 1, "ed": 3, "text": "based approaches"}, {"st": 10, "ed": 12, "text": "image processing"}, {"st": 14, "ed": 16, "text": "compressed sensing"}, {"st": 38, "ed": 40, "text": "inverse problems"}, {"st": 43, "ed": 45, "text": "recent research"}, {"st": 52, "ed": 54, "text": "low rank"}, {"st": 86, "ed": 88, "text": "compressed sensing"}, {"st": 99, "ed": 101, "text": "low rank"}, {"st": 142, "ed": 144, "text": "low rank"}, {"st": 189, "ed": 191, "text": "compressed sensing"}, {"st": 193, "ed": 195, "text": "special cases"}, {"st": 200, "ed": 202, "text": "numerical experiments"}, {"st": 204, "ed": 206, "text": "promising performance"}, {"st": 211, "ed": 213, "text": "magnetic resonance"}, {"st": 213, "ed": 215, "text": "image reconstruction"}, {"st": 223, "ed": 225, "text": "recent methods"}, {"st": 240, "ed": 242, "text": "compressed sensing"}]
[{"st": 2, "ed": 4, "text": "fast convergence"}, {"st": 11, "ed": 13, "text": "per iteration"}, {"st": 19, "ed": 21, "text": "big data"}, {"st": 29, "ed": 31, "text": "highly accurate"}, {"st": 44, "ed": 46, "text": "high dimensional"}, {"st": 47, "ed": 49, "text": "existing algorithms"}, {"st": 63, "ed": 65, "text": "big data"}, {"st": 80, "ed": 82, "text": "convergence rate"}, {"st": 83, "ed": 85, "text": "frac 1"}, {"st": 88, "ed": 90, "text": "empirical studies"}, {"st": 91, "ed": 93, "text": "large scale"}]
[{"st": 0, "ed": 2, "text": "inverse classification"}, {"st": 38, "ed": 40, "text": "approach yields"}, {"st": 57, "ed": 59, "text": "longitudinal data"}, {"st": 109, "ed": 111, "text": "inverse classification"}, {"st": 155, "ed": 157, "text": "inverse classification"}, {"st": 164, "ed": 166, "text": "experiments demonstrate"}]
[{"st": 92, "ed": 94, "text": "nuclear norm"}, {"st": 97, "ed": 99, "text": "low rank"}, {"st": 143, "ed": 146, "text": "directed acyclic graph"}, {"st": 168, "ed": 170, "text": "computational complexity"}, {"st": 183, "ed": 185, "text": "nuclear norm"}, {"st": 195, "ed": 197, "text": "empirical results"}, {"st": 213, "ed": 215, "text": "cognitive neuroscience"}]
[{"st": 3, "ed": 5, "text": "main challenges"}, {"st": 6, "ed": 8, "text": "deep learning"}, {"st": 23, "ed": 26, "text": "unsupervised pre training"}, {"st": 87, "ed": 89, "text": "statistical model"}]
[{"st": 10, "ed": 12, "text": "supervised learning"}, {"st": 13, "ed": 15, "text": "batch normalization"}, {"st": 93, "ed": 95, "text": "batch normalization"}, {"st": 143, "ed": 145, "text": "convolutional neural"}, {"st": 147, "ed": 150, "text": "recurrent neural networks"}, {"st": 155, "ed": 157, "text": "image classification"}]
[{"st": 6, "ed": 8, "text": "prediction accuracy"}, {"st": 9, "ed": 11, "text": "decision tree"}, {"st": 42, "ed": 44, "text": "training set"}, {"st": 62, "ed": 64, "text": "prediction error"}, {"st": 191, "ed": 193, "text": "decision tree"}, {"st": 193, "ed": 195, "text": "models including"}, {"st": 197, "ed": 199, "text": "random forest"}]
[{"st": 0, "ed": 3, "text": "gaussian graphical models"}, {"st": 8, "ed": 10, "text": "statistical modeling"}, {"st": 21, "ed": 23, "text": "normal distribution"}, {"st": 73, "ed": 75, "text": "hidden variables"}, {"st": 90, "ed": 94, "text": "rectified linear unit relu"}, {"st": 126, "ed": 128, "text": "real valued"}, {"st": 143, "ed": 145, "text": "deep models"}, {"st": 149, "ed": 152, "text": "unsupervised pre training"}, {"st": 156, "ed": 159, "text": "extensive experimental results"}]
[{"st": 0, "ed": 2, "text": "predictive models"}, {"st": 43, "ed": 45, "text": "iterative procedure"}, {"st": 62, "ed": 64, "text": "iterative procedure"}, {"st": 86, "ed": 88, "text": "predictive model"}]
[{"st": 48, "ed": 50, "text": "machine learning"}, {"st": 74, "ed": 76, "text": "depth perception"}, {"st": 81, "ed": 83, "text": "classification methods"}, {"st": 108, "ed": 110, "text": "ground truth"}, {"st": 122, "ed": 124, "text": "proposed framework"}, {"st": 142, "ed": 144, "text": "machine learning"}, {"st": 172, "ed": 175, "text": "easy to implement"}]
[{"st": 19, "ed": 22, "text": "intensive care unit"}, {"st": 86, "ed": 89, "text": "electronic health records"}, {"st": 114, "ed": 116, "text": "online fashion"}, {"st": 121, "ed": 123, "text": "experiments conducted"}, {"st": 144, "ed": 146, "text": "significantly outperforms"}]
[{"st": 15, "ed": 17, "text": "large networks"}, {"st": 23, "ed": 25, "text": "complex models"}, {"st": 32, "ed": 34, "text": "prediction accuracy"}, {"st": 45, "ed": 47, "text": "network layer"}, {"st": 56, "ed": 58, "text": "convex optimization"}, {"st": 91, "ed": 93, "text": "neural networks"}, {"st": 96, "ed": 100, "text": "rectified linear unit relu"}, {"st": 125, "ed": 127, "text": "generalization performance"}, {"st": 141, "ed": 143, "text": "significantly reduces"}, {"st": 182, "ed": 184, "text": "sample complexity"}, {"st": 188, "ed": 190, "text": "sufficient conditions"}, {"st": 205, "ed": 207, "text": "random vectors"}, {"st": 239, "ed": 241, "text": "mathcal o"}]
[{"st": 30, "ed": 32, "text": "proposed framework"}, {"st": 38, "ed": 40, "text": "learning problems"}, {"st": 50, "ed": 52, "text": "learning problems"}, {"st": 62, "ed": 64, "text": "observed data"}, {"st": 66, "ed": 68, "text": "structural constraints"}, {"st": 86, "ed": 89, "text": "a posteriori map"}, {"st": 89, "ed": 91, "text": "parameter estimation"}, {"st": 93, "ed": 96, "text": "markov random field"}, {"st": 100, "ed": 102, "text": "inverse covariance"}, {"st": 111, "ed": 113, "text": "learning problems"}, {"st": 144, "ed": 147, "text": "accuracy and computational"}]
[{"st": 0, "ed": 2, "text": "previous research"}, {"st": 14, "ed": 16, "text": "significant speedup"}, {"st": 73, "ed": 75, "text": "main contributions"}, {"st": 109, "ed": 111, "text": "activation functions"}, {"st": 134, "ed": 136, "text": "activation function"}]
[{"st": 8, "ed": 10, "text": "machine learning"}, {"st": 12, "ed": 14, "text": "low precision"}, {"st": 50, "ed": 53, "text": "end to end"}, {"st": 53, "ed": 55, "text": "low precision"}, {"st": 56, "ed": 58, "text": "provable guarantees"}, {"st": 70, "ed": 72, "text": "framework called"}, {"st": 78, "ed": 80, "text": "linear models"}, {"st": 107, "ed": 109, "text": "low precision"}, {"st": 178, "ed": 180, "text": "linear models"}, {"st": 199, "ed": 201, "text": "deep networks"}, {"st": 206, "ed": 208, "text": "higher accuracy"}, {"st": 225, "ed": 227, "text": "linear models"}, {"st": 235, "ed": 237, "text": "low precision"}]
[{"st": 0, "ed": 2, "text": "variational inference"}, {"st": 19, "ed": 21, "text": "optimization problem"}, {"st": 103, "ed": 105, "text": "efficient inference"}, {"st": 118, "ed": 120, "text": "variational inference"}, {"st": 162, "ed": 164, "text": "mean field"}]
[{"st": 3, "ed": 5, "text": "decision tree"}, {"st": 54, "ed": 56, "text": "decision support"}, {"st": 58, "ed": 60, "text": "decision making"}, {"st": 76, "ed": 78, "text": "decision trees"}, {"st": 81, "ed": 83, "text": "decision tree"}, {"st": 86, "ed": 88, "text": "predictive performance"}, {"st": 98, "ed": 100, "text": "decision tree"}, {"st": 118, "ed": 120, "text": "predictive performance"}, {"st": 127, "ed": 129, "text": "decision tree"}, {"st": 133, "ed": 135, "text": "predictive performance"}, {"st": 154, "ed": 156, "text": "low complexity"}]
[{"st": 14, "ed": 17, "text": "multi armed bandit"}, {"st": 71, "ed": 73, "text": "expected reward"}, {"st": 76, "ed": 78, "text": "expected reward"}, {"st": 81, "ed": 83, "text": "previous results"}, {"st": 99, "ed": 101, "text": "thompson sampling"}]
[{"st": 45, "ed": 47, "text": "multiple modalities"}, {"st": 115, "ed": 117, "text": "big data"}, {"st": 130, "ed": 132, "text": "relevant information"}, {"st": 164, "ed": 166, "text": "unlabeled samples"}, {"st": 168, "ed": 171, "text": "support vector machines"}, {"st": 172, "ed": 177, "text": "graph based semi supervised learning"}, {"st": 238, "ed": 240, "text": "labeled samples"}]
[{"st": 8, "ed": 10, "text": "large scale"}, {"st": 109, "ed": 111, "text": "random projections"}, {"st": 132, "ed": 135, "text": "generalized linear model"}, {"st": 168, "ed": 170, "text": "100 000"}, {"st": 181, "ed": 183, "text": "training samples"}, {"st": 184, "ed": 186, "text": "predictive accuracy"}, {"st": 205, "ed": 207, "text": "predictive accuracy"}, {"st": 228, "ed": 230, "text": "case study"}]
[{"st": 1, "ed": 4, "text": "deep neural networks"}, {"st": 8, "ed": 10, "text": "application domains"}, {"st": 36, "ed": 39, "text": "recurrent neural networks"}, {"st": 45, "ed": 47, "text": "speech recognition"}, {"st": 59, "ed": 62, "text": "short term memory"}, {"st": 66, "ed": 69, "text": "hidden markov model"}, {"st": 84, "ed": 86, "text": "output layer"}, {"st": 148, "ed": 150, "text": "text data"}, {"st": 152, "ed": 154, "text": "time series"}, {"st": 163, "ed": 165, "text": "complementary information"}]
[{"st": 11, "ed": 14, "text": "gaussian process gp"}, {"st": 20, "ed": 22, "text": "big data"}, {"st": 33, "ed": 35, "text": "low rank"}, {"st": 63, "ed": 65, "text": "gp models"}, {"st": 66, "ed": 68, "text": "achieve competitive"}, {"st": 68, "ed": 70, "text": "predictive performance"}, {"st": 78, "ed": 80, "text": "variational bayesian"}, {"st": 98, "ed": 100, "text": "avoid overfitting"}, {"st": 106, "ed": 108, "text": "variational distribution"}, {"st": 130, "ed": 133, "text": "variational lower bound"}, {"st": 172, "ed": 174, "text": "stochastic optimization"}, {"st": 179, "ed": 181, "text": "per iteration"}, {"st": 199, "ed": 201, "text": "empirical evaluation"}, {"st": 202, "ed": 204, "text": "real world"}]
[{"st": 12, "ed": 14, "text": "learning algorithms"}, {"st": 15, "ed": 17, "text": "real world"}, {"st": 17, "ed": 19, "text": "decision making"}, {"st": 56, "ed": 58, "text": "linear bandits"}, {"st": 89, "ed": 91, "text": "bandit algorithm"}, {"st": 155, "ed": 157, "text": "ucb algorithm"}]
[{"st": 10, "ed": 12, "text": "neural networks"}, {"st": 23, "ed": 25, "text": "fine tuning"}, {"st": 28, "ed": 30, "text": "computationally efficient"}, {"st": 46, "ed": 48, "text": "taylor expansion"}, {"st": 54, "ed": 56, "text": "cost function"}, {"st": 64, "ed": 66, "text": "transfer learning"}, {"st": 92, "ed": 94, "text": "feature map"}, {"st": 102, "ed": 104, "text": "fine grained"}, {"st": 104, "ed": 106, "text": "classification tasks"}, {"st": 137, "ed": 139, "text": "convolutional filters"}, {"st": 156, "ed": 158, "text": "large scale"}, {"st": 158, "ed": 160, "text": "imagenet dataset"}]
[{"st": 10, "ed": 13, "text": "real valued function"}, {"st": 25, "ed": 27, "text": "unknown distribution"}, {"st": 154, "ed": 156, "text": "random variable"}, {"st": 174, "ed": 176, "text": "expected error"}, {"st": 179, "ed": 181, "text": "standard deviation"}, {"st": 194, "ed": 196, "text": "scales linearly"}, {"st": 197, "ed": 199, "text": "standard deviation"}, {"st": 230, "ed": 232, "text": "convex optimization"}]
[{"st": 9, "ed": 11, "text": "thompson sampling"}, {"st": 22, "ed": 24, "text": "regret bound"}, {"st": 31, "ed": 33, "text": "sqrt t"}, {"st": 35, "ed": 37, "text": "previous results"}, {"st": 71, "ed": 73, "text": "objective function"}, {"st": 97, "ed": 99, "text": "randomized algorithm"}, {"st": 101, "ed": 103, "text": "sampling distribution"}, {"st": 141, "ed": 143, "text": "linear optimization"}, {"st": 144, "ed": 147, "text": "generalized linear model"}]
[{"st": 26, "ed": 28, "text": "diagonal matrix"}, {"st": 43, "ed": 45, "text": "step size"}, {"st": 76, "ed": 78, "text": "computationally efficient"}, {"st": 98, "ed": 100, "text": "similar performance"}, {"st": 163, "ed": 166, "text": "convolutional neural networks"}, {"st": 169, "ed": 172, "text": "recurrent neural networks"}, {"st": 174, "ed": 176, "text": "faster convergence"}]
[{"st": 0, "ed": 2, "text": "existing methods"}, {"st": 6, "ed": 8, "text": "time series"}, {"st": 14, "ed": 16, "text": "gaussian process"}, {"st": 21, "ed": 23, "text": "gaussian process"}, {"st": 57, "ed": 59, "text": "fully bayesian"}]
[{"st": 0, "ed": 2, "text": "convolutional networks"}, {"st": 26, "ed": 28, "text": "supervised learning"}, {"st": 34, "ed": 36, "text": "previous attempts"}, {"st": 41, "ed": 43, "text": "unlabeled data"}, {"st": 68, "ed": 70, "text": "unsupervised training"}, {"st": 71, "ed": 73, "text": "convolutional networks"}, {"st": 79, "ed": 81, "text": "spatial regions"}, {"st": 90, "ed": 92, "text": "neural networks"}, {"st": 101, "ed": 103, "text": "back propagation"}]
[{"st": 0, "ed": 2, "text": "deep learning"}, {"st": 4, "ed": 6, "text": "promising performance"}, {"st": 7, "ed": 9, "text": "predictive modeling"}, {"st": 21, "ed": 23, "text": "predictive modeling"}, {"st": 24, "ed": 26, "text": "sample size"}, {"st": 29, "ed": 31, "text": "deep learning"}, {"st": 41, "ed": 43, "text": "deep learning"}, {"st": 58, "ed": 60, "text": "attention model"}, {"st": 63, "ed": 66, "text": "electronic health records"}, {"st": 103, "ed": 105, "text": "predictive performance"}, {"st": 117, "ed": 121, "text": "recurrent neural network rnn"}, {"st": 125, "ed": 127, "text": "prediction tasks"}, {"st": 141, "ed": 143, "text": "higher accuracy"}, {"st": 150, "ed": 152, "text": "training data"}, {"st": 200, "ed": 202, "text": "higher level"}]
[{"st": 6, "ed": 8, "text": "supervised learning"}, {"st": 9, "ed": 11, "text": "structured output"}, {"st": 29, "ed": 31, "text": "loss function"}, {"st": 52, "ed": 54, "text": "least squares"}, {"st": 62, "ed": 64, "text": "inference problem"}, {"st": 85, "ed": 88, "text": "training and inference"}, {"st": 92, "ed": 94, "text": "convex surrogate"}, {"st": 97, "ed": 99, "text": "loss function"}, {"st": 104, "ed": 106, "text": "empirical evaluations"}, {"st": 107, "ed": 109, "text": "real world"}, {"st": 110, "ed": 112, "text": "synthetic data"}]
[{"st": 2, "ed": 4, "text": "decision trees"}, {"st": 6, "ed": 8, "text": "prediction accuracy"}, {"st": 34, "ed": 36, "text": "similarity measure"}, {"st": 44, "ed": 46, "text": "random forests"}, {"st": 70, "ed": 72, "text": "prediction accuracy"}, {"st": 105, "ed": 107, "text": "higher dimensional"}]
[{"st": 0, "ed": 4, "text": "recurrent neural networks rnns"}, {"st": 21, "ed": 23, "text": "internal structure"}, {"st": 36, "ed": 38, "text": "rnn based"}, {"st": 50, "ed": 52, "text": "sparse recovery"}, {"st": 107, "ed": 109, "text": "statistical model"}, {"st": 118, "ed": 120, "text": "step size"}, {"st": 129, "ed": 131, "text": "compressive sensing"}, {"st": 151, "ed": 156, "text": "long short term memory lstm"}]
[{"st": 1, "ed": 3, "text": "neural networks"}, {"st": 4, "ed": 6, "text": "becoming increasingly"}, {"st": 61, "ed": 63, "text": "theoretical insights"}]
[{"st": 5, "ed": 7, "text": "auto encoder"}, {"st": 11, "ed": 13, "text": "unsupervised learning"}, {"st": 14, "ed": 17, "text": "graph structured data"}, {"st": 20, "ed": 23, "text": "variational auto encoder"}, {"st": 30, "ed": 32, "text": "latent variables"}, {"st": 38, "ed": 40, "text": "latent representations"}, {"st": 50, "ed": 52, "text": "convolutional network"}, {"st": 57, "ed": 59, "text": "inner product"}, {"st": 63, "ed": 65, "text": "competitive results"}, {"st": 67, "ed": 69, "text": "link prediction"}, {"st": 77, "ed": 79, "text": "existing models"}, {"st": 80, "ed": 82, "text": "unsupervised learning"}, {"st": 83, "ed": 86, "text": "graph structured data"}, {"st": 87, "ed": 89, "text": "link prediction"}, {"st": 97, "ed": 99, "text": "significantly improves"}, {"st": 99, "ed": 101, "text": "predictive performance"}]
[{"st": 8, "ed": 10, "text": "deep learning"}, {"st": 27, "ed": 29, "text": "existing approaches"}, {"st": 85, "ed": 87, "text": "without compromising"}, {"st": 106, "ed": 108, "text": "feature space"}, {"st": 115, "ed": 117, "text": "class labels"}]
[{"st": 4, "ed": 6, "text": "machine learning"}, {"st": 28, "ed": 30, "text": "interpretable models"}, {"st": 65, "ed": 67, "text": "predictive model"}, {"st": 106, "ed": 110, "text": "multi layer neural networks"}, {"st": 114, "ed": 116, "text": "natural language"}]
[{"st": 5, "ed": 7, "text": "variational autoencoder"}, {"st": 21, "ed": 23, "text": "mixture model"}, {"st": 71, "ed": 74, "text": "semi supervised learning"}, {"st": 78, "ed": 82, "text": "number of training samples"}]
[{"st": 0, "ed": 2, "text": "complex nonlinear"}, {"st": 5, "ed": 8, "text": "deep neural network"}, {"st": 15, "ed": 17, "text": "image classification"}, {"st": 17, "ed": 19, "text": "speech recognition"}, {"st": 19, "ed": 21, "text": "natural language"}, {"st": 36, "ed": 38, "text": "complex nonlinear"}, {"st": 42, "ed": 44, "text": "complex data"}, {"st": 113, "ed": 115, "text": "classification decision"}]
[{"st": 14, "ed": 16, "text": "sparse representation"}, {"st": 112, "ed": 114, "text": "computational complexity"}, {"st": 120, "ed": 122, "text": "local minimum"}, {"st": 143, "ed": 145, "text": "computational complexity"}, {"st": 150, "ed": 152, "text": "sparse representations"}, {"st": 171, "ed": 173, "text": "image processing"}]
[{"st": 15, "ed": 17, "text": "statistically significant"}, {"st": 40, "ed": 43, "text": "goodness of fit"}]
[{"st": 13, "ed": 15, "text": "representation learning"}, {"st": 32, "ed": 34, "text": "deep architectures"}, {"st": 35, "ed": 37, "text": "widely applied"}, {"st": 38, "ed": 40, "text": "representation learning"}, {"st": 41, "ed": 43, "text": "recent years"}, {"st": 53, "ed": 55, "text": "image classification"}, {"st": 55, "ed": 57, "text": "object detection"}, {"st": 68, "ed": 70, "text": "data representation"}, {"st": 77, "ed": 79, "text": "feature learning"}, {"st": 85, "ed": 87, "text": "deep learning"}, {"st": 91, "ed": 93, "text": "data representation"}, {"st": 123, "ed": 125, "text": "data representation"}]
[{"st": 21, "ed": 23, "text": "computational resources"}, {"st": 116, "ed": 118, "text": "text classification"}]
[{"st": 7, "ed": 10, "text": "deep generative models"}, {"st": 28, "ed": 30, "text": "variational autoencoder"}, {"st": 64, "ed": 66, "text": "conditional generative"}, {"st": 100, "ed": 102, "text": "training procedure"}, {"st": 105, "ed": 108, "text": "achieve competitive results"}, {"st": 112, "ed": 114, "text": "prediction task"}, {"st": 116, "ed": 118, "text": "fully supervised"}, {"st": 125, "ed": 127, "text": "semi supervised"}, {"st": 129, "ed": 131, "text": "mnist svhn"}]
[{"st": 1, "ed": 3, "text": "active learning"}, {"st": 6, "ed": 8, "text": "logistic regression"}, {"st": 42, "ed": 44, "text": "synthetic datasets"}, {"st": 46, "ed": 48, "text": "real world"}, {"st": 56, "ed": 58, "text": "active learning"}, {"st": 62, "ed": 64, "text": "classification accuracy"}]
[{"st": 2, "ed": 4, "text": "machine learning"}, {"st": 49, "ed": 51, "text": "machine learning"}, {"st": 86, "ed": 88, "text": "linear classification"}]
[{"st": 0, "ed": 2, "text": "variational inference"}, {"st": 4, "ed": 6, "text": "powerful tool"}, {"st": 15, "ed": 18, "text": "evidence lower bound"}, {"st": 28, "ed": 30, "text": "variational inference"}, {"st": 59, "ed": 61, "text": "training set"}, {"st": 63, "ed": 65, "text": "random noise"}, {"st": 70, "ed": 72, "text": "variational autoencoders"}, {"st": 80, "ed": 83, "text": "evidence lower bound"}, {"st": 84, "ed": 86, "text": "synthetic datasets"}]
[{"st": 16, "ed": 18, "text": "input features"}, {"st": 20, "ed": 23, "text": "orders of magnitude"}, {"st": 28, "ed": 30, "text": "training examples"}, {"st": 34, "ed": 36, "text": "avoid overfitting"}, {"st": 64, "ed": 67, "text": "single nucleotide polymorphisms"}, {"st": 77, "ed": 79, "text": "deep learning"}, {"st": 136, "ed": 138, "text": "neural networks"}, {"st": 167, "ed": 169, "text": "neural network"}, {"st": 192, "ed": 194, "text": "distributed representation"}, {"st": 214, "ed": 216, "text": "neural network"}, {"st": 227, "ed": 229, "text": "distributed representation"}, {"st": 241, "ed": 243, "text": "neural network"}, {"st": 256, "ed": 258, "text": "hidden units"}, {"st": 274, "ed": 276, "text": "proposed approach"}, {"st": 277, "ed": 279, "text": "significantly reduce"}, {"st": 286, "ed": 288, "text": "error rate"}]
[{"st": 23, "ed": 25, "text": "probabilistic inference"}, {"st": 78, "ed": 80, "text": "inductive bias"}, {"st": 123, "ed": 125, "text": "inductive bias"}]
[{"st": 1, "ed": 3, "text": "neural networks"}, {"st": 4, "ed": 6, "text": "piecewise linear"}, {"st": 15, "ed": 17, "text": "neural network"}]
[{"st": 0, "ed": 2, "text": "neural networks"}, {"st": 3, "ed": 6, "text": "rectified linear unit"}, {"st": 26, "ed": 28, "text": "neural network"}, {"st": 46, "ed": 48, "text": "fully connected"}, {"st": 48, "ed": 51, "text": "feedforward neural networks"}, {"st": 52, "ed": 55, "text": "rectified linear unit"}, {"st": 61, "ed": 63, "text": "neural networks"}, {"st": 69, "ed": 71, "text": "theoretical analyses"}, {"st": 92, "ed": 94, "text": "mathbb r"}, {"st": 95, "ed": 97, "text": "mathbb r"}, {"st": 159, "ed": 161, "text": "mathbb r"}, {"st": 163, "ed": 165, "text": "mathbb r"}]
[{"st": 4, "ed": 7, "text": "multi label classification"}, {"st": 9, "ed": 11, "text": "multi label"}, {"st": 11, "ed": 14, "text": "error correcting code"}, {"st": 33, "ed": 35, "text": "special cases"}, {"st": 58, "ed": 60, "text": "evaluation criteria"}, {"st": 84, "ed": 86, "text": "evaluation criteria"}, {"st": 90, "ed": 92, "text": "error correcting"}, {"st": 95, "ed": 97, "text": "cost sensitive"}, {"st": 109, "ed": 111, "text": "binary classification"}, {"st": 126, "ed": 128, "text": "multi class"}, {"st": 149, "ed": 151, "text": "random sampling"}, {"st": 156, "ed": 158, "text": "nearest neighbor"}, {"st": 159, "ed": 162, "text": "extensive experimental results"}, {"st": 182, "ed": 184, "text": "cost sensitive"}, {"st": 207, "ed": 209, "text": "multi label"}, {"st": 209, "ed": 211, "text": "active learning"}, {"st": 238, "ed": 240, "text": "proposed algorithm"}, {"st": 244, "ed": 246, "text": "multi label"}, {"st": 246, "ed": 248, "text": "active learning"}]
[{"st": 31, "ed": 33, "text": "spectral graph"}, {"st": 81, "ed": 83, "text": "spectral graph"}, {"st": 107, "ed": 110, "text": "approach significantly outperforms"}, {"st": 132, "ed": 135, "text": "k nearest neighbor"}]
[{"st": 0, "ed": 3, "text": "variational auto encoders"}, {"st": 25, "ed": 27, "text": "latent variables"}, {"st": 31, "ed": 33, "text": "normal distribution"}, {"st": 41, "ed": 43, "text": "computational efficiency"}, {"st": 62, "ed": 64, "text": "posterior distribution"}, {"st": 67, "ed": 69, "text": "normalizing flows"}, {"st": 76, "ed": 78, "text": "latent variables"}, {"st": 108, "ed": 110, "text": "mnist dataset"}, {"st": 125, "ed": 127, "text": "competitive results"}]
[{"st": 35, "ed": 37, "text": "recommendation systems"}, {"st": 69, "ed": 71, "text": "previous studies"}, {"st": 74, "ed": 76, "text": "empirical results"}]
[{"st": 6, "ed": 8, "text": "time series"}, {"st": 14, "ed": 16, "text": "autism spectrum"}, {"st": 24, "ed": 26, "text": "kernel machines"}, {"st": 34, "ed": 36, "text": "dot product"}, {"st": 46, "ed": 48, "text": "spatio temporal"}, {"st": 83, "ed": 85, "text": "sparse coding"}, {"st": 132, "ed": 134, "text": "time series"}]
[{"st": 5, "ed": 7, "text": "decision making"}, {"st": 22, "ed": 26, "text": "multi armed bandit problems"}, {"st": 36, "ed": 38, "text": "armed bandit"}, {"st": 53, "ed": 56, "text": "upper confidence bound"}, {"st": 88, "ed": 90, "text": "l 2"}, {"st": 146, "ed": 148, "text": "regret bound"}, {"st": 241, "ed": 243, "text": "numerical examples"}]
[{"st": 20, "ed": 22, "text": "input space"}, {"st": 45, "ed": 47, "text": "support vector"}, {"st": 65, "ed": 67, "text": "achieve high"}]
[{"st": 7, "ed": 9, "text": "noisy data"}, {"st": 68, "ed": 70, "text": "online learning"}, {"st": 87, "ed": 91, "text": "robust principal component analysis"}, {"st": 113, "ed": 115, "text": "large scale"}]
[{"st": 3, "ed": 6, "text": "deep neural networks"}, {"st": 11, "ed": 13, "text": "feed forward"}, {"st": 13, "ed": 16, "text": "deep neural network"}, {"st": 65, "ed": 67, "text": "feed forward"}, {"st": 67, "ed": 70, "text": "deep neural network"}, {"st": 70, "ed": 73, "text": "directed acyclic graph"}, {"st": 117, "ed": 119, "text": "jointly trained"}, {"st": 136, "ed": 138, "text": "extensive experiments"}, {"st": 143, "ed": 145, "text": "image classification"}]
[{"st": 8, "ed": 10, "text": "missing information"}, {"st": 19, "ed": 23, "text": "mean squared error mse"}, {"st": 39, "ed": 41, "text": "bias variance"}, {"st": 61, "ed": 63, "text": "previous works"}, {"st": 89, "ed": 91, "text": "prediction accuracy"}]
[{"st": 1, "ed": 3, "text": "activity recognition"}, {"st": 67, "ed": 69, "text": "latent variables"}, {"st": 104, "ed": 106, "text": "temporal dependencies"}, {"st": 119, "ed": 121, "text": "network structure"}, {"st": 151, "ed": 153, "text": "empirical evaluations"}, {"st": 154, "ed": 156, "text": "benchmark datasets"}]
[{"st": 1, "ed": 3, "text": "real world"}, {"st": 17, "ed": 19, "text": "multi objective"}, {"st": 99, "ed": 101, "text": "multi objective"}, {"st": 110, "ed": 112, "text": "multi objective"}, {"st": 136, "ed": 138, "text": "theoretical analysis"}, {"st": 140, "ed": 142, "text": "thompson sampling"}, {"st": 154, "ed": 156, "text": "theoretical results"}, {"st": 165, "ed": 167, "text": "multi objective"}]
[{"st": 4, "ed": 6, "text": "machine learning"}, {"st": 13, "ed": 15, "text": "machine learning"}, {"st": 52, "ed": 54, "text": "machine learning"}, {"st": 75, "ed": 77, "text": "machine learning"}]
[{"st": 1, "ed": 3, "text": "recently proposed"}, {"st": 4, "ed": 6, "text": "min max"}, {"st": 16, "ed": 18, "text": "large scale"}, {"st": 18, "ed": 20, "text": "statistical learning"}, {"st": 35, "ed": 38, "text": "radial basis function"}, {"st": 45, "ed": 47, "text": "classification tasks"}, {"st": 175, "ed": 177, "text": "deep learning"}, {"st": 199, "ed": 201, "text": "previous studies"}, {"st": 211, "ed": 213, "text": "excellent performance"}, {"st": 232, "ed": 234, "text": "multi class"}]
[{"st": 4, "ed": 7, "text": "fast and accurate"}, {"st": 7, "ed": 9, "text": "network architecture"}, {"st": 20, "ed": 22, "text": "deep architectures"}, {"st": 51, "ed": 53, "text": "et al"}, {"st": 76, "ed": 79, "text": "rectified linear units"}, {"st": 84, "ed": 87, "text": "rectified linear units"}, {"st": 113, "ed": 115, "text": "network architecture"}, {"st": 152, "ed": 154, "text": "significantly faster"}, {"st": 172, "ed": 174, "text": "cifar 10"}]
[{"st": 0, "ed": 3, "text": "generative adversarial networks"}, {"st": 14, "ed": 16, "text": "generative models"}, {"st": 17, "ed": 19, "text": "complex data"}, {"st": 57, "ed": 59, "text": "iterative procedure"}, {"st": 72, "ed": 74, "text": "mixture model"}, {"st": 87, "ed": 89, "text": "boosting algorithms"}, {"st": 116, "ed": 118, "text": "true distribution"}]
[{"st": 0, "ed": 2, "text": "transfer learning"}, {"st": 9, "ed": 11, "text": "source domain"}, {"st": 22, "ed": 24, "text": "transfer learning"}, {"st": 35, "ed": 37, "text": "feature spaces"}, {"st": 54, "ed": 56, "text": "transfer learning"}, {"st": 89, "ed": 91, "text": "negative transfer"}, {"st": 192, "ed": 194, "text": "specifically designed"}, {"st": 219, "ed": 221, "text": "learned representations"}, {"st": 247, "ed": 249, "text": "source domain"}]
[{"st": 3, "ed": 6, "text": "distance metric learning"}, {"st": 18, "ed": 20, "text": "pairwise similarity"}, {"st": 36, "ed": 39, "text": "classification and clustering"}, {"st": 62, "ed": 64, "text": "generation process"}, {"st": 76, "ed": 78, "text": "ground truth"}, {"st": 170, "ed": 172, "text": "learning rates"}, {"st": 174, "ed": 176, "text": "parameter free"}, {"st": 178, "ed": 180, "text": "metric learning"}, {"st": 182, "ed": 184, "text": "synthetic data"}, {"st": 193, "ed": 195, "text": "performance improvements"}, {"st": 203, "ed": 205, "text": "previously proposed"}, {"st": 205, "ed": 208, "text": "batch and online"}, {"st": 208, "ed": 211, "text": "distance metric learning"}]
[{"st": 0, "ed": 4, "text": "markov chain monte carlo"}, {"st": 15, "ed": 17, "text": "machine learning"}, {"st": 73, "ed": 75, "text": "theoretical guarantees"}, {"st": 86, "ed": 88, "text": "gibbs sampler"}, {"st": 90, "ed": 92, "text": "posterior distribution"}, {"st": 98, "ed": 101, "text": "latent dirichlet allocation"}, {"st": 107, "ed": 109, "text": "unsupervised learning"}, {"st": 111, "ed": 113, "text": "text generation"}]
[{"st": 3, "ed": 5, "text": "compressive sensing"}, {"st": 8, "ed": 10, "text": "factor analysis"}, {"st": 24, "ed": 30, "text": "alternating direction method of multipliers admm"}, {"st": 32, "ed": 34, "text": "compressive sensing"}, {"st": 38, "ed": 40, "text": "factor analysis"}, {"st": 43, "ed": 45, "text": "proposed algorithm"}, {"st": 46, "ed": 48, "text": "reconstructed images"}, {"st": 118, "ed": 120, "text": "classification accuracy"}]
[{"st": 19, "ed": 21, "text": "high dimensional"}, {"st": 23, "ed": 25, "text": "observed data"}, {"st": 47, "ed": 49, "text": "unsupervised learning"}, {"st": 62, "ed": 64, "text": "highly efficient"}, {"st": 70, "ed": 72, "text": "spatial temporal"}, {"st": 132, "ed": 134, "text": "low rank"}, {"st": 150, "ed": 152, "text": "promising performance"}, {"st": 154, "ed": 156, "text": "benchmark problems"}]
[{"st": 4, "ed": 6, "text": "recently proposed"}, {"st": 23, "ed": 25, "text": "training data"}, {"st": 37, "ed": 39, "text": "low rank"}, {"st": 50, "ed": 52, "text": "low rank"}, {"st": 58, "ed": 60, "text": "synthetic experiments"}]
[{"st": 0, "ed": 3, "text": "dirichlet process mixture"}, {"st": 26, "ed": 28, "text": "variational inference"}, {"st": 58, "ed": 60, "text": "sampling based"}, {"st": 89, "ed": 91, "text": "empirical performance"}, {"st": 102, "ed": 105, "text": "easy to implement"}]
[{"st": 47, "ed": 49, "text": "temporal dynamics"}, {"st": 63, "ed": 65, "text": "existing methods"}, {"st": 87, "ed": 89, "text": "temporal dynamics"}, {"st": 99, "ed": 101, "text": "highly dependent"}, {"st": 126, "ed": 128, "text": "temporal dynamics"}, {"st": 150, "ed": 152, "text": "extensive experiments"}, {"st": 158, "ed": 160, "text": "proposed method"}]
[{"st": 4, "ed": 8, "text": "convolutional neural network cnn"}, {"st": 8, "ed": 10, "text": "based method"}, {"st": 17, "ed": 19, "text": "large scale"}, {"st": 41, "ed": 43, "text": "traffic flow"}, {"st": 63, "ed": 65, "text": "feature extraction"}, {"st": 75, "ed": 77, "text": "proposed method"}, {"st": 82, "ed": 84, "text": "real world"}, {"st": 88, "ed": 90, "text": "ring road"}, {"st": 109, "ed": 111, "text": "least squares"}, {"st": 111, "ed": 114, "text": "k nearest neighbors"}, {"st": 114, "ed": 117, "text": "artificial neural network"}, {"st": 118, "ed": 120, "text": "random forest"}, {"st": 122, "ed": 124, "text": "deep learning"}, {"st": 128, "ed": 131, "text": "recurrent neural network"}, {"st": 133, "ed": 136, "text": "short term memory"}, {"st": 142, "ed": 144, "text": "proposed method"}, {"st": 174, "ed": 176, "text": "large scale"}]
[{"st": 9, "ed": 12, "text": "field of view"}, {"st": 31, "ed": 33, "text": "deep learning"}, {"st": 39, "ed": 41, "text": "computer vision"}, {"st": 73, "ed": 75, "text": "hyper parameters"}, {"st": 95, "ed": 97, "text": "exhaustive search"}, {"st": 108, "ed": 110, "text": "random search"}, {"st": 118, "ed": 120, "text": "gaussian processes"}, {"st": 137, "ed": 139, "text": "substantial improvement"}]
[{"st": 9, "ed": 13, "text": "deep convolutional neural network"}, {"st": 32, "ed": 34, "text": "deep architectures"}]
[{"st": 1, "ed": 3, "text": "class classification"}, {"st": 17, "ed": 19, "text": "traditional methods"}, {"st": 25, "ed": 27, "text": "time consuming"}, {"st": 46, "ed": 49, "text": "extreme learning machine"}, {"st": 103, "ed": 105, "text": "feature mapping"}, {"st": 113, "ed": 115, "text": "feature mapping"}, {"st": 115, "ed": 117, "text": "based approaches"}, {"st": 197, "ed": 199, "text": "benchmark datasets"}]
[{"st": 6, "ed": 8, "text": "model selection"}, {"st": 12, "ed": 14, "text": "conditional independence"}, {"st": 19, "ed": 21, "text": "non stationary"}, {"st": 22, "ed": 24, "text": "random process"}, {"st": 24, "ed": 26, "text": "time series"}, {"st": 48, "ed": 50, "text": "sample complexity"}, {"st": 51, "ed": 53, "text": "graphical model"}, {"st": 61, "ed": 63, "text": "selection method"}, {"st": 93, "ed": 95, "text": "conditional independence"}]
[{"st": 34, "ed": 36, "text": "theoretical analysis"}, {"st": 77, "ed": 79, "text": "problems including"}]
[{"st": 20, "ed": 22, "text": "x ray"}, {"st": 23, "ed": 25, "text": "recent progress"}, {"st": 26, "ed": 28, "text": "machine learning"}, {"st": 39, "ed": 41, "text": "probabilistic framework"}, {"st": 62, "ed": 64, "text": "high dimensional"}, {"st": 67, "ed": 69, "text": "low dimensional"}, {"st": 72, "ed": 74, "text": "adjacency matrix"}, {"st": 96, "ed": 98, "text": "latent variables"}, {"st": 126, "ed": 128, "text": "parallel transport"}, {"st": 209, "ed": 211, "text": "prediction accuracy"}]
[{"st": 20, "ed": 22, "text": "machine learning"}, {"st": 34, "ed": 36, "text": "machine learning"}, {"st": 71, "ed": 73, "text": "prediction error"}, {"st": 81, "ed": 83, "text": "prediction error"}, {"st": 128, "ed": 130, "text": "method yields"}, {"st": 159, "ed": 161, "text": "effect size"}, {"st": 168, "ed": 170, "text": "effect size"}, {"st": 183, "ed": 185, "text": "p value"}, {"st": 232, "ed": 234, "text": "nonparametric regression"}]
[{"st": 0, "ed": 3, "text": "artificial neural networks"}, {"st": 10, "ed": 12, "text": "pattern recognition"}, {"st": 17, "ed": 19, "text": "neural networks"}, {"st": 46, "ed": 48, "text": "multilayer perceptron"}, {"st": 48, "ed": 50, "text": "neural networks"}]
[{"st": 0, "ed": 3, "text": "sum product networks"}, {"st": 20, "ed": 23, "text": "deep neural network"}, {"st": 31, "ed": 34, "text": "probabilistic graphical model"}, {"st": 71, "ed": 73, "text": "sum product"}, {"st": 78, "ed": 80, "text": "structure learning"}, {"st": 92, "ed": 94, "text": "structure learning"}, {"st": 107, "ed": 109, "text": "parameter learning"}]
[{"st": 32, "ed": 34, "text": "kernel function"}, {"st": 42, "ed": 45, "text": "k means clustering"}]
[{"st": 3, "ed": 5, "text": "recently proposed"}, {"st": 5, "ed": 7, "text": "variational dropout"}, {"st": 19, "ed": 21, "text": "variational dropout"}, {"st": 60, "ed": 62, "text": "fully connected"}]
[{"st": 3, "ed": 5, "text": "recently proposed"}, {"st": 8, "ed": 10, "text": "generative models"}, {"st": 23, "ed": 27, "text": "available at https github.com"}, {"st": 128, "ed": 130, "text": "log likelihood"}, {"st": 132, "ed": 134, "text": "cifar 10"}]
[{"st": 50, "ed": 52, "text": "inference algorithm"}, {"st": 71, "ed": 73, "text": "inductive bias"}, {"st": 77, "ed": 80, "text": "real world data"}, {"st": 86, "ed": 88, "text": "predictive performance"}]
[{"st": 1, "ed": 3, "text": "rare disease"}, {"st": 32, "ed": 34, "text": "rare disease"}, {"st": 69, "ed": 71, "text": "rare disease"}, {"st": 83, "ed": 85, "text": "graphical model"}, {"st": 117, "ed": 119, "text": "proposed approach"}]
[{"st": 3, "ed": 5, "text": "important role"}, {"st": 132, "ed": 134, "text": "random forest"}, {"st": 141, "ed": 143, "text": "classification results"}]
[{"st": 5, "ed": 7, "text": "multi class"}, {"st": 23, "ed": 25, "text": "sampling strategy"}, {"st": 29, "ed": 31, "text": "multi class"}, {"st": 39, "ed": 41, "text": "multi class"}, {"st": 44, "ed": 46, "text": "binary classification"}, {"st": 55, "ed": 57, "text": "sampling strategy"}, {"st": 65, "ed": 67, "text": "class distributions"}, {"st": 71, "ed": 73, "text": "large scale"}, {"st": 73, "ed": 76, "text": "multi class classification"}, {"st": 102, "ed": 105, "text": "empirical risk minimization"}, {"st": 125, "ed": 127, "text": "100 000"}, {"st": 135, "ed": 137, "text": "proposed approach"}, {"st": 147, "ed": 149, "text": "predictive performance"}]
[{"st": 7, "ed": 10, "text": "gaussian process gp"}, {"st": 30, "ed": 32, "text": "gp model"}, {"st": 36, "ed": 38, "text": "unlike existing"}, {"st": 51, "ed": 53, "text": "gp models"}, {"st": 60, "ed": 62, "text": "gp models"}, {"st": 73, "ed": 75, "text": "prediction error"}, {"st": 157, "ed": 159, "text": "prediction methods"}, {"st": 186, "ed": 188, "text": "uncertainty quantification"}, {"st": 207, "ed": 209, "text": "computationally efficient"}, {"st": 209, "ed": 211, "text": "closed form"}, {"st": 235, "ed": 237, "text": "principal component"}, {"st": 241, "ed": 243, "text": "proposed approach"}, {"st": 258, "ed": 260, "text": "higher dimensional"}]
[{"st": 4, "ed": 6, "text": "multi label"}, {"st": 17, "ed": 20, "text": "multi label learning"}, {"st": 39, "ed": 41, "text": "low rank"}, {"st": 87, "ed": 89, "text": "low rank"}, {"st": 90, "ed": 92, "text": "multi label"}, {"st": 94, "ed": 96, "text": "proposed algorithm"}, {"st": 113, "ed": 115, "text": "low rank"}, {"st": 122, "ed": 124, "text": "efficiently solved"}, {"st": 126, "ed": 129, "text": "coordinate descent algorithm"}, {"st": 130, "ed": 132, "text": "iterative optimization"}, {"st": 138, "ed": 140, "text": "benchmark datasets"}, {"st": 150, "ed": 152, "text": "significantly improved"}, {"st": 158, "ed": 160, "text": "competing methods"}]
[{"st": 2, "ed": 5, "text": "k nearest neighbors"}, {"st": 16, "ed": 18, "text": "pattern recognition"}, {"st": 37, "ed": 40, "text": "received much attention"}, {"st": 68, "ed": 70, "text": "bias variance"}, {"st": 101, "ed": 104, "text": "each data point"}]
[{"st": 4, "ed": 7, "text": "problem of recovering"}, {"st": 13, "ed": 15, "text": "sparse representation"}, {"st": 22, "ed": 24, "text": "l 2"}, {"st": 26, "ed": 29, "text": "mean square error"}, {"st": 58, "ed": 60, "text": "modified version"}, {"st": 62, "ed": 64, "text": "structural similarity"}, {"st": 109, "ed": 111, "text": "sparse recovery"}, {"st": 116, "ed": 119, "text": "l 1 norm"}, {"st": 119, "ed": 121, "text": "minimization problem"}, {"st": 129, "ed": 131, "text": "convex optimization"}, {"st": 139, "ed": 145, "text": "alternating direction method of multipliers admm"}, {"st": 146, "ed": 149, "text": "taking advantage of"}, {"st": 165, "ed": 167, "text": "globally optimal"}, {"st": 171, "ed": 173, "text": "optimization problem"}, {"st": 192, "ed": 194, "text": "proposed algorithm"}]
[{"st": 4, "ed": 7, "text": "electronic health records"}, {"st": 36, "ed": 39, "text": "deep neural network"}, {"st": 43, "ed": 45, "text": "feature embedding"}, {"st": 59, "ed": 61, "text": "multi layer"}, {"st": 61, "ed": 65, "text": "convolutional neural network cnn"}, {"st": 74, "ed": 76, "text": "capture complex"}, {"st": 124, "ed": 126, "text": "promising results"}, {"st": 134, "ed": 136, "text": "feature embedding"}, {"st": 139, "ed": 142, "text": "convolutional neural network"}, {"st": 148, "ed": 151, "text": "congestive heart failure"}]
[{"st": 0, "ed": 2, "text": "feature selection"}, {"st": 15, "ed": 17, "text": "text categorization"}, {"st": 17, "ed": 19, "text": "image classification"}, {"st": 20, "ed": 22, "text": "face recognition"}, {"st": 22, "ed": 24, "text": "multi label"}, {"st": 37, "ed": 39, "text": "feature selection"}, {"st": 42, "ed": 44, "text": "mutual information"}, {"st": 77, "ed": 79, "text": "theoretical framework"}, {"st": 99, "ed": 101, "text": "objective function"}, {"st": 133, "ed": 135, "text": "objective function"}]
[{"st": 13, "ed": 15, "text": "non stationary"}, {"st": 15, "ed": 17, "text": "time series"}, {"st": 18, "ed": 20, "text": "time series"}, {"st": 47, "ed": 49, "text": "riemannian manifold"}, {"st": 52, "ed": 54, "text": "time series"}, {"st": 68, "ed": 70, "text": "riemannian geometry"}, {"st": 75, "ed": 77, "text": "time series"}, {"st": 108, "ed": 110, "text": "granger causality"}, {"st": 116, "ed": 118, "text": "moving average"}, {"st": 121, "ed": 123, "text": "low rank"}, {"st": 152, "ed": 154, "text": "kernel based"}, {"st": 163, "ed": 165, "text": "positive definite"}, {"st": 168, "ed": 170, "text": "recently developed"}, {"st": 181, "ed": 183, "text": "time series"}, {"st": 193, "ed": 195, "text": "extensive numerical"}, {"st": 199, "ed": 201, "text": "proposed framework"}, {"st": 219, "ed": 221, "text": "time series"}]
[{"st": 11, "ed": 13, "text": "strongly convex"}, {"st": 23, "ed": 26, "text": "under mild conditions"}, {"st": 37, "ed": 39, "text": "statistical models"}, {"st": 41, "ed": 43, "text": "group lasso"}, {"st": 44, "ed": 46, "text": "logistic regression"}, {"st": 47, "ed": 49, "text": "ell 1"}, {"st": 53, "ed": 55, "text": "linear regression"}, {"st": 59, "ed": 61, "text": "significantly improves"}, {"st": 89, "ed": 91, "text": "regularization term"}]
[{"st": 32, "ed": 34, "text": "mode collapse"}, {"st": 51, "ed": 53, "text": "optimization problem"}]
[{"st": 2, "ed": 4, "text": "differentially private"}, {"st": 10, "ed": 12, "text": "linear optimization"}, {"st": 22, "ed": 24, "text": "o sqrt"}, {"st": 37, "ed": 39, "text": "differential privacy"}, {"st": 47, "ed": 49, "text": "regret bounds"}, {"st": 51, "ed": 53, "text": "o sqrt"}, {"st": 55, "ed": 58, "text": "o left frac"}, {"st": 64, "ed": 66, "text": "linear optimization"}, {"st": 69, "ed": 71, "text": "special case"}, {"st": 74, "ed": 77, "text": "multi armed bandits"}, {"st": 78, "ed": 80, "text": "proposed algorithm"}, {"st": 85, "ed": 88, "text": "o left frac"}, {"st": 90, "ed": 92, "text": "sqrt t"}, {"st": 98, "ed": 100, "text": "regret bound"}, {"st": 102, "ed": 105, "text": "o left frac"}]
[{"st": 27, "ed": 29, "text": "novelty detection"}, {"st": 30, "ed": 32, "text": "statistical models"}, {"st": 65, "ed": 68, "text": "maximum likelihood estimator"}, {"st": 77, "ed": 79, "text": "novelty detection"}, {"st": 89, "ed": 91, "text": "substantially improve"}]
[{"st": 45, "ed": 47, "text": "mutual information"}, {"st": 82, "ed": 84, "text": "higher accuracy"}]
[{"st": 44, "ed": 46, "text": "multitask learning"}, {"st": 63, "ed": 65, "text": "multitask learning"}, {"st": 85, "ed": 87, "text": "multitask learning"}, {"st": 115, "ed": 117, "text": "group lasso"}, {"st": 151, "ed": 153, "text": "recent works"}, {"st": 157, "ed": 159, "text": "learning task"}, {"st": 165, "ed": 167, "text": "special cases"}, {"st": 171, "ed": 174, "text": "synthetic and real"}, {"st": 194, "ed": 196, "text": "significantly outperforms"}]
[{"st": 1, "ed": 3, "text": "mixture models"}, {"st": 11, "ed": 13, "text": "probabilistic framework"}, {"st": 64, "ed": 66, "text": "variable selection"}, {"st": 68, "ed": 70, "text": "model based"}]
[{"st": 10, "ed": 12, "text": "clustering problems"}, {"st": 17, "ed": 19, "text": "mixture model"}, {"st": 26, "ed": 28, "text": "point processes"}, {"st": 59, "ed": 61, "text": "prior distribution"}, {"st": 77, "ed": 80, "text": "maximum likelihood estimator"}, {"st": 85, "ed": 87, "text": "variational bayesian"}, {"st": 124, "ed": 126, "text": "sample complexity"}, {"st": 130, "ed": 133, "text": "theoretical and empirical"}, {"st": 145, "ed": 147, "text": "proposed method"}, {"st": 149, "ed": 152, "text": "number of clusters"}, {"st": 162, "ed": 166, "text": "synthetic and real world"}]
[{"st": 10, "ed": 13, "text": "deep neural networks"}, {"st": 28, "ed": 31, "text": "deep neural network"}, {"st": 45, "ed": 47, "text": "layer wise"}, {"st": 49, "ed": 52, "text": "deep belief network"}, {"st": 54, "ed": 56, "text": "auto encoder"}, {"st": 65, "ed": 67, "text": "transfer learning"}, {"st": 71, "ed": 73, "text": "training epochs"}, {"st": 90, "ed": 92, "text": "fine tuning"}, {"st": 107, "ed": 109, "text": "auto encoder"}, {"st": 174, "ed": 176, "text": "weight initialization"}]
[{"st": 4, "ed": 6, "text": "positive unlabeled"}, {"st": 14, "ed": 17, "text": "labeled and unlabeled"}, {"st": 29, "ed": 32, "text": "under mild conditions"}, {"st": 52, "ed": 54, "text": "performance measures"}, {"st": 63, "ed": 65, "text": "precision recall"}, {"st": 84, "ed": 87, "text": "positive and negative"}, {"st": 139, "ed": 141, "text": "positive class"}]
[{"st": 11, "ed": 13, "text": "machine learning"}, {"st": 26, "ed": 28, "text": "efficiently solve"}, {"st": 28, "ed": 30, "text": "real world"}, {"st": 30, "ed": 32, "text": "artificial intelligence"}, {"st": 33, "ed": 35, "text": "data mining"}, {"st": 39, "ed": 41, "text": "big data"}, {"st": 46, "ed": 48, "text": "wide variety"}, {"st": 90, "ed": 92, "text": "future research"}]
[{"st": 18, "ed": 21, "text": "cross modal retrieval"}, {"st": 30, "ed": 32, "text": "multi modal"}, {"st": 32, "ed": 34, "text": "embedding space"}, {"st": 41, "ed": 43, "text": "previous methods"}, {"st": 47, "ed": 49, "text": "embedding space"}, {"st": 97, "ed": 99, "text": "embedding space"}, {"st": 113, "ed": 115, "text": "generalization capability"}, {"st": 133, "ed": 136, "text": "self paced learning"}, {"st": 141, "ed": 143, "text": "cross modal"}, {"st": 143, "ed": 146, "text": "learning to rank"}, {"st": 150, "ed": 152, "text": "multi modal"}, {"st": 152, "ed": 154, "text": "embedding space"}, {"st": 198, "ed": 200, "text": "challenging problem"}, {"st": 201, "ed": 203, "text": "fast convergence"}, {"st": 205, "ed": 208, "text": "extensive experimental results"}, {"st": 210, "ed": 212, "text": "benchmark datasets"}, {"st": 215, "ed": 217, "text": "proposed method"}, {"st": 218, "ed": 220, "text": "significant improvements"}]
[{"st": 7, "ed": 9, "text": "semi supervised"}, {"st": 86, "ed": 88, "text": "sample space"}, {"st": 131, "ed": 134, "text": "publicly available datasets"}, {"st": 143, "ed": 145, "text": "pancreatic cancer"}, {"st": 147, "ed": 149, "text": "ovarian cancer"}, {"st": 163, "ed": 165, "text": "recently published"}, {"st": 179, "ed": 181, "text": "based methods"}, {"st": 183, "ed": 185, "text": "sample space"}, {"st": 220, "ed": 223, "text": "taking into account"}, {"st": 249, "ed": 251, "text": "sample space"}]
[{"st": 1, "ed": 3, "text": "dimensionality reduction"}, {"st": 5, "ed": 7, "text": "learning algorithms"}, {"st": 11, "ed": 13, "text": "pairwise similarities"}, {"st": 20, "ed": 22, "text": "spectral methods"}, {"st": 23, "ed": 25, "text": "kernel pca"}, {"st": 33, "ed": 37, "text": "singular value decomposition svd"}, {"st": 39, "ed": 41, "text": "similarity matrix"}, {"st": 44, "ed": 46, "text": "low dimensional"}, {"st": 54, "ed": 56, "text": "computationally expensive"}, {"st": 60, "ed": 62, "text": "training examples"}, {"st": 84, "ed": 86, "text": "training examples"}, {"st": 102, "ed": 106, "text": "feed forward neural network"}, {"st": 111, "ed": 113, "text": "embedding space"}, {"st": 148, "ed": 150, "text": "neural network"}, {"st": 165, "ed": 167, "text": "pairwise similarities"}, {"st": 169, "ed": 171, "text": "training set"}, {"st": 186, "ed": 189, "text": "image and text"}, {"st": 204, "ed": 206, "text": "spectral methods"}]
[{"st": 3, "ed": 5, "text": "closed form"}, {"st": 6, "ed": 8, "text": "least squares"}, {"st": 8, "ed": 11, "text": "support vector machine"}, {"st": 17, "ed": 20, "text": "classification and regression"}, {"st": 23, "ed": 25, "text": "comparable performance"}, {"st": 52, "ed": 54, "text": "loss function"}, {"st": 76, "ed": 78, "text": "large scale"}, {"st": 114, "ed": 116, "text": "kernel matrix"}, {"st": 122, "ed": 124, "text": "kernel matrix"}, {"st": 126, "ed": 128, "text": "low rank"}, {"st": 132, "ed": 134, "text": "loss function"}, {"st": 165, "ed": 167, "text": "proposed algorithm"}, {"st": 172, "ed": 174, "text": "existing algorithms"}, {"st": 180, "ed": 182, "text": "large scale"}, {"st": 194, "ed": 196, "text": "comparable performance"}, {"st": 206, "ed": 208, "text": "large scale"}]
[{"st": 31, "ed": 33, "text": "neural network"}, {"st": 103, "ed": 106, "text": "mixture of experts"}]
[{"st": 0, "ed": 3, "text": "inference and learning"}, {"st": 17, "ed": 19, "text": "large networks"}, {"st": 29, "ed": 31, "text": "large scale"}, {"st": 37, "ed": 40, "text": "semi supervised learning"}, {"st": 44, "ed": 46, "text": "recent developments"}, {"st": 47, "ed": 49, "text": "neural network"}, {"st": 69, "ed": 71, "text": "theoretical guarantees"}, {"st": 96, "ed": 98, "text": "standard benchmarks"}, {"st": 142, "ed": 144, "text": "error rates"}, {"st": 170, "ed": 172, "text": "semi supervised"}, {"st": 192, "ed": 194, "text": "unlabeled data"}]
[{"st": 7, "ed": 9, "text": "unsupervised learning"}, {"st": 11, "ed": 13, "text": "machine learning"}, {"st": 16, "ed": 18, "text": "clustering algorithms"}, {"st": 53, "ed": 55, "text": "numerous applications"}, {"st": 86, "ed": 88, "text": "model based"}, {"st": 92, "ed": 94, "text": "finite set"}, {"st": 102, "ed": 104, "text": "numerical experiments"}, {"st": 113, "ed": 116, "text": "simulated and real"}]
[{"st": 46, "ed": 48, "text": "building blocks"}, {"st": 50, "ed": 52, "text": "multi level"}, {"st": 109, "ed": 111, "text": "computational requirements"}, {"st": 184, "ed": 186, "text": "rule based"}]
[{"st": 2, "ed": 4, "text": "training set"}, {"st": 8, "ed": 11, "text": "support vector machine"}, {"st": 106, "ed": 108, "text": "standard deviation"}]
[{"st": 16, "ed": 18, "text": "k 1"}, {"st": 38, "ed": 40, "text": "activation function"}, {"st": 51, "ed": 54, "text": "single hidden layer"}, {"st": 54, "ed": 56, "text": "neural networks"}, {"st": 81, "ed": 84, "text": "mean square error"}, {"st": 93, "ed": 95, "text": "ell 1"}, {"st": 151, "ed": 153, "text": "v 1"}, {"st": 155, "ed": 157, "text": "ell 1"}]
[{"st": 4, "ed": 7, "text": "multi task learning"}, {"st": 12, "ed": 14, "text": "related tasks"}, {"st": 20, "ed": 22, "text": "task specific"}, {"st": 27, "ed": 29, "text": "algorithmic framework"}, {"st": 59, "ed": 61, "text": "key idea"}, {"st": 102, "ed": 104, "text": "regret bounds"}]
[{"st": 1, "ed": 3, "text": "generative model"}, {"st": 6, "ed": 8, "text": "deep architectures"}, {"st": 36, "ed": 38, "text": "input data"}, {"st": 102, "ed": 104, "text": "objective function"}, {"st": 114, "ed": 116, "text": "neural networks"}, {"st": 136, "ed": 138, "text": "hand written"}, {"st": 154, "ed": 156, "text": "real world"}]
[{"st": 2, "ed": 4, "text": "reinforcement learning"}, {"st": 7, "ed": 10, "text": "recurrent neural network"}, {"st": 34, "ed": 36, "text": "quality assurance"}, {"st": 43, "ed": 45, "text": "time consuming"}, {"st": 48, "ed": 50, "text": "reinforcement learning"}, {"st": 52, "ed": 54, "text": "natural language"}, {"st": 69, "ed": 71, "text": "demonstrate empirically"}, {"st": 83, "ed": 85, "text": "off policy"}, {"st": 86, "ed": 88, "text": "policy gradient"}, {"st": 102, "ed": 104, "text": "synthetic experiments"}, {"st": 106, "ed": 109, "text": "amazon mechanical turk"}]
[{"st": 3, "ed": 5, "text": "deep learning"}, {"st": 7, "ed": 9, "text": "application domains"}, {"st": 56, "ed": 58, "text": "neural network"}, {"st": 84, "ed": 86, "text": "weight sharing"}, {"st": 91, "ed": 93, "text": "method achieves"}, {"st": 104, "ed": 107, "text": "point of view"}, {"st": 115, "ed": 118, "text": "minimum description length"}]
[{"st": 17, "ed": 20, "text": "multi label classification"}, {"st": 31, "ed": 34, "text": "benchmark data sets"}]
[{"st": 1, "ed": 4, "text": "the past decade"}, {"st": 14, "ed": 16, "text": "predictive state"}, {"st": 23, "ed": 25, "text": "theoretical guarantees"}, {"st": 35, "ed": 37, "text": "inference tasks"}, {"st": 57, "ed": 59, "text": "loss function"}, {"st": 64, "ed": 66, "text": "estimation error"}, {"st": 75, "ed": 77, "text": "improve performance"}, {"st": 106, "ed": 108, "text": "log likelihood"}, {"st": 128, "ed": 130, "text": "predictive state"}, {"st": 182, "ed": 184, "text": "parameter space"}, {"st": 189, "ed": 191, "text": "loss function"}, {"st": 219, "ed": 223, "text": "real and synthetic data"}]
[{"st": 5, "ed": 7, "text": "widely studied"}, {"st": 20, "ed": 22, "text": "np hard"}, {"st": 119, "ed": 121, "text": "feature engineering"}, {"st": 124, "ed": 127, "text": "curse of dimensionality"}, {"st": 128, "ed": 130, "text": "feature selection"}, {"st": 134, "ed": 136, "text": "deep learning"}, {"st": 145, "ed": 147, "text": "training data"}, {"st": 149, "ed": 151, "text": "feature selection"}]
[{"st": 0, "ed": 3, "text": "multi label classification"}, {"st": 7, "ed": 9, "text": "supervised learning"}, {"st": 15, "ed": 17, "text": "multiple labels"}, {"st": 53, "ed": 55, "text": "training data"}, {"st": 58, "ed": 60, "text": "weighted sum"}, {"st": 66, "ed": 68, "text": "feature space"}, {"st": 121, "ed": 123, "text": "training data"}, {"st": 125, "ed": 128, "text": "taking into account"}, {"st": 133, "ed": 135, "text": "multi label"}, {"st": 140, "ed": 142, "text": "proposed method"}, {"st": 158, "ed": 160, "text": "multi label"}]
[{"st": 10, "ed": 12, "text": "distributed representations"}]
[{"st": 3, "ed": 5, "text": "semi supervised"}, {"st": 34, "ed": 37, "text": "semi supervised learning"}, {"st": 39, "ed": 42, "text": "expectation maximization algorithm"}, {"st": 100, "ed": 102, "text": "large scale"}, {"st": 111, "ed": 113, "text": "prediction accuracy"}, {"st": 114, "ed": 116, "text": "computational effort"}]
[{"st": 19, "ed": 21, "text": "existing methods"}, {"st": 33, "ed": 35, "text": "sampling strategy"}, {"st": 44, "ed": 46, "text": "sampling strategy"}, {"st": 99, "ed": 101, "text": "bandit problem"}, {"st": 105, "ed": 107, "text": "pure exploration"}, {"st": 125, "ed": 127, "text": "sample complexity"}, {"st": 130, "ed": 132, "text": "sample complexity"}, {"st": 145, "ed": 147, "text": "instance based"}, {"st": 198, "ed": 200, "text": "near optimal"}]
[{"st": 33, "ed": 35, "text": "real life"}, {"st": 60, "ed": 62, "text": "extracted features"}, {"st": 64, "ed": 66, "text": "prediction task"}, {"st": 77, "ed": 80, "text": "massive amounts of"}, {"st": 84, "ed": 86, "text": "big data"}, {"st": 95, "ed": 97, "text": "computational resources"}, {"st": 122, "ed": 124, "text": "dimensionality reduction"}, {"st": 129, "ed": 131, "text": "classification accuracy"}, {"st": 145, "ed": 147, "text": "deep learning"}, {"st": 158, "ed": 160, "text": "feature extraction"}, {"st": 164, "ed": 166, "text": "cloud computing"}, {"st": 190, "ed": 192, "text": "patient specific"}, {"st": 202, "ed": 204, "text": "real life"}]
[{"st": 10, "ed": 12, "text": "domain specific"}, {"st": 17, "ed": 19, "text": "training set"}, {"st": 32, "ed": 34, "text": "visual recognition"}, {"st": 40, "ed": 42, "text": "carefully designed"}, {"st": 97, "ed": 99, "text": "input space"}, {"st": 111, "ed": 114, "text": "unsupervised representation learning"}]
[{"st": 3, "ed": 6, "text": "a long standing"}, {"st": 8, "ed": 10, "text": "efficiently learn"}, {"st": 63, "ed": 65, "text": "frac 1"}, {"st": 70, "ed": 72, "text": "near optimal"}, {"st": 76, "ed": 79, "text": "o left frac"}, {"st": 85, "ed": 87, "text": "frac 1"}, {"st": 92, "ed": 95, "text": "o left frac"}, {"st": 106, "ed": 108, "text": "adversarial noise"}, {"st": 130, "ed": 132, "text": "near optimal"}, {"st": 140, "ed": 142, "text": "frac 1"}, {"st": 147, "ed": 150, "text": "o left frac"}, {"st": 160, "ed": 162, "text": "active learning"}, {"st": 170, "ed": 172, "text": "learning algorithm"}, {"st": 174, "ed": 176, "text": "near optimal"}]
[{"st": 32, "ed": 34, "text": "t sne"}, {"st": 73, "ed": 75, "text": "large datasets"}, {"st": 109, "ed": 111, "text": "mathcal o"}]
[]
[{"st": 4, "ed": 7, "text": "deep neural networks"}, {"st": 31, "ed": 33, "text": "et al"}, {"st": 43, "ed": 45, "text": "deep networks"}, {"st": 63, "ed": 65, "text": "recurrent networks"}, {"st": 71, "ed": 74, "text": "vanishing and exploding"}, {"st": 140, "ed": 142, "text": "higher accuracy"}]
[{"st": 5, "ed": 7, "text": "outlier detection"}, {"st": 64, "ed": 66, "text": "logistic regression"}, {"st": 90, "ed": 92, "text": "high dimensional"}, {"st": 94, "ed": 96, "text": "synthetic experiments"}, {"st": 100, "ed": 102, "text": "proposed algorithm"}, {"st": 106, "ed": 108, "text": "important features"}, {"st": 115, "ed": 117, "text": "proposed algorithm"}, {"st": 119, "ed": 121, "text": "outperform existing"}]
[{"st": 4, "ed": 6, "text": "joint distribution"}, {"st": 8, "ed": 10, "text": "random variables"}, {"st": 22, "ed": 24, "text": "causal direction"}, {"st": 100, "ed": 103, "text": "minimum description length"}, {"st": 142, "ed": 145, "text": "classification and regression"}, {"st": 152, "ed": 154, "text": "np hard"}, {"st": 159, "ed": 161, "text": "greedy algorithm"}, {"st": 166, "ed": 168, "text": "causal direction"}, {"st": 172, "ed": 174, "text": "empirical evaluation"}, {"st": 191, "ed": 193, "text": "causal direction"}, {"st": 198, "ed": 200, "text": "cause effect"}]
[{"st": 11, "ed": 13, "text": "matching pursuit"}, {"st": 14, "ed": 16, "text": "frank wolfe"}, {"st": 35, "ed": 37, "text": "convergence rates"}, {"st": 38, "ed": 40, "text": "matching pursuit"}, {"st": 64, "ed": 66, "text": "linear convergence"}, {"st": 67, "ed": 69, "text": "strongly convex"}]
[{"st": 4, "ed": 6, "text": "sample complexity"}, {"st": 10, "ed": 12, "text": "learning algorithm"}]
[{"st": 0, "ed": 2, "text": "metric learning"}, {"st": 4, "ed": 6, "text": "dimensionality reduction"}, {"st": 9, "ed": 12, "text": "k nearest neighbors"}, {"st": 33, "ed": 35, "text": "training data"}, {"st": 40, "ed": 42, "text": "computational complexity"}, {"st": 48, "ed": 50, "text": "training set"}, {"st": 67, "ed": 69, "text": "training data"}, {"st": 77, "ed": 79, "text": "computational cost"}, {"st": 110, "ed": 112, "text": "embedding model"}, {"st": 116, "ed": 118, "text": "metric learning"}, {"st": 144, "ed": 146, "text": "cost function"}, {"st": 148, "ed": 150, "text": "computational complexity"}, {"st": 157, "ed": 159, "text": "empirically demonstrate"}, {"st": 161, "ed": 163, "text": "benchmark datasets"}, {"st": 169, "ed": 171, "text": "embedding space"}]
[{"st": 7, "ed": 9, "text": "generative model"}, {"st": 11, "ed": 13, "text": "joint distribution"}, {"st": 30, "ed": 32, "text": "reward function"}, {"st": 60, "ed": 62, "text": "latent space"}, {"st": 64, "ed": 66, "text": "generative model"}, {"st": 87, "ed": 90, "text": "deep q networks"}, {"st": 113, "ed": 115, "text": "generative model"}, {"st": 118, "ed": 120, "text": "joint distribution"}, {"st": 138, "ed": 140, "text": "gain insight"}]
[{"st": 5, "ed": 7, "text": "stochastic approximation"}, {"st": 9, "ed": 13, "text": "canonical correlation analysis cca"}, {"st": 41, "ed": 43, "text": "frac 1"}]
[{"st": 5, "ed": 7, "text": "adversarial examples"}, {"st": 8, "ed": 11, "text": "deep generative models"}, {"st": 14, "ed": 16, "text": "variational autoencoder"}, {"st": 21, "ed": 23, "text": "deep learning"}, {"st": 28, "ed": 32, "text": "vulnerable to adversarial examples"}, {"st": 41, "ed": 43, "text": "adversarial examples"}, {"st": 46, "ed": 49, "text": "deep generative models"}, {"st": 59, "ed": 61, "text": "input data"}, {"st": 63, "ed": 65, "text": "generate realistic"}, {"st": 89, "ed": 91, "text": "mnist svhn"}, {"st": 111, "ed": 113, "text": "generative model"}, {"st": 131, "ed": 133, "text": "loss function"}, {"st": 164, "ed": 167, "text": "source and target"}]
[{"st": 14, "ed": 17, "text": "classification and regression"}, {"st": 23, "ed": 25, "text": "hyper parameter"}, {"st": 68, "ed": 70, "text": "command line"}]
[{"st": 0, "ed": 3, "text": "independent component analysis"}, {"st": 10, "ed": 12, "text": "square matrix"}, {"st": 28, "ed": 30, "text": "existing algorithms"}, {"st": 48, "ed": 50, "text": "practical applications"}, {"st": 67, "ed": 69, "text": "heavy tailed"}, {"st": 76, "ed": 78, "text": "random walks"}, {"st": 89, "ed": 91, "text": "main contributions"}, {"st": 100, "ed": 102, "text": "heavy tailed"}, {"st": 108, "ed": 111, "text": "provide theoretical guarantees"}, {"st": 120, "ed": 122, "text": "heavy tailed"}, {"st": 235, "ed": 239, "text": "synthetic and real world"}, {"st": 239, "ed": 241, "text": "heavy tailed"}]
[{"st": 2, "ed": 4, "text": "machine learning"}, {"st": 22, "ed": 24, "text": "multi label"}, {"st": 24, "ed": 26, "text": "multi instance"}, {"st": 27, "ed": 29, "text": "multi view"}, {"st": 47, "ed": 49, "text": "predictive performance"}, {"st": 66, "ed": 69, "text": "deep neural network"}, {"st": 95, "ed": 98, "text": "input and output"}, {"st": 128, "ed": 130, "text": "multi label"}, {"st": 131, "ed": 133, "text": "multi view"}, {"st": 134, "ed": 136, "text": "multi view"}, {"st": 136, "ed": 138, "text": "multi label"}, {"st": 147, "ed": 149, "text": "competitive performance"}]
[{"st": 5, "ed": 7, "text": "off policy"}, {"st": 33, "ed": 35, "text": "policy improvement"}, {"st": 38, "ed": 41, "text": "temporal difference td"}, {"st": 75, "ed": 77, "text": "off policy"}, {"st": 77, "ed": 79, "text": "temporal difference"}, {"st": 150, "ed": 152, "text": "empirical results"}, {"st": 172, "ed": 174, "text": "recently proposed"}]
[{"st": 5, "ed": 7, "text": "recommender systems"}, {"st": 64, "ed": 66, "text": "reinforcement learning"}, {"st": 110, "ed": 112, "text": "recommendation systems"}, {"st": 117, "ed": 119, "text": "hand tuned"}, {"st": 142, "ed": 144, "text": "off line"}, {"st": 144, "ed": 146, "text": "real world"}]
[{"st": 1, "ed": 4, "text": "multi armed bandits"}, {"st": 26, "ed": 28, "text": "decision maker"}, {"st": 43, "ed": 45, "text": "decision maker"}, {"st": 52, "ed": 54, "text": "expected reward"}, {"st": 98, "ed": 100, "text": "expected reward"}, {"st": 118, "ed": 120, "text": "real world"}, {"st": 123, "ed": 125, "text": "online advertising"}]
[{"st": 5, "ed": 7, "text": "theoretical analysis"}, {"st": 8, "ed": 10, "text": "boosting algorithms"}, {"st": 24, "ed": 26, "text": "batch setting"}, {"st": 59, "ed": 61, "text": "boosting algorithm"}, {"st": 67, "ed": 69, "text": "weak learners"}, {"st": 82, "ed": 84, "text": "near optimal"}, {"st": 87, "ed": 89, "text": "excellent performance"}, {"st": 90, "ed": 92, "text": "real data"}]
[{"st": 12, "ed": 14, "text": "causal discovery"}, {"st": 17, "ed": 19, "text": "cause effect"}, {"st": 45, "ed": 47, "text": "cause effect"}, {"st": 101, "ed": 103, "text": "cause effect"}, {"st": 111, "ed": 113, "text": "causal discovery"}, {"st": 120, "ed": 122, "text": "cause effect"}, {"st": 157, "ed": 159, "text": "case study"}, {"st": 195, "ed": 197, "text": "machine learning"}]
[{"st": 9, "ed": 11, "text": "neural networks"}, {"st": 12, "ed": 14, "text": "decision trees"}, {"st": 18, "ed": 20, "text": "decision trees"}, {"st": 28, "ed": 30, "text": "decision tree"}, {"st": 37, "ed": 39, "text": "multilayer perceptron"}, {"st": 74, "ed": 76, "text": "weight sharing"}, {"st": 86, "ed": 88, "text": "neural network"}, {"st": 92, "ed": 94, "text": "multilayer perceptron"}, {"st": 96, "ed": 98, "text": "activation function"}, {"st": 109, "ed": 111, "text": "output units"}, {"st": 123, "ed": 125, "text": "proposed framework"}, {"st": 127, "ed": 129, "text": "global optimization"}, {"st": 155, "ed": 157, "text": "computer vision"}, {"st": 201, "ed": 203, "text": "complex data"}]
[{"st": 23, "ed": 25, "text": "probabilistic model"}, {"st": 88, "ed": 90, "text": "dynamic programming"}, {"st": 98, "ed": 100, "text": "without resorting"}, {"st": 111, "ed": 113, "text": "speech recognition"}, {"st": 117, "ed": 119, "text": "quantitative results"}]
[{"st": 5, "ed": 7, "text": "increasingly important"}, {"st": 21, "ed": 24, "text": "support vector machine"}, {"st": 32, "ed": 34, "text": "learning rates"}, {"st": 39, "ed": 42, "text": "a logarithmic factor"}, {"st": 60, "ed": 62, "text": "special case"}, {"st": 63, "ed": 65, "text": "learning rates"}, {"st": 71, "ed": 73, "text": "kernel based"}, {"st": 73, "ed": 75, "text": "least squares"}, {"st": 83, "ed": 85, "text": "statistical analysis"}, {"st": 93, "ed": 95, "text": "least squares"}]
[{"st": 0, "ed": 2, "text": "machine learning"}, {"st": 23, "ed": 25, "text": "computer vision"}, {"st": 67, "ed": 70, "text": "mixture of experts"}]
[{"st": 1, "ed": 3, "text": "activation functions"}, {"st": 25, "ed": 27, "text": "activation functions"}, {"st": 29, "ed": 31, "text": "neural network"}, {"st": 48, "ed": 50, "text": "activation functions"}, {"st": 72, "ed": 74, "text": "activation functions"}, {"st": 121, "ed": 123, "text": "feed forward"}, {"st": 153, "ed": 155, "text": "activation functions"}]
[{"st": 21, "ed": 24, "text": "click through rate"}, {"st": 37, "ed": 40, "text": "learning to rank"}, {"st": 49, "ed": 52, "text": "learning to rank"}, {"st": 60, "ed": 62, "text": "training data"}, {"st": 110, "ed": 113, "text": "learning to rank"}, {"st": 115, "ed": 117, "text": "maximum likelihood"}, {"st": 126, "ed": 128, "text": "machine learning"}, {"st": 145, "ed": 147, "text": "generic framework"}, {"st": 167, "ed": 169, "text": "inference procedures"}, {"st": 186, "ed": 188, "text": "real life"}]
[{"st": 31, "ed": 33, "text": "computational complexity"}, {"st": 80, "ed": 83, "text": "method of moments"}, {"st": 86, "ed": 89, "text": "provide sufficient conditions"}, {"st": 100, "ed": 102, "text": "empirical results"}, {"st": 104, "ed": 107, "text": "simulated and real"}]
[{"st": 4, "ed": 6, "text": "active learning"}, {"st": 11, "ed": 14, "text": "generative adversarial networks"}, {"st": 19, "ed": 21, "text": "active learning"}, {"st": 26, "ed": 28, "text": "training instances"}, {"st": 40, "ed": 42, "text": "uncertainty principle"}, {"st": 49, "ed": 51, "text": "active learning"}, {"st": 57, "ed": 59, "text": "numerical experiments"}, {"st": 70, "ed": 72, "text": "proposed algorithm"}, {"st": 86, "ed": 88, "text": "active learning"}]
[{"st": 10, "ed": 12, "text": "frac 1"}, {"st": 13, "ed": 15, "text": "sqrt t"}, {"st": 23, "ed": 25, "text": "regret bound"}, {"st": 33, "ed": 35, "text": "loss functions"}, {"st": 53, "ed": 55, "text": "loss functions"}, {"st": 57, "ed": 59, "text": "hinge loss"}, {"st": 63, "ed": 65, "text": "hinge loss"}, {"st": 74, "ed": 76, "text": "open problem"}, {"st": 84, "ed": 86, "text": "bandit algorithm"}, {"st": 87, "ed": 89, "text": "sqrt t"}]
[{"st": 11, "ed": 13, "text": "big data"}, {"st": 56, "ed": 58, "text": "existing methods"}, {"st": 90, "ed": 92, "text": "based method"}, {"st": 109, "ed": 111, "text": "classification task"}, {"st": 131, "ed": 134, "text": "human activity recognition"}, {"st": 139, "ed": 141, "text": "significantly outperforms"}]
[{"st": 11, "ed": 14, "text": "online learning algorithm"}, {"st": 18, "ed": 20, "text": "bandit problems"}, {"st": 38, "ed": 40, "text": "mathbb r"}, {"st": 79, "ed": 81, "text": "expected reward"}, {"st": 115, "ed": 117, "text": "non stationary"}, {"st": 117, "ed": 119, "text": "bandit problem"}, {"st": 135, "ed": 137, "text": "step size"}, {"st": 142, "ed": 144, "text": "sliding window"}, {"st": 171, "ed": 173, "text": "learning rates"}]
[{"st": 9, "ed": 12, "text": "support vector machine"}, {"st": 17, "ed": 19, "text": "sample size"}]
[{"st": 0, "ed": 4, "text": "generative adversarial networks gans"}, {"st": 13, "ed": 15, "text": "generative models"}, {"st": 51, "ed": 53, "text": "variational inference"}, {"st": 88, "ed": 90, "text": "existing algorithms"}, {"st": 93, "ed": 95, "text": "variational autoencoders"}, {"st": 102, "ed": 104, "text": "image reconstruction"}, {"st": 136, "ed": 138, "text": "inference algorithms"}]
[{"st": 10, "ed": 12, "text": "expected loss"}, {"st": 52, "ed": 54, "text": "loss functions"}, {"st": 69, "ed": 72, "text": "k means clustering"}, {"st": 90, "ed": 92, "text": "mathcal o"}, {"st": 101, "ed": 103, "text": "mathcal o"}]
[{"st": 4, "ed": 6, "text": "approximate inference"}, {"st": 60, "ed": 62, "text": "generic framework"}, {"st": 63, "ed": 65, "text": "approximate inference"}, {"st": 69, "ed": 71, "text": "highly complex"}, {"st": 98, "ed": 101, "text": "deep generative models"}, {"st": 108, "ed": 110, "text": "deep models"}, {"st": 117, "ed": 119, "text": "generate realistic"}]
[{"st": 9, "ed": 11, "text": "text data"}, {"st": 17, "ed": 19, "text": "individual words"}, {"st": 40, "ed": 42, "text": "embedding vectors"}, {"st": 56, "ed": 58, "text": "embedding vectors"}, {"st": 71, "ed": 73, "text": "variational inference"}, {"st": 74, "ed": 76, "text": "skip gram"}, {"st": 78, "ed": 80, "text": "skip gram"}, {"st": 118, "ed": 120, "text": "word embedding"}, {"st": 132, "ed": 134, "text": "competing methods"}]
[{"st": 0, "ed": 3, "text": "deep neural networks"}, {"st": 15, "ed": 17, "text": "supervised learning"}, {"st": 18, "ed": 20, "text": "generative models"}, {"st": 28, "ed": 30, "text": "hierarchical models"}, {"st": 31, "ed": 33, "text": "multiple layers"}, {"st": 42, "ed": 44, "text": "hierarchical latent"}, {"st": 52, "ed": 54, "text": "hierarchical structure"}, {"st": 58, "ed": 60, "text": "variational methods"}, {"st": 69, "ed": 71, "text": "existing models"}, {"st": 100, "ed": 102, "text": "natural image"}, {"st": 105, "ed": 107, "text": "task specific"}]
[{"st": 11, "ed": 14, "text": "generative adversarial networks"}, {"st": 28, "ed": 30, "text": "finite dimensional"}]
[{"st": 0, "ed": 4, "text": "generative adversarial networks gans"}, {"st": 6, "ed": 8, "text": "learning framework"}, {"st": 33, "ed": 35, "text": "generated samples"}, {"st": 58, "ed": 60, "text": "discrete data"}, {"st": 74, "ed": 76, "text": "generated samples"}, {"st": 79, "ed": 81, "text": "policy gradient"}, {"st": 94, "ed": 96, "text": "decision boundary"}, {"st": 115, "ed": 117, "text": "proposed algorithm"}, {"st": 123, "ed": 125, "text": "natural language"}, {"st": 134, "ed": 136, "text": "continuous data"}, {"st": 151, "ed": 153, "text": "large scale"}, {"st": 153, "ed": 155, "text": "scene understanding"}]
[{"st": 3, "ed": 5, "text": "network structure"}, {"st": 13, "ed": 15, "text": "causal models"}, {"st": 18, "ed": 20, "text": "gaussian process"}, {"st": 31, "ed": 33, "text": "weight matrices"}, {"st": 53, "ed": 55, "text": "uncertainty quantification"}, {"st": 68, "ed": 70, "text": "multi task"}, {"st": 75, "ed": 78, "text": "stochastic variational inference"}, {"st": 106, "ed": 108, "text": "random variables"}, {"st": 120, "ed": 122, "text": "outperforms previous"}, {"st": 126, "ed": 129, "text": "qualitative and quantitative"}]
[{"st": 60, "ed": 62, "text": "latent variable"}, {"st": 82, "ed": 85, "text": "orders of magnitude"}, {"st": 99, "ed": 101, "text": "performance gains"}, {"st": 109, "ed": 111, "text": "probability distributions"}, {"st": 123, "ed": 125, "text": "computationally efficient"}, {"st": 141, "ed": 143, "text": "new york"}]
[{"st": 1, "ed": 3, "text": "active learning"}, {"st": 8, "ed": 10, "text": "based approaches"}, {"st": 19, "ed": 21, "text": "training set"}, {"st": 42, "ed": 44, "text": "true label"}, {"st": 114, "ed": 116, "text": "extensive experiments"}, {"st": 118, "ed": 120, "text": "wide variety"}, {"st": 121, "ed": 123, "text": "real world"}, {"st": 132, "ed": 134, "text": "proposed method"}, {"st": 144, "ed": 146, "text": "real life"}]
[{"st": 10, "ed": 12, "text": "active learning"}, {"st": 50, "ed": 52, "text": "empirically demonstrate"}]
[{"st": 9, "ed": 11, "text": "transition matrix"}, {"st": 86, "ed": 88, "text": "transition matrix"}, {"st": 182, "ed": 187, "text": "synthetic and real world datasets"}]
[{"st": 6, "ed": 8, "text": "latent variable"}, {"st": 9, "ed": 11, "text": "graphical model"}, {"st": 14, "ed": 16, "text": "precision matrix"}, {"st": 21, "ed": 23, "text": "sparse matrix"}, {"st": 25, "ed": 27, "text": "low rank"}, {"st": 39, "ed": 41, "text": "low rank"}, {"st": 47, "ed": 50, "text": "maximum likelihood estimator"}, {"st": 52, "ed": 54, "text": "matrix factorization"}, {"st": 70, "ed": 73, "text": "orders of magnitude"}, {"st": 76, "ed": 78, "text": "convex relaxation"}, {"st": 78, "ed": 80, "text": "based methods"}, {"st": 99, "ed": 101, "text": "low rank"}]
[{"st": 6, "ed": 8, "text": "optimization criteria"}, {"st": 9, "ed": 11, "text": "variational auto"}, {"st": 31, "ed": 33, "text": "latent features"}, {"st": 45, "ed": 47, "text": "latent features"}, {"st": 74, "ed": 76, "text": "image dataset"}, {"st": 89, "ed": 91, "text": "unsupervised learning"}]
[{"st": 8, "ed": 10, "text": "learning algorithms"}, {"st": 25, "ed": 27, "text": "learning algorithm"}, {"st": 29, "ed": 31, "text": "normed space"}, {"st": 39, "ed": 41, "text": "main result"}, {"st": 46, "ed": 48, "text": "generalization error"}, {"st": 50, "ed": 52, "text": "learning algorithm"}, {"st": 67, "ed": 69, "text": "banach space"}, {"st": 85, "ed": 87, "text": "learning algorithms"}, {"st": 89, "ed": 92, "text": "empirical risk minimization"}]
[{"st": 8, "ed": 10, "text": "machine learning"}, {"st": 32, "ed": 35, "text": "deep neural networks"}, {"st": 45, "ed": 47, "text": "non linearity"}, {"st": 50, "ed": 53, "text": "scale to large"}, {"st": 89, "ed": 91, "text": "method called"}, {"st": 134, "ed": 137, "text": "end to end"}, {"st": 155, "ed": 157, "text": "extensive experiments"}, {"st": 158, "ed": 160, "text": "benchmark datasets"}]
[{"st": 3, "ed": 5, "text": "domain invariant"}, {"st": 10, "ed": 12, "text": "domain adaptation"}, {"st": 13, "ed": 15, "text": "neural networks"}, {"st": 28, "ed": 30, "text": "domain specific"}, {"st": 30, "ed": 32, "text": "latent feature"}, {"st": 42, "ed": 44, "text": "distribution matching"}, {"st": 59, "ed": 63, "text": "maximum mean discrepancy mmd"}, {"st": 69, "ed": 71, "text": "higher order"}, {"st": 82, "ed": 84, "text": "higher order"}, {"st": 87, "ed": 89, "text": "probability distributions"}, {"st": 101, "ed": 103, "text": "computationally expensive"}, {"st": 105, "ed": 107, "text": "kernel matrix"}, {"st": 114, "ed": 116, "text": "probability distributions"}, {"st": 123, "ed": 125, "text": "distance function"}, {"st": 126, "ed": 128, "text": "central moment"}, {"st": 142, "ed": 144, "text": "probability distributions"}, {"st": 154, "ed": 156, "text": "probability distributions"}, {"st": 179, "ed": 182, "text": "benchmark data sets"}, {"st": 183, "ed": 185, "text": "object recognition"}, {"st": 187, "ed": 189, "text": "sentiment analysis"}, {"st": 206, "ed": 208, "text": "domain adaptation"}, {"st": 221, "ed": 223, "text": "domain adversarial"}, {"st": 223, "ed": 225, "text": "neural networks"}, {"st": 234, "ed": 236, "text": "sensitivity analysis"}, {"st": 251, "ed": 253, "text": "source code"}]
[{"st": 7, "ed": 9, "text": "decision tree"}, {"st": 9, "ed": 11, "text": "ensemble approach"}, {"st": 13, "ed": 15, "text": "highly competitive"}, {"st": 22, "ed": 25, "text": "deep neural networks"}, {"st": 30, "ed": 33, "text": "hyper parameter tuning"}, {"st": 51, "ed": 53, "text": "excellent performance"}, {"st": 64, "ed": 66, "text": "training process"}, {"st": 87, "ed": 90, "text": "deep neural networks"}, {"st": 114, "ed": 117, "text": "deep neural networks"}, {"st": 119, "ed": 121, "text": "large scale"}, {"st": 121, "ed": 123, "text": "training data"}, {"st": 132, "ed": 134, "text": "small scale"}, {"st": 139, "ed": 141, "text": "tree based"}, {"st": 147, "ed": 149, "text": "theoretical analysis"}]
[{"st": 8, "ed": 10, "text": "human intelligence"}, {"st": 11, "ed": 13, "text": "low cost"}, {"st": 56, "ed": 58, "text": "classification tasks"}, {"st": 89, "ed": 91, "text": "low rank"}, {"st": 108, "ed": 110, "text": "probabilistic model"}, {"st": 131, "ed": 133, "text": "message passing"}, {"st": 141, "ed": 143, "text": "belief propagation"}, {"st": 155, "ed": 157, "text": "near optimal"}, {"st": 157, "ed": 160, "text": "mean squared error"}, {"st": 177, "ed": 179, "text": "proposed algorithm"}, {"st": 187, "ed": 189, "text": "synthetic experiments"}, {"st": 205, "ed": 207, "text": "visual object"}, {"st": 222, "ed": 224, "text": "significantly improve"}]
[{"st": 20, "ed": 22, "text": "machine learning"}, {"st": 64, "ed": 66, "text": "input space"}, {"st": 75, "ed": 77, "text": "existing techniques"}, {"st": 124, "ed": 126, "text": "validation set"}, {"st": 130, "ed": 132, "text": "poor performance"}, {"st": 134, "ed": 136, "text": "local minima"}]
[{"st": 9, "ed": 12, "text": "number of clusters"}, {"st": 17, "ed": 19, "text": "k means"}, {"st": 27, "ed": 30, "text": "number of clusters"}, {"st": 34, "ed": 36, "text": "description length"}, {"st": 44, "ed": 46, "text": "compression ratio"}, {"st": 53, "ed": 55, "text": "description length"}, {"st": 71, "ed": 73, "text": "k means"}, {"st": 75, "ed": 77, "text": "hierarchical structure"}, {"st": 80, "ed": 82, "text": "multi stage"}, {"st": 96, "ed": 99, "text": "number of clusters"}, {"st": 100, "ed": 102, "text": "synthetic data"}, {"st": 138, "ed": 140, "text": "clustering results"}, {"st": 148, "ed": 150, "text": "numerical results"}, {"st": 166, "ed": 168, "text": "description length"}, {"st": 178, "ed": 181, "text": "number of clusters"}]
[{"st": 6, "ed": 8, "text": "increasing attention"}, {"st": 10, "ed": 12, "text": "data mining"}, {"st": 13, "ed": 15, "text": "machine learning"}, {"st": 30, "ed": 32, "text": "training data"}, {"st": 45, "ed": 47, "text": "theoretical guarantee"}, {"st": 81, "ed": 83, "text": "training data"}, {"st": 91, "ed": 93, "text": "causal model"}, {"st": 115, "ed": 117, "text": "theoretical results"}]
[{"st": 22, "ed": 25, "text": "efficiency and scalability"}, {"st": 60, "ed": 62, "text": "np hard"}, {"st": 81, "ed": 83, "text": "ell 2"}, {"st": 84, "ed": 86, "text": "loss function"}, {"st": 111, "ed": 113, "text": "np hard"}, {"st": 147, "ed": 149, "text": "sparse recovery"}, {"st": 185, "ed": 187, "text": "large scale"}, {"st": 189, "ed": 191, "text": "numerical results"}, {"st": 211, "ed": 214, "text": "accuracy and computational"}]
[{"st": 1, "ed": 3, "text": "neural networks"}, {"st": 10, "ed": 12, "text": "applications including"}, {"st": 12, "ed": 14, "text": "image processing"}, {"st": 14, "ed": 16, "text": "speech recognition"}, {"st": 16, "ed": 18, "text": "natural language"}, {"st": 38, "ed": 40, "text": "neural network"}, {"st": 42, "ed": 44, "text": "internal representation"}, {"st": 66, "ed": 68, "text": "neural networks"}, {"st": 93, "ed": 95, "text": "network analysis"}, {"st": 96, "ed": 98, "text": "proposed method"}, {"st": 126, "ed": 129, "text": "trained neural network"}, {"st": 157, "ed": 159, "text": "randomly chosen"}]
[{"st": 2, "ed": 5, "text": "ability to capture"}, {"st": 5, "ed": 7, "text": "non linearities"}, {"st": 12, "ed": 15, "text": "scale to large"}, {"st": 15, "ed": 17, "text": "training sets"}, {"st": 18, "ed": 21, "text": "support vector machines"}, {"st": 40, "ed": 42, "text": "method called"}, {"st": 48, "ed": 50, "text": "input space"}, {"st": 52, "ed": 54, "text": "dimensionality reduction"}, {"st": 61, "ed": 63, "text": "jointly learns"}, {"st": 64, "ed": 66, "text": "linear combination"}, {"st": 69, "ed": 72, "text": "simple and effective"}, {"st": 111, "ed": 113, "text": "randomly selected"}]
[{"st": 0, "ed": 3, "text": "deep neural networks"}, {"st": 24, "ed": 27, "text": "vulnerable to adversarial"}, {"st": 47, "ed": 49, "text": "adversarial samples"}, {"st": 60, "ed": 62, "text": "adversarial samples"}, {"st": 66, "ed": 68, "text": "uncertainty estimates"}, {"st": 71, "ed": 73, "text": "neural networks"}, {"st": 82, "ed": 84, "text": "deep features"}, {"st": 112, "ed": 114, "text": "standard datasets"}, {"st": 115, "ed": 119, "text": "mnist and cifar 10"}, {"st": 145, "ed": 147, "text": "standard classification"}]
[{"st": 0, "ed": 2, "text": "recommendation systems"}, {"st": 34, "ed": 37, "text": "multi armed bandit"}, {"st": 103, "ed": 105, "text": "recommendation systems"}]
[{"st": 3, "ed": 6, "text": "sequential decision making"}, {"st": 18, "ed": 20, "text": "linear models"}, {"st": 34, "ed": 36, "text": "decision maker"}, {"st": 41, "ed": 43, "text": "linear models"}, {"st": 51, "ed": 54, "text": "corrupted by noise"}, {"st": 75, "ed": 77, "text": "noise levels"}, {"st": 84, "ed": 86, "text": "linear functions"}, {"st": 90, "ed": 92, "text": "simple regret"}, {"st": 114, "ed": 116, "text": "linear models"}, {"st": 133, "ed": 135, "text": "real data"}]
[{"st": 38, "ed": 40, "text": "training data"}, {"st": 84, "ed": 87, "text": "deep neural networks"}, {"st": 98, "ed": 101, "text": "mean squared error"}, {"st": 110, "ed": 112, "text": "estimation error"}, {"st": 115, "ed": 117, "text": "empirical risk"}, {"st": 118, "ed": 120, "text": "experiments demonstrate"}]
[{"st": 2, "ed": 4, "text": "kernel methods"}, {"st": 13, "ed": 15, "text": "explicit feature"}, {"st": 20, "ed": 22, "text": "large scale"}, {"st": 26, "ed": 28, "text": "explicit feature"}, {"st": 40, "ed": 43, "text": "real world data"}, {"st": 48, "ed": 50, "text": "complex data"}, {"st": 86, "ed": 88, "text": "real valued"}, {"st": 93, "ed": 95, "text": "explicit feature"}, {"st": 113, "ed": 115, "text": "base kernels"}, {"st": 139, "ed": 141, "text": "explicit feature"}, {"st": 152, "ed": 154, "text": "feature maps"}, {"st": 155, "ed": 157, "text": "random walk"}, {"st": 165, "ed": 167, "text": "real world"}, {"st": 173, "ed": 175, "text": "theoretical results"}, {"st": 181, "ed": 183, "text": "phase transition"}, {"st": 202, "ed": 204, "text": "explicit feature"}, {"st": 212, "ed": 214, "text": "real valued"}, {"st": 223, "ed": 225, "text": "extensive experiments"}, {"st": 233, "ed": 235, "text": "classification accuracy"}]
[{"st": 0, "ed": 2, "text": "neural networks"}, {"st": 4, "ed": 6, "text": "successfully applied"}, {"st": 26, "ed": 28, "text": "training data"}, {"st": 32, "ed": 34, "text": "previously learned"}, {"st": 41, "ed": 43, "text": "neural network"}, {"st": 51, "ed": 53, "text": "meta learning"}, {"st": 68, "ed": 70, "text": "inductive biases"}, {"st": 96, "ed": 98, "text": "baseline approaches"}]
[{"st": 15, "ed": 17, "text": "statistical analysis"}, {"st": 18, "ed": 20, "text": "machine learning"}, {"st": 26, "ed": 28, "text": "detailed analysis"}, {"st": 40, "ed": 42, "text": "least squares"}, {"st": 44, "ed": 46, "text": "ridge regression"}, {"st": 69, "ed": 71, "text": "theoretical results"}, {"st": 110, "ed": 112, "text": "without compromising"}]
[{"st": 1, "ed": 4, "text": "labeled training data"}, {"st": 18, "ed": 20, "text": "generative models"}, {"st": 26, "ed": 28, "text": "weak supervision"}, {"st": 30, "ed": 32, "text": "generative model"}, {"st": 33, "ed": 35, "text": "dependency structure"}, {"st": 50, "ed": 52, "text": "labeled data"}, {"st": 65, "ed": 67, "text": "ell 1"}, {"st": 75, "ed": 77, "text": "analysis shows"}, {"st": 81, "ed": 83, "text": "unlabeled data"}, {"st": 110, "ed": 113, "text": "times faster than"}, {"st": 114, "ed": 116, "text": "maximum likelihood"}, {"st": 148, "ed": 151, "text": "real world data"}]
[{"st": 3, "ed": 5, "text": "self paced"}, {"st": 8, "ed": 10, "text": "multitask learning"}, {"st": 14, "ed": 16, "text": "closely related"}, {"st": 74, "ed": 76, "text": "proposed method"}, {"st": 82, "ed": 84, "text": "feature learning"}, {"st": 84, "ed": 86, "text": "multitask learning"}, {"st": 100, "ed": 102, "text": "self paced"}]
[{"st": 7, "ed": 9, "text": "classification algorithm"}, {"st": 25, "ed": 27, "text": "feature space"}, {"st": 68, "ed": 70, "text": "optimization problem"}, {"st": 97, "ed": 99, "text": "continuous space"}, {"st": 108, "ed": 110, "text": "linear transformation"}, {"st": 126, "ed": 128, "text": "classification methods"}, {"st": 129, "ed": 131, "text": "naive bayes"}, {"st": 131, "ed": 134, "text": "support vector machines"}, {"st": 134, "ed": 137, "text": "linear discriminant analysis"}, {"st": 137, "ed": 139, "text": "multi layer"}, {"st": 140, "ed": 142, "text": "decision trees"}, {"st": 143, "ed": 146, "text": "k nearest neighbors"}, {"st": 148, "ed": 150, "text": "standard classification"}, {"st": 181, "ed": 183, "text": "classification methods"}]
[{"st": 5, "ed": 7, "text": "multitask learning"}, {"st": 11, "ed": 13, "text": "shared representation"}, {"st": 47, "ed": 49, "text": "multitask learning"}, {"st": 73, "ed": 75, "text": "highly scalable"}, {"st": 75, "ed": 77, "text": "multitask learning"}, {"st": 84, "ed": 86, "text": "conjugate gradient"}, {"st": 98, "ed": 100, "text": "benchmark datasets"}, {"st": 103, "ed": 105, "text": "proposed method"}, {"st": 112, "ed": 114, "text": "multitask learning"}]
[{"st": 3, "ed": 5, "text": "active learning"}, {"st": 7, "ed": 9, "text": "cost sensitive"}, {"st": 10, "ed": 12, "text": "classification problems"}, {"st": 84, "ed": 86, "text": "squared loss"}, {"st": 95, "ed": 97, "text": "predictive performance"}, {"st": 109, "ed": 111, "text": "active learning"}, {"st": 113, "ed": 115, "text": "significant improvements"}, {"st": 122, "ed": 124, "text": "real world"}]
[{"st": 2, "ed": 5, "text": "directed acyclic graph"}, {"st": 12, "ed": 14, "text": "observational data"}, {"st": 16, "ed": 18, "text": "notoriously difficult"}, {"st": 61, "ed": 63, "text": "observational data"}, {"st": 109, "ed": 111, "text": "theoretical guarantees"}, {"st": 133, "ed": 135, "text": "conditional independence"}, {"st": 141, "ed": 143, "text": "sample complexity"}]
[{"st": 0, "ed": 3, "text": "recurrent neural networks"}, {"st": 8, "ed": 10, "text": "statistical model"}, {"st": 39, "ed": 41, "text": "rnn architectures"}, {"st": 63, "ed": 65, "text": "rnn model"}, {"st": 69, "ed": 71, "text": "weighted average"}, {"st": 88, "ed": 90, "text": "computational overhead"}, {"st": 103, "ed": 105, "text": "attention mechanism"}]
[{"st": 6, "ed": 8, "text": "input space"}, {"st": 10, "ed": 12, "text": "output space"}, {"st": 20, "ed": 22, "text": "training data"}, {"st": 35, "ed": 37, "text": "domain shift"}, {"st": 55, "ed": 57, "text": "unlabeled samples"}, {"st": 69, "ed": 71, "text": "unlabeled samples"}, {"st": 103, "ed": 105, "text": "unlabeled samples"}, {"st": 113, "ed": 115, "text": "output spaces"}, {"st": 117, "ed": 119, "text": "domain transfer"}, {"st": 129, "ed": 131, "text": "generalization bounds"}]
[{"st": 3, "ed": 6, "text": "multi armed bandit"}, {"st": 29, "ed": 31, "text": "regret bounds"}, {"st": 34, "ed": 36, "text": "exponentially large"}, {"st": 106, "ed": 108, "text": "regret bounds"}, {"st": 111, "ed": 113, "text": "regret bounds"}]
[{"st": 1, "ed": 5, "text": "markov chain monte carlo"}, {"st": 62, "ed": 64, "text": "closed form"}]
[{"st": 3, "ed": 5, "text": "least squares"}, {"st": 13, "ed": 16, "text": "easy to implement"}, {"st": 21, "ed": 23, "text": "local optima"}, {"st": 55, "ed": 57, "text": "random initialization"}, {"st": 85, "ed": 87, "text": "synthetic data"}, {"st": 105, "ed": 107, "text": "word embeddings"}]
[{"st": 5, "ed": 7, "text": "neural networks"}, {"st": 9, "ed": 11, "text": "random variables"}, {"st": 14, "ed": 16, "text": "approximate posterior"}, {"st": 42, "ed": 44, "text": "normalizing flows"}, {"st": 66, "ed": 68, "text": "significantly improve"}, {"st": 70, "ed": 72, "text": "mean field"}, {"st": 74, "ed": 76, "text": "neural networks"}, {"st": 78, "ed": 80, "text": "predictive accuracy"}]
[{"st": 0, "ed": 3, "text": "deep reinforcement learning"}, {"st": 38, "ed": 41, "text": "deep reinforcement learning"}, {"st": 93, "ed": 95, "text": "agent learns"}, {"st": 95, "ed": 97, "text": "significantly faster"}, {"st": 105, "ed": 108, "text": "deep reinforcement learning"}]
[{"st": 0, "ed": 2, "text": "off policy"}, {"st": 11, "ed": 13, "text": "policy gradient"}, {"st": 25, "ed": 27, "text": "optimal policy"}, {"st": 42, "ed": 44, "text": "policy improvement"}, {"st": 60, "ed": 62, "text": "natural gradient"}, {"st": 72, "ed": 74, "text": "deterministic policy"}, {"st": 82, "ed": 84, "text": "off policy"}, {"st": 100, "ed": 102, "text": "policy gradient"}]
[{"st": 117, "ed": 119, "text": "training data"}, {"st": 128, "ed": 130, "text": "maximum likelihood"}, {"st": 146, "ed": 148, "text": "maximum likelihood"}, {"st": 155, "ed": 158, "text": "gaussian mixture model"}, {"st": 195, "ed": 198, "text": "classification and clustering"}, {"st": 203, "ed": 207, "text": "synthetic and real world"}, {"st": 215, "ed": 219, "text": "available at https github.com"}]
[{"st": 1, "ed": 3, "text": "multiple instance"}, {"st": 38, "ed": 40, "text": "point process"}, {"st": 49, "ed": 51, "text": "point process"}, {"st": 64, "ed": 66, "text": "novelty detection"}, {"st": 84, "ed": 86, "text": "decision making"}]
[{"st": 61, "ed": 64, "text": "local and global"}, {"st": 78, "ed": 80, "text": "kernel based"}, {"st": 131, "ed": 133, "text": "theoretical results"}, {"st": 137, "ed": 139, "text": "graph classification"}]
[{"st": 3, "ed": 5, "text": "theoretical insights"}, {"st": 6, "ed": 8, "text": "structured prediction"}, {"st": 13, "ed": 15, "text": "convex surrogate"}, {"st": 15, "ed": 17, "text": "loss minimization"}, {"st": 27, "ed": 29, "text": "convex surrogate"}, {"st": 34, "ed": 37, "text": "stochastic gradient descent"}]
[{"st": 5, "ed": 7, "text": "machine learning"}, {"st": 25, "ed": 27, "text": "spatio temporal"}, {"st": 52, "ed": 54, "text": "decision tree"}, {"st": 57, "ed": 59, "text": "decision trees"}, {"st": 59, "ed": 61, "text": "random forest"}, {"st": 61, "ed": 64, "text": "boosted decision trees"}, {"st": 65, "ed": 68, "text": "artificial neural network"}, {"st": 81, "ed": 83, "text": "square root"}, {"st": 83, "ed": 86, "text": "mean square error"}, {"st": 104, "ed": 106, "text": "case study"}, {"st": 132, "ed": 134, "text": "spatio temporal"}, {"st": 169, "ed": 172, "text": "boosted decision trees"}, {"st": 175, "ed": 177, "text": "prediction accuracy"}, {"st": 188, "ed": 191, "text": "artificial neural network"}, {"st": 192, "ed": 194, "text": "random forest"}, {"st": 196, "ed": 198, "text": "decision trees"}, {"st": 201, "ed": 203, "text": "decision tree"}]
[{"st": 26, "ed": 28, "text": "mathcal o"}, {"st": 38, "ed": 40, "text": "mathcal o"}, {"st": 57, "ed": 59, "text": "sparse matrix"}, {"st": 70, "ed": 72, "text": "least square"}, {"st": 115, "ed": 117, "text": "impressive performance"}]
[{"st": 0, "ed": 2, "text": "online learning"}, {"st": 16, "ed": 18, "text": "efficient algorithms"}, {"st": 20, "ed": 22, "text": "recently proposed"}, {"st": 68, "ed": 70, "text": "convergence guarantees"}, {"st": 81, "ed": 83, "text": "online learning"}, {"st": 128, "ed": 130, "text": "web search"}]
[{"st": 14, "ed": 17, "text": "the research community"}, {"st": 28, "ed": 31, "text": "generative adversarial networks"}, {"st": 175, "ed": 177, "text": "log likelihood"}, {"st": 200, "ed": 202, "text": "optimal solution"}]
[{"st": 42, "ed": 44, "text": "side information"}, {"st": 52, "ed": 54, "text": "feature selection"}, {"st": 62, "ed": 64, "text": "side information"}, {"st": 100, "ed": 102, "text": "side information"}, {"st": 121, "ed": 123, "text": "learned models"}, {"st": 147, "ed": 149, "text": "benchmark datasets"}, {"st": 152, "ed": 154, "text": "predictive performance"}]
[{"st": 3, "ed": 6, "text": "online convex optimization"}, {"st": 10, "ed": 12, "text": "optimal regret"}, {"st": 17, "ed": 19, "text": "prior knowledge"}, {"st": 39, "ed": 41, "text": "existing algorithms"}, {"st": 48, "ed": 50, "text": "loss functions"}, {"st": 77, "ed": 79, "text": "demonstrate empirically"}, {"st": 83, "ed": 85, "text": "optimization algorithms"}]
[{"st": 6, "ed": 9, "text": "online learning algorithms"}, {"st": 12, "ed": 14, "text": "prior information"}]
[{"st": 56, "ed": 58, "text": "low complexity"}, {"st": 77, "ed": 79, "text": "error correcting"}, {"st": 89, "ed": 91, "text": "error correction"}]
[{"st": 48, "ed": 50, "text": "nearest neighbor"}, {"st": 58, "ed": 60, "text": "recognition performance"}, {"st": 67, "ed": 70, "text": "ability to capture"}, {"st": 73, "ed": 75, "text": "proposed method"}, {"st": 84, "ed": 86, "text": "decision trees"}, {"st": 188, "ed": 190, "text": "proposed method"}, {"st": 191, "ed": 193, "text": "impressive results"}, {"st": 194, "ed": 196, "text": "outperforms existing"}, {"st": 201, "ed": 203, "text": "graph database"}]
[{"st": 2, "ed": 4, "text": "uncertainty estimates"}, {"st": 5, "ed": 7, "text": "real world"}, {"st": 8, "ed": 10, "text": "deep learning"}, {"st": 17, "ed": 19, "text": "variational inference"}, {"st": 26, "ed": 28, "text": "machine vision"}, {"st": 65, "ed": 67, "text": "existing techniques"}, {"st": 75, "ed": 77, "text": "existing models"}, {"st": 112, "ed": 114, "text": "existing models"}, {"st": 125, "ed": 127, "text": "uncertainty estimates"}, {"st": 148, "ed": 150, "text": "adversarial images"}, {"st": 158, "ed": 160, "text": "adversarial images"}]
[{"st": 8, "ed": 10, "text": "labeled data"}, {"st": 46, "ed": 48, "text": "accurately estimate"}, {"st": 115, "ed": 117, "text": "principal components"}, {"st": 123, "ed": 125, "text": "provide theoretical"}, {"st": 132, "ed": 134, "text": "improved accuracy"}]
[{"st": 11, "ed": 13, "text": "prediction tasks"}, {"st": 45, "ed": 47, "text": "graph based"}, {"st": 50, "ed": 52, "text": "pairwise similarities"}, {"st": 70, "ed": 72, "text": "feature vectors"}, {"st": 95, "ed": 97, "text": "convolutional networks"}, {"st": 124, "ed": 126, "text": "feature vectors"}, {"st": 178, "ed": 181, "text": "proof of concept"}, {"st": 186, "ed": 188, "text": "contextual information"}, {"st": 229, "ed": 231, "text": "linear classifiers"}]
[{"st": 85, "ed": 87, "text": "global optimization"}, {"st": 97, "ed": 99, "text": "genetic algorithms"}, {"st": 117, "ed": 119, "text": "statistically significant"}]
[{"st": 9, "ed": 11, "text": "sparse signal"}, {"st": 11, "ed": 13, "text": "recovery problem"}, {"st": 19, "ed": 21, "text": "message passing"}, {"st": 23, "ed": 26, "text": "expectation maximization em"}, {"st": 55, "ed": 58, "text": "guaranteed to converge"}]
[{"st": 1, "ed": 3, "text": "reinforcement learning"}, {"st": 7, "ed": 9, "text": "real world"}, {"st": 31, "ed": 33, "text": "feature selection"}, {"st": 50, "ed": 53, "text": "markov decision process"}, {"st": 56, "ed": 59, "text": "under mild assumptions"}, {"st": 60, "ed": 62, "text": "sample complexity"}, {"st": 90, "ed": 92, "text": "sample complexity"}]
[{"st": 28, "ed": 30, "text": "powerful tool"}, {"st": 42, "ed": 44, "text": "deep learning"}, {"st": 52, "ed": 54, "text": "feature vectors"}, {"st": 78, "ed": 81, "text": "deep neural networks"}, {"st": 86, "ed": 88, "text": "unsupervised manner"}, {"st": 133, "ed": 135, "text": "data science"}, {"st": 137, "ed": 139, "text": "lancaster university"}, {"st": 144, "ed": 146, "text": "intellectual property"}]
[{"st": 0, "ed": 2, "text": "learning parameters"}, {"st": 19, "ed": 21, "text": "latent variables"}, {"st": 24, "ed": 26, "text": "log likelihood"}, {"st": 29, "ed": 31, "text": "expectation maximization"}, {"st": 47, "ed": 49, "text": "recent years"}, {"st": 50, "ed": 53, "text": "method of moments"}, {"st": 128, "ed": 130, "text": "learning framework"}]
[{"st": 0, "ed": 3, "text": "deep neural network"}, {"st": 24, "ed": 26, "text": "convolutional networks"}, {"st": 28, "ed": 31, "text": "deep convolutional networks"}, {"st": 37, "ed": 39, "text": "higher dimensional"}, {"st": 62, "ed": 64, "text": "efficient implementation"}, {"st": 86, "ed": 88, "text": "image data"}]
[{"st": 6, "ed": 8, "text": "objective functions"}, {"st": 28, "ed": 30, "text": "training process"}, {"st": 37, "ed": 39, "text": "generalization performance"}, {"st": 65, "ed": 67, "text": "fine tune"}, {"st": 88, "ed": 90, "text": "langevin dynamics"}, {"st": 93, "ed": 96, "text": "efficient and effective"}, {"st": 122, "ed": 124, "text": "local minima"}, {"st": 133, "ed": 135, "text": "neural networks"}, {"st": 139, "ed": 141, "text": "theoretical analysis"}]
[{"st": 8, "ed": 10, "text": "convergence rate"}, {"st": 16, "ed": 19, "text": "easy to implement"}, {"st": 35, "ed": 37, "text": "optimization problems"}, {"st": 41, "ed": 44, "text": "stochastic gradient descent"}, {"st": 44, "ed": 47, "text": "stochastic gradient descent"}, {"st": 55, "ed": 57, "text": "significantly reduces"}, {"st": 66, "ed": 68, "text": "learning rate"}, {"st": 74, "ed": 76, "text": "method works"}, {"st": 80, "ed": 82, "text": "learning rate"}, {"st": 91, "ed": 93, "text": "learning rate"}, {"st": 95, "ed": 97, "text": "update rule"}]
[{"st": 37, "ed": 39, "text": "highly successful"}, {"st": 98, "ed": 100, "text": "based approach"}, {"st": 111, "ed": 113, "text": "link prediction"}, {"st": 128, "ed": 130, "text": "semi supervised"}, {"st": 135, "ed": 137, "text": "kernel method"}, {"st": 176, "ed": 178, "text": "extensively evaluate"}, {"st": 187, "ed": 189, "text": "positive unlabeled"}, {"st": 197, "ed": 199, "text": "false positive"}]
[{"st": 2, "ed": 4, "text": "prototypical networks"}, {"st": 8, "ed": 10, "text": "few shot"}, {"st": 23, "ed": 25, "text": "training set"}, {"st": 36, "ed": 38, "text": "prototypical networks"}, {"st": 40, "ed": 42, "text": "metric space"}, {"st": 59, "ed": 61, "text": "recent approaches"}, {"st": 62, "ed": 65, "text": "few shot learning"}, {"st": 69, "ed": 71, "text": "inductive bias"}, {"st": 76, "ed": 78, "text": "limited data"}, {"st": 95, "ed": 97, "text": "substantial improvements"}, {"st": 98, "ed": 100, "text": "recent approaches"}, {"st": 110, "ed": 112, "text": "prototypical networks"}, {"st": 113, "ed": 116, "text": "zero shot learning"}]
[{"st": 41, "ed": 43, "text": "meta data"}, {"st": 53, "ed": 55, "text": "proposed method"}, {"st": 62, "ed": 64, "text": "base classifiers"}, {"st": 95, "ed": 97, "text": "learning algorithms"}, {"st": 120, "ed": 122, "text": "proposed approach"}]
[{"st": 0, "ed": 2, "text": "random forests"}, {"st": 8, "ed": 10, "text": "training samples"}, {"st": 26, "ed": 28, "text": "cross validation"}, {"st": 36, "ed": 38, "text": "training samples"}, {"st": 41, "ed": 43, "text": "decision tree"}, {"st": 60, "ed": 62, "text": "generalization error"}, {"st": 65, "ed": 67, "text": "decision trees"}, {"st": 70, "ed": 72, "text": "random forest"}, {"st": 77, "ed": 79, "text": "empirical study"}]
[{"st": 24, "ed": 26, "text": "kernel methods"}, {"st": 34, "ed": 36, "text": "neural networks"}, {"st": 47, "ed": 49, "text": "neural network"}, {"st": 66, "ed": 68, "text": "social network"}, {"st": 104, "ed": 106, "text": "graph classification"}, {"st": 111, "ed": 113, "text": "social network"}]
[{"st": 0, "ed": 2, "text": "structured prediction"}, {"st": 12, "ed": 14, "text": "structured prediction"}, {"st": 25, "ed": 27, "text": "structured outputs"}, {"st": 31, "ed": 33, "text": "deep network"}, {"st": 38, "ed": 40, "text": "gradient based"}, {"st": 44, "ed": 47, "text": "end to end"}, {"st": 109, "ed": 112, "text": "end to end"}, {"st": 123, "ed": 125, "text": "image denoising"}, {"st": 144, "ed": 146, "text": "baseline methods"}]
[{"st": 9, "ed": 12, "text": "stochastic gradient descent"}, {"st": 25, "ed": 27, "text": "convergence rate"}, {"st": 27, "ed": 30, "text": "o left frac"}, {"st": 40, "ed": 42, "text": "frank wolfe"}, {"st": 43, "ed": 46, "text": "stochastic gradient descent"}, {"st": 52, "ed": 54, "text": "convergence rate"}, {"st": 54, "ed": 57, "text": "o left frac"}]
[{"st": 6, "ed": 8, "text": "multi label"}, {"st": 31, "ed": 33, "text": "non trivial"}, {"st": 33, "ed": 35, "text": "special case"}, {"st": 55, "ed": 57, "text": "proposed algorithm"}, {"st": 63, "ed": 65, "text": "low rank"}, {"st": 84, "ed": 86, "text": "proposed algorithm"}, {"st": 122, "ed": 124, "text": "significantly improves"}, {"st": 134, "ed": 136, "text": "multi label"}]
[{"st": 9, "ed": 11, "text": "objective functions"}, {"st": 12, "ed": 14, "text": "machine learning"}, {"st": 25, "ed": 27, "text": "objective functions"}, {"st": 29, "ed": 31, "text": "machine learning"}, {"st": 34, "ed": 36, "text": "finite dimensional"}, {"st": 39, "ed": 41, "text": "objective functions"}, {"st": 66, "ed": 68, "text": "anomaly detection"}, {"st": 89, "ed": 91, "text": "objective functions"}, {"st": 102, "ed": 104, "text": "objective function"}, {"st": 120, "ed": 123, "text": "deep network architecture"}, {"st": 140, "ed": 143, "text": "unsupervised and supervised"}, {"st": 156, "ed": 158, "text": "point cloud"}]
[{"st": 89, "ed": 91, "text": "learning agent"}, {"st": 95, "ed": 98, "text": "row and column"}, {"st": 114, "ed": 116, "text": "special case"}, {"st": 121, "ed": 123, "text": "bandit problem"}, {"st": 140, "ed": 142, "text": "scales linearly"}, {"st": 162, "ed": 165, "text": "row and column"}, {"st": 203, "ed": 205, "text": "confidence intervals"}, {"st": 208, "ed": 210, "text": "confidence intervals"}, {"st": 212, "ed": 215, "text": "kullback leibler kl"}, {"st": 250, "ed": 252, "text": "synthetic data"}, {"st": 275, "ed": 277, "text": "real data"}]
[{"st": 10, "ed": 12, "text": "neural networks"}, {"st": 14, "ed": 16, "text": "variational approximation"}, {"st": 22, "ed": 24, "text": "neural network"}, {"st": 26, "ed": 28, "text": "variational parameters"}]
[{"st": 11, "ed": 13, "text": "machine learning"}, {"st": 22, "ed": 24, "text": "parameter settings"}, {"st": 29, "ed": 31, "text": "cross validation"}, {"st": 60, "ed": 62, "text": "learning algorithms"}, {"st": 87, "ed": 89, "text": "machine learning"}, {"st": 90, "ed": 93, "text": "support vector machine"}, {"st": 93, "ed": 95, "text": "random forest"}, {"st": 101, "ed": 103, "text": "parameter settings"}, {"st": 110, "ed": 112, "text": "parameter values"}, {"st": 129, "ed": 131, "text": "classification problems"}, {"st": 146, "ed": 148, "text": "random forest"}]
[{"st": 7, "ed": 9, "text": "neural networks"}, {"st": 129, "ed": 131, "text": "input vector"}, {"st": 149, "ed": 152, "text": "artificial neural networks"}, {"st": 166, "ed": 168, "text": "hidden layers"}, {"st": 178, "ed": 180, "text": "extensive numerical"}, {"st": 213, "ed": 216, "text": "k nearest neighbors"}, {"st": 216, "ed": 218, "text": "k nn"}, {"st": 220, "ed": 223, "text": "k nearest neighbors"}]
[{"st": 7, "ed": 9, "text": "training procedure"}, {"st": 12, "ed": 14, "text": "generative model"}, {"st": 30, "ed": 32, "text": "random noise"}, {"st": 51, "ed": 53, "text": "training procedure"}, {"st": 125, "ed": 127, "text": "competitive results"}, {"st": 135, "ed": 137, "text": "generative adversarial"}]
[{"st": 4, "ed": 6, "text": "optimization problems"}, {"st": 14, "ed": 16, "text": "feature sets"}, {"st": 19, "ed": 21, "text": "supervised classification"}, {"st": 41, "ed": 43, "text": "optimization problem"}, {"st": 54, "ed": 56, "text": "feature selection"}, {"st": 95, "ed": 97, "text": "text classification"}, {"st": 116, "ed": 118, "text": "feature selection"}]
[{"st": 1, "ed": 3, "text": "recently developed"}, {"st": 3, "ed": 5, "text": "variational autoencoders"}, {"st": 16, "ed": 18, "text": "representational power"}, {"st": 19, "ed": 21, "text": "neural networks"}, {"st": 36, "ed": 38, "text": "latent variables"}, {"st": 40, "ed": 43, "text": "standard normal distribution"}, {"st": 58, "ed": 60, "text": "variational autoencoders"}, {"st": 75, "ed": 77, "text": "latent representation"}, {"st": 86, "ed": 88, "text": "learned jointly"}, {"st": 97, "ed": 99, "text": "hierarchical structure"}, {"st": 100, "ed": 102, "text": "latent semantic"}, {"st": 135, "ed": 137, "text": "clustering accuracy"}]
[{"st": 19, "ed": 21, "text": "feature selection"}, {"st": 31, "ed": 33, "text": "key challenge"}, {"st": 45, "ed": 47, "text": "np hard"}, {"st": 101, "ed": 103, "text": "linear programming"}, {"st": 113, "ed": 115, "text": "linear programming"}, {"st": 146, "ed": 148, "text": "synthetic experiments"}]
[{"st": 5, "ed": 7, "text": "latent variables"}, {"st": 47, "ed": 49, "text": "continuous relaxation"}, {"st": 50, "ed": 52, "text": "discrete variables"}, {"st": 54, "ed": 56, "text": "low variance"}, {"st": 75, "ed": 77, "text": "low variance"}, {"st": 88, "ed": 90, "text": "continuous relaxation"}, {"st": 118, "ed": 120, "text": "generative modeling"}, {"st": 124, "ed": 126, "text": "faster convergence"}]
[{"st": 20, "ed": 22, "text": "neural networks"}, {"st": 24, "ed": 26, "text": "weight sharing"}, {"st": 38, "ed": 41, "text": "standard benchmark datasets"}, {"st": 103, "ed": 105, "text": "proposed approach"}, {"st": 106, "ed": 108, "text": "autoregressive models"}, {"st": 110, "ed": 112, "text": "neural networks"}]
[{"st": 10, "ed": 12, "text": "data mining"}, {"st": 18, "ed": 21, "text": "electronic health records"}, {"st": 35, "ed": 37, "text": "machine learning"}, {"st": 76, "ed": 78, "text": "intensive care"}, {"st": 88, "ed": 90, "text": "problems including"}, {"st": 115, "ed": 117, "text": "jointly learn"}, {"st": 120, "ed": 122, "text": "prediction tasks"}, {"st": 126, "ed": 128, "text": "time series"}, {"st": 137, "ed": 141, "text": "recurrent neural network rnn"}, {"st": 160, "ed": 162, "text": "neural architecture"}, {"st": 170, "ed": 172, "text": "strong baselines"}]
[{"st": 1, "ed": 3, "text": "nearest neighbor"}, {"st": 7, "ed": 11, "text": "dynamic time warping dtw"}, {"st": 20, "ed": 22, "text": "time series"}, {"st": 66, "ed": 68, "text": "empirical results"}, {"st": 86, "ed": 88, "text": "nearest neighbor"}]
[{"st": 5, "ed": 7, "text": "empirical results"}, {"st": 24, "ed": 26, "text": "theoretical understanding"}, {"st": 36, "ed": 38, "text": "reinforcement learning"}, {"st": 73, "ed": 76, "text": "markov decision processes"}]
[{"st": 57, "ed": 59, "text": "posterior distribution"}, {"st": 86, "ed": 88, "text": "unified framework"}, {"st": 117, "ed": 119, "text": "level set"}, {"st": 122, "ed": 124, "text": "inverse problems"}, {"st": 147, "ed": 149, "text": "level set"}, {"st": 155, "ed": 157, "text": "harmonic function"}, {"st": 161, "ed": 163, "text": "et al"}, {"st": 168, "ed": 170, "text": "numerical methods"}, {"st": 172, "ed": 175, "text": "large data sets"}, {"st": 188, "ed": 190, "text": "numerical experiments"}, {"st": 192, "ed": 194, "text": "classification accuracy"}, {"st": 195, "ed": 197, "text": "uncertainty quantification"}, {"st": 211, "ed": 216, "text": "graph based semi supervised learning"}]
[{"st": 26, "ed": 28, "text": "total derivative"}, {"st": 32, "ed": 34, "text": "variational parameters"}, {"st": 54, "ed": 56, "text": "approximate posterior"}, {"st": 68, "ed": 71, "text": "theoretically and empirically"}]
[{"st": 13, "ed": 16, "text": "markov decision processes"}, {"st": 65, "ed": 67, "text": "monte carlo"}, {"st": 89, "ed": 91, "text": "off policy"}, {"st": 91, "ed": 94, "text": "monte carlo simulation"}, {"st": 94, "ed": 96, "text": "method works"}]
[{"st": 89, "ed": 91, "text": "reward functions"}, {"st": 109, "ed": 111, "text": "optimization algorithm"}, {"st": 121, "ed": 123, "text": "reward function"}, {"st": 150, "ed": 152, "text": "optimal policy"}, {"st": 157, "ed": 159, "text": "reward functions"}, {"st": 204, "ed": 206, "text": "surrogate model"}, {"st": 270, "ed": 273, "text": "natural resource management"}]
[{"st": 7, "ed": 9, "text": "time series"}, {"st": 97, "ed": 99, "text": "feature spaces"}, {"st": 143, "ed": 145, "text": "open source"}, {"st": 160, "ed": 162, "text": "classifier performance"}, {"st": 214, "ed": 216, "text": "ensemble approach"}]
[{"st": 0, "ed": 2, "text": "early stopping"}, {"st": 10, "ed": 12, "text": "generalization performance"}, {"st": 21, "ed": 23, "text": "gradient based"}, {"st": 34, "ed": 36, "text": "common practice"}, {"st": 47, "ed": 49, "text": "validation set"}, {"st": 62, "ed": 64, "text": "early stopping"}, {"st": 100, "ed": 102, "text": "least squares"}, {"st": 103, "ed": 105, "text": "logistic regression"}]
[{"st": 23, "ed": 25, "text": "low rank"}, {"st": 32, "ed": 36, "text": "nonnegative matrix factorization nmf"}, {"st": 36, "ed": 38, "text": "objective function"}, {"st": 46, "ed": 48, "text": "objective function"}, {"st": 60, "ed": 62, "text": "objective function"}, {"st": 65, "ed": 68, "text": "block coordinate descent"}, {"st": 124, "ed": 126, "text": "method produces"}, {"st": 126, "ed": 128, "text": "higher quality"}, {"st": 128, "ed": 130, "text": "clustering results"}, {"st": 135, "ed": 137, "text": "clustering methods"}, {"st": 159, "ed": 162, "text": "real world data"}, {"st": 185, "ed": 187, "text": "feature space"}, {"st": 189, "ed": 191, "text": "pairwise similarities"}, {"st": 200, "ed": 202, "text": "feature spaces"}]
[{"st": 0, "ed": 3, "text": "stochastic gradient descent"}, {"st": 15, "ed": 17, "text": "deep learning"}, {"st": 19, "ed": 23, "text": "restricted boltzmann machine rbm"}, {"st": 26, "ed": 28, "text": "generative model"}, {"st": 42, "ed": 44, "text": "euclidean algorithm"}, {"st": 52, "ed": 54, "text": "specifically designed"}, {"st": 75, "ed": 77, "text": "theoretical justification"}, {"st": 109, "ed": 111, "text": "continuous data"}, {"st": 156, "ed": 159, "text": "stochastic gradient descent"}]
[{"st": 5, "ed": 8, "text": "inverse reinforcement learning"}, {"st": 9, "ed": 12, "text": "markov decision processes"}, {"st": 26, "ed": 28, "text": "reinforcement learning"}, {"st": 36, "ed": 38, "text": "decision making"}, {"st": 44, "ed": 46, "text": "behavioral economics"}, {"st": 53, "ed": 56, "text": "inverse reinforcement learning"}, {"st": 60, "ed": 62, "text": "loss function"}, {"st": 95, "ed": 98, "text": "markov decision process"}, {"st": 122, "ed": 124, "text": "transition probabilities"}]
[{"st": 28, "ed": 30, "text": "hyper parameters"}, {"st": 87, "ed": 90, "text": "mean squared error"}, {"st": 105, "ed": 107, "text": "real world"}, {"st": 107, "ed": 109, "text": "case study"}, {"st": 111, "ed": 113, "text": "financial services"}]
[{"st": 66, "ed": 68, "text": "gaussian process"}, {"st": 91, "ed": 94, "text": "low computational cost"}, {"st": 110, "ed": 112, "text": "learning rate"}]
[{"st": 63, "ed": 65, "text": "deep residual"}, {"st": 79, "ed": 81, "text": "neural nets"}, {"st": 86, "ed": 89, "text": "deep neural nets"}, {"st": 106, "ed": 108, "text": "total number"}, {"st": 109, "ed": 111, "text": "hidden units"}, {"st": 113, "ed": 116, "text": "single hidden layer"}, {"st": 152, "ed": 154, "text": "skip connections"}, {"st": 160, "ed": 163, "text": "single hidden layer"}, {"st": 175, "ed": 178, "text": "single hidden layer"}]
[{"st": 11, "ed": 13, "text": "streaming data"}, {"st": 47, "ed": 49, "text": "space complexity"}, {"st": 53, "ed": 55, "text": "streaming data"}, {"st": 66, "ed": 68, "text": "learning algorithms"}]
[{"st": 25, "ed": 27, "text": "model selection"}, {"st": 51, "ed": 53, "text": "penalty term"}, {"st": 71, "ed": 73, "text": "mutual information"}, {"st": 96, "ed": 98, "text": "open problems"}, {"st": 102, "ed": 104, "text": "penalty term"}, {"st": 121, "ed": 123, "text": "linear model"}]
[{"st": 12, "ed": 14, "text": "based optimization"}, {"st": 65, "ed": 67, "text": "computational budget"}, {"st": 148, "ed": 150, "text": "large data"}, {"st": 206, "ed": 208, "text": "large scale"}, {"st": 222, "ed": 224, "text": "large scale"}, {"st": 235, "ed": 237, "text": "infinite dimensional"}, {"st": 249, "ed": 251, "text": "finite dimensional"}]
[{"st": 14, "ed": 16, "text": "wasserstein distance"}, {"st": 18, "ed": 20, "text": "auto encoder"}, {"st": 69, "ed": 71, "text": "image generation"}, {"st": 77, "ed": 79, "text": "visual quality"}]
[{"st": 0, "ed": 2, "text": "nonparametric models"}, {"st": 5, "ed": 7, "text": "computationally expensive"}, {"st": 17, "ed": 19, "text": "spectral methods"}, {"st": 24, "ed": 26, "text": "nonparametric models"}, {"st": 33, "ed": 36, "text": "hierarchical dirichlet process"}, {"st": 42, "ed": 44, "text": "spectral methods"}, {"st": 48, "ed": 50, "text": "nonparametric models"}, {"st": 90, "ed": 92, "text": "hierarchical models"}, {"st": 95, "ed": 97, "text": "hierarchical structure"}]
[{"st": 0, "ed": 4, "text": "generative adversarial networks gans"}, {"st": 6, "ed": 8, "text": "generative models"}, {"st": 14, "ed": 16, "text": "recently proposed"}, {"st": 16, "ed": 18, "text": "wasserstein gan"}, {"st": 32, "ed": 34, "text": "low quality"}, {"st": 90, "ed": 92, "text": "proposed method"}, {"st": 103, "ed": 105, "text": "wide variety"}, {"st": 111, "ed": 113, "text": "hyperparameter tuning"}, {"st": 125, "ed": 127, "text": "achieve high"}, {"st": 130, "ed": 132, "text": "cifar 10"}]
[{"st": 37, "ed": 39, "text": "group selection"}, {"st": 85, "ed": 87, "text": "conditional distribution"}, {"st": 154, "ed": 156, "text": "relevant information"}]
[{"st": 1, "ed": 4, "text": "support vector machine"}, {"st": 70, "ed": 72, "text": "naive bayes"}, {"st": 83, "ed": 85, "text": "training data"}, {"st": 136, "ed": 138, "text": "high dimensional"}]
[{"st": 3, "ed": 5, "text": "comparative study"}, {"st": 9, "ed": 11, "text": "off policy"}, {"st": 15, "ed": 17, "text": "importance sampling"}, {"st": 19, "ed": 21, "text": "importance sampling"}]
[{"st": 0, "ed": 2, "text": "similarity based"}, {"st": 8, "ed": 10, "text": "time series"}, {"st": 17, "ed": 19, "text": "parameter tuning"}, {"st": 25, "ed": 27, "text": "time series"}, {"st": 37, "ed": 39, "text": "time series"}, {"st": 54, "ed": 56, "text": "kernel methods"}, {"st": 61, "ed": 63, "text": "time series"}, {"st": 72, "ed": 74, "text": "missing data"}, {"st": 77, "ed": 80, "text": "gaussian mixture models"}, {"st": 87, "ed": 89, "text": "ensemble learning"}, {"st": 100, "ed": 102, "text": "clustering results"}, {"st": 115, "ed": 119, "text": "synthetic and real data"}, {"st": 141, "ed": 143, "text": "competitive results"}, {"st": 146, "ed": 148, "text": "missing data"}]
[{"st": 26, "ed": 28, "text": "higher order"}, {"st": 52, "ed": 54, "text": "theoretical properties"}, {"st": 58, "ed": 60, "text": "steady state"}, {"st": 70, "ed": 72, "text": "alternating minimization"}, {"st": 84, "ed": 86, "text": "real world"}]
[{"st": 9, "ed": 11, "text": "machine learning"}, {"st": 25, "ed": 27, "text": "modeling language"}, {"st": 29, "ed": 32, "text": "probabilistic graphical models"}, {"st": 33, "ed": 35, "text": "latent variables"}, {"st": 45, "ed": 48, "text": "large data sets"}, {"st": 56, "ed": 58, "text": "learning algorithms"}, {"st": 72, "ed": 74, "text": "message passing"}, {"st": 113, "ed": 115, "text": "open source"}]
[{"st": 75, "ed": 77, "text": "highly accurate"}, {"st": 109, "ed": 111, "text": "feature construction"}, {"st": 137, "ed": 139, "text": "mathematical model"}, {"st": 141, "ed": 143, "text": "natural language"}, {"st": 145, "ed": 148, "text": "bag of words"}, {"st": 163, "ed": 165, "text": "natural language"}, {"st": 174, "ed": 176, "text": "natural language"}, {"st": 182, "ed": 185, "text": "gaussian mixture model"}, {"st": 192, "ed": 195, "text": "bag of words"}, {"st": 215, "ed": 218, "text": "mean squared error"}]
[{"st": 6, "ed": 8, "text": "approximation algorithms"}, {"st": 9, "ed": 11, "text": "np hard"}, {"st": 11, "ed": 13, "text": "combinatorial optimization"}, {"st": 37, "ed": 39, "text": "real world"}, {"st": 48, "ed": 50, "text": "optimization problem"}, {"st": 94, "ed": 96, "text": "reinforcement learning"}, {"st": 110, "ed": 112, "text": "meta algorithm"}, {"st": 150, "ed": 152, "text": "optimization problems"}, {"st": 161, "ed": 163, "text": "vertex cover"}]
[{"st": 9, "ed": 11, "text": "discrete optimization"}, {"st": 34, "ed": 36, "text": "empirical risk"}, {"st": 54, "ed": 57, "text": "orders of magnitude"}, {"st": 71, "ed": 73, "text": "approach produces"}, {"st": 77, "ed": 79, "text": "practical problems"}, {"st": 109, "ed": 112, "text": "broward county florida"}, {"st": 127, "ed": 129, "text": "decision tree"}]
[{"st": 2, "ed": 4, "text": "clustering methods"}, {"st": 19, "ed": 22, "text": "number of clusters"}, {"st": 30, "ed": 32, "text": "greedy algorithm"}, {"st": 34, "ed": 36, "text": "hierarchical clustering"}, {"st": 72, "ed": 75, "text": "accuracy and speed"}, {"st": 100, "ed": 102, "text": "greedy algorithm"}, {"st": 116, "ed": 118, "text": "experiments demonstrate"}, {"st": 128, "ed": 130, "text": "clustering algorithms"}, {"st": 140, "ed": 142, "text": "higher quality"}]
[{"st": 18, "ed": 20, "text": "binary tree"}, {"st": 41, "ed": 43, "text": "basis functions"}, {"st": 58, "ed": 60, "text": "binary tree"}, {"st": 94, "ed": 96, "text": "greedy algorithm"}, {"st": 103, "ed": 105, "text": "theoretical findings"}]
[{"st": 34, "ed": 37, "text": "generative adversarial nets"}, {"st": 47, "ed": 49, "text": "representation learning"}, {"st": 67, "ed": 69, "text": "mnist datasets"}, {"st": 71, "ed": 73, "text": "significant improvement"}, {"st": 75, "ed": 77, "text": "classification performance"}, {"st": 80, "ed": 82, "text": "k nn"}]
[{"st": 1, "ed": 3, "text": "gibbs sampler"}, {"st": 11, "ed": 14, "text": "learning and inference"}, {"st": 29, "ed": 31, "text": "gibbs sampler"}, {"st": 86, "ed": 88, "text": "mild conditions"}, {"st": 94, "ed": 96, "text": "gibbs sampler"}, {"st": 109, "ed": 111, "text": "significantly outperforms"}, {"st": 112, "ed": 114, "text": "gibbs sampler"}, {"st": 119, "ed": 121, "text": "learning parameters"}]
[{"st": 9, "ed": 11, "text": "spectral clustering"}, {"st": 12, "ed": 14, "text": "spectral clustering"}, {"st": 17, "ed": 19, "text": "clustering algorithm"}, {"st": 23, "ed": 25, "text": "computational complexity"}, {"st": 36, "ed": 38, "text": "adjacency matrix"}]
[{"st": 2, "ed": 5, "text": "linear discriminant analysis"}, {"st": 159, "ed": 161, "text": "likelihood ratio"}, {"st": 163, "ed": 165, "text": "null hypothesis"}, {"st": 179, "ed": 181, "text": "alternative hypothesis"}]
[{"st": 7, "ed": 9, "text": "variational bayes"}, {"st": 30, "ed": 32, "text": "uncertainty estimates"}, {"st": 40, "ed": 42, "text": "computational cost"}, {"st": 79, "ed": 81, "text": "approximate posterior"}, {"st": 98, "ed": 101, "text": "recurrent neural networks"}, {"st": 114, "ed": 116, "text": "empirically demonstrate"}, {"st": 131, "ed": 133, "text": "image captioning"}]
[{"st": 13, "ed": 15, "text": "time series"}, {"st": 56, "ed": 58, "text": "temporal dependencies"}, {"st": 76, "ed": 78, "text": "attention based"}, {"st": 78, "ed": 81, "text": "recurrent neural network"}, {"st": 96, "ed": 98, "text": "attention mechanism"}, {"st": 105, "ed": 107, "text": "input features"}, {"st": 127, "ed": 129, "text": "attention mechanism"}, {"st": 133, "ed": 135, "text": "hidden states"}, {"st": 160, "ed": 162, "text": "empirical studies"}, {"st": 187, "ed": 189, "text": "time series"}]
[{"st": 10, "ed": 13, "text": "electronic health records"}, {"st": 94, "ed": 100, "text": "alternating direction method of multipliers admm"}]
[{"st": 7, "ed": 9, "text": "gaussian processes"}, {"st": 21, "ed": 23, "text": "gaussian processes"}, {"st": 25, "ed": 27, "text": "gaussian processes"}, {"st": 34, "ed": 36, "text": "big data"}, {"st": 58, "ed": 61, "text": "stochastic variational inference"}, {"st": 72, "ed": 74, "text": "proposed approach"}, {"st": 81, "ed": 83, "text": "simulated data"}, {"st": 85, "ed": 87, "text": "benchmark dataset"}, {"st": 89, "ed": 91, "text": "airline industry"}]
[{"st": 6, "ed": 9, "text": "generative adversarial networks"}, {"st": 13, "ed": 15, "text": "training procedure"}, {"st": 26, "ed": 28, "text": "hinge loss"}, {"st": 33, "ed": 35, "text": "hinge loss"}, {"st": 62, "ed": 64, "text": "global optimum"}, {"st": 73, "ed": 75, "text": "image generation"}, {"st": 77, "ed": 79, "text": "training procedure"}, {"st": 91, "ed": 94, "text": "qualitative and quantitative"}]
[{"st": 53, "ed": 55, "text": "similarity based"}, {"st": 81, "ed": 83, "text": "machine learning"}, {"st": 87, "ed": 89, "text": "tf idf"}, {"st": 105, "ed": 107, "text": "k nn"}, {"st": 114, "ed": 116, "text": "linear regression"}]
[{"st": 9, "ed": 11, "text": "adversarial loss"}, {"st": 22, "ed": 24, "text": "adversarial loss"}, {"st": 33, "ed": 35, "text": "posterior distribution"}, {"st": 40, "ed": 42, "text": "input data"}, {"st": 48, "ed": 50, "text": "adversarial training"}, {"st": 53, "ed": 55, "text": "adversarial training"}, {"st": 66, "ed": 68, "text": "output distribution"}, {"st": 75, "ed": 77, "text": "semi supervised"}, {"st": 95, "ed": 97, "text": "adversarial training"}, {"st": 100, "ed": 102, "text": "computational cost"}, {"st": 108, "ed": 110, "text": "neural networks"}, {"st": 115, "ed": 117, "text": "adversarial loss"}, {"st": 137, "ed": 141, "text": "supervised and semi supervised"}, {"st": 166, "ed": 168, "text": "cifar 10"}, {"st": 169, "ed": 172, "text": "semi supervised learning"}]
[{"st": 0, "ed": 3, "text": "stochastic gradient descent"}, {"st": 6, "ed": 8, "text": "learning rate"}, {"st": 37, "ed": 39, "text": "approximate bayesian"}, {"st": 39, "ed": 41, "text": "posterior inference"}, {"st": 49, "ed": 51, "text": "tuning parameters"}, {"st": 65, "ed": 68, "text": "kullback leibler divergence"}, {"st": 84, "ed": 86, "text": "em algorithm"}, {"st": 117, "ed": 119, "text": "langevin dynamics"}]
[{"st": 4, "ed": 6, "text": "multi armed"}, {"st": 7, "ed": 9, "text": "bandit problem"}, {"st": 37, "ed": 39, "text": "proposed algorithm"}, {"st": 68, "ed": 70, "text": "parameter free"}, {"st": 98, "ed": 100, "text": "significantly improve"}, {"st": 129, "ed": 131, "text": "bandit algorithms"}, {"st": 181, "ed": 183, "text": "significantly improves"}, {"st": 187, "ed": 189, "text": "theoretical analysis"}]
[{"st": 12, "ed": 14, "text": "machine learning"}, {"st": 22, "ed": 24, "text": "neural nets"}, {"st": 25, "ed": 27, "text": "decision trees"}, {"st": 54, "ed": 56, "text": "problem solving"}, {"st": 57, "ed": 59, "text": "voice recognition"}, {"st": 78, "ed": 80, "text": "industrial revolution"}]
[{"st": 10, "ed": 12, "text": "total variation"}, {"st": 43, "ed": 45, "text": "sampling strategy"}, {"st": 53, "ed": 55, "text": "numerical experiments"}, {"st": 68, "ed": 70, "text": "random graph"}, {"st": 75, "ed": 77, "text": "real world"}]
[{"st": 77, "ed": 79, "text": "machine learning"}]
[{"st": 12, "ed": 14, "text": "impressive results"}, {"st": 50, "ed": 52, "text": "automatically generated"}, {"st": 132, "ed": 134, "text": "statistical model"}, {"st": 142, "ed": 144, "text": "recent advances"}, {"st": 145, "ed": 147, "text": "neural network"}, {"st": 147, "ed": 149, "text": "generative modeling"}, {"st": 154, "ed": 156, "text": "hierarchical structure"}]
[{"st": 9, "ed": 11, "text": "multi output"}, {"st": 37, "ed": 39, "text": "basis functions"}, {"st": 48, "ed": 50, "text": "proposed method"}]
[{"st": 0, "ed": 3, "text": "temporal difference learning"}, {"st": 12, "ed": 14, "text": "temporal difference"}, {"st": 15, "ed": 17, "text": "learning algorithms"}, {"st": 26, "ed": 28, "text": "objective functions"}, {"st": 68, "ed": 70, "text": "off policy"}, {"st": 81, "ed": 83, "text": "computational cost"}, {"st": 85, "ed": 87, "text": "near optimal"}, {"st": 95, "ed": 97, "text": "off policy"}, {"st": 105, "ed": 108, "text": "temporal difference learning"}, {"st": 113, "ed": 115, "text": "off policy"}, {"st": 117, "ed": 119, "text": "off policy"}]
[{"st": 3, "ed": 5, "text": "real world"}, {"st": 9, "ed": 11, "text": "machine learning"}, {"st": 13, "ed": 15, "text": "large scale"}, {"st": 30, "ed": 32, "text": "large scale"}, {"st": 34, "ed": 36, "text": "linear model"}, {"st": 48, "ed": 50, "text": "l 2"}, {"st": 117, "ed": 119, "text": "feature engineering"}, {"st": 129, "ed": 131, "text": "prediction model"}, {"st": 135, "ed": 137, "text": "display advertising"}]
[{"st": 17, "ed": 19, "text": "large scale"}, {"st": 19, "ed": 21, "text": "multi label"}, {"st": 43, "ed": 45, "text": "main contribution"}, {"st": 50, "ed": 52, "text": "multi label"}, {"st": 52, "ed": 54, "text": "ensemble method"}, {"st": 58, "ed": 60, "text": "statistical significance"}, {"st": 70, "ed": 72, "text": "machine learning"}, {"st": 117, "ed": 119, "text": "ensemble method"}, {"st": 169, "ed": 171, "text": "fully automated"}, {"st": 171, "ed": 173, "text": "machine learning"}, {"st": 181, "ed": 183, "text": "rule based"}, {"st": 186, "ed": 188, "text": "highly competitive"}]
[{"st": 8, "ed": 10, "text": "feature selection"}, {"st": 19, "ed": 21, "text": "auto encoders"}, {"st": 22, "ed": 24, "text": "feature representation"}, {"st": 25, "ed": 27, "text": "higher level"}, {"st": 34, "ed": 36, "text": "feature learning"}, {"st": 59, "ed": 61, "text": "african american"}, {"st": 67, "ed": 69, "text": "deep learning"}, {"st": 96, "ed": 98, "text": "feature learning"}]
[{"st": 0, "ed": 2, "text": "deep learning"}, {"st": 10, "ed": 12, "text": "application domains"}, {"st": 13, "ed": 15, "text": "machine learning"}, {"st": 16, "ed": 18, "text": "recent years"}, {"st": 20, "ed": 23, "text": "multi class classification"}, {"st": 27, "ed": 29, "text": "loss function"}, {"st": 35, "ed": 38, "text": "cross entropy loss"}, {"st": 42, "ed": 44, "text": "log loss"}]
[{"st": 2, "ed": 4, "text": "classification tasks"}, {"st": 7, "ed": 9, "text": "structured data"}, {"st": 39, "ed": 41, "text": "structured data"}, {"st": 42, "ed": 44, "text": "temporal information"}, {"st": 49, "ed": 51, "text": "neural network"}, {"st": 80, "ed": 83, "text": "short term memory"}, {"st": 86, "ed": 88, "text": "convolutional networks"}, {"st": 107, "ed": 109, "text": "promising results"}]
[{"st": 4, "ed": 6, "text": "phase retrieval"}, {"st": 6, "ed": 9, "text": "problem of recovering"}, {"st": 40, "ed": 42, "text": "phase retrieval"}, {"st": 60, "ed": 62, "text": "proposed algorithm"}, {"st": 63, "ed": 66, "text": "guaranteed to converge"}, {"st": 88, "ed": 90, "text": "phase retrieval"}, {"st": 95, "ed": 97, "text": "sample complexity"}, {"st": 104, "ed": 106, "text": "noise free"}, {"st": 111, "ed": 114, "text": "synthetic and real"}]
[{"st": 5, "ed": 7, "text": "challenging problem"}, {"st": 9, "ed": 12, "text": "fast and accurate"}, {"st": 14, "ed": 16, "text": "neural networks"}, {"st": 35, "ed": 37, "text": "training data"}, {"st": 40, "ed": 43, "text": "orders of magnitude"}, {"st": 57, "ed": 60, "text": "difficult to train"}, {"st": 75, "ed": 77, "text": "sparse signal"}, {"st": 96, "ed": 98, "text": "neural network"}, {"st": 108, "ed": 110, "text": "message passing"}, {"st": 192, "ed": 195, "text": "times faster than"}, {"st": 200, "ed": 203, "text": "times faster than"}]
[{"st": 7, "ed": 9, "text": "feature selection"}, {"st": 54, "ed": 56, "text": "proposed algorithm"}, {"st": 62, "ed": 64, "text": "feature selection"}, {"st": 65, "ed": 67, "text": "breast cancer"}]
[{"st": 12, "ed": 14, "text": "natural gradient"}, {"st": 18, "ed": 20, "text": "natural gradient"}, {"st": 26, "ed": 29, "text": "point of view"}, {"st": 55, "ed": 57, "text": "neural networks"}, {"st": 66, "ed": 68, "text": "feed forward"}, {"st": 78, "ed": 80, "text": "back propagation"}, {"st": 92, "ed": 94, "text": "neural networks"}, {"st": 98, "ed": 100, "text": "character recognition"}]
[{"st": 71, "ed": 73, "text": "learning representations"}, {"st": 80, "ed": 82, "text": "learned features"}, {"st": 137, "ed": 139, "text": "recent successes"}, {"st": 140, "ed": 142, "text": "word embedding"}, {"st": 164, "ed": 166, "text": "convex optimization"}, {"st": 180, "ed": 183, "text": "a logarithmic factor"}, {"st": 196, "ed": 198, "text": "sample complexity"}, {"st": 258, "ed": 260, "text": "numerical simulations"}, {"st": 261, "ed": 264, "text": "synthetic and real"}]
[{"st": 7, "ed": 9, "text": "low rank"}, {"st": 15, "ed": 17, "text": "linear subspace"}, {"st": 38, "ed": 40, "text": "low rank"}, {"st": 42, "ed": 44, "text": "structural constraints"}, {"st": 50, "ed": 52, "text": "optimization problem"}, {"st": 64, "ed": 66, "text": "computationally efficient"}, {"st": 66, "ed": 68, "text": "conjugate gradient"}, {"st": 69, "ed": 71, "text": "trust region"}, {"st": 82, "ed": 84, "text": "matrix completion"}, {"st": 86, "ed": 88, "text": "matrix completion"}]
[{"st": 0, "ed": 3, "text": "self paced learning"}, {"st": 8, "ed": 10, "text": "weight training"}, {"st": 29, "ed": 32, "text": "stochastic gradient descent"}, {"st": 60, "ed": 63, "text": "extensive experimental results"}, {"st": 75, "ed": 77, "text": "network architectures"}, {"st": 89, "ed": 91, "text": "residual learning"}, {"st": 93, "ed": 95, "text": "batch normalization"}]
[{"st": 1, "ed": 3, "text": "predictive models"}, {"st": 27, "ed": 29, "text": "autism spectrum"}, {"st": 33, "ed": 35, "text": "non invasive"}, {"st": 56, "ed": 58, "text": "wide variety"}, {"st": 76, "ed": 78, "text": "graph based"}, {"st": 98, "ed": 100, "text": "brain activity"}, {"st": 123, "ed": 125, "text": "predictive models"}, {"st": 142, "ed": 145, "text": "convolutional neural networks"}, {"st": 157, "ed": 159, "text": "avoid overfitting"}, {"st": 180, "ed": 182, "text": "brain imaging"}, {"st": 190, "ed": 192, "text": "proposed approach"}]
[{"st": 8, "ed": 10, "text": "resource constrained"}, {"st": 23, "ed": 25, "text": "prediction model"}, {"st": 41, "ed": 43, "text": "without sacrificing"}, {"st": 48, "ed": 50, "text": "prediction models"}, {"st": 52, "ed": 55, "text": "labeled training data"}, {"st": 82, "ed": 84, "text": "low complexity"}, {"st": 86, "ed": 88, "text": "prediction model"}, {"st": 101, "ed": 103, "text": "low cost"}, {"st": 108, "ed": 110, "text": "highly accurate"}, {"st": 114, "ed": 116, "text": "empirical loss"}, {"st": 116, "ed": 118, "text": "minimization problem"}, {"st": 132, "ed": 134, "text": "benchmark datasets"}, {"st": 135, "ed": 137, "text": "method outperforms"}, {"st": 142, "ed": 144, "text": "higher accuracy"}]
[{"st": 0, "ed": 2, "text": "class imbalance"}, {"st": 8, "ed": 10, "text": "classification problems"}, {"st": 11, "ed": 13, "text": "deep learning"}, {"st": 33, "ed": 35, "text": "structured data"}, {"st": 37, "ed": 39, "text": "deep learning"}, {"st": 62, "ed": 64, "text": "feature space"}, {"st": 67, "ed": 71, "text": "convolutional neural network cnn"}, {"st": 73, "ed": 75, "text": "key feature"}, {"st": 79, "ed": 81, "text": "representation learning"}, {"st": 84, "ed": 86, "text": "training data"}, {"st": 99, "ed": 101, "text": "feature space"}, {"st": 106, "ed": 108, "text": "linear subspace"}, {"st": 137, "ed": 139, "text": "discriminative power"}, {"st": 146, "ed": 148, "text": "empirical study"}, {"st": 160, "ed": 162, "text": "class imbalance"}]
[{"st": 2, "ed": 4, "text": "machine learning"}, {"st": 21, "ed": 23, "text": "distributed stochastic"}, {"st": 55, "ed": 57, "text": "convex optimization"}, {"st": 78, "ed": 80, "text": "distributed stochastic"}, {"st": 88, "ed": 90, "text": "distributed stochastic"}, {"st": 131, "ed": 133, "text": "collaborative learning"}, {"st": 156, "ed": 158, "text": "main contributions"}, {"st": 166, "ed": 168, "text": "convergence rates"}, {"st": 181, "ed": 183, "text": "network topology"}, {"st": 194, "ed": 196, "text": "sufficient conditions"}]
[{"st": 4, "ed": 7, "text": "multi armed bandit"}, {"st": 31, "ed": 33, "text": "bipartite graph"}, {"st": 66, "ed": 68, "text": "social networks"}, {"st": 77, "ed": 79, "text": "side information"}, {"st": 107, "ed": 109, "text": "network structure"}, {"st": 141, "ed": 144, "text": "upper confidence bound"}, {"st": 165, "ed": 168, "text": "under mild assumptions"}, {"st": 192, "ed": 194, "text": "numerical examples"}, {"st": 196, "ed": 198, "text": "real world"}, {"st": 198, "ed": 200, "text": "social network"}]
[{"st": 104, "ed": 106, "text": "quasi newton"}, {"st": 121, "ed": 124, "text": "linear convergence rate"}, {"st": 130, "ed": 132, "text": "strongly convex"}, {"st": 141, "ed": 143, "text": "logistic regression"}, {"st": 144, "ed": 147, "text": "convolutional neural networks"}]
[{"st": 1, "ed": 3, "text": "machine learning"}, {"st": 4, "ed": 6, "text": "supervised learning"}, {"st": 11, "ed": 13, "text": "input output"}, {"st": 18, "ed": 20, "text": "past observations"}, {"st": 23, "ed": 25, "text": "decision trees"}, {"st": 27, "ed": 29, "text": "input output"}, {"st": 65, "ed": 67, "text": "random forest"}, {"st": 73, "ed": 75, "text": "decision trees"}, {"st": 84, "ed": 86, "text": "decision trees"}, {"st": 103, "ed": 105, "text": "supervised learning"}, {"st": 109, "ed": 111, "text": "computational power"}, {"st": 124, "ed": 126, "text": "without sacrificing"}, {"st": 136, "ed": 138, "text": "decision tree"}, {"st": 156, "ed": 158, "text": "output spaces"}]
[{"st": 6, "ed": 8, "text": "ensemble learning"}, {"st": 9, "ed": 11, "text": "gained increasing"}, {"st": 15, "ed": 17, "text": "great potential"}, {"st": 32, "ed": 34, "text": "ensemble learning"}, {"st": 103, "ed": 105, "text": "stability selection"}, {"st": 117, "ed": 122, "text": "simulated and real world data"}, {"st": 137, "ed": 139, "text": "achieves higher"}]
[{"st": 0, "ed": 3, "text": "end to end"}, {"st": 23, "ed": 26, "text": "end to end"}, {"st": 29, "ed": 31, "text": "specifically designed"}, {"st": 51, "ed": 53, "text": "representation learning"}, {"st": 66, "ed": 69, "text": "end to end"}, {"st": 83, "ed": 85, "text": "video games"}, {"st": 95, "ed": 97, "text": "challenging tasks"}, {"st": 97, "ed": 99, "text": "network architectures"}, {"st": 117, "ed": 120, "text": "end to end"}, {"st": 149, "ed": 152, "text": "end to end"}, {"st": 160, "ed": 162, "text": "modular design"}, {"st": 169, "ed": 171, "text": "experiments demonstrate"}]
[{"st": 49, "ed": 51, "text": "predictive models"}, {"st": 61, "ed": 63, "text": "patient data"}, {"st": 101, "ed": 103, "text": "spectral clustering"}, {"st": 104, "ed": 107, "text": "gaussian mixture models"}, {"st": 116, "ed": 118, "text": "lower dimensional"}, {"st": 128, "ed": 130, "text": "non linearity"}, {"st": 142, "ed": 144, "text": "classification algorithms"}, {"st": 146, "ed": 148, "text": "logistic regression"}, {"st": 149, "ed": 152, "text": "boosted decision trees"}, {"st": 180, "ed": 183, "text": "radial basis function"}, {"st": 183, "ed": 185, "text": "kernel pca"}, {"st": 193, "ed": 195, "text": "decision tree"}, {"st": 200, "ed": 202, "text": "improved accuracy"}]
[{"st": 6, "ed": 9, "text": "achieved great success"}, {"st": 52, "ed": 54, "text": "relevant features"}, {"st": 69, "ed": 71, "text": "feature selection"}, {"st": 76, "ed": 78, "text": "large scale"}, {"st": 88, "ed": 90, "text": "learning process"}, {"st": 96, "ed": 98, "text": "group lasso"}, {"st": 102, "ed": 104, "text": "irrelevant features"}, {"st": 120, "ed": 122, "text": "group lasso"}, {"st": 122, "ed": 124, "text": "feature selection"}, {"st": 134, "ed": 136, "text": "stability selection"}, {"st": 162, "ed": 164, "text": "feature selection"}, {"st": 167, "ed": 169, "text": "group lasso"}, {"st": 169, "ed": 171, "text": "feature selection"}, {"st": 184, "ed": 186, "text": "empirical studies"}]
[{"st": 2, "ed": 4, "text": "deep forest"}, {"st": 15, "ed": 17, "text": "deep forest"}, {"st": 52, "ed": 54, "text": "training set"}, {"st": 64, "ed": 66, "text": "class distributions"}, {"st": 68, "ed": 70, "text": "deep forest"}, {"st": 72, "ed": 74, "text": "weighted sum"}, {"st": 77, "ed": 79, "text": "class probabilities"}, {"st": 118, "ed": 120, "text": "prevent overfitting"}, {"st": 121, "ed": 123, "text": "takes place"}, {"st": 124, "ed": 126, "text": "neural networks"}, {"st": 129, "ed": 131, "text": "training data"}, {"st": 134, "ed": 136, "text": "numerical experiments"}, {"st": 139, "ed": 141, "text": "distance metric"}]
[{"st": 23, "ed": 25, "text": "large scale"}, {"st": 25, "ed": 27, "text": "dag models"}, {"st": 31, "ed": 33, "text": "computational complexity"}, {"st": 58, "ed": 60, "text": "dag models"}, {"st": 76, "ed": 78, "text": "dag models"}, {"st": 89, "ed": 91, "text": "quadratic function"}, {"st": 95, "ed": 97, "text": "dag models"}, {"st": 100, "ed": 102, "text": "dag models"}, {"st": 135, "ed": 137, "text": "dag models"}, {"st": 151, "ed": 153, "text": "large scale"}, {"st": 166, "ed": 169, "text": "undirected graphical model"}, {"st": 177, "ed": 179, "text": "search space"}, {"st": 194, "ed": 196, "text": "theoretical results"}]
[{"st": 1, "ed": 3, "text": "deep learning"}, {"st": 22, "ed": 24, "text": "hyperparameter optimization"}, {"st": 34, "ed": 36, "text": "deep architectures"}, {"st": 56, "ed": 59, "text": "trial and error"}, {"st": 94, "ed": 96, "text": "search spaces"}, {"st": 103, "ed": 105, "text": "search spaces"}, {"st": 136, "ed": 138, "text": "search space"}, {"st": 142, "ed": 144, "text": "search algorithms"}, {"st": 146, "ed": 148, "text": "random search"}, {"st": 148, "ed": 150, "text": "monte carlo"}, {"st": 155, "ed": 157, "text": "model based"}, {"st": 168, "ed": 170, "text": "cifar 10"}, {"st": 201, "ed": 203, "text": "search spaces"}]
[{"st": 15, "ed": 18, "text": "peer to peer"}, {"st": 117, "ed": 119, "text": "real world"}, {"st": 127, "ed": 129, "text": "existing approaches"}, {"st": 133, "ed": 135, "text": "kernel based"}, {"st": 156, "ed": 158, "text": "open problems"}]
[{"st": 1, "ed": 3, "text": "contextual bandit"}, {"st": 12, "ed": 14, "text": "exploration exploitation"}, {"st": 17, "ed": 19, "text": "greedy algorithms"}, {"st": 35, "ed": 37, "text": "greedy algorithms"}, {"st": 50, "ed": 52, "text": "clinical trials"}, {"st": 59, "ed": 61, "text": "greedy algorithm"}, {"st": 85, "ed": 87, "text": "armed bandit"}, {"st": 111, "ed": 113, "text": "greedy algorithm"}, {"st": 122, "ed": 124, "text": "bandit algorithms"}, {"st": 151, "ed": 153, "text": "greedy algorithm"}, {"st": 166, "ed": 168, "text": "additional assumptions"}, {"st": 177, "ed": 179, "text": "extensive simulations"}, {"st": 187, "ed": 189, "text": "outperforms existing"}, {"st": 191, "ed": 193, "text": "contextual bandit"}, {"st": 196, "ed": 198, "text": "thompson sampling"}]
[{"st": 33, "ed": 35, "text": "big data"}, {"st": 69, "ed": 71, "text": "similarity function"}, {"st": 115, "ed": 117, "text": "similarity function"}, {"st": 124, "ed": 126, "text": "similarity function"}, {"st": 129, "ed": 131, "text": "dot product"}, {"st": 132, "ed": 134, "text": "feature vectors"}, {"st": 164, "ed": 167, "text": "o n 2"}, {"st": 168, "ed": 170, "text": "per iteration"}, {"st": 211, "ed": 213, "text": "semi supervised"}]
[{"st": 0, "ed": 2, "text": "matrix completion"}, {"st": 18, "ed": 20, "text": "partially observed"}, {"st": 22, "ed": 24, "text": "recommender systems"}, {"st": 32, "ed": 34, "text": "matrix completion"}, {"st": 38, "ed": 40, "text": "partially observed"}, {"st": 91, "ed": 93, "text": "low rank"}, {"st": 98, "ed": 100, "text": "low rank"}, {"st": 120, "ed": 122, "text": "low rank"}, {"st": 127, "ed": 129, "text": "matrix completion"}, {"st": 132, "ed": 134, "text": "low rank"}, {"st": 161, "ed": 163, "text": "significantly smaller"}, {"st": 168, "ed": 170, "text": "matrix completion"}, {"st": 186, "ed": 188, "text": "low rank"}, {"st": 192, "ed": 194, "text": "partially observed"}]
[{"st": 13, "ed": 15, "text": "reinforcement learning"}, {"st": 21, "ed": 23, "text": "transition function"}, {"st": 54, "ed": 57, "text": "deep generative models"}, {"st": 58, "ed": 60, "text": "capture complex"}, {"st": 72, "ed": 74, "text": "variational inference"}, {"st": 91, "ed": 94, "text": "ability to learn"}]
[{"st": 8, "ed": 11, "text": "stochastic gradient descent"}, {"st": 31, "ed": 33, "text": "uniform sampling"}, {"st": 37, "ed": 39, "text": "determinantal point"}, {"st": 47, "ed": 49, "text": "similarity measure"}, {"st": 107, "ed": 109, "text": "stratified sampling"}, {"st": 117, "ed": 119, "text": "stratified sampling"}, {"st": 150, "ed": 152, "text": "classification accuracies"}]
[{"st": 42, "ed": 44, "text": "additive models"}, {"st": 48, "ed": 50, "text": "continuous functions"}, {"st": 63, "ed": 65, "text": "convex functions"}, {"st": 73, "ed": 75, "text": "avoid overfitting"}, {"st": 89, "ed": 91, "text": "per iteration"}, {"st": 95, "ed": 99, "text": "synthetic and real data"}, {"st": 111, "ed": 113, "text": "additive models"}, {"st": 114, "ed": 116, "text": "improved performance"}]
[{"st": 0, "ed": 2, "text": "multi label"}, {"st": 2, "ed": 4, "text": "text classification"}, {"st": 7, "ed": 9, "text": "machine learning"}, {"st": 31, "ed": 33, "text": "multi label"}, {"st": 48, "ed": 50, "text": "high dimensional"}, {"st": 64, "ed": 66, "text": "accurate predictions"}, {"st": 81, "ed": 83, "text": "training phase"}, {"st": 88, "ed": 90, "text": "search space"}, {"st": 100, "ed": 102, "text": "elastic net"}, {"st": 112, "ed": 114, "text": "early stopping"}, {"st": 127, "ed": 129, "text": "search space"}, {"st": 135, "ed": 137, "text": "training set"}, {"st": 211, "ed": 213, "text": "approximate inference"}]
[{"st": 7, "ed": 9, "text": "semi supervised"}, {"st": 10, "ed": 12, "text": "class classification"}, {"st": 18, "ed": 20, "text": "linearly separable"}, {"st": 29, "ed": 31, "text": "linearly separable"}, {"st": 55, "ed": 57, "text": "linearly separable"}, {"st": 72, "ed": 74, "text": "gaussian distribution"}, {"st": 93, "ed": 95, "text": "proposed algorithm"}]
[{"st": 28, "ed": 30, "text": "feature extraction"}, {"st": 40, "ed": 42, "text": "deep learning"}, {"st": 47, "ed": 49, "text": "feature extraction"}, {"st": 67, "ed": 69, "text": "deep learning"}, {"st": 85, "ed": 88, "text": "deep convolutional networks"}, {"st": 124, "ed": 126, "text": "classification tasks"}, {"st": 151, "ed": 153, "text": "learned models"}, {"st": 158, "ed": 160, "text": "sensitivity analysis"}, {"st": 161, "ed": 163, "text": "biologically plausible"}, {"st": 180, "ed": 182, "text": "starting point"}, {"st": 186, "ed": 188, "text": "deep learning"}]
[{"st": 30, "ed": 34, "text": "recurrent neural network rnn"}, {"st": 36, "ed": 40, "text": "convolutional neural network cnn"}, {"st": 74, "ed": 76, "text": "result shows"}, {"st": 87, "ed": 89, "text": "multilayer perceptron"}, {"st": 93, "ed": 95, "text": "learning framework"}, {"st": 98, "ed": 100, "text": "convolutional networks"}]
[{"st": 10, "ed": 12, "text": "spatio temporal"}, {"st": 13, "ed": 15, "text": "gaussian processes"}, {"st": 17, "ed": 19, "text": "gp regression"}, {"st": 79, "ed": 81, "text": "gp regression"}, {"st": 104, "ed": 106, "text": "finite dimensional"}, {"st": 120, "ed": 122, "text": "finite set"}, {"st": 132, "ed": 134, "text": "kalman filter"}, {"st": 141, "ed": 143, "text": "sufficient statistic"}, {"st": 203, "ed": 206, "text": "synthetic and real"}, {"st": 216, "ed": 218, "text": "gp regression"}]
[{"st": 6, "ed": 9, "text": "semi supervised clustering"}, {"st": 19, "ed": 21, "text": "gaussian distributions"}, {"st": 36, "ed": 38, "text": "side information"}, {"st": 44, "ed": 46, "text": "cross entropy"}, {"st": 86, "ed": 88, "text": "experiments demonstrate"}, {"st": 96, "ed": 99, "text": "gaussian mixture models"}, {"st": 103, "ed": 105, "text": "semi supervised"}, {"st": 112, "ed": 114, "text": "noisy labels"}, {"st": 118, "ed": 121, "text": "number of clusters"}, {"st": 139, "ed": 141, "text": "semi supervised"}, {"st": 145, "ed": 147, "text": "successfully applied"}, {"st": 155, "ed": 157, "text": "side information"}]
[{"st": 22, "ed": 24, "text": "optimization methods"}, {"st": 33, "ed": 35, "text": "semi supervised"}, {"st": 39, "ed": 41, "text": "small sample"}, {"st": 44, "ed": 46, "text": "semi supervised"}, {"st": 47, "ed": 49, "text": "optimization methods"}, {"st": 59, "ed": 61, "text": "real world"}, {"st": 69, "ed": 71, "text": "semi supervised"}, {"st": 72, "ed": 74, "text": "optimization method"}, {"st": 86, "ed": 88, "text": "optimization method"}, {"st": 93, "ed": 95, "text": "unlabeled data"}, {"st": 102, "ed": 105, "text": "semi supervised learning"}, {"st": 123, "ed": 125, "text": "unlabeled data"}, {"st": 129, "ed": 131, "text": "generalization performance"}, {"st": 134, "ed": 136, "text": "semi supervised"}]
[{"st": 16, "ed": 18, "text": "semi supervised"}, {"st": 20, "ed": 23, "text": "gaussian mixture model"}, {"st": 36, "ed": 38, "text": "semi supervised"}, {"st": 43, "ed": 45, "text": "user defined"}, {"st": 67, "ed": 69, "text": "expert systems"}, {"st": 74, "ed": 76, "text": "expert knowledge"}, {"st": 77, "ed": 79, "text": "true distribution"}, {"st": 93, "ed": 95, "text": "clustering techniques"}, {"st": 100, "ed": 103, "text": "the paper presents"}, {"st": 104, "ed": 106, "text": "theoretical analysis"}]
[{"st": 10, "ed": 12, "text": "training examples"}, {"st": 41, "ed": 43, "text": "open problem"}, {"st": 53, "ed": 56, "text": "positive and negative"}, {"st": 101, "ed": 103, "text": "closed form"}, {"st": 125, "ed": 128, "text": "mnist and cifar"}, {"st": 144, "ed": 146, "text": "training examples"}]
[{"st": 31, "ed": 33, "text": "text documents"}, {"st": 44, "ed": 46, "text": "power law"}, {"st": 72, "ed": 74, "text": "hidden layer"}, {"st": 121, "ed": 124, "text": "deep generative models"}, {"st": 124, "ed": 127, "text": "probabilistic topic models"}, {"st": 138, "ed": 140, "text": "downstream tasks"}, {"st": 142, "ed": 144, "text": "document classification"}]
[{"st": 5, "ed": 7, "text": "partially observed"}, {"st": 10, "ed": 12, "text": "wide applicability"}, {"st": 17, "ed": 19, "text": "recommender systems"}, {"st": 19, "ed": 21, "text": "computational biology"}, {"st": 29, "ed": 31, "text": "theoretical guarantees"}, {"st": 42, "ed": 44, "text": "low rank"}, {"st": 51, "ed": 54, "text": "row and column"}, {"st": 59, "ed": 61, "text": "recent research"}, {"st": 64, "ed": 66, "text": "side information"}, {"st": 68, "ed": 71, "text": "low rank matrix"}, {"st": 83, "ed": 85, "text": "higher order"}, {"st": 108, "ed": 110, "text": "real world"}, {"st": 125, "ed": 127, "text": "recommendation systems"}, {"st": 159, "ed": 162, "text": "coordinate descent algorithm"}, {"st": 173, "ed": 175, "text": "higher order"}, {"st": 197, "ed": 199, "text": "higher order"}, {"st": 207, "ed": 209, "text": "side information"}, {"st": 224, "ed": 227, "text": "deep neural networks"}]
[{"st": 36, "ed": 38, "text": "training process"}, {"st": 75, "ed": 77, "text": "speech recognition"}, {"st": 95, "ed": 97, "text": "recognition performance"}]
[{"st": 0, "ed": 4, "text": "generative adversarial networks gans"}, {"st": 28, "ed": 30, "text": "great potential"}, {"st": 54, "ed": 56, "text": "wasserstein distance"}, {"st": 58, "ed": 60, "text": "training objective"}, {"st": 64, "ed": 66, "text": "gan training"}, {"st": 80, "ed": 82, "text": "wasserstein distance"}, {"st": 104, "ed": 106, "text": "wasserstein gan"}, {"st": 115, "ed": 117, "text": "gan training"}, {"st": 119, "ed": 121, "text": "wasserstein distance"}]
[{"st": 0, "ed": 2, "text": "missing data"}, {"st": 16, "ed": 18, "text": "missing data"}, {"st": 63, "ed": 65, "text": "real life"}, {"st": 70, "ed": 72, "text": "significantly outperforms"}]
[{"st": 6, "ed": 11, "text": "non negative matrix factorization nmf"}, {"st": 127, "ed": 129, "text": "empirical risk"}, {"st": 220, "ed": 223, "text": "non convex optimization"}, {"st": 228, "ed": 230, "text": "optimization methods"}]
[{"st": 7, "ed": 9, "text": "neural network"}, {"st": 11, "ed": 13, "text": "adversarial examples"}, {"st": 27, "ed": 29, "text": "generator network"}, {"st": 31, "ed": 33, "text": "adversarial perturbation"}, {"st": 59, "ed": 61, "text": "adversarial images"}, {"st": 80, "ed": 82, "text": "adversarial training"}, {"st": 89, "ed": 91, "text": "regularization methods"}, {"st": 99, "ed": 101, "text": "supervised learning"}, {"st": 114, "ed": 116, "text": "generalization error"}]
[{"st": 33, "ed": 35, "text": "greedy algorithm"}, {"st": 44, "ed": 46, "text": "constant factor"}, {"st": 83, "ed": 85, "text": "active learning"}]
[{"st": 5, "ed": 7, "text": "optimization problem"}, {"st": 42, "ed": 44, "text": "strongly convex"}, {"st": 51, "ed": 53, "text": "convergence rate"}, {"st": 101, "ed": 103, "text": "conduct experiments"}, {"st": 107, "ed": 109, "text": "outperforms existing"}]
[{"st": 15, "ed": 17, "text": "supervised learning"}, {"st": 23, "ed": 25, "text": "group level"}, {"st": 31, "ed": 33, "text": "current approaches"}, {"st": 98, "ed": 100, "text": "neural network"}, {"st": 104, "ed": 106, "text": "map inference"}, {"st": 136, "ed": 138, "text": "challenging problem"}]
[{"st": 40, "ed": 42, "text": "network structure"}]
[{"st": 1, "ed": 3, "text": "neural networks"}, {"st": 7, "ed": 9, "text": "challenging task"}, {"st": 29, "ed": 31, "text": "neural network"}, {"st": 51, "ed": 53, "text": "detection performance"}, {"st": 72, "ed": 75, "text": "accuracy and efficiency"}, {"st": 80, "ed": 82, "text": "input features"}, {"st": 91, "ed": 93, "text": "activation functions"}, {"st": 120, "ed": 122, "text": "synthetic datasets"}, {"st": 123, "ed": 125, "text": "real world"}]
[{"st": 15, "ed": 17, "text": "low dimensional"}, {"st": 46, "ed": 48, "text": "semi supervised"}, {"st": 71, "ed": 73, "text": "training data"}, {"st": 81, "ed": 83, "text": "training data"}, {"st": 92, "ed": 94, "text": "task performance"}, {"st": 113, "ed": 115, "text": "active learning"}, {"st": 119, "ed": 121, "text": "semi supervised"}, {"st": 151, "ed": 153, "text": "node classification"}, {"st": 224, "ed": 226, "text": "https github.com"}]
[{"st": 8, "ed": 11, "text": "multi armed bandits"}, {"st": 54, "ed": 56, "text": "additional assumptions"}, {"st": 94, "ed": 97, "text": "multi armed bandit"}]
[{"st": 9, "ed": 12, "text": "a posteriori map"}, {"st": 21, "ed": 23, "text": "statistical properties"}, {"st": 38, "ed": 40, "text": "prior probability"}, {"st": 40, "ed": 42, "text": "density function"}, {"st": 44, "ed": 49, "text": "alternating direction method of multipliers"}, {"st": 50, "ed": 52, "text": "based approach"}, {"st": 130, "ed": 132, "text": "theoretical guarantees"}, {"st": 135, "ed": 137, "text": "proposed framework"}, {"st": 163, "ed": 165, "text": "sparse models"}, {"st": 174, "ed": 177, "text": "mean square error"}, {"st": 193, "ed": 197, "text": "signal to noise ratio"}]
[{"st": 33, "ed": 35, "text": "multi layer"}, {"st": 47, "ed": 49, "text": "linear models"}, {"st": 59, "ed": 61, "text": "neural nets"}, {"st": 77, "ed": 79, "text": "linear models"}, {"st": 93, "ed": 95, "text": "theoretically sound"}, {"st": 96, "ed": 98, "text": "linear models"}]
[{"st": 7, "ed": 9, "text": "random forest"}, {"st": 12, "ed": 14, "text": "supervised learning"}, {"st": 61, "ed": 63, "text": "classification error"}, {"st": 86, "ed": 88, "text": "theoretical results"}, {"st": 91, "ed": 93, "text": "expected error"}, {"st": 114, "ed": 116, "text": "theoretical results"}, {"st": 127, "ed": 129, "text": "performance measures"}, {"st": 142, "ed": 145, "text": "mean squared error"}, {"st": 180, "ed": 182, "text": "computationally feasible"}, {"st": 186, "ed": 188, "text": "convergence properties"}]
[{"st": 13, "ed": 17, "text": "deep recurrent neural networks"}, {"st": 26, "ed": 28, "text": "recurrent units"}, {"st": 83, "ed": 85, "text": "deep learning"}]
[{"st": 0, "ed": 3, "text": "principal component analysis"}, {"st": 8, "ed": 10, "text": "dimensionality reduction"}, {"st": 39, "ed": 41, "text": "principal components"}, {"st": 65, "ed": 67, "text": "low rank"}, {"st": 71, "ed": 73, "text": "least squares"}, {"st": 73, "ed": 75, "text": "linear regression"}, {"st": 85, "ed": 87, "text": "loss function"}, {"st": 94, "ed": 96, "text": "least squares"}, {"st": 104, "ed": 106, "text": "dimensionality reduction"}, {"st": 109, "ed": 111, "text": "classification problems"}, {"st": 121, "ed": 123, "text": "classification problems"}, {"st": 146, "ed": 148, "text": "extensive experiments"}, {"st": 163, "ed": 165, "text": "classification error"}, {"st": 171, "ed": 173, "text": "statistically significant"}, {"st": 184, "ed": 186, "text": "least squares"}]
[{"st": 3, "ed": 6, "text": "problem of recovering"}, {"st": 8, "ed": 10, "text": "mathbf x"}, {"st": 25, "ed": 27, "text": "mathbf x"}, {"st": 36, "ed": 38, "text": "phase retrieval"}, {"st": 54, "ed": 56, "text": "ill posed"}, {"st": 56, "ed": 58, "text": "additional assumptions"}, {"st": 78, "ed": 80, "text": "mathbf x"}, {"st": 92, "ed": 94, "text": "phase retrieval"}, {"st": 95, "ed": 97, "text": "alternating minimization"}, {"st": 107, "ed": 109, "text": "alternating minimization"}, {"st": 111, "ed": 113, "text": "phase retrieval"}, {"st": 129, "ed": 131, "text": "sample complexity"}, {"st": 147, "ed": 149, "text": "existing results"}, {"st": 152, "ed": 154, "text": "linear convergence"}, {"st": 163, "ed": 165, "text": "tuning parameters"}, {"st": 181, "ed": 183, "text": "sparse signal"}, {"st": 185, "ed": 187, "text": "power law"}, {"st": 194, "ed": 196, "text": "sample complexity"}, {"st": 217, "ed": 219, "text": "mathbf x"}, {"st": 221, "ed": 223, "text": "structured sparsity"}, {"st": 260, "ed": 262, "text": "sample complexity"}, {"st": 269, "ed": 271, "text": "sufficiently large"}, {"st": 293, "ed": 296, "text": "end to end"}, {"st": 298, "ed": 300, "text": "phase retrieval"}, {"st": 303, "ed": 305, "text": "sample complexity"}]
[{"st": 0, "ed": 2, "text": "adversarial examples"}, {"st": 11, "ed": 13, "text": "deep learning"}, {"st": 14, "ed": 17, "text": "deep reinforcement learning"}, {"st": 18, "ed": 21, "text": "shown promising results"}, {"st": 42, "ed": 44, "text": "adversarial attacks"}, {"st": 45, "ed": 48, "text": "deep reinforcement learning"}, {"st": 57, "ed": 59, "text": "adversarial examples"}, {"st": 73, "ed": 75, "text": "adversarial examples"}, {"st": 95, "ed": 97, "text": "random noise"}]
[{"st": 3, "ed": 5, "text": "cluster analysis"}, {"st": 7, "ed": 9, "text": "k means"}, {"st": 58, "ed": 61, "text": "k means clustering"}, {"st": 76, "ed": 78, "text": "proposed method"}, {"st": 126, "ed": 128, "text": "empirical study"}, {"st": 147, "ed": 149, "text": "clustering results"}, {"st": 155, "ed": 157, "text": "real datasets"}]
[{"st": 42, "ed": 44, "text": "prediction tasks"}, {"st": 52, "ed": 54, "text": "predictive performance"}, {"st": 60, "ed": 63, "text": "bag of words"}, {"st": 64, "ed": 66, "text": "topic model"}]
[{"st": 0, "ed": 2, "text": "autoregressive models"}, {"st": 41, "ed": 43, "text": "autoregressive models"}, {"st": 77, "ed": 79, "text": "closely related"}]
[{"st": 3, "ed": 5, "text": "generic framework"}, {"st": 59, "ed": 61, "text": "mini batch"}, {"st": 105, "ed": 107, "text": "performance guarantees"}, {"st": 126, "ed": 128, "text": "strongly convex"}]
[{"st": 12, "ed": 14, "text": "point wise"}, {"st": 20, "ed": 22, "text": "real world"}, {"st": 25, "ed": 27, "text": "machine learning"}, {"st": 29, "ed": 31, "text": "hot topic"}, {"st": 35, "ed": 37, "text": "examples include"}, {"st": 48, "ed": 52, "text": "generative adversarial networks gans"}, {"st": 53, "ed": 55, "text": "image synthesis"}, {"st": 60, "ed": 62, "text": "approximate inference"}, {"st": 70, "ed": 72, "text": "existing approaches"}, {"st": 118, "ed": 120, "text": "score function"}, {"st": 138, "ed": 140, "text": "meta learning"}, {"st": 141, "ed": 143, "text": "approximate inference"}]
[{"st": 17, "ed": 19, "text": "wasserstein distance"}, {"st": 53, "ed": 55, "text": "total variation"}, {"st": 58, "ed": 60, "text": "l 2"}, {"st": 89, "ed": 91, "text": "image generation"}, {"st": 96, "ed": 99, "text": "kullback leibler kl"}, {"st": 101, "ed": 103, "text": "competitive performance"}, {"st": 118, "ed": 120, "text": "convergence properties"}, {"st": 130, "ed": 132, "text": "existing literature"}, {"st": 136, "ed": 138, "text": "ad hoc"}, {"st": 142, "ed": 144, "text": "cost functions"}, {"st": 146, "ed": 148, "text": "conceptual framework"}, {"st": 156, "ed": 158, "text": "cost functions"}, {"st": 167, "ed": 169, "text": "cost functions"}]
[{"st": 6, "ed": 8, "text": "logistic regression"}, {"st": 17, "ed": 19, "text": "convex surrogate"}, {"st": 46, "ed": 48, "text": "surrogate loss"}, {"st": 61, "ed": 63, "text": "weight vector"}, {"st": 67, "ed": 69, "text": "surrogate loss"}, {"st": 81, "ed": 83, "text": "surrogate loss"}, {"st": 87, "ed": 89, "text": "linear activation"}, {"st": 161, "ed": 163, "text": "surrogate loss"}, {"st": 174, "ed": 176, "text": "previous methods"}, {"st": 211, "ed": 213, "text": "surrogate loss"}]
[{"st": 3, "ed": 6, "text": "deep neural networks"}, {"st": 23, "ed": 25, "text": "general framework"}, {"st": 29, "ed": 31, "text": "deep learning"}, {"st": 39, "ed": 42, "text": "deep neural networks"}, {"st": 89, "ed": 91, "text": "input data"}, {"st": 112, "ed": 114, "text": "multiple layers"}, {"st": 133, "ed": 135, "text": "final output"}, {"st": 147, "ed": 150, "text": "deep neural nets"}, {"st": 153, "ed": 155, "text": "decision trees"}, {"st": 163, "ed": 165, "text": "random forest"}, {"st": 170, "ed": 173, "text": "proof of concept"}, {"st": 193, "ed": 195, "text": "deep learning"}]
[{"st": 4, "ed": 7, "text": "multi armed bandit"}, {"st": 36, "ed": 38, "text": "armed bandit"}, {"st": 77, "ed": 79, "text": "clinical trials"}, {"st": 173, "ed": 175, "text": "regret bounds"}]
[{"st": 2, "ed": 4, "text": "low rank"}, {"st": 7, "ed": 9, "text": "big data"}, {"st": 11, "ed": 13, "text": "recommender systems"}, {"st": 15, "ed": 17, "text": "topic models"}, {"st": 31, "ed": 33, "text": "low rank"}, {"st": 45, "ed": 47, "text": "low rank"}, {"st": 58, "ed": 60, "text": "low rank"}, {"st": 62, "ed": 64, "text": "big data"}, {"st": 68, "ed": 70, "text": "latent variable"}, {"st": 86, "ed": 88, "text": "latent variable"}, {"st": 99, "ed": 101, "text": "low rank"}]
[{"st": 6, "ed": 8, "text": "adversarial training"}, {"st": 44, "ed": 46, "text": "objective function"}, {"st": 66, "ed": 68, "text": "generative network"}, {"st": 87, "ed": 89, "text": "synthetic data"}]
[{"st": 1, "ed": 3, "text": "labeled data"}, {"st": 11, "ed": 13, "text": "real world"}, {"st": 27, "ed": 29, "text": "complementary labels"}, {"st": 30, "ed": 32, "text": "multi class"}, {"st": 47, "ed": 49, "text": "complementary labels"}, {"st": 76, "ed": 78, "text": "complementary labels"}, {"st": 115, "ed": 117, "text": "labeled data"}, {"st": 119, "ed": 121, "text": "loss function"}, {"st": 128, "ed": 130, "text": "estimation error"}, {"st": 133, "ed": 135, "text": "proposed method"}, {"st": 141, "ed": 143, "text": "convergence rate"}, {"st": 151, "ed": 153, "text": "complementary labels"}, {"st": 164, "ed": 166, "text": "supervised learning"}, {"st": 177, "ed": 179, "text": "experimentally demonstrate"}]
[{"st": 3, "ed": 6, "text": "stochastic gradient descent"}, {"st": 9, "ed": 11, "text": "nonconvex optimization"}, {"st": 14, "ed": 17, "text": "point of view"}, {"st": 47, "ed": 49, "text": "step size"}, {"st": 75, "ed": 77, "text": "saddle point"}, {"st": 81, "ed": 84, "text": "number of iterations"}, {"st": 104, "ed": 106, "text": "dynamical systems"}, {"st": 119, "ed": 121, "text": "stationary points"}, {"st": 129, "ed": 131, "text": "batch size"}, {"st": 133, "ed": 136, "text": "deep neural networks"}, {"st": 141, "ed": 143, "text": "batch size"}, {"st": 151, "ed": 153, "text": "stationary points"}, {"st": 164, "ed": 166, "text": "batch size"}]
[{"st": 13, "ed": 15, "text": "low rank"}, {"st": 22, "ed": 24, "text": "multi output"}, {"st": 34, "ed": 36, "text": "multi class"}, {"st": 37, "ed": 39, "text": "multi task"}, {"st": 77, "ed": 79, "text": "global convergence"}, {"st": 92, "ed": 94, "text": "classification tasks"}, {"st": 127, "ed": 129, "text": "multi output"}]
[{"st": 7, "ed": 10, "text": "goodness of fit"}, {"st": 11, "ed": 13, "text": "computational cost"}, {"st": 39, "ed": 41, "text": "false negative"}, {"st": 79, "ed": 81, "text": "mean shift"}, {"st": 143, "ed": 146, "text": "goodness of fit"}, {"st": 160, "ed": 163, "text": "maximum mean discrepancy"}]
[{"st": 8, "ed": 10, "text": "deep learning"}, {"st": 78, "ed": 80, "text": "recent results"}]
[{"st": 0, "ed": 3, "text": "stochastic gradient descent"}, {"st": 65, "ed": 67, "text": "convergence rates"}, {"st": 105, "ed": 107, "text": "local models"}, {"st": 136, "ed": 138, "text": "shared memory"}]
[{"st": 0, "ed": 2, "text": "point processes"}, {"st": 9, "ed": 11, "text": "sequential data"}, {"st": 24, "ed": 26, "text": "real world"}, {"st": 56, "ed": 58, "text": "maximum likelihood"}, {"st": 65, "ed": 67, "text": "multi modal"}, {"st": 80, "ed": 82, "text": "point processes"}, {"st": 101, "ed": 103, "text": "wasserstein distance"}, {"st": 109, "ed": 113, "text": "synthetic and real world"}, {"st": 120, "ed": 122, "text": "point process"}]
[{"st": 5, "ed": 7, "text": "robust optimization"}, {"st": 21, "ed": 23, "text": "probability distributions"}, {"st": 25, "ed": 27, "text": "observed data"}, {"st": 39, "ed": 41, "text": "machine learning"}, {"st": 49, "ed": 51, "text": "objective functions"}, {"st": 54, "ed": 56, "text": "wasserstein distance"}, {"st": 60, "ed": 63, "text": "kullback leibler kl"}, {"st": 85, "ed": 87, "text": "bregman divergence"}]
[{"st": 2, "ed": 4, "text": "multitask learning"}, {"st": 64, "ed": 66, "text": "main contribution"}, {"st": 72, "ed": 74, "text": "multiple tasks"}]
[{"st": 1, "ed": 4, "text": "short term memory"}, {"st": 5, "ed": 8, "text": "recurrent neural networks"}, {"st": 39, "ed": 41, "text": "machine learning"}, {"st": 53, "ed": 56, "text": "convolutional neural network"}]
[{"st": 11, "ed": 13, "text": "data science"}, {"st": 38, "ed": 41, "text": "generative adversarial networks"}, {"st": 50, "ed": 52, "text": "training data"}, {"st": 285, "ed": 287, "text": "social networks"}]
[{"st": 1, "ed": 3, "text": "optimization methods"}, {"st": 17, "ed": 19, "text": "becoming increasingly"}, {"st": 25, "ed": 27, "text": "examples include"}, {"st": 50, "ed": 53, "text": "stochastic gradient descent"}, {"st": 59, "ed": 61, "text": "binary classification"}, {"st": 66, "ed": 68, "text": "linearly separable"}, {"st": 92, "ed": 94, "text": "generalization capability"}, {"st": 103, "ed": 105, "text": "deep learning"}]
[{"st": 23, "ed": 25, "text": "sharing economy"}, {"st": 93, "ed": 95, "text": "public transport"}, {"st": 95, "ed": 97, "text": "open data"}, {"st": 112, "ed": 114, "text": "transfer learning"}, {"st": 129, "ed": 131, "text": "random forest"}, {"st": 175, "ed": 179, "text": "convolutional neural network cnn"}, {"st": 193, "ed": 195, "text": "feature set"}, {"st": 230, "ed": 232, "text": "transfer learning"}, {"st": 244, "ed": 246, "text": "similar accuracy"}, {"st": 248, "ed": 250, "text": "supervised learning"}]
[{"st": 4, "ed": 6, "text": "active learning"}, {"st": 45, "ed": 47, "text": "greedy algorithms"}, {"st": 51, "ed": 53, "text": "classification problem"}, {"st": 92, "ed": 94, "text": "frac 1"}, {"st": 95, "ed": 97, "text": "constant factor"}, {"st": 103, "ed": 105, "text": "worst case"}]
[{"st": 0, "ed": 2, "text": "large scale"}, {"st": 2, "ed": 4, "text": "kernel approximation"}, {"st": 9, "ed": 11, "text": "machine learning"}, {"st": 14, "ed": 17, "text": "random fourier features"}, {"st": 19, "ed": 21, "text": "increasingly popular"}, {"st": 26, "ed": 28, "text": "kernel approximation"}, {"st": 35, "ed": 37, "text": "monte carlo"}, {"st": 40, "ed": 42, "text": "monte carlo"}, {"st": 53, "ed": 55, "text": "current approaches"}, {"st": 87, "ed": 89, "text": "random features"}, {"st": 105, "ed": 107, "text": "randomized algorithm"}, {"st": 108, "ed": 110, "text": "large scale"}, {"st": 116, "ed": 118, "text": "empirical results"}, {"st": 120, "ed": 123, "text": "benchmark data sets"}, {"st": 135, "ed": 137, "text": "kernel approximation"}, {"st": 138, "ed": 140, "text": "supervised learning"}]
[{"st": 7, "ed": 9, "text": "machine learning"}, {"st": 14, "ed": 16, "text": "recent research"}, {"st": 63, "ed": 65, "text": "mixture model"}, {"st": 72, "ed": 74, "text": "mixture model"}, {"st": 102, "ed": 104, "text": "text mining"}, {"st": 112, "ed": 114, "text": "proposed approach"}]
[{"st": 0, "ed": 2, "text": "contextual bandits"}, {"st": 6, "ed": 9, "text": "multi armed bandit"}, {"st": 17, "ed": 19, "text": "side information"}, {"st": 50, "ed": 53, "text": "multi task learning"}, {"st": 55, "ed": 57, "text": "contextual bandit"}, {"st": 59, "ed": 62, "text": "multi task learning"}, {"st": 64, "ed": 66, "text": "batch setting"}, {"st": 93, "ed": 96, "text": "upper confidence bound"}, {"st": 97, "ed": 100, "text": "multi task learning"}, {"st": 102, "ed": 104, "text": "contextual bandits"}, {"st": 107, "ed": 109, "text": "regret bound"}]
[{"st": 19, "ed": 21, "text": "partially observed"}, {"st": 97, "ed": 100, "text": "k nearest neighbors"}, {"st": 132, "ed": 134, "text": "conduct experiments"}, {"st": 151, "ed": 153, "text": "based methods"}, {"st": 165, "ed": 167, "text": "matrix factorization"}]
[{"st": 1, "ed": 3, "text": "recent works"}, {"st": 7, "ed": 9, "text": "convolutional neural"}, {"st": 36, "ed": 38, "text": "mathematical model"}, {"st": 39, "ed": 41, "text": "sparse signal"}, {"st": 62, "ed": 64, "text": "compressive sensing"}, {"st": 93, "ed": 95, "text": "theoretical framework"}]
[{"st": 2, "ed": 4, "text": "computational efficiency"}, {"st": 5, "ed": 7, "text": "deep learning"}, {"st": 35, "ed": 38, "text": "point of view"}, {"st": 40, "ed": 42, "text": "sparsity inducing"}, {"st": 80, "ed": 82, "text": "fixed point"}]
[{"st": 2, "ed": 4, "text": "non stationary"}, {"st": 7, "ed": 9, "text": "gaussian process"}, {"st": 15, "ed": 17, "text": "spectral density"}, {"st": 19, "ed": 21, "text": "non stationary"}, {"st": 21, "ed": 23, "text": "kernel function"}, {"st": 27, "ed": 29, "text": "input dependent"}, {"st": 29, "ed": 31, "text": "gaussian process"}, {"st": 49, "ed": 51, "text": "non stationary"}, {"st": 58, "ed": 60, "text": "input dependent"}, {"st": 71, "ed": 73, "text": "efficient inference"}, {"st": 82, "ed": 84, "text": "case studies"}, {"st": 94, "ed": 96, "text": "time series"}, {"st": 101, "ed": 103, "text": "non stationary"}]
[{"st": 1, "ed": 3, "text": "deep learning"}, {"st": 8, "ed": 11, "text": "stochastic gradient descent"}, {"st": 41, "ed": 43, "text": "batch sizes"}, {"st": 49, "ed": 51, "text": "generalization performance"}, {"st": 77, "ed": 79, "text": "learning rate"}, {"st": 102, "ed": 104, "text": "random walk"}, {"st": 107, "ed": 109, "text": "statistical model"}, {"st": 123, "ed": 125, "text": "conducted experiments"}, {"st": 143, "ed": 145, "text": "batch size"}, {"st": 176, "ed": 178, "text": "batch normalization"}, {"st": 202, "ed": 205, "text": "mnist cifar 10"}, {"st": 205, "ed": 207, "text": "cifar 100"}, {"st": 219, "ed": 221, "text": "deep models"}]
[{"st": 1, "ed": 3, "text": "individual level"}, {"st": 6, "ed": 8, "text": "observational data"}, {"st": 37, "ed": 39, "text": "observational data"}, {"st": 54, "ed": 56, "text": "carefully designed"}, {"st": 56, "ed": 58, "text": "observational study"}, {"st": 90, "ed": 92, "text": "recent advances"}, {"st": 93, "ed": 95, "text": "latent variable"}, {"st": 101, "ed": 103, "text": "latent space"}, {"st": 115, "ed": 117, "text": "variational autoencoders"}, {"st": 121, "ed": 123, "text": "causal structure"}, {"st": 136, "ed": 138, "text": "existing methods"}]
[{"st": 18, "ed": 20, "text": "supervised learning"}, {"st": 79, "ed": 81, "text": "convex function"}, {"st": 89, "ed": 91, "text": "convex optimization"}, {"st": 99, "ed": 101, "text": "gradient based"}, {"st": 120, "ed": 122, "text": "loss functions"}, {"st": 130, "ed": 132, "text": "correctly classified"}, {"st": 137, "ed": 139, "text": "learning theory"}, {"st": 153, "ed": 155, "text": "error bounds"}, {"st": 169, "ed": 171, "text": "binary classification"}, {"st": 174, "ed": 177, "text": "synthetic and real"}]
[{"st": 15, "ed": 18, "text": "factors of variation"}, {"st": 33, "ed": 35, "text": "latent representation"}, {"st": 64, "ed": 66, "text": "face images"}, {"st": 92, "ed": 94, "text": "probabilistic models"}, {"st": 107, "ed": 109, "text": "multi level"}, {"st": 109, "ed": 111, "text": "variational autoencoder"}, {"st": 116, "ed": 118, "text": "probabilistic model"}, {"st": 134, "ed": 136, "text": "latent representation"}, {"st": 137, "ed": 139, "text": "semantically meaningful"}, {"st": 145, "ed": 147, "text": "group level"}, {"st": 157, "ed": 160, "text": "quantitative and qualitative"}, {"st": 170, "ed": 172, "text": "semantically meaningful"}, {"st": 181, "ed": 183, "text": "latent representation"}]
[{"st": 5, "ed": 8, "text": "unsupervised domain adaptation"}, {"st": 34, "ed": 36, "text": "source domain"}, {"st": 50, "ed": 52, "text": "linear transformation"}, {"st": 79, "ed": 81, "text": "optimal transport"}, {"st": 144, "ed": 146, "text": "loss functions"}, {"st": 149, "ed": 151, "text": "real world"}, {"st": 151, "ed": 154, "text": "classification and regression"}]
[{"st": 3, "ed": 5, "text": "kernel density"}, {"st": 9, "ed": 11, "text": "kernel bandwidth"}, {"st": 16, "ed": 18, "text": "sample size"}, {"st": 74, "ed": 76, "text": "square integrable"}, {"st": 80, "ed": 83, "text": "rates of convergence"}, {"st": 107, "ed": 110, "text": "compares favorably to"}, {"st": 115, "ed": 117, "text": "previously proposed"}]
[{"st": 4, "ed": 6, "text": "computational complexity"}, {"st": 7, "ed": 10, "text": "deep neural networks"}, {"st": 13, "ed": 16, "text": "taking advantage of"}, {"st": 53, "ed": 55, "text": "coarse grained"}, {"st": 65, "ed": 67, "text": "hardware acceleration"}, {"st": 88, "ed": 90, "text": "prediction accuracy"}, {"st": 102, "ed": 104, "text": "structured sparsity"}, {"st": 110, "ed": 112, "text": "coarse grained"}, {"st": 133, "ed": 135, "text": "coarse grained"}, {"st": 142, "ed": 144, "text": "compression ratio"}, {"st": 145, "ed": 147, "text": "fine grained"}, {"st": 158, "ed": 161, "text": "convolutional neural network"}, {"st": 168, "ed": 170, "text": "coarse grained"}, {"st": 179, "ed": 181, "text": "fine grained"}, {"st": 189, "ed": 192, "text": "orders of magnitude"}]
[{"st": 0, "ed": 3, "text": "generative adversarial networks"}, {"st": 13, "ed": 15, "text": "objective function"}, {"st": 28, "ed": 30, "text": "empirical success"}, {"st": 66, "ed": 68, "text": "objective functions"}, {"st": 83, "ed": 85, "text": "objective function"}, {"st": 123, "ed": 125, "text": "recently proposed"}, {"st": 132, "ed": 134, "text": "objective function"}, {"st": 150, "ed": 152, "text": "moment matching"}, {"st": 158, "ed": 160, "text": "objective functions"}, {"st": 168, "ed": 170, "text": "objective function"}]
[{"st": 8, "ed": 10, "text": "thompson sampling"}, {"st": 30, "ed": 32, "text": "theoretical analysis"}, {"st": 40, "ed": 42, "text": "thompson sampling"}, {"st": 128, "ed": 131, "text": "hyper parameter tuning"}]
[{"st": 35, "ed": 37, "text": "step sizes"}, {"st": 55, "ed": 57, "text": "nuclear norm"}]
[{"st": 9, "ed": 11, "text": "generative model"}, {"st": 47, "ed": 49, "text": "nearest neighbor"}, {"st": 52, "ed": 54, "text": "training set"}, {"st": 63, "ed": 65, "text": "generative model"}, {"st": 67, "ed": 69, "text": "probability density"}, {"st": 86, "ed": 88, "text": "latent representation"}]
[{"st": 5, "ed": 7, "text": "neural network"}, {"st": 55, "ed": 57, "text": "neural network"}]
[{"st": 0, "ed": 3, "text": "deep generative models"}, {"st": 5, "ed": 9, "text": "generative adversarial networks gans"}, {"st": 79, "ed": 82, "text": "low computational cost"}, {"st": 86, "ed": 88, "text": "gan training"}, {"st": 103, "ed": 105, "text": "image generation"}, {"st": 113, "ed": 115, "text": "building blocks"}]
[{"st": 3, "ed": 5, "text": "k nn"}, {"st": 8, "ed": 11, "text": "classification and regression"}, {"st": 35, "ed": 37, "text": "posterior probability"}, {"st": 54, "ed": 58, "text": "markov chain monte carlo"}, {"st": 106, "ed": 108, "text": "distance metric"}, {"st": 119, "ed": 122, "text": "change point detection"}, {"st": 138, "ed": 140, "text": "change point"}, {"st": 148, "ed": 150, "text": "de facto"}, {"st": 152, "ed": 154, "text": "posterior probability"}, {"st": 188, "ed": 190, "text": "posterior probability"}]
[{"st": 14, "ed": 17, "text": "multi armed bandit"}, {"st": 21, "ed": 23, "text": "heavy tailed"}, {"st": 23, "ed": 25, "text": "reward distributions"}, {"st": 58, "ed": 60, "text": "probability distributions"}, {"st": 172, "ed": 175, "text": "upper confidence bound"}, {"st": 182, "ed": 184, "text": "regret bound"}, {"st": 186, "ed": 188, "text": "mathcal o"}, {"st": 192, "ed": 195, "text": "exploration and exploitation"}, {"st": 204, "ed": 206, "text": "heavy tailed"}]
[{"st": 2, "ed": 4, "text": "deep forest"}, {"st": 7, "ed": 9, "text": "metric learning"}, {"st": 20, "ed": 22, "text": "deep forest"}, {"st": 41, "ed": 43, "text": "fully supervised"}, {"st": 48, "ed": 50, "text": "class labels"}, {"st": 52, "ed": 54, "text": "training examples"}, {"st": 57, "ed": 59, "text": "main idea"}, {"st": 67, "ed": 69, "text": "decision trees"}, {"st": 70, "ed": 72, "text": "random forest"}, {"st": 99, "ed": 101, "text": "objective function"}, {"st": 110, "ed": 112, "text": "optimization problem"}, {"st": 119, "ed": 121, "text": "numerical experiments"}, {"st": 124, "ed": 126, "text": "distance metric"}]
[{"st": 0, "ed": 4, "text": "generative adversarial networks gans"}, {"st": 20, "ed": 22, "text": "recent works"}, {"st": 124, "ed": 126, "text": "semi supervised"}]
[{"st": 0, "ed": 2, "text": "lifelong learning"}, {"st": 18, "ed": 20, "text": "previous tasks"}, {"st": 49, "ed": 51, "text": "lifelong learning"}, {"st": 53, "ed": 55, "text": "generative modeling"}, {"st": 161, "ed": 164, "text": "ability to learn"}, {"st": 166, "ed": 168, "text": "latent representation"}, {"st": 171, "ed": 173, "text": "transfer learning"}]
[{"st": 2, "ed": 4, "text": "machine learning"}, {"st": 55, "ed": 57, "text": "latent variable"}, {"st": 57, "ed": 59, "text": "multiple output"}, {"st": 59, "ed": 61, "text": "gaussian processes"}, {"st": 91, "ed": 93, "text": "gaussian processes"}, {"st": 96, "ed": 98, "text": "latent space"}, {"st": 108, "ed": 110, "text": "variational inference"}, {"st": 116, "ed": 118, "text": "computational complexity"}, {"st": 129, "ed": 131, "text": "significantly outperforms"}, {"st": 132, "ed": 134, "text": "gaussian process"}]
[{"st": 3, "ed": 6, "text": "problem of classifying"}, {"st": 31, "ed": 33, "text": "training examples"}, {"st": 40, "ed": 42, "text": "iterative algorithm"}, {"st": 48, "ed": 50, "text": "cutting plane"}, {"st": 63, "ed": 65, "text": "maximum margin"}, {"st": 80, "ed": 83, "text": "number of iterations"}, {"st": 130, "ed": 132, "text": "generalization performance"}, {"st": 135, "ed": 137, "text": "maximum margin"}]
[{"st": 44, "ed": 46, "text": "decision theoretic"}, {"st": 79, "ed": 82, "text": "best arm identification"}, {"st": 141, "ed": 143, "text": "asymptotically optimal"}, {"st": 145, "ed": 148, "text": "best arm identification"}]
[{"st": 7, "ed": 9, "text": "invariant representations"}, {"st": 25, "ed": 27, "text": "coordinate descent"}, {"st": 30, "ed": 32, "text": "efficiently solve"}, {"st": 33, "ed": 35, "text": "ell 1"}, {"st": 35, "ed": 37, "text": "minimization problems"}, {"st": 70, "ed": 72, "text": "optimal solution"}, {"st": 89, "ed": 91, "text": "numerical experiments"}]
[{"st": 24, "ed": 26, "text": "mathbb r"}, {"st": 51, "ed": 53, "text": "linear subspace"}, {"st": 54, "ed": 56, "text": "mathbb r"}, {"st": 82, "ed": 84, "text": "problems including"}, {"st": 84, "ed": 87, "text": "principal component analysis"}, {"st": 87, "ed": 89, "text": "latent semantic"}, {"st": 116, "ed": 118, "text": "randomized algorithm"}, {"st": 157, "ed": 159, "text": "randomized algorithm"}, {"st": 160, "ed": 164, "text": "synthetic and real data"}]
[{"st": 10, "ed": 12, "text": "resource constrained"}, {"st": 25, "ed": 27, "text": "prediction model"}, {"st": 43, "ed": 45, "text": "without sacrificing"}, {"st": 50, "ed": 52, "text": "prediction models"}, {"st": 54, "ed": 57, "text": "labeled training data"}, {"st": 78, "ed": 80, "text": "low complexity"}, {"st": 82, "ed": 84, "text": "prediction model"}, {"st": 97, "ed": 99, "text": "low cost"}, {"st": 104, "ed": 106, "text": "highly accurate"}, {"st": 110, "ed": 112, "text": "empirical loss"}, {"st": 112, "ed": 114, "text": "minimization problem"}, {"st": 128, "ed": 130, "text": "benchmark datasets"}, {"st": 131, "ed": 133, "text": "method outperforms"}, {"st": 138, "ed": 140, "text": "higher accuracy"}]
[{"st": 4, "ed": 6, "text": "patient specific"}, {"st": 15, "ed": 17, "text": "decision making"}, {"st": 30, "ed": 32, "text": "deep learning"}, {"st": 77, "ed": 79, "text": "log likelihood"}, {"st": 81, "ed": 83, "text": "deep learning"}, {"st": 83, "ed": 85, "text": "approach outperforms"}, {"st": 99, "ed": 101, "text": "survival analysis"}]
[{"st": 2, "ed": 4, "text": "neural network"}, {"st": 120, "ed": 123, "text": "cross entropy loss"}, {"st": 138, "ed": 140, "text": "loss functions"}, {"st": 145, "ed": 147, "text": "loss functions"}, {"st": 182, "ed": 184, "text": "a 20"}, {"st": 186, "ed": 188, "text": "performance improvement"}, {"st": 210, "ed": 213, "text": "sheds light on"}, {"st": 220, "ed": 222, "text": "loss functions"}, {"st": 224, "ed": 226, "text": "neural network"}, {"st": 239, "ed": 241, "text": "https github.com"}]
[{"st": 7, "ed": 10, "text": "sequential decision making"}, {"st": 21, "ed": 23, "text": "reinforcement learning"}, {"st": 33, "ed": 35, "text": "theoretical understanding"}, {"st": 78, "ed": 80, "text": "learning rate"}, {"st": 101, "ed": 103, "text": "main results"}, {"st": 108, "ed": 110, "text": "exploration strategy"}, {"st": 113, "ed": 115, "text": "learning rate"}, {"st": 132, "ed": 134, "text": "near optimal"}, {"st": 174, "ed": 176, "text": "learning rates"}, {"st": 184, "ed": 186, "text": "regret bound"}, {"st": 226, "ed": 228, "text": "performance bounds"}]
[{"st": 6, "ed": 8, "text": "great success"}, {"st": 9, "ed": 11, "text": "natural language"}, {"st": 16, "ed": 18, "text": "compact representations"}, {"st": 57, "ed": 60, "text": "graph structured data"}, {"st": 72, "ed": 74, "text": "improve performance"}, {"st": 75, "ed": 77, "text": "tasks including"}, {"st": 154, "ed": 156, "text": "significantly improves"}, {"st": 158, "ed": 160, "text": "downstream tasks"}, {"st": 162, "ed": 164, "text": "real world"}]
[{"st": 10, "ed": 12, "text": "machine learning"}, {"st": 24, "ed": 27, "text": "multi task learning"}, {"st": 44, "ed": 46, "text": "optimization method"}, {"st": 70, "ed": 72, "text": "fault tolerance"}, {"st": 79, "ed": 81, "text": "method achieves"}, {"st": 81, "ed": 83, "text": "significant speedups"}, {"st": 96, "ed": 98, "text": "real world"}]
[{"st": 8, "ed": 10, "text": "machine teaching"}, {"st": 11, "ed": 13, "text": "inverse problem"}, {"st": 19, "ed": 21, "text": "machine teaching"}, {"st": 38, "ed": 40, "text": "iterative algorithm"}, {"st": 81, "ed": 83, "text": "training set"}, {"st": 87, "ed": 89, "text": "machine teaching"}, {"st": 92, "ed": 94, "text": "fast convergence"}, {"st": 126, "ed": 128, "text": "faster convergence"}, {"st": 136, "ed": 138, "text": "theoretical findings"}, {"st": 139, "ed": 141, "text": "extensive experiments"}]
[{"st": 3, "ed": 5, "text": "prior knowledge"}, {"st": 57, "ed": 59, "text": "related tasks"}, {"st": 89, "ed": 91, "text": "multiple tasks"}, {"st": 110, "ed": 113, "text": "feedforward neural network"}, {"st": 139, "ed": 141, "text": "weight sharing"}, {"st": 146, "ed": 148, "text": "takes place"}, {"st": 152, "ed": 154, "text": "feedforward networks"}, {"st": 155, "ed": 158, "text": "stochastic gradient descent"}, {"st": 165, "ed": 167, "text": "domain adaptation"}, {"st": 168, "ed": 171, "text": "multi task learning"}, {"st": 187, "ed": 189, "text": "numerical experiments"}, {"st": 195, "ed": 197, "text": "domain adaptation"}, {"st": 198, "ed": 200, "text": "transfer learning"}, {"st": 202, "ed": 204, "text": "provide evidence"}, {"st": 208, "ed": 210, "text": "task oriented"}]
[{"st": 5, "ed": 8, "text": "received much attention"}, {"st": 10, "ed": 12, "text": "machine learning"}, {"st": 15, "ed": 18, "text": "kullback leibler divergence"}, {"st": 24, "ed": 27, "text": "the wasserstein metric"}, {"st": 67, "ed": 69, "text": "machine learning"}, {"st": 77, "ed": 80, "text": "the wasserstein metric"}, {"st": 88, "ed": 91, "text": "kullback leibler divergence"}, {"st": 98, "ed": 100, "text": "empirical evidence"}, {"st": 119, "ed": 122, "text": "the wasserstein metric"}, {"st": 136, "ed": 138, "text": "desired properties"}, {"st": 145, "ed": 147, "text": "kullback leibler"}, {"st": 167, "ed": 171, "text": "generative adversarial network gan"}]
[{"st": 11, "ed": 13, "text": "real valued"}, {"st": 25, "ed": 29, "text": "recurrent neural networks rnns"}, {"st": 34, "ed": 36, "text": "real valued"}, {"st": 47, "ed": 49, "text": "latent space"}, {"st": 64, "ed": 66, "text": "conditional distributions"}, {"st": 93, "ed": 96, "text": "real world data"}, {"st": 108, "ed": 110, "text": "negative log"}, {"st": 113, "ed": 115, "text": "neural network"}, {"st": 133, "ed": 135, "text": "anomaly detection"}]
[{"st": 12, "ed": 14, "text": "feature selection"}, {"st": 37, "ed": 39, "text": "significantly improves"}, {"st": 129, "ed": 131, "text": "computational efficiency"}, {"st": 134, "ed": 137, "text": "orders of magnitude"}, {"st": 154, "ed": 156, "text": "proposed algorithm"}, {"st": 157, "ed": 159, "text": "feature selection"}, {"st": 174, "ed": 176, "text": "proposed algorithm"}]
[{"st": 10, "ed": 12, "text": "sample based"}, {"st": 40, "ed": 44, "text": "generative adversarial networks gans"}, {"st": 45, "ed": 47, "text": "reinforcement learning"}, {"st": 59, "ed": 61, "text": "generation process"}, {"st": 69, "ed": 71, "text": "reward function"}, {"st": 84, "ed": 86, "text": "previous results"}, {"st": 131, "ed": 133, "text": "generation process"}]
[{"st": 27, "ed": 29, "text": "low rank"}, {"st": 88, "ed": 90, "text": "sample complexity"}, {"st": 116, "ed": 118, "text": "statistical analysis"}]
[{"st": 21, "ed": 23, "text": "low complexity"}, {"st": 23, "ed": 25, "text": "linear model"}, {"st": 29, "ed": 31, "text": "linear transformations"}, {"st": 35, "ed": 37, "text": "linear combinations"}, {"st": 40, "ed": 42, "text": "linear transformations"}, {"st": 44, "ed": 46, "text": "linear model"}]
[{"st": 0, "ed": 3, "text": "deep neural network"}, {"st": 3, "ed": 5, "text": "dnn based"}, {"st": 10, "ed": 12, "text": "reinforcement learning"}, {"st": 64, "ed": 66, "text": "dnn based"}, {"st": 69, "ed": 71, "text": "deep architectures"}, {"st": 103, "ed": 105, "text": "computationally efficient"}, {"st": 115, "ed": 117, "text": "sequential decision"}, {"st": 159, "ed": 161, "text": "without sacrificing"}, {"st": 170, "ed": 172, "text": "loss functions"}]
[{"st": 5, "ed": 7, "text": "deep learning"}, {"st": 39, "ed": 42, "text": "simple and effective"}, {"st": 48, "ed": 50, "text": "norm regularization"}, {"st": 57, "ed": 59, "text": "weight matrices"}, {"st": 79, "ed": 81, "text": "norm regularization"}]
[{"st": 0, "ed": 2, "text": "kernel methods"}, {"st": 36, "ed": 38, "text": "large scale"}, {"st": 42, "ed": 44, "text": "computational requirements"}, {"st": 62, "ed": 64, "text": "kernel methods"}, {"st": 92, "ed": 94, "text": "theoretical analysis"}, {"st": 113, "ed": 115, "text": "extensive experimental"}, {"st": 117, "ed": 119, "text": "large scale"}, {"st": 128, "ed": 130, "text": "outperforms previous"}]
[{"st": 1, "ed": 3, "text": "optimization methods"}, {"st": 5, "ed": 7, "text": "matching pursuit"}, {"st": 9, "ed": 11, "text": "frank wolfe"}, {"st": 16, "ed": 18, "text": "recent years"}, {"st": 33, "ed": 35, "text": "linear span"}, {"st": 37, "ed": 39, "text": "convex hull"}, {"st": 57, "ed": 59, "text": "convex cone"}, {"st": 85, "ed": 87, "text": "convergence rates"}, {"st": 97, "ed": 99, "text": "mathcal o"}, {"st": 106, "ed": 108, "text": "convex objectives"}, {"st": 109, "ed": 111, "text": "linear convergence"}, {"st": 111, "ed": 113, "text": "mathcal o"}, {"st": 116, "ed": 118, "text": "strongly convex"}, {"st": 156, "ed": 158, "text": "objective functions"}]
[{"st": 22, "ed": 26, "text": "deep convolutional neural networks"}, {"st": 28, "ed": 30, "text": "labeled datasets"}, {"st": 41, "ed": 44, "text": "undirected graphical model"}, {"st": 56, "ed": 58, "text": "semi supervised"}, {"st": 109, "ed": 111, "text": "cifar 10"}, {"st": 112, "ed": 114, "text": "ms coco"}]
[{"st": 7, "ed": 9, "text": "decision making"}, {"st": 26, "ed": 28, "text": "machine learning"}, {"st": 53, "ed": 55, "text": "decision making"}, {"st": 63, "ed": 65, "text": "experimentally demonstrate"}]
[{"st": 0, "ed": 2, "text": "generalized linear"}, {"st": 5, "ed": 7, "text": "natural extension"}, {"st": 10, "ed": 12, "text": "linear bandits"}, {"st": 32, "ed": 35, "text": "number of arms"}, {"st": 54, "ed": 56, "text": "unlike existing"}, {"st": 95, "ed": 97, "text": "generalized linear"}, {"st": 111, "ed": 114, "text": "online learning algorithm"}, {"st": 123, "ed": 125, "text": "special case"}, {"st": 178, "ed": 180, "text": "inner product"}, {"st": 204, "ed": 206, "text": "thompson sampling"}, {"st": 213, "ed": 215, "text": "regret bound"}, {"st": 216, "ed": 218, "text": "d dimensional"}, {"st": 228, "ed": 230, "text": "regret bound"}, {"st": 246, "ed": 248, "text": "regret bound"}, {"st": 258, "ed": 260, "text": "fast approximate"}, {"st": 263, "ed": 265, "text": "inner product"}]
[{"st": 4, "ed": 6, "text": "probabilistic framework"}, {"st": 19, "ed": 21, "text": "large scale"}, {"st": 21, "ed": 23, "text": "classification task"}, {"st": 42, "ed": 44, "text": "feature based"}, {"st": 48, "ed": 50, "text": "neural network"}, {"st": 72, "ed": 74, "text": "probabilistic model"}, {"st": 81, "ed": 83, "text": "neural network"}, {"st": 99, "ed": 101, "text": "probabilistic model"}, {"st": 127, "ed": 129, "text": "well calibrated"}, {"st": 138, "ed": 140, "text": "recent approaches"}]
[{"st": 0, "ed": 3, "text": "deep generative models"}, {"st": 4, "ed": 6, "text": "achieved impressive"}, {"st": 10, "ed": 14, "text": "generative adversarial networks gans"}, {"st": 15, "ed": 17, "text": "variational autoencoders"}, {"st": 22, "ed": 24, "text": "deep generative"}, {"st": 46, "ed": 48, "text": "deep generative"}, {"st": 94, "ed": 96, "text": "powerful tool"}]
[{"st": 0, "ed": 2, "text": "causal discovery"}, {"st": 13, "ed": 15, "text": "observational data"}, {"st": 105, "ed": 107, "text": "sample complexity"}, {"st": 117, "ed": 120, "text": "continuous and discrete"}, {"st": 133, "ed": 135, "text": "sample complexity"}]
[{"st": 1, "ed": 3, "text": "statistical theory"}]
[{"st": 7, "ed": 9, "text": "real world"}, {"st": 29, "ed": 31, "text": "learning theory"}, {"st": 34, "ed": 36, "text": "compressive sensing"}, {"st": 54, "ed": 56, "text": "sparsity inducing"}, {"st": 63, "ed": 66, "text": "a sufficient condition"}, {"st": 72, "ed": 74, "text": "globally optimal"}, {"st": 81, "ed": 83, "text": "globally optimal"}, {"st": 110, "ed": 112, "text": "ell 0"}, {"st": 115, "ed": 117, "text": "ell 1"}, {"st": 139, "ed": 142, "text": "times faster than"}, {"st": 156, "ed": 158, "text": "ell 1"}, {"st": 172, "ed": 174, "text": "ell 1"}]
[{"st": 1, "ed": 3, "text": "machine learning"}, {"st": 3, "ed": 5, "text": "ensemble methods"}, {"st": 26, "ed": 28, "text": "random forests"}, {"st": 90, "ed": 92, "text": "proposed algorithm"}, {"st": 102, "ed": 104, "text": "large scale"}]
[{"st": 0, "ed": 2, "text": "missing data"}, {"st": 7, "ed": 9, "text": "real life"}, {"st": 11, "ed": 13, "text": "machine learning"}, {"st": 21, "ed": 23, "text": "missing values"}, {"st": 47, "ed": 49, "text": "machine learning"}, {"st": 57, "ed": 59, "text": "classification algorithm"}, {"st": 90, "ed": 92, "text": "genetic programming"}, {"st": 110, "ed": 112, "text": "recently introduced"}, {"st": 128, "ed": 130, "text": "machine learning"}, {"st": 135, "ed": 137, "text": "genetic programming"}, {"st": 153, "ed": 155, "text": "pre processing"}, {"st": 161, "ed": 163, "text": "classification problems"}]
[{"st": 0, "ed": 2, "text": "large scale"}, {"st": 35, "ed": 37, "text": "large scale"}, {"st": 52, "ed": 54, "text": "prediction errors"}, {"st": 56, "ed": 58, "text": "higher levels"}, {"st": 76, "ed": 78, "text": "higher levels"}, {"st": 109, "ed": 112, "text": "local and global"}, {"st": 113, "ed": 115, "text": "hierarchical structure"}, {"st": 127, "ed": 129, "text": "extensive empirical"}, {"st": 136, "ed": 139, "text": "image and text"}, {"st": 147, "ed": 149, "text": "training instances"}, {"st": 153, "ed": 155, "text": "classification performance"}, {"st": 178, "ed": 180, "text": "source code"}]
[{"st": 40, "ed": 42, "text": "exploration strategy"}, {"st": 44, "ed": 46, "text": "upper confidence"}, {"st": 52, "ed": 54, "text": "significant gains"}]
[{"st": 6, "ed": 8, "text": "hyperparameter optimization"}, {"st": 31, "ed": 33, "text": "examples include"}, {"st": 33, "ed": 35, "text": "grid search"}, {"st": 36, "ed": 38, "text": "random search"}, {"st": 53, "ed": 56, "text": "determinantal point processes"}, {"st": 57, "ed": 59, "text": "hyperparameter optimization"}, {"st": 66, "ed": 68, "text": "random search"}, {"st": 86, "ed": 88, "text": "search spaces"}, {"st": 127, "ed": 130, "text": "discrete and continuous"}]
[{"st": 0, "ed": 2, "text": "large scale"}, {"st": 2, "ed": 4, "text": "hierarchical classification"}, {"st": 15, "ed": 17, "text": "training instances"}, {"st": 23, "ed": 25, "text": "big data"}, {"st": 26, "ed": 28, "text": "feature selection"}, {"st": 44, "ed": 46, "text": "large scale"}, {"st": 52, "ed": 54, "text": "training process"}, {"st": 80, "ed": 82, "text": "feature selection"}, {"st": 90, "ed": 92, "text": "classification accuracy"}, {"st": 104, "ed": 106, "text": "feature selection"}, {"st": 108, "ed": 110, "text": "dimensionality reduction"}, {"st": 113, "ed": 115, "text": "large scale"}, {"st": 121, "ed": 124, "text": "text and image"}, {"st": 152, "ed": 154, "text": "weight vectors"}, {"st": 169, "ed": 171, "text": "source code"}]
[{"st": 0, "ed": 3, "text": "multi task learning"}, {"st": 6, "ed": 8, "text": "supervised learning"}, {"st": 12, "ed": 14, "text": "prediction models"}, {"st": 16, "ed": 18, "text": "related tasks"}, {"st": 19, "ed": 21, "text": "learned jointly"}, {"st": 32, "ed": 34, "text": "training examples"}, {"st": 57, "ed": 59, "text": "based approach"}, {"st": 76, "ed": 79, "text": "multi class classification"}, {"st": 85, "ed": 87, "text": "binary classification"}, {"st": 103, "ed": 105, "text": "linear discriminant"}, {"st": 132, "ed": 134, "text": "nearest neighbor"}, {"st": 141, "ed": 143, "text": "transfer learning"}, {"st": 159, "ed": 162, "text": "semi supervised learning"}, {"st": 164, "ed": 166, "text": "empirical results"}, {"st": 184, "ed": 186, "text": "training examples"}]
[{"st": 0, "ed": 3, "text": "multi task learning"}, {"st": 9, "ed": 11, "text": "generalization performance"}, {"st": 14, "ed": 16, "text": "related tasks"}, {"st": 24, "ed": 26, "text": "batch setting"}, {"st": 40, "ed": 42, "text": "online learning"}, {"st": 52, "ed": 54, "text": "existing algorithms"}, {"st": 67, "ed": 69, "text": "weight matrix"}, {"st": 98, "ed": 100, "text": "weight matrix"}, {"st": 108, "ed": 110, "text": "low rank"}, {"st": 116, "ed": 118, "text": "nuclear norm"}, {"st": 133, "ed": 135, "text": "theoretical analysis"}, {"st": 137, "ed": 139, "text": "proposed algorithm"}, {"st": 150, "ed": 152, "text": "linear model"}, {"st": 163, "ed": 165, "text": "nuclear norm"}, {"st": 170, "ed": 172, "text": "singular values"}, {"st": 178, "ed": 180, "text": "low rank"}, {"st": 211, "ed": 213, "text": "closed form"}, {"st": 224, "ed": 226, "text": "real world"}]
[{"st": 0, "ed": 2, "text": "generalization error"}, {"st": 19, "ed": 21, "text": "feature space"}, {"st": 45, "ed": 47, "text": "image classification"}, {"st": 58, "ed": 60, "text": "max pooling"}, {"st": 75, "ed": 77, "text": "learning rate"}, {"st": 84, "ed": 86, "text": "deep model"}]
[{"st": 5, "ed": 7, "text": "interactive learning"}, {"st": 86, "ed": 89, "text": "markov decision process"}, {"st": 140, "ed": 142, "text": "pure exploration"}, {"st": 150, "ed": 153, "text": "multi armed bandits"}, {"st": 162, "ed": 164, "text": "posterior sampling"}, {"st": 165, "ed": 167, "text": "pure exploration"}, {"st": 180, "ed": 182, "text": "simple regret"}, {"st": 204, "ed": 206, "text": "cumulative regret"}, {"st": 214, "ed": 216, "text": "reinforcement learning"}, {"st": 222, "ed": 224, "text": "posterior sampling"}, {"st": 225, "ed": 227, "text": "reinforcement learning"}, {"st": 261, "ed": 263, "text": "empirical results"}, {"st": 269, "ed": 271, "text": "simple regret"}, {"st": 282, "ed": 284, "text": "cumulative regret"}]
[{"st": 11, "ed": 13, "text": "variational inference"}, {"st": 15, "ed": 17, "text": "trust region"}, {"st": 61, "ed": 63, "text": "automatic differentiation"}, {"st": 63, "ed": 65, "text": "variational inference"}, {"st": 71, "ed": 73, "text": "variational inference"}]
[{"st": 7, "ed": 9, "text": "random variables"}, {"st": 116, "ed": 118, "text": "synthetic data"}]
[{"st": 11, "ed": 13, "text": "logistic regression"}, {"st": 89, "ed": 91, "text": "comparative study"}]
[{"st": 2, "ed": 4, "text": "large scale"}, {"st": 4, "ed": 6, "text": "machine learning"}, {"st": 101, "ed": 103, "text": "prior knowledge"}, {"st": 118, "ed": 120, "text": "theoretical guarantee"}]
[{"st": 7, "ed": 10, "text": "deep neural networks"}, {"st": 23, "ed": 25, "text": "deep networks"}, {"st": 47, "ed": 49, "text": "main idea"}, {"st": 65, "ed": 67, "text": "input data"}, {"st": 87, "ed": 89, "text": "multiple layers"}, {"st": 109, "ed": 111, "text": "final output"}, {"st": 124, "ed": 127, "text": "proof of concept"}, {"st": 136, "ed": 138, "text": "mnist dataset"}, {"st": 158, "ed": 160, "text": "deep learning"}]
[{"st": 0, "ed": 2, "text": "deep learning"}, {"st": 6, "ed": 10, "text": "convolutional neural networks cnns"}, {"st": 11, "ed": 13, "text": "natural language"}, {"st": 15, "ed": 19, "text": "recurrent neural networks rnns"}, {"st": 24, "ed": 26, "text": "deep learning"}, {"st": 28, "ed": 32, "text": "feed forward neural networks"}, {"st": 55, "ed": 57, "text": "neural networks"}, {"st": 65, "ed": 67, "text": "batch normalization"}, {"st": 83, "ed": 85, "text": "activation function"}, {"st": 101, "ed": 103, "text": "fixed point"}, {"st": 120, "ed": 122, "text": "network layers"}, {"st": 147, "ed": 149, "text": "deep networks"}, {"st": 182, "ed": 185, "text": "vanishing and exploding"}, {"st": 198, "ed": 200, "text": "machine learning"}, {"st": 216, "ed": 218, "text": "machine learning"}, {"st": 221, "ed": 223, "text": "random forests"}, {"st": 228, "ed": 230, "text": "significantly outperformed"}, {"st": 240, "ed": 242, "text": "competing methods"}]
[{"st": 5, "ed": 7, "text": "challenging problem"}, {"st": 24, "ed": 26, "text": "search algorithm"}, {"st": 27, "ed": 29, "text": "gaussian processes"}, {"st": 33, "ed": 35, "text": "statistical models"}, {"st": 64, "ed": 66, "text": "search algorithm"}, {"st": 87, "ed": 89, "text": "marginal likelihood"}, {"st": 92, "ed": 94, "text": "marginal likelihood"}, {"st": 96, "ed": 99, "text": "variational lower bound"}]
[{"st": 1, "ed": 3, "text": "distributed stochastic"}, {"st": 5, "ed": 7, "text": "t sne"}, {"st": 10, "ed": 12, "text": "visualization method"}, {"st": 64, "ed": 66, "text": "t sne"}, {"st": 78, "ed": 80, "text": "t sne"}, {"st": 86, "ed": 88, "text": "optimization technique"}, {"st": 119, "ed": 121, "text": "step size"}, {"st": 123, "ed": 125, "text": "numerical examples"}, {"st": 151, "ed": 153, "text": "spectral clustering"}]
[{"st": 0, "ed": 4, "text": "generative adversarial networks gans"}, {"st": 6, "ed": 8, "text": "remarkable success"}, {"st": 30, "ed": 32, "text": "conditional gan"}, {"st": 36, "ed": 38, "text": "real valued"}, {"st": 38, "ed": 40, "text": "multi dimensional"}, {"st": 40, "ed": 42, "text": "time series"}, {"st": 55, "ed": 58, "text": "recurrent neural networks"}, {"st": 98, "ed": 101, "text": "maximum mean discrepancy"}, {"st": 105, "ed": 107, "text": "generate realistic"}, {"st": 140, "ed": 142, "text": "synthetic data"}, {"st": 154, "ed": 156, "text": "time series"}, {"st": 159, "ed": 161, "text": "supervised training"}, {"st": 175, "ed": 177, "text": "digit classification"}, {"st": 215, "ed": 217, "text": "generate realistic"}, {"st": 219, "ed": 221, "text": "time series"}]
[{"st": 0, "ed": 2, "text": "active learning"}, {"st": 16, "ed": 18, "text": "predictive model"}, {"st": 26, "ed": 28, "text": "generalization bounds"}, {"st": 58, "ed": 62, "text": "maximum mean discrepancy mmd"}, {"st": 107, "ed": 109, "text": "active learning"}, {"st": 129, "ed": 131, "text": "generalization bound"}, {"st": 167, "ed": 169, "text": "active learning"}]
[{"st": 17, "ed": 20, "text": "simple and effective"}, {"st": 29, "ed": 31, "text": "pre trained"}, {"st": 46, "ed": 48, "text": "small perturbations"}, {"st": 82, "ed": 84, "text": "network architectures"}, {"st": 87, "ed": 89, "text": "consistently outperforms"}, {"st": 94, "ed": 96, "text": "large margin"}, {"st": 112, "ed": 115, "text": "false positive rate"}, {"st": 126, "ed": 128, "text": "cifar 10"}, {"st": 131, "ed": 133, "text": "positive rate"}, {"st": 133, "ed": 135, "text": "is 95"}]
[{"st": 0, "ed": 3, "text": "kernel k means"}, {"st": 20, "ed": 23, "text": "k means clustering"}, {"st": 25, "ed": 28, "text": "kernel k means"}, {"st": 30, "ed": 32, "text": "computationally expensive"}, {"st": 36, "ed": 38, "text": "feature map"}, {"st": 47, "ed": 49, "text": "kernel approximation"}, {"st": 51, "ed": 53, "text": "nystr om"}, {"st": 58, "ed": 60, "text": "previous works"}, {"st": 63, "ed": 65, "text": "kernel learning"}, {"st": 83, "ed": 86, "text": "kernel k means"}, {"st": 93, "ed": 96, "text": "k means clustering"}, {"st": 112, "ed": 114, "text": "nystr om"}, {"st": 124, "ed": 126, "text": "approximation ratio"}, {"st": 130, "ed": 133, "text": "kernel k means"}, {"st": 133, "ed": 135, "text": "cost function"}, {"st": 149, "ed": 151, "text": "nystr om"}, {"st": 164, "ed": 166, "text": "relative error"}, {"st": 166, "ed": 168, "text": "trace norm"}, {"st": 170, "ed": 172, "text": "low rank"}, {"st": 177, "ed": 179, "text": "nystr om"}, {"st": 180, "ed": 182, "text": "empirical evaluations"}, {"st": 195, "ed": 198, "text": "kernel k means"}, {"st": 200, "ed": 202, "text": "nystr om"}, {"st": 207, "ed": 209, "text": "spectral clustering"}, {"st": 210, "ed": 212, "text": "nystr om"}, {"st": 216, "ed": 218, "text": "computationally efficient"}, {"st": 233, "ed": 235, "text": "theoretically sound"}, {"st": 237, "ed": 240, "text": "kernel k means"}, {"st": 242, "ed": 244, "text": "nystr om"}]
[{"st": 52, "ed": 54, "text": "real world"}, {"st": 72, "ed": 74, "text": "recent years"}, {"st": 77, "ed": 79, "text": "machine learning"}, {"st": 84, "ed": 86, "text": "prediction model"}, {"st": 97, "ed": 99, "text": "supervised learning"}, {"st": 116, "ed": 118, "text": "loss function"}, {"st": 120, "ed": 122, "text": "training objective"}, {"st": 126, "ed": 128, "text": "quadratic loss"}, {"st": 138, "ed": 140, "text": "synthetic data"}, {"st": 141, "ed": 144, "text": "real world data"}]
[{"st": 0, "ed": 2, "text": "recent advances"}, {"st": 30, "ed": 32, "text": "game tree"}, {"st": 47, "ed": 49, "text": "game tree"}, {"st": 75, "ed": 77, "text": "confidence intervals"}, {"st": 83, "ed": 86, "text": "best arm identification"}, {"st": 93, "ed": 95, "text": "sample complexity"}, {"st": 110, "ed": 112, "text": "outperform existing"}]
[{"st": 76, "ed": 79, "text": "convolutional neural networks"}, {"st": 81, "ed": 83, "text": "activation functions"}]
[{"st": 3, "ed": 6, "text": "expectation maximization algorithm"}, {"st": 9, "ed": 11, "text": "inverse problem"}, {"st": 26, "ed": 28, "text": "iterated function"}, {"st": 41, "ed": 43, "text": "point cloud"}, {"st": 44, "ed": 46, "text": "mathbb r"}, {"st": 59, "ed": 61, "text": "mathbb r"}, {"st": 66, "ed": 68, "text": "inverse problem"}]
[{"st": 8, "ed": 10, "text": "linear regression"}, {"st": 44, "ed": 47, "text": "large data sets"}, {"st": 85, "ed": 88, "text": "o n 2"}, {"st": 120, "ed": 123, "text": "o n 2"}, {"st": 162, "ed": 164, "text": "financial market"}, {"st": 213, "ed": 215, "text": "feature set"}]
[{"st": 6, "ed": 8, "text": "generative models"}, {"st": 10, "ed": 14, "text": "generative adversarial network gan"}, {"st": 27, "ed": 30, "text": "difficult to train"}, {"st": 64, "ed": 66, "text": "online learning"}, {"st": 71, "ed": 73, "text": "method named"}, {"st": 123, "ed": 125, "text": "theoretical results"}, {"st": 136, "ed": 138, "text": "real world"}]
[{"st": 0, "ed": 3, "text": "the past decade"}, {"st": 15, "ed": 18, "text": "electronic health records"}, {"st": 51, "ed": 53, "text": "machine learning"}, {"st": 59, "ed": 61, "text": "deep learning"}, {"st": 66, "ed": 68, "text": "successfully applied"}, {"st": 102, "ed": 104, "text": "future research"}]
[{"st": 0, "ed": 2, "text": "ensemble methods"}, {"st": 13, "ed": 15, "text": "machine learning"}, {"st": 35, "ed": 38, "text": "deep neural networks"}, {"st": 54, "ed": 56, "text": "ensemble methods"}, {"st": 58, "ed": 61, "text": "deep neural networks"}, {"st": 63, "ed": 65, "text": "multiple choice"}, {"st": 72, "ed": 74, "text": "multiple choice"}, {"st": 125, "ed": 127, "text": "image classification"}, {"st": 144, "ed": 146, "text": "residual networks"}, {"st": 156, "ed": 158, "text": "error rates"}, {"st": 165, "ed": 167, "text": "classification task"}]
[{"st": 5, "ed": 7, "text": "decision trees"}, {"st": 52, "ed": 54, "text": "decision tree"}, {"st": 123, "ed": 125, "text": "random forests"}, {"st": 134, "ed": 136, "text": "case studies"}, {"st": 169, "ed": 171, "text": "real data"}, {"st": 190, "ed": 192, "text": "existing approaches"}, {"st": 207, "ed": 209, "text": "decision tree"}]
[{"st": 1, "ed": 3, "text": "kernel methods"}, {"st": 3, "ed": 5, "text": "temporal information"}, {"st": 31, "ed": 34, "text": "a reproducing kernel"}, {"st": 68, "ed": 70, "text": "kernel matrix"}, {"st": 80, "ed": 82, "text": "important information"}, {"st": 93, "ed": 96, "text": "multiple kernel learning"}, {"st": 124, "ed": 127, "text": "batch and online"}, {"st": 128, "ed": 130, "text": "automatically learn"}, {"st": 132, "ed": 134, "text": "highly nonlinear"}, {"st": 134, "ed": 136, "text": "temporal information"}, {"st": 139, "ed": 141, "text": "input signal"}, {"st": 168, "ed": 171, "text": "batch and online"}]
[{"st": 14, "ed": 16, "text": "important applications"}, {"st": 35, "ed": 37, "text": "labeled examples"}, {"st": 43, "ed": 45, "text": "deep learning"}, {"st": 50, "ed": 52, "text": "semi supervised"}, {"st": 54, "ed": 56, "text": "semi supervised"}, {"st": 137, "ed": 140, "text": "end to end"}, {"st": 145, "ed": 147, "text": "extensive experiments"}, {"st": 154, "ed": 156, "text": "significantly outperforms"}, {"st": 162, "ed": 164, "text": "semi supervised"}, {"st": 166, "ed": 168, "text": "labeled data"}, {"st": 177, "ed": 179, "text": "fully supervised"}]
[{"st": 1, "ed": 3, "text": "belief propagation"}, {"st": 12, "ed": 14, "text": "large scale"}, {"st": 18, "ed": 20, "text": "smart grid"}, {"st": 23, "ed": 25, "text": "social networks"}, {"st": 55, "ed": 57, "text": "direct current"}, {"st": 102, "ed": 104, "text": "convergence properties"}, {"st": 129, "ed": 131, "text": "positive definite"}, {"st": 134, "ed": 136, "text": "positive semidefinite"}]
[{"st": 50, "ed": 52, "text": "related methods"}, {"st": 102, "ed": 104, "text": "low rank"}, {"st": 126, "ed": 129, "text": "log partition function"}]
[{"st": 81, "ed": 83, "text": "statistically efficient"}, {"st": 127, "ed": 129, "text": "statistical efficiency"}]
[{"st": 8, "ed": 12, "text": "recurrent neural networks rnns"}, {"st": 29, "ed": 31, "text": "structured prediction"}, {"st": 34, "ed": 36, "text": "machine translation"}, {"st": 43, "ed": 45, "text": "maximum likelihood"}, {"st": 50, "ed": 52, "text": "training loss"}, {"st": 66, "ed": 68, "text": "ground truth"}, {"st": 103, "ed": 105, "text": "search space"}, {"st": 121, "ed": 123, "text": "improved performance"}, {"st": 143, "ed": 146, "text": "scale to large"}, {"st": 160, "ed": 162, "text": "machine translation"}]
[{"st": 5, "ed": 7, "text": "learning representations"}, {"st": 18, "ed": 20, "text": "labeled data"}, {"st": 20, "ed": 23, "text": "semi supervised learning"}, {"st": 23, "ed": 25, "text": "transfer learning"}, {"st": 34, "ed": 36, "text": "representation learning"}, {"st": 40, "ed": 42, "text": "kernel learning"}, {"st": 64, "ed": 66, "text": "representation learning"}, {"st": 119, "ed": 121, "text": "mixture models"}, {"st": 128, "ed": 130, "text": "representation learning"}, {"st": 137, "ed": 139, "text": "representation learning"}, {"st": 150, "ed": 152, "text": "np hard"}, {"st": 161, "ed": 163, "text": "labeled data"}, {"st": 163, "ed": 166, "text": "semi supervised learning"}, {"st": 171, "ed": 173, "text": "classification tasks"}, {"st": 177, "ed": 179, "text": "nearest neighbors"}]
[{"st": 1, "ed": 3, "text": "efficient algorithms"}, {"st": 4, "ed": 7, "text": "strong theoretical guarantees"}, {"st": 29, "ed": 31, "text": "domain expertise"}, {"st": 47, "ed": 49, "text": "contextual bandit"}, {"st": 68, "ed": 70, "text": "decision trees"}, {"st": 76, "ed": 78, "text": "decision trees"}, {"st": 92, "ed": 94, "text": "exploration exploitation"}, {"st": 103, "ed": 105, "text": "thompson sampling"}]
[{"st": 2, "ed": 4, "text": "reinforcement learning"}, {"st": 41, "ed": 43, "text": "reinforcement learning"}, {"st": 111, "ed": 113, "text": "policy iteration"}, {"st": 116, "ed": 118, "text": "value iteration"}, {"st": 121, "ed": 124, "text": "under mild assumptions"}, {"st": 130, "ed": 132, "text": "loss function"}, {"st": 141, "ed": 144, "text": "stochastic gradient descent"}, {"st": 147, "ed": 150, "text": "guaranteed to converge"}]
[{"st": 10, "ed": 12, "text": "neural networks"}, {"st": 18, "ed": 20, "text": "recent developments"}, {"st": 24, "ed": 27, "text": "non convex optimization"}, {"st": 57, "ed": 59, "text": "strongly convex"}, {"st": 73, "ed": 75, "text": "quasi newton"}, {"st": 88, "ed": 90, "text": "neural network"}, {"st": 119, "ed": 121, "text": "loss function"}, {"st": 122, "ed": 124, "text": "squared loss"}, {"st": 125, "ed": 128, "text": "cross entropy loss"}, {"st": 141, "ed": 143, "text": "medium sized"}, {"st": 143, "ed": 145, "text": "benchmark problems"}, {"st": 148, "ed": 150, "text": "large scale"}, {"st": 168, "ed": 170, "text": "faster convergence"}, {"st": 211, "ed": 213, "text": "input variables"}]
[{"st": 1, "ed": 4, "text": "online convex optimization"}, {"st": 34, "ed": 36, "text": "mathcal o"}, {"st": 40, "ed": 42, "text": "per iteration"}, {"st": 57, "ed": 59, "text": "mathcal o"}, {"st": 59, "ed": 61, "text": "sqrt t"}, {"st": 71, "ed": 73, "text": "squared loss"}, {"st": 73, "ed": 75, "text": "logistic loss"}, {"st": 77, "ed": 79, "text": "hinge loss"}, {"st": 94, "ed": 96, "text": "mathcal o"}, {"st": 107, "ed": 109, "text": "mathcal o"}, {"st": 131, "ed": 133, "text": "mathcal o"}, {"st": 133, "ed": 135, "text": "sqrt t"}, {"st": 147, "ed": 149, "text": "mathcal o"}, {"st": 174, "ed": 176, "text": "mathcal o"}, {"st": 185, "ed": 187, "text": "computational complexity"}, {"st": 200, "ed": 202, "text": "kernel matrix"}, {"st": 231, "ed": 233, "text": "mathcal o"}, {"st": 240, "ed": 242, "text": "per iteration"}]
[{"st": 8, "ed": 10, "text": "submodular function"}, {"st": 37, "ed": 39, "text": "worst case"}, {"st": 49, "ed": 51, "text": "constant factor"}, {"st": 57, "ed": 59, "text": "o sqrt"}, {"st": 68, "ed": 70, "text": "open problem"}, {"st": 107, "ed": 109, "text": "submodular optimization"}, {"st": 138, "ed": 140, "text": "greedy algorithm"}]
[{"st": 5, "ed": 7, "text": "deep neural"}, {"st": 28, "ed": 30, "text": "hidden layers"}, {"st": 56, "ed": 58, "text": "empirically demonstrate"}, {"st": 73, "ed": 76, "text": "loss in accuracy"}, {"st": 78, "ed": 80, "text": "a 20"}, {"st": 97, "ed": 101, "text": "available at https github.com"}]
[{"st": 2, "ed": 6, "text": "generative adversarial networks gans"}, {"st": 17, "ed": 19, "text": "generated data"}, {"st": 33, "ed": 35, "text": "mode collapse"}, {"st": 38, "ed": 40, "text": "generative model"}, {"st": 61, "ed": 63, "text": "auto encoders"}, {"st": 67, "ed": 70, "text": "generative adversarial networks"}, {"st": 73, "ed": 75, "text": "hierarchical structure"}, {"st": 84, "ed": 86, "text": "variational inference"}, {"st": 108, "ed": 110, "text": "posterior distribution"}, {"st": 120, "ed": 122, "text": "posterior distributions"}, {"st": 136, "ed": 139, "text": "variational auto encoders"}, {"st": 140, "ed": 143, "text": "generative adversarial networks"}]
[{"st": 0, "ed": 2, "text": "deep learning"}, {"st": 9, "ed": 11, "text": "speech recognition"}, {"st": 11, "ed": 13, "text": "image classification"}, {"st": 21, "ed": 23, "text": "deep model"}, {"st": 67, "ed": 69, "text": "image captioning"}, {"st": 69, "ed": 71, "text": "coco dataset"}, {"st": 72, "ed": 74, "text": "speech recognition"}, {"st": 84, "ed": 86, "text": "building blocks"}, {"st": 91, "ed": 93, "text": "convolutional layers"}, {"st": 94, "ed": 96, "text": "attention mechanism"}, {"st": 155, "ed": 157, "text": "joint training"}]
[{"st": 2, "ed": 4, "text": "streaming data"}, {"st": 5, "ed": 8, "text": "attracted much attention"}, {"st": 17, "ed": 19, "text": "data stream"}, {"st": 57, "ed": 59, "text": "learning paradigm"}, {"st": 164, "ed": 166, "text": "performance guarantee"}, {"st": 174, "ed": 178, "text": "synthetic and real data"}]
[{"st": 0, "ed": 3, "text": "unsupervised domain adaptation"}, {"st": 21, "ed": 23, "text": "labeled dataset"}, {"st": 30, "ed": 32, "text": "domain adaptation"}, {"st": 53, "ed": 55, "text": "generalization error"}, {"st": 57, "ed": 59, "text": "large margin"}, {"st": 59, "ed": 61, "text": "linear classifiers"}, {"st": 93, "ed": 95, "text": "random walk"}, {"st": 97, "ed": 99, "text": "directed graph"}, {"st": 100, "ed": 102, "text": "transition probabilities"}, {"st": 107, "ed": 109, "text": "majority vote"}, {"st": 126, "ed": 128, "text": "domain adaptation"}, {"st": 131, "ed": 134, "text": "easy to implement"}, {"st": 154, "ed": 156, "text": "input features"}, {"st": 163, "ed": 165, "text": "source data"}, {"st": 178, "ed": 180, "text": "feature space"}, {"st": 189, "ed": 191, "text": "deep features"}, {"st": 193, "ed": 195, "text": "pre trained"}, {"st": 195, "ed": 197, "text": "neural networks"}]
[{"st": 0, "ed": 2, "text": "batch normalization"}, {"st": 16, "ed": 18, "text": "neural networks"}, {"st": 19, "ed": 21, "text": "l2 regularization"}, {"st": 23, "ed": 25, "text": "weight decay"}, {"st": 33, "ed": 35, "text": "l2 regularization"}, {"st": 73, "ed": 75, "text": "optimization methods"}]
[{"st": 2, "ed": 4, "text": "recent progress"}, {"st": 6, "ed": 9, "text": "deep neural network"}, {"st": 16, "ed": 18, "text": "local feature"}, {"st": 36, "ed": 38, "text": "local feature"}, {"st": 42, "ed": 44, "text": "low dimensional"}, {"st": 47, "ed": 49, "text": "neural network"}, {"st": 59, "ed": 61, "text": "local feature"}, {"st": 67, "ed": 69, "text": "initial conditions"}, {"st": 70, "ed": 72, "text": "learning parameters"}, {"st": 101, "ed": 103, "text": "local feature"}, {"st": 108, "ed": 110, "text": "computationally efficient"}, {"st": 113, "ed": 115, "text": "recognition rate"}, {"st": 120, "ed": 122, "text": "experiments demonstrate"}, {"st": 125, "ed": 127, "text": "local feature"}]
[{"st": 3, "ed": 5, "text": "policy gradients"}, {"st": 9, "ed": 11, "text": "policy gradients"}, {"st": 13, "ed": 15, "text": "deterministic policy"}, {"st": 49, "ed": 51, "text": "policy gradient"}, {"st": 57, "ed": 59, "text": "deterministic policy"}, {"st": 129, "ed": 131, "text": "empirical results"}]
[{"st": 7, "ed": 9, "text": "deep learning"}, {"st": 18, "ed": 20, "text": "deep networks"}, {"st": 46, "ed": 48, "text": "gradient based"}, {"st": 50, "ed": 53, "text": "deep neural networks"}, {"st": 73, "ed": 75, "text": "dnn training"}, {"st": 79, "ed": 81, "text": "without compromising"}, {"st": 103, "ed": 105, "text": "generalization performance"}, {"st": 106, "ed": 108, "text": "deep networks"}, {"st": 111, "ed": 113, "text": "gradient based"}, {"st": 115, "ed": 117, "text": "training data"}, {"st": 120, "ed": 122, "text": "important role"}]
[{"st": 6, "ed": 9, "text": "support vector machine"}, {"st": 22, "ed": 24, "text": "support vector"}, {"st": 26, "ed": 29, "text": "support vector regression"}]
[{"st": 9, "ed": 11, "text": "training sample"}, {"st": 19, "ed": 21, "text": "supervised classification"}, {"st": 61, "ed": 63, "text": "hierarchical structure"}]
[{"st": 4, "ed": 6, "text": "generalization bounds"}, {"st": 10, "ed": 12, "text": "cross validation"}, {"st": 62, "ed": 65, "text": "fold cross validation"}, {"st": 91, "ed": 94, "text": "bias and variance"}, {"st": 107, "ed": 109, "text": "learning rule"}]
[{"st": 7, "ed": 10, "text": "canonical correlation analysis"}, {"st": 47, "ed": 49, "text": "previous methods"}, {"st": 57, "ed": 59, "text": "intrinsic dimensionality"}, {"st": 70, "ed": 72, "text": "learning dynamics"}, {"st": 88, "ed": 90, "text": "class specific"}, {"st": 109, "ed": 111, "text": "https github.com"}]
[{"st": 21, "ed": 23, "text": "causal inference"}, {"st": 25, "ed": 27, "text": "multitask learning"}, {"st": 62, "ed": 64, "text": "selection bias"}, {"st": 66, "ed": 68, "text": "observational data"}, {"st": 73, "ed": 75, "text": "dropout regularization"}, {"st": 111, "ed": 113, "text": "training examples"}, {"st": 138, "ed": 140, "text": "experiments conducted"}, {"st": 145, "ed": 147, "text": "real world"}, {"st": 147, "ed": 149, "text": "observational study"}]
[{"st": 33, "ed": 35, "text": "consensus clustering"}, {"st": 48, "ed": 50, "text": "clustering methods"}]
[{"st": 28, "ed": 30, "text": "worst case"}, {"st": 65, "ed": 67, "text": "significant improvements"}, {"st": 122, "ed": 124, "text": "clustering algorithm"}]
[{"st": 6, "ed": 8, "text": "approximate inference"}, {"st": 9, "ed": 13, "text": "markov chain monte carlo"}, {"st": 30, "ed": 32, "text": "variational inference"}, {"st": 54, "ed": 56, "text": "variational methods"}, {"st": 62, "ed": 64, "text": "fine grained"}, {"st": 71, "ed": 73, "text": "variational parameters"}, {"st": 84, "ed": 86, "text": "marginal distribution"}, {"st": 109, "ed": 111, "text": "existing methods"}, {"st": 113, "ed": 115, "text": "langevin dynamics"}, {"st": 118, "ed": 120, "text": "variational inference"}]
[{"st": 6, "ed": 8, "text": "machine learning"}, {"st": 74, "ed": 76, "text": "medical doctors"}]
[{"st": 0, "ed": 2, "text": "deep learning"}, {"st": 12, "ed": 15, "text": "visual object recognition"}, {"st": 52, "ed": 54, "text": "hidden units"}, {"st": 56, "ed": 58, "text": "learning process"}, {"st": 68, "ed": 70, "text": "hidden units"}, {"st": 90, "ed": 92, "text": "hidden units"}, {"st": 100, "ed": 102, "text": "ensemble learning"}]
[{"st": 86, "ed": 88, "text": "change detection"}, {"st": 122, "ed": 124, "text": "problems involving"}, {"st": 139, "ed": 141, "text": "baseline methods"}]
[{"st": 19, "ed": 21, "text": "objective function"}, {"st": 34, "ed": 36, "text": "objective function"}, {"st": 53, "ed": 55, "text": "objective function"}, {"st": 82, "ed": 84, "text": "statistical mechanics"}, {"st": 92, "ed": 94, "text": "differential equations"}, {"st": 108, "ed": 110, "text": "generalization error"}, {"st": 113, "ed": 115, "text": "differential equations"}]
[{"st": 17, "ed": 20, "text": "click through rate"}, {"st": 21, "ed": 23, "text": "prediction model"}, {"st": 25, "ed": 27, "text": "online advertising"}, {"st": 29, "ed": 31, "text": "e commerce"}, {"st": 55, "ed": 57, "text": "e commerce"}, {"st": 113, "ed": 115, "text": "display advertising"}, {"st": 132, "ed": 134, "text": "network structure"}, {"st": 152, "ed": 154, "text": "significantly outperforms"}, {"st": 166, "ed": 168, "text": "deep network"}, {"st": 169, "ed": 171, "text": "large scale"}]
[{"st": 24, "ed": 26, "text": "neural networks"}]
[{"st": 1, "ed": 3, "text": "inverse problems"}, {"st": 6, "ed": 8, "text": "optimization problems"}, {"st": 27, "ed": 29, "text": "optimization problems"}, {"st": 33, "ed": 35, "text": "worst case"}, {"st": 61, "ed": 64, "text": "ability to learn"}, {"st": 88, "ed": 90, "text": "predictive model"}, {"st": 121, "ed": 123, "text": "assignment problem"}, {"st": 138, "ed": 140, "text": "neural networks"}]
[{"st": 0, "ed": 2, "text": "mutual information"}, {"st": 7, "ed": 9, "text": "feature selection"}, {"st": 31, "ed": 33, "text": "conditional independence"}, {"st": 45, "ed": 47, "text": "mutual information"}, {"st": 82, "ed": 84, "text": "low rank"}, {"st": 90, "ed": 92, "text": "feature selection"}, {"st": 101, "ed": 103, "text": "np hard"}, {"st": 105, "ed": 107, "text": "feature selection"}, {"st": 109, "ed": 111, "text": "experimentally demonstrate"}, {"st": 117, "ed": 119, "text": "multiple datasets"}, {"st": 129, "ed": 131, "text": "feature selection"}]
[{"st": 0, "ed": 2, "text": "active learning"}, {"st": 23, "ed": 25, "text": "active learning"}, {"st": 35, "ed": 37, "text": "unlabeled data"}, {"st": 60, "ed": 62, "text": "active learning"}, {"st": 71, "ed": 73, "text": "unlabeled instances"}, {"st": 111, "ed": 113, "text": "unlabeled data"}, {"st": 175, "ed": 177, "text": "logistic regression"}, {"st": 178, "ed": 181, "text": "support vector machines"}]
[{"st": 7, "ed": 9, "text": "deep learning"}, {"st": 74, "ed": 76, "text": "deep learning"}, {"st": 97, "ed": 99, "text": "learning agents"}, {"st": 114, "ed": 116, "text": "convergence properties"}, {"st": 118, "ed": 120, "text": "proposed algorithm"}, {"st": 121, "ed": 123, "text": "strongly convex"}, {"st": 125, "ed": 127, "text": "objective functions"}, {"st": 131, "ed": 133, "text": "step sizes"}, {"st": 136, "ed": 138, "text": "lyapunov function"}, {"st": 155, "ed": 157, "text": "recently proposed"}, {"st": 167, "ed": 169, "text": "benchmark datasets"}, {"st": 171, "ed": 174, "text": "mnist cifar 10"}]
[{"st": 12, "ed": 15, "text": "deep neural network"}, {"st": 33, "ed": 35, "text": "recently proposed"}]
[{"st": 1, "ed": 3, "text": "domain adaptation"}, {"st": 8, "ed": 10, "text": "source domain"}, {"st": 67, "ed": 71, "text": "source and target domains"}, {"st": 119, "ed": 121, "text": "least squares"}, {"st": 142, "ed": 144, "text": "selection bias"}, {"st": 151, "ed": 153, "text": "domain adaptation"}]
[{"st": 0, "ed": 2, "text": "large scale"}, {"st": 11, "ed": 13, "text": "neural network"}, {"st": 14, "ed": 16, "text": "deep learning"}, {"st": 22, "ed": 24, "text": "benchmark dataset"}, {"st": 26, "ed": 28, "text": "multi label"}, {"st": 66, "ed": 68, "text": "audio visual"}, {"st": 76, "ed": 78, "text": "feature vectors"}, {"st": 103, "ed": 105, "text": "classification algorithms"}, {"st": 108, "ed": 110, "text": "level labels"}, {"st": 132, "ed": 134, "text": "deep learning"}, {"st": 157, "ed": 159, "text": "prediction accuracy"}, {"st": 161, "ed": 163, "text": "base level"}]
[{"st": 51, "ed": 53, "text": "computer aided"}, {"st": 96, "ed": 99, "text": "convolutional neural network"}, {"st": 135, "ed": 137, "text": "without resorting"}, {"st": 142, "ed": 144, "text": "performance metrics"}, {"st": 150, "ed": 152, "text": "existing techniques"}, {"st": 155, "ed": 157, "text": "feature selection"}]
[{"st": 0, "ed": 4, "text": "generative adversarial networks gans"}, {"st": 7, "ed": 9, "text": "realistic images"}, {"st": 10, "ed": 12, "text": "complex models"}, {"st": 14, "ed": 16, "text": "maximum likelihood"}, {"st": 22, "ed": 24, "text": "gan training"}, {"st": 35, "ed": 37, "text": "update rule"}, {"st": 42, "ed": 45, "text": "stochastic gradient descent"}, {"st": 54, "ed": 56, "text": "learning rate"}, {"st": 75, "ed": 78, "text": "under mild assumptions"}, {"st": 126, "ed": 128, "text": "image generation"}, {"st": 141, "ed": 143, "text": "generated images"}, {"st": 160, "ed": 162, "text": "wasserstein gans"}, {"st": 166, "ed": 168, "text": "gan training"}, {"st": 170, "ed": 172, "text": "cifar 10"}]
[{"st": 6, "ed": 8, "text": "local feature"}, {"st": 35, "ed": 37, "text": "cost function"}, {"st": 37, "ed": 39, "text": "minimization problem"}, {"st": 46, "ed": 48, "text": "local feature"}, {"st": 53, "ed": 55, "text": "cost function"}, {"st": 62, "ed": 64, "text": "cost function"}, {"st": 69, "ed": 71, "text": "local feature"}, {"st": 76, "ed": 78, "text": "synthetic datasets"}, {"st": 88, "ed": 90, "text": "relevant information"}, {"st": 94, "ed": 96, "text": "local feature"}, {"st": 109, "ed": 112, "text": "image and video"}, {"st": 116, "ed": 118, "text": "method outperforms"}, {"st": 123, "ed": 125, "text": "local feature"}, {"st": 129, "ed": 132, "text": "bag of words"}]
[{"st": 5, "ed": 8, "text": "multivariate time series"}, {"st": 37, "ed": 41, "text": "reproducing kernel hilbert space"}, {"st": 71, "ed": 73, "text": "search space"}, {"st": 85, "ed": 87, "text": "valued kernels"}, {"st": 109, "ed": 111, "text": "semi definite"}, {"st": 116, "ed": 118, "text": "predictive performance"}, {"st": 123, "ed": 125, "text": "non linearities"}]
[{"st": 4, "ed": 6, "text": "deep learning"}, {"st": 16, "ed": 19, "text": "deep neural networks"}, {"st": 46, "ed": 48, "text": "deep learning"}, {"st": 61, "ed": 64, "text": "deep belief network"}, {"st": 70, "ed": 73, "text": "deep belief network"}, {"st": 78, "ed": 80, "text": "main idea"}, {"st": 83, "ed": 85, "text": "differential privacy"}, {"st": 96, "ed": 98, "text": "objective functions"}, {"st": 106, "ed": 108, "text": "key contribution"}, {"st": 130, "ed": 132, "text": "theoretical analysis"}, {"st": 141, "ed": 143, "text": "error bounds"}, {"st": 152, "ed": 154, "text": "differential privacy"}, {"st": 165, "ed": 167, "text": "social network"}, {"st": 188, "ed": 190, "text": "digit recognition"}, {"st": 191, "ed": 193, "text": "theoretical analysis"}, {"st": 205, "ed": 208, "text": "significantly outperforms existing"}]
[{"st": 0, "ed": 2, "text": "trace norm"}, {"st": 10, "ed": 12, "text": "low rank"}, {"st": 15, "ed": 17, "text": "optimization strategy"}, {"st": 26, "ed": 29, "text": "low rank matrix"}, {"st": 56, "ed": 58, "text": "proximal gradient"}, {"st": 63, "ed": 66, "text": "guaranteed to converge"}, {"st": 68, "ed": 70, "text": "global optimum"}, {"st": 111, "ed": 113, "text": "critical point"}, {"st": 123, "ed": 125, "text": "meta algorithm"}, {"st": 129, "ed": 131, "text": "parameter space"}, {"st": 140, "ed": 142, "text": "critical point"}, {"st": 157, "ed": 159, "text": "matrix completion"}, {"st": 160, "ed": 162, "text": "multitask learning"}, {"st": 168, "ed": 170, "text": "random initialization"}, {"st": 184, "ed": 187, "text": "synthetic and real"}]
[{"st": 108, "ed": 110, "text": "contextual bandit"}, {"st": 131, "ed": 133, "text": "web applications"}, {"st": 145, "ed": 147, "text": "reward function"}, {"st": 150, "ed": 152, "text": "reward function"}, {"st": 199, "ed": 201, "text": "numerical experiments"}, {"st": 218, "ed": 220, "text": "contextual bandit"}]
[{"st": 0, "ed": 2, "text": "machine learning"}, {"st": 5, "ed": 7, "text": "successfully applied"}, {"st": 23, "ed": 25, "text": "successful applications"}, {"st": 26, "ed": 28, "text": "ensemble learning"}, {"st": 40, "ed": 42, "text": "random forests"}, {"st": 77, "ed": 79, "text": "machine learning"}, {"st": 96, "ed": 98, "text": "optimization techniques"}, {"st": 122, "ed": 124, "text": "recent advances"}, {"st": 128, "ed": 131, "text": "learning to rank"}]
[{"st": 8, "ed": 10, "text": "recovery problem"}, {"st": 27, "ed": 29, "text": "inverse problem"}, {"st": 50, "ed": 52, "text": "image processing"}, {"st": 60, "ed": 62, "text": "sparse recovery"}, {"st": 66, "ed": 69, "text": "l 1 norm"}, {"st": 89, "ed": 91, "text": "structural similarity"}, {"st": 100, "ed": 102, "text": "optimization problem"}, {"st": 109, "ed": 115, "text": "alternating direction method of multipliers admm"}, {"st": 123, "ed": 125, "text": "proposed method"}]
[{"st": 36, "ed": 38, "text": "predictive model"}, {"st": 53, "ed": 55, "text": "text analysis"}, {"st": 75, "ed": 77, "text": "deep learning"}, {"st": 85, "ed": 87, "text": "textual data"}, {"st": 90, "ed": 92, "text": "neural network"}, {"st": 104, "ed": 106, "text": "latent semantic"}, {"st": 111, "ed": 113, "text": "neural network"}, {"st": 151, "ed": 153, "text": "predictive performance"}]
[{"st": 3, "ed": 5, "text": "binary classification"}, {"st": 25, "ed": 28, "text": "support vector machine"}, {"st": 51, "ed": 53, "text": "low dimensional"}, {"st": 60, "ed": 63, "text": "random fourier features"}, {"st": 65, "ed": 67, "text": "nystr om"}, {"st": 85, "ed": 87, "text": "proximal gradient"}]
[{"st": 7, "ed": 10, "text": "convolutional neural network"}, {"st": 24, "ed": 27, "text": "graph structured data"}, {"st": 47, "ed": 49, "text": "convolutional network"}, {"st": 74, "ed": 76, "text": "extensive experiments"}, {"st": 95, "ed": 97, "text": "node classification"}, {"st": 120, "ed": 122, "text": "real world"}]
[{"st": 12, "ed": 14, "text": "objective function"}, {"st": 23, "ed": 25, "text": "objective function"}, {"st": 27, "ed": 29, "text": "maximum likelihood"}, {"st": 32, "ed": 34, "text": "reinforcement learning"}, {"st": 36, "ed": 38, "text": "objective functions"}, {"st": 39, "ed": 41, "text": "special cases"}, {"st": 62, "ed": 64, "text": "objective function"}, {"st": 81, "ed": 83, "text": "machine translation"}, {"st": 88, "ed": 90, "text": "objective function"}]
[{"st": 19, "ed": 21, "text": "optimization methods"}, {"st": 47, "ed": 49, "text": "optimization algorithms"}, {"st": 63, "ed": 65, "text": "supervised learning"}, {"st": 73, "ed": 75, "text": "optimization problems"}, {"st": 92, "ed": 94, "text": "optimization problems"}, {"st": 99, "ed": 101, "text": "logistic regression"}, {"st": 116, "ed": 118, "text": "optimization algorithms"}, {"st": 121, "ed": 123, "text": "logistic regression"}, {"st": 158, "ed": 161, "text": "deep neural networks"}]
[{"st": 3, "ed": 5, "text": "decision making"}, {"st": 9, "ed": 11, "text": "machine learning"}, {"st": 28, "ed": 30, "text": "decision making"}]
[{"st": 4, "ed": 7, "text": "deep reinforcement learning"}, {"st": 76, "ed": 78, "text": "epsilon greedy"}, {"st": 90, "ed": 92, "text": "atari games"}]
[{"st": 6, "ed": 8, "text": "decision making"}, {"st": 32, "ed": 34, "text": "recent studies"}, {"st": 73, "ed": 75, "text": "decision making"}, {"st": 84, "ed": 86, "text": "fair division"}, {"st": 142, "ed": 144, "text": "margin based"}, {"st": 160, "ed": 165, "text": "synthetic and real world datasets"}]
[{"st": 16, "ed": 18, "text": "binary classification"}, {"st": 35, "ed": 37, "text": "false positive"}, {"st": 43, "ed": 45, "text": "false negative"}, {"st": 51, "ed": 54, "text": "proof of concept"}, {"st": 59, "ed": 61, "text": "empirically evaluate"}]
[{"st": 9, "ed": 12, "text": "stochastic gradient descent"}, {"st": 19, "ed": 21, "text": "parameter estimates"}, {"st": 23, "ed": 25, "text": "gained increasing"}, {"st": 47, "ed": 49, "text": "statistical inference"}, {"st": 66, "ed": 68, "text": "computationally feasible"}, {"st": 110, "ed": 113, "text": "stochastic gradient descent"}, {"st": 136, "ed": 138, "text": "proposed method"}, {"st": 139, "ed": 142, "text": "easy to implement"}, {"st": 147, "ed": 149, "text": "theoretical properties"}, {"st": 157, "ed": 160, "text": "generalized linear models"}, {"st": 168, "ed": 170, "text": "finite sample"}, {"st": 181, "ed": 183, "text": "real data"}]
[{"st": 4, "ed": 6, "text": "widely applied"}, {"st": 42, "ed": 44, "text": "gaussian processes"}, {"st": 66, "ed": 68, "text": "mean field"}, {"st": 68, "ed": 70, "text": "variational inference"}, {"st": 86, "ed": 88, "text": "image segmentation"}]
[{"st": 18, "ed": 20, "text": "gaussian distribution"}, {"st": 74, "ed": 76, "text": "standard benchmark"}, {"st": 76, "ed": 78, "text": "image datasets"}, {"st": 84, "ed": 86, "text": "output distribution"}]
[{"st": 1, "ed": 3, "text": "supervised learning"}, {"st": 19, "ed": 21, "text": "speech recognition"}, {"st": 22, "ed": 25, "text": "text to speech"}, {"st": 26, "ed": 28, "text": "image classification"}, {"st": 102, "ed": 104, "text": "proposed approach"}, {"st": 106, "ed": 108, "text": "supervised learning"}, {"st": 113, "ed": 115, "text": "supervised learning"}, {"st": 125, "ed": 127, "text": "applications including"}, {"st": 127, "ed": 129, "text": "machine translation"}, {"st": 129, "ed": 131, "text": "image processing"}]
[{"st": 14, "ed": 16, "text": "low rank"}, {"st": 61, "ed": 63, "text": "models including"}, {"st": 66, "ed": 68, "text": "multi view"}, {"st": 94, "ed": 96, "text": "low rank"}, {"st": 169, "ed": 171, "text": "numerical experiments"}, {"st": 181, "ed": 183, "text": "nuclear norm"}, {"st": 187, "ed": 189, "text": "low rank"}]
[{"st": 5, "ed": 7, "text": "neural network"}, {"st": 18, "ed": 20, "text": "multi scale"}, {"st": 65, "ed": 67, "text": "character level"}, {"st": 76, "ed": 78, "text": "word level"}]
[{"st": 1, "ed": 3, "text": "recent advances"}, {"st": 3, "ed": 5, "text": "large scale"}, {"st": 77, "ed": 79, "text": "input space"}]
[{"st": 4, "ed": 7, "text": "simple and efficient"}, {"st": 21, "ed": 23, "text": "kernel density"}, {"st": 78, "ed": 80, "text": "kernel density"}, {"st": 83, "ed": 85, "text": "computationally expensive"}, {"st": 98, "ed": 101, "text": "orders of magnitude"}, {"st": 118, "ed": 120, "text": "large datasets"}]
[]
[{"st": 2, "ed": 4, "text": "time series"}, {"st": 10, "ed": 12, "text": "examples include"}, {"st": 13, "ed": 15, "text": "kalman filter"}, {"st": 16, "ed": 18, "text": "collaborative filtering"}, {"st": 29, "ed": 31, "text": "mean field"}, {"st": 31, "ed": 33, "text": "variational approximations"}, {"st": 41, "ed": 43, "text": "variational inference"}, {"st": 60, "ed": 62, "text": "time series"}, {"st": 96, "ed": 98, "text": "scales linearly"}, {"st": 108, "ed": 110, "text": "variational distribution"}, {"st": 123, "ed": 125, "text": "recently proposed"}, {"st": 126, "ed": 128, "text": "word embedding"}]
[{"st": 83, "ed": 85, "text": "positive definite"}, {"st": 153, "ed": 155, "text": "digit recognition"}, {"st": 166, "ed": 170, "text": "center for disease control"}, {"st": 193, "ed": 195, "text": "recently published"}, {"st": 198, "ed": 200, "text": "public health"}]
[{"st": 0, "ed": 2, "text": "domain adaptation"}, {"st": 20, "ed": 22, "text": "source domain"}, {"st": 33, "ed": 35, "text": "domain adaptation"}, {"st": 38, "ed": 40, "text": "domain invariant"}, {"st": 40, "ed": 42, "text": "feature representations"}, {"st": 44, "ed": 46, "text": "learned representations"}, {"st": 56, "ed": 58, "text": "domain adaptation"}, {"st": 62, "ed": 64, "text": "domain invariant"}, {"st": 64, "ed": 66, "text": "representation learning"}, {"st": 83, "ed": 85, "text": "wasserstein gan"}, {"st": 95, "ed": 97, "text": "domain invariant"}, {"st": 97, "ed": 99, "text": "feature representations"}, {"st": 100, "ed": 102, "text": "wasserstein distance"}, {"st": 103, "ed": 105, "text": "representation learning"}, {"st": 110, "ed": 112, "text": "neural network"}, {"st": 120, "ed": 122, "text": "wasserstein distance"}, {"st": 124, "ed": 127, "text": "source and target"}, {"st": 131, "ed": 133, "text": "feature extractor"}, {"st": 138, "ed": 140, "text": "wasserstein distance"}, {"st": 148, "ed": 150, "text": "wasserstein distance"}, {"st": 151, "ed": 153, "text": "domain adaptation"}, {"st": 162, "ed": 164, "text": "empirical studies"}, {"st": 168, "ed": 170, "text": "image classification"}, {"st": 183, "ed": 185, "text": "domain invariant"}, {"st": 185, "ed": 187, "text": "representation learning"}]
[{"st": 2, "ed": 4, "text": "neural network"}, {"st": 32, "ed": 34, "text": "neural networks"}, {"st": 69, "ed": 71, "text": "neural network"}, {"st": 83, "ed": 85, "text": "unlike existing"}, {"st": 121, "ed": 123, "text": "tasks including"}, {"st": 141, "ed": 144, "text": "trained neural network"}, {"st": 147, "ed": 149, "text": "labeled data"}, {"st": 188, "ed": 190, "text": "domain specific"}]
[{"st": 11, "ed": 13, "text": "excellent performance"}, {"st": 16, "ed": 18, "text": "link prediction"}, {"st": 36, "ed": 38, "text": "knowledge graph"}, {"st": 75, "ed": 77, "text": "scoring functions"}, {"st": 125, "ed": 128, "text": "advantages and disadvantages"}]
[{"st": 0, "ed": 2, "text": "early stopping"}, {"st": 3, "ed": 5, "text": "iterative algorithms"}, {"st": 56, "ed": 58, "text": "loss functions"}, {"st": 59, "ed": 61, "text": "boosting algorithms"}, {"st": 99, "ed": 101, "text": "fixed point"}]
[{"st": 1, "ed": 3, "text": "kernel learning"}, {"st": 45, "ed": 47, "text": "logistic regression"}, {"st": 49, "ed": 51, "text": "semi definite"}, {"st": 60, "ed": 62, "text": "logistic regression"}, {"st": 134, "ed": 136, "text": "multi modal"}, {"st": 147, "ed": 149, "text": "logistic regression"}, {"st": 150, "ed": 152, "text": "positive definite"}]
[{"st": 37, "ed": 39, "text": "temporal dynamics"}, {"st": 60, "ed": 62, "text": "traffic flow"}, {"st": 68, "ed": 70, "text": "directed graph"}, {"st": 74, "ed": 77, "text": "recurrent neural network"}, {"st": 79, "ed": 81, "text": "deep learning"}, {"st": 88, "ed": 91, "text": "spatial and temporal"}, {"st": 104, "ed": 106, "text": "random walks"}, {"st": 115, "ed": 118, "text": "encoder decoder architecture"}, {"st": 127, "ed": 129, "text": "real world"}, {"st": 129, "ed": 131, "text": "large scale"}]
[{"st": 0, "ed": 3, "text": "independent component analysis"}, {"st": 6, "ed": 8, "text": "dimensionality reduction"}, {"st": 14, "ed": 16, "text": "machine learning"}, {"st": 20, "ed": 22, "text": "probability density"}, {"st": 121, "ed": 123, "text": "proposed algorithm"}, {"st": 134, "ed": 136, "text": "machine learning"}, {"st": 139, "ed": 142, "text": "stochastic gradient descent"}]
[{"st": 2, "ed": 5, "text": "click through rate"}, {"st": 13, "ed": 15, "text": "online advertising"}, {"st": 20, "ed": 23, "text": "click through rate"}, {"st": 45, "ed": 47, "text": "search engines"}, {"st": 50, "ed": 52, "text": "machine learning"}, {"st": 92, "ed": 94, "text": "character level"}, {"st": 99, "ed": 101, "text": "word level"}, {"st": 103, "ed": 107, "text": "deep convolutional neural networks"}, {"st": 110, "ed": 113, "text": "click through rate"}, {"st": 140, "ed": 143, "text": "click through rate"}, {"st": 147, "ed": 149, "text": "character level"}, {"st": 152, "ed": 154, "text": "word level"}, {"st": 166, "ed": 168, "text": "character level"}, {"st": 174, "ed": 176, "text": "extensive experiments"}, {"st": 186, "ed": 188, "text": "search engine"}, {"st": 193, "ed": 195, "text": "significantly outperform"}, {"st": 220, "ed": 222, "text": "deep models"}, {"st": 238, "ed": 240, "text": "search engine"}, {"st": 241, "ed": 243, "text": "significantly improve"}, {"st": 250, "ed": 253, "text": "click through rate"}]
[{"st": 64, "ed": 66, "text": "explicitly model"}, {"st": 77, "ed": 79, "text": "variational inference"}, {"st": 91, "ed": 93, "text": "computational efficiency"}, {"st": 94, "ed": 96, "text": "variational methods"}, {"st": 105, "ed": 107, "text": "real data"}, {"st": 110, "ed": 112, "text": "latent variable"}]
[{"st": 10, "ed": 13, "text": "mixture of gaussians"}, {"st": 14, "ed": 16, "text": "streaming data"}, {"st": 95, "ed": 97, "text": "k 1"}, {"st": 188, "ed": 190, "text": "cluster centers"}, {"st": 197, "ed": 199, "text": "based clustering"}, {"st": 217, "ed": 219, "text": "space complexity"}, {"st": 232, "ed": 235, "text": "bias and variance"}, {"st": 260, "ed": 262, "text": "approximation error"}, {"st": 283, "ed": 285, "text": "gaussian distribution"}, {"st": 303, "ed": 305, "text": "estimation error"}]
[{"st": 0, "ed": 2, "text": "variational inference"}, {"st": 68, "ed": 70, "text": "mnist dataset"}, {"st": 74, "ed": 76, "text": "variational autoencoder"}]
[{"st": 4, "ed": 7, "text": "best arm identification"}, {"st": 75, "ed": 77, "text": "nonlinear function"}, {"st": 89, "ed": 91, "text": "decision rule"}, {"st": 151, "ed": 153, "text": "numerical experiments"}]
[{"st": 4, "ed": 6, "text": "computed tomography"}, {"st": 10, "ed": 12, "text": "x ray"}, {"st": 19, "ed": 21, "text": "low level"}, {"st": 41, "ed": 43, "text": "least squares"}, {"st": 72, "ed": 74, "text": "cost function"}, {"st": 84, "ed": 86, "text": "sparse coding"}, {"st": 117, "ed": 119, "text": "numerical experiments"}, {"st": 126, "ed": 128, "text": "low dose"}, {"st": 139, "ed": 141, "text": "reconstructed images"}]
[{"st": 1, "ed": 3, "text": "recent years"}, {"st": 12, "ed": 14, "text": "unsupervised learning"}, {"st": 17, "ed": 19, "text": "autoencoder vae"}, {"st": 28, "ed": 30, "text": "variational auto"}, {"st": 40, "ed": 42, "text": "generative process"}, {"st": 60, "ed": 62, "text": "variational autoencoder"}, {"st": 100, "ed": 102, "text": "log likelihood"}, {"st": 143, "ed": 145, "text": "latent variable"}, {"st": 154, "ed": 156, "text": "least square"}, {"st": 156, "ed": 158, "text": "loss function"}, {"st": 168, "ed": 170, "text": "least square"}, {"st": 170, "ed": 172, "text": "loss function"}, {"st": 177, "ed": 179, "text": "reconstructed images"}, {"st": 182, "ed": 184, "text": "faster training"}]
[{"st": 6, "ed": 8, "text": "mixture model"}, {"st": 15, "ed": 17, "text": "binary data"}, {"st": 45, "ed": 47, "text": "mixture models"}, {"st": 49, "ed": 51, "text": "em algorithm"}, {"st": 70, "ed": 72, "text": "optimization algorithm"}, {"st": 81, "ed": 83, "text": "extensive experimental"}, {"st": 111, "ed": 113, "text": "internal structure"}]
[{"st": 5, "ed": 11, "text": "alternating direction method of multipliers admm"}, {"st": 26, "ed": 28, "text": "linear convergence"}, {"st": 30, "ed": 32, "text": "strongly convex"}, {"st": 37, "ed": 39, "text": "convergence rate"}, {"st": 41, "ed": 43, "text": "convex problems"}, {"st": 72, "ed": 74, "text": "convergence rates"}, {"st": 119, "ed": 121, "text": "update rules"}, {"st": 122, "ed": 124, "text": "strongly convex"}, {"st": 136, "ed": 138, "text": "strongly convex"}, {"st": 143, "ed": 145, "text": "per iteration"}, {"st": 155, "ed": 157, "text": "convergence rate"}, {"st": 159, "ed": 161, "text": "convex problems"}]
[{"st": 2, "ed": 4, "text": "log linear"}, {"st": 5, "ed": 7, "text": "scales linearly"}, {"st": 11, "ed": 13, "text": "output space"}, {"st": 23, "ed": 25, "text": "natural language"}, {"st": 27, "ed": 29, "text": "computer vision"}, {"st": 32, "ed": 34, "text": "output space"}, {"st": 48, "ed": 50, "text": "log linear"}, {"st": 61, "ed": 63, "text": "random variable"}, {"st": 69, "ed": 71, "text": "inner product"}, {"st": 85, "ed": 87, "text": "method yields"}, {"st": 100, "ed": 102, "text": "word embeddings"}, {"st": 103, "ed": 105, "text": "significant speedups"}, {"st": 107, "ed": 110, "text": "inference and learning"}, {"st": 111, "ed": 113, "text": "log linear"}]
[{"st": 55, "ed": 57, "text": "near optimal"}, {"st": 75, "ed": 77, "text": "compressive sensing"}, {"st": 85, "ed": 87, "text": "convex optimization"}, {"st": 88, "ed": 90, "text": "iterative algorithms"}, {"st": 99, "ed": 101, "text": "ell 1"}, {"st": 104, "ed": 106, "text": "phase transition"}, {"st": 106, "ed": 109, "text": "point of view"}, {"st": 114, "ed": 116, "text": "ell 1"}, {"st": 121, "ed": 123, "text": "phase transition"}, {"st": 125, "ed": 127, "text": "ell 1"}, {"st": 136, "ed": 138, "text": "experimentally demonstrate"}, {"st": 156, "ed": 158, "text": "fewer parameters"}]
[{"st": 32, "ed": 34, "text": "intelligent systems"}, {"st": 79, "ed": 81, "text": "finite set"}, {"st": 83, "ed": 85, "text": "building blocks"}, {"st": 87, "ed": 89, "text": "exponentially large"}, {"st": 117, "ed": 119, "text": "previously published"}, {"st": 133, "ed": 135, "text": "latent structure"}, {"st": 144, "ed": 146, "text": "abstract concepts"}, {"st": 183, "ed": 185, "text": "bi directional"}, {"st": 212, "ed": 214, "text": "visual concepts"}, {"st": 231, "ed": 233, "text": "visual concepts"}]
[{"st": 13, "ed": 16, "text": "multi task learning"}, {"st": 42, "ed": 44, "text": "empirical results"}, {"st": 46, "ed": 48, "text": "consistently outperforms"}, {"st": 50, "ed": 52, "text": "kernel learning"}, {"st": 61, "ed": 63, "text": "base kernels"}, {"st": 77, "ed": 79, "text": "promising results"}, {"st": 85, "ed": 87, "text": "rademacher complexity"}, {"st": 93, "ed": 95, "text": "multi task"}, {"st": 95, "ed": 98, "text": "multiple kernel learning"}, {"st": 108, "ed": 111, "text": "support vector machine"}, {"st": 154, "ed": 157, "text": "classification and regression"}]
[{"st": 3, "ed": 5, "text": "probabilistic framework"}, {"st": 44, "ed": 46, "text": "probabilistic model"}, {"st": 49, "ed": 51, "text": "prior distributions"}, {"st": 59, "ed": 61, "text": "desired properties"}, {"st": 99, "ed": 102, "text": "learning and inference"}, {"st": 112, "ed": 116, "text": "markov chain monte carlo"}, {"st": 126, "ed": 128, "text": "proposed framework"}, {"st": 132, "ed": 134, "text": "time series"}, {"st": 136, "ed": 140, "text": "synthetic and real world"}, {"st": 149, "ed": 152, "text": "mean square error"}]
[{"st": 0, "ed": 3, "text": "mixture of experts"}, {"st": 15, "ed": 17, "text": "complex data"}]
[{"st": 57, "ed": 59, "text": "unknown distribution"}, {"st": 75, "ed": 77, "text": "total number"}, {"st": 109, "ed": 111, "text": "optimization algorithm"}]
[{"st": 1, "ed": 4, "text": "deep reinforcement learning"}, {"st": 26, "ed": 28, "text": "multitask learning"}, {"st": 30, "ed": 32, "text": "neural network"}, {"st": 97, "ed": 99, "text": "joint training"}, {"st": 100, "ed": 102, "text": "multiple tasks"}, {"st": 109, "ed": 111, "text": "transfer learning"}, {"st": 171, "ed": 173, "text": "learning process"}, {"st": 200, "ed": 202, "text": "learning process"}]
[{"st": 8, "ed": 10, "text": "time series"}, {"st": 15, "ed": 18, "text": "ability to learn"}, {"st": 33, "ed": 35, "text": "accurate predictions"}, {"st": 41, "ed": 43, "text": "computational complexity"}, {"st": 121, "ed": 123, "text": "gaussian kernel"}, {"st": 139, "ed": 141, "text": "real world"}, {"st": 150, "ed": 153, "text": "mean square error"}]
[{"st": 2, "ed": 4, "text": "et al"}, {"st": 81, "ed": 83, "text": "experimental literature"}, {"st": 103, "ed": 105, "text": "exponential families"}, {"st": 109, "ed": 111, "text": "exponential families"}, {"st": 132, "ed": 134, "text": "deep architectures"}, {"st": 155, "ed": 157, "text": "deep architectures"}, {"st": 169, "ed": 172, "text": "a sufficient condition"}, {"st": 174, "ed": 176, "text": "activation functions"}, {"st": 205, "ed": 207, "text": "exponential families"}, {"st": 244, "ed": 246, "text": "activation functions"}]
[{"st": 26, "ed": 28, "text": "statistically efficient"}, {"st": 41, "ed": 43, "text": "observational data"}, {"st": 55, "ed": 57, "text": "computational efficiency"}, {"st": 82, "ed": 85, "text": "directed acyclic graph"}, {"st": 119, "ed": 121, "text": "mathcal o"}, {"st": 129, "ed": 131, "text": "finite sample"}, {"st": 135, "ed": 137, "text": "precision matrix"}, {"st": 147, "ed": 149, "text": "mathcal o"}, {"st": 157, "ed": 159, "text": "precision matrix"}, {"st": 169, "ed": 171, "text": "mathcal o"}, {"st": 176, "ed": 178, "text": "gaussian noise"}, {"st": 185, "ed": 187, "text": "sample complexity"}, {"st": 188, "ed": 190, "text": "mathcal o"}, {"st": 232, "ed": 234, "text": "positive integer"}, {"st": 238, "ed": 240, "text": "sample complexity"}, {"st": 241, "ed": 243, "text": "mathcal o"}]
[{"st": 2, "ed": 4, "text": "time series"}, {"st": 45, "ed": 47, "text": "decision making"}, {"st": 54, "ed": 56, "text": "deep learning"}, {"st": 65, "ed": 67, "text": "icu mortality"}, {"st": 74, "ed": 77, "text": "short term memory"}]
[{"st": 23, "ed": 25, "text": "missing values"}, {"st": 48, "ed": 50, "text": "probabilistic inference"}, {"st": 50, "ed": 52, "text": "gibbs sampling"}, {"st": 52, "ed": 54, "text": "variational bayesian"}]
[{"st": 2, "ed": 4, "text": "hierarchical structure"}, {"st": 21, "ed": 23, "text": "text mining"}, {"st": 27, "ed": 30, "text": "multi label classification"}, {"st": 39, "ed": 41, "text": "hierarchical structure"}, {"st": 103, "ed": 105, "text": "hierarchical structure"}, {"st": 113, "ed": 116, "text": "hierarchical dirichlet process"}, {"st": 126, "ed": 128, "text": "hierarchical structure"}, {"st": 148, "ed": 151, "text": "hierarchical dirichlet process"}, {"st": 194, "ed": 200, "text": "experiments on synthetic and real world"}, {"st": 215, "ed": 217, "text": "hierarchical structure"}]
[{"st": 33, "ed": 35, "text": "attention based"}, {"st": 108, "ed": 110, "text": "building blocks"}, {"st": 160, "ed": 162, "text": "user interactions"}, {"st": 200, "ed": 202, "text": "significant improvements"}]
[{"st": 3, "ed": 5, "text": "fast inference"}, {"st": 9, "ed": 12, "text": "support vector machines"}, {"st": 14, "ed": 17, "text": "stochastic variational inference"}, {"st": 25, "ed": 27, "text": "proposed method"}, {"st": 52, "ed": 54, "text": "uncertainty estimates"}]
[{"st": 3, "ed": 5, "text": "global optimization"}, {"st": 18, "ed": 20, "text": "linear regression"}, {"st": 33, "ed": 35, "text": "global optimality"}, {"st": 45, "ed": 47, "text": "proposed approach"}, {"st": 62, "ed": 65, "text": "branch and bound"}, {"st": 91, "ed": 93, "text": "directly optimize"}, {"st": 98, "ed": 100, "text": "continuous optimization"}, {"st": 105, "ed": 107, "text": "numerical experiments"}, {"st": 114, "ed": 116, "text": "higher accuracy"}, {"st": 117, "ed": 119, "text": "convex relaxations"}, {"st": 122, "ed": 124, "text": "computational burden"}]
[{"st": 3, "ed": 5, "text": "gaussian process"}, {"st": 9, "ed": 11, "text": "latent variable"}, {"st": 15, "ed": 17, "text": "non stationary"}, {"st": 17, "ed": 19, "text": "multi modal"}, {"st": 29, "ed": 31, "text": "input space"}, {"st": 37, "ed": 39, "text": "latent variable"}, {"st": 61, "ed": 63, "text": "multi modal"}, {"st": 64, "ed": 66, "text": "non stationary"}, {"st": 75, "ed": 77, "text": "synthetic data"}, {"st": 81, "ed": 83, "text": "real data"}, {"st": 84, "ed": 86, "text": "motion capture"}]
[{"st": 42, "ed": 44, "text": "feature extractor"}, {"st": 86, "ed": 88, "text": "competitive performance"}, {"st": 103, "ed": 105, "text": "deep networks"}]
[{"st": 23, "ed": 25, "text": "inverse problem"}, {"st": 45, "ed": 48, "text": "artificial neural networks"}, {"st": 48, "ed": 50, "text": "k means"}, {"st": 96, "ed": 98, "text": "efficient learning"}, {"st": 137, "ed": 139, "text": "deep residual"}]
[{"st": 16, "ed": 18, "text": "invariant feature"}, {"st": 49, "ed": 51, "text": "higher order"}, {"st": 110, "ed": 112, "text": "extremely high"}, {"st": 145, "ed": 147, "text": "machine learning"}, {"st": 189, "ed": 192, "text": "deep artificial neural"}]
[{"st": 1, "ed": 3, "text": "sensing applications"}, {"st": 49, "ed": 51, "text": "gaussian process"}, {"st": 62, "ed": 64, "text": "gaussian process"}, {"st": 151, "ed": 153, "text": "real world"}, {"st": 155, "ed": 157, "text": "time series"}, {"st": 159, "ed": 161, "text": "step function"}, {"st": 170, "ed": 172, "text": "estimation error"}]
[{"st": 9, "ed": 12, "text": "computer aided diagnosis"}, {"st": 28, "ed": 30, "text": "deep learning"}, {"st": 33, "ed": 35, "text": "computer vision"}, {"st": 42, "ed": 44, "text": "deep learning"}, {"st": 56, "ed": 58, "text": "deep learning"}, {"st": 60, "ed": 63, "text": "computer aided diagnosis"}, {"st": 99, "ed": 101, "text": "time consuming"}, {"st": 119, "ed": 121, "text": "multi stage"}, {"st": 121, "ed": 124, "text": "self paced learning"}, {"st": 127, "ed": 131, "text": "convolutional neural network cnn"}, {"st": 133, "ed": 135, "text": "computed tomography"}, {"st": 139, "ed": 141, "text": "key contribution"}, {"st": 151, "ed": 153, "text": "training samples"}, {"st": 156, "ed": 158, "text": "unlabeled instances"}, {"st": 160, "ed": 163, "text": "self paced learning"}, {"st": 169, "ed": 172, "text": "high performance computing"}, {"st": 202, "ed": 204, "text": "performance gain"}, {"st": 209, "ed": 211, "text": "training samples"}, {"st": 214, "ed": 216, "text": "image analysis"}]
[{"st": 25, "ed": 29, "text": "generative adversarial networks gans"}]
[{"st": 4, "ed": 6, "text": "finite sample"}, {"st": 11, "ed": 13, "text": "k nn"}, {"st": 28, "ed": 30, "text": "k nn"}, {"st": 36, "ed": 38, "text": "intrinsic dimension"}, {"st": 43, "ed": 45, "text": "k nn"}]
[{"st": 5, "ed": 7, "text": "transfer learning"}, {"st": 8, "ed": 10, "text": "causal inference"}, {"st": 13, "ed": 15, "text": "accurate predictions"}, {"st": 24, "ed": 26, "text": "training set"}, {"st": 44, "ed": 46, "text": "generating process"}, {"st": 65, "ed": 67, "text": "transfer learning"}, {"st": 70, "ed": 72, "text": "training sets"}, {"st": 123, "ed": 125, "text": "prior knowledge"}, {"st": 127, "ed": 129, "text": "causal graph"}, {"st": 144, "ed": 149, "text": "simulated and real world data"}]
[{"st": 8, "ed": 11, "text": "real world data"}, {"st": 13, "ed": 15, "text": "face images"}, {"st": 71, "ed": 73, "text": "supervised learning"}, {"st": 99, "ed": 101, "text": "optimization problem"}, {"st": 109, "ed": 111, "text": "theoretical analysis"}, {"st": 113, "ed": 115, "text": "generalization bounds"}, {"st": 119, "ed": 121, "text": "rademacher complexity"}, {"st": 133, "ed": 135, "text": "proposed method"}]
[{"st": 37, "ed": 39, "text": "metric spaces"}, {"st": 98, "ed": 100, "text": "continuous data"}, {"st": 109, "ed": 111, "text": "deep learning"}, {"st": 113, "ed": 116, "text": "conduct extensive experiments"}, {"st": 124, "ed": 126, "text": "similarity measures"}, {"st": 143, "ed": 145, "text": "ground truth"}, {"st": 151, "ed": 153, "text": "structured data"}, {"st": 156, "ed": 158, "text": "continuous data"}, {"st": 177, "ed": 179, "text": "similarity measures"}, {"st": 190, "ed": 192, "text": "deep learning"}]
[{"st": 8, "ed": 12, "text": "functional magnetic resonance imaging"}, {"st": 24, "ed": 26, "text": "functional connectivity"}, {"st": 30, "ed": 32, "text": "sparse coding"}, {"st": 50, "ed": 52, "text": "prior knowledge"}, {"st": 76, "ed": 78, "text": "based method"}, {"st": 89, "ed": 91, "text": "functional connectivity"}, {"st": 105, "ed": 107, "text": "significantly improve"}, {"st": 109, "ed": 111, "text": "connectivity patterns"}, {"st": 140, "ed": 142, "text": "promising results"}, {"st": 145, "ed": 147, "text": "proposed method"}, {"st": 156, "ed": 158, "text": "functional connectivity"}]
[{"st": 11, "ed": 13, "text": "generative models"}, {"st": 48, "ed": 50, "text": "inference algorithm"}, {"st": 55, "ed": 57, "text": "generative model"}, {"st": 60, "ed": 63, "text": "generative adversarial networks"}, {"st": 71, "ed": 73, "text": "training procedure"}, {"st": 74, "ed": 76, "text": "wasserstein gans"}, {"st": 80, "ed": 82, "text": "generative network"}, {"st": 84, "ed": 86, "text": "domain specific"}, {"st": 112, "ed": 114, "text": "proposal distribution"}, {"st": 120, "ed": 122, "text": "wasserstein distance"}, {"st": 124, "ed": 126, "text": "marginal distribution"}, {"st": 128, "ed": 130, "text": "synthetic data"}, {"st": 135, "ed": 137, "text": "observed data"}, {"st": 149, "ed": 152, "text": "discrete and continuous"}]
[{"st": 24, "ed": 26, "text": "ground truth"}, {"st": 36, "ed": 38, "text": "subspace clustering"}, {"st": 63, "ed": 65, "text": "clustering accuracy"}, {"st": 70, "ed": 72, "text": "computational complexity"}, {"st": 86, "ed": 88, "text": "dimensionality reduction"}, {"st": 131, "ed": 133, "text": "random projections"}, {"st": 135, "ed": 138, "text": "fast and accurate"}, {"st": 138, "ed": 140, "text": "large scale"}, {"st": 146, "ed": 148, "text": "extensive numerical"}, {"st": 150, "ed": 152, "text": "real data"}, {"st": 160, "ed": 162, "text": "competitive performance"}]
[{"st": 29, "ed": 31, "text": "neural networks"}, {"st": 65, "ed": 67, "text": "uncertainty quantification"}, {"st": 93, "ed": 95, "text": "learning process"}, {"st": 109, "ed": 111, "text": "neural networks"}, {"st": 126, "ed": 128, "text": "log likelihood"}]
[{"st": 3, "ed": 5, "text": "large datasets"}, {"st": 101, "ed": 103, "text": "ensemble methods"}, {"st": 105, "ed": 107, "text": "boosted trees"}, {"st": 110, "ed": 112, "text": "predictive performance"}, {"st": 129, "ed": 131, "text": "predictive performance"}, {"st": 133, "ed": 135, "text": "proposed approach"}, {"st": 137, "ed": 139, "text": "predictive performance"}]
[{"st": 7, "ed": 11, "text": "multi armed bandit problem"}, {"st": 38, "ed": 41, "text": "upper confidence bound"}, {"st": 54, "ed": 56, "text": "thompson sampling"}, {"st": 66, "ed": 68, "text": "thompson sampling"}, {"st": 81, "ed": 83, "text": "o sqrt"}, {"st": 93, "ed": 95, "text": "previous works"}, {"st": 103, "ed": 105, "text": "o sqrt"}, {"st": 131, "ed": 133, "text": "real world"}, {"st": 165, "ed": 167, "text": "total number"}, {"st": 178, "ed": 180, "text": "numerical results"}, {"st": 182, "ed": 184, "text": "theoretical findings"}]
[{"st": 19, "ed": 22, "text": "attracted much attention"}, {"st": 45, "ed": 47, "text": "nonconvex optimization"}, {"st": 63, "ed": 65, "text": "theoretical analysis"}, {"st": 84, "ed": 88, "text": "simulated and real world"}, {"st": 97, "ed": 99, "text": "similar performance"}, {"st": 119, "ed": 121, "text": "prior knowledge"}]
[{"st": 22, "ed": 24, "text": "classification tasks"}, {"st": 88, "ed": 91, "text": "data generating distribution"}, {"st": 92, "ed": 94, "text": "empirically demonstrate"}]
[{"st": 0, "ed": 2, "text": "differential privacy"}, {"st": 67, "ed": 69, "text": "fine grained"}, {"st": 71, "ed": 73, "text": "differential privacy"}, {"st": 75, "ed": 77, "text": "differential privacy"}, {"st": 109, "ed": 111, "text": "desirable properties"}, {"st": 115, "ed": 117, "text": "side information"}, {"st": 283, "ed": 286, "text": "orders of magnitude"}]
[{"st": 2, "ed": 5, "text": "statistical relational learning"}, {"st": 16, "ed": 18, "text": "large scale"}, {"st": 25, "ed": 27, "text": "successfully applied"}, {"st": 68, "ed": 70, "text": "parameter estimates"}, {"st": 113, "ed": 115, "text": "logistic regression"}, {"st": 117, "ed": 120, "text": "stochastic gradient descent"}, {"st": 128, "ed": 130, "text": "proposed method"}, {"st": 132, "ed": 134, "text": "parameter estimates"}]
[{"st": 1, "ed": 3, "text": "probabilistic models"}, {"st": 76, "ed": 78, "text": "naive bayes"}, {"st": 78, "ed": 80, "text": "logistic regression"}, {"st": 96, "ed": 98, "text": "existing models"}]
[{"st": 11, "ed": 13, "text": "prediction models"}, {"st": 24, "ed": 26, "text": "joint distribution"}, {"st": 36, "ed": 38, "text": "predictive performance"}, {"st": 41, "ed": 43, "text": "concept drift"}, {"st": 60, "ed": 62, "text": "concept drift"}, {"st": 72, "ed": 74, "text": "data stream"}, {"st": 96, "ed": 98, "text": "concept drift"}, {"st": 104, "ed": 106, "text": "imbalanced data"}, {"st": 118, "ed": 120, "text": "online fashion"}, {"st": 128, "ed": 130, "text": "training strategy"}, {"st": 140, "ed": 142, "text": "proposed framework"}, {"st": 149, "ed": 153, "text": "simulated and real world"}, {"st": 158, "ed": 160, "text": "concept drift"}, {"st": 162, "ed": 164, "text": "proposed approach"}, {"st": 164, "ed": 166, "text": "significantly outperforms"}]
[{"st": 12, "ed": 14, "text": "class conditional"}, {"st": 18, "ed": 20, "text": "linear discriminant"}, {"st": 118, "ed": 120, "text": "class conditional"}, {"st": 123, "ed": 125, "text": "proposed method"}, {"st": 132, "ed": 134, "text": "generalization performance"}]
[{"st": 34, "ed": 36, "text": "input space"}, {"st": 46, "ed": 48, "text": "generalization error"}, {"st": 101, "ed": 103, "text": "error bounds"}, {"st": 187, "ed": 189, "text": "kernel based"}]
[{"st": 1, "ed": 5, "text": "convolutional neural networks cnns"}, {"st": 15, "ed": 17, "text": "wide variety"}, {"st": 116, "ed": 118, "text": "convolutional layer"}, {"st": 129, "ed": 131, "text": "feature maps"}, {"st": 146, "ed": 148, "text": "deep cnn"}, {"st": 154, "ed": 156, "text": "proposed algorithm"}]
[{"st": 6, "ed": 8, "text": "large scale"}, {"st": 8, "ed": 11, "text": "approximate nearest neighbor"}, {"st": 25, "ed": 27, "text": "supervised hashing"}, {"st": 28, "ed": 30, "text": "significantly outperform"}, {"st": 32, "ed": 34, "text": "supervised hashing"}, {"st": 41, "ed": 43, "text": "supervised hashing"}, {"st": 52, "ed": 54, "text": "hash function"}, {"st": 68, "ed": 70, "text": "supervised hashing"}, {"st": 73, "ed": 75, "text": "time consuming"}, {"st": 88, "ed": 90, "text": "large scale"}, {"st": 99, "ed": 101, "text": "supervised hashing"}, {"st": 101, "ed": 103, "text": "method called"}, {"st": 105, "ed": 107, "text": "supervised hashing"}, {"st": 109, "ed": 111, "text": "large scale"}, {"st": 132, "ed": 134, "text": "hash function"}, {"st": 140, "ed": 142, "text": "hash codes"}, {"st": 162, "ed": 164, "text": "supervised hashing"}]
[{"st": 8, "ed": 10, "text": "latent feature"}, {"st": 89, "ed": 91, "text": "latent structure"}, {"st": 96, "ed": 98, "text": "latent features"}, {"st": 111, "ed": 113, "text": "latent features"}]
[{"st": 11, "ed": 14, "text": "armed bandit problem"}, {"st": 67, "ed": 69, "text": "bandit problem"}]
[{"st": 5, "ed": 7, "text": "submodular functions"}, {"st": 15, "ed": 17, "text": "optimization problem"}, {"st": 22, "ed": 24, "text": "convex optimization"}, {"st": 49, "ed": 52, "text": "simple and efficient"}, {"st": 59, "ed": 61, "text": "higher order"}, {"st": 77, "ed": 80, "text": "convex loss functions"}]
[{"st": 6, "ed": 8, "text": "generative models"}, {"st": 9, "ed": 12, "text": "deep neural networks"}, {"st": 62, "ed": 64, "text": "generative models"}, {"st": 67, "ed": 69, "text": "latent structure"}]
[{"st": 17, "ed": 19, "text": "finite state"}, {"st": 28, "ed": 30, "text": "reverse engineering"}, {"st": 51, "ed": 53, "text": "domain expertise"}, {"st": 72, "ed": 74, "text": "domain expertise"}, {"st": 84, "ed": 86, "text": "learning algorithm"}]
[{"st": 0, "ed": 4, "text": "recurrent neural networks rnns"}, {"st": 8, "ed": 10, "text": "sequential data"}, {"st": 21, "ed": 24, "text": "recurrent neural networks"}, {"st": 41, "ed": 44, "text": "short term memory"}, {"st": 59, "ed": 61, "text": "weight matrices"}, {"st": 63, "ed": 65, "text": "complex valued"}, {"st": 73, "ed": 75, "text": "skew symmetric"}, {"st": 101, "ed": 103, "text": "weight matrix"}, {"st": 105, "ed": 107, "text": "diagonal matrix"}, {"st": 115, "ed": 117, "text": "training scheme"}, {"st": 133, "ed": 136, "text": "recurrent neural network"}, {"st": 142, "ed": 144, "text": "trainable parameters"}]
[{"st": 1, "ed": 3, "text": "predictive power"}, {"st": 4, "ed": 6, "text": "neural networks"}, {"st": 22, "ed": 24, "text": "input features"}, {"st": 84, "ed": 86, "text": "input image"}, {"st": 153, "ed": 155, "text": "attention mechanisms"}]
[{"st": 3, "ed": 7, "text": "multi armed bandit problem"}, {"st": 8, "ed": 10, "text": "non stationary"}, {"st": 21, "ed": 23, "text": "thompson sampling"}, {"st": 40, "ed": 42, "text": "prior distribution"}, {"st": 85, "ed": 87, "text": "extensive empirical"}]
[{"st": 0, "ed": 2, "text": "active learning"}, {"st": 26, "ed": 28, "text": "active learning"}, {"st": 46, "ed": 48, "text": "active learning"}, {"st": 68, "ed": 70, "text": "active learning"}, {"st": 85, "ed": 87, "text": "active learning"}, {"st": 119, "ed": 121, "text": "active learning"}, {"st": 123, "ed": 125, "text": "spatial regions"}, {"st": 171, "ed": 173, "text": "active learning"}, {"st": 209, "ed": 211, "text": "user defined"}, {"st": 213, "ed": 215, "text": "automatically generated"}]
[{"st": 5, "ed": 7, "text": "feature learning"}, {"st": 12, "ed": 14, "text": "deep representations"}, {"st": 16, "ed": 18, "text": "multiple tasks"}, {"st": 21, "ed": 23, "text": "negative transfer"}, {"st": 52, "ed": 54, "text": "feature learning"}, {"st": 115, "ed": 117, "text": "benchmark datasets"}, {"st": 118, "ed": 120, "text": "multitask learning"}, {"st": 121, "ed": 123, "text": "image classification"}, {"st": 126, "ed": 128, "text": "significantly outperforms"}, {"st": 132, "ed": 134, "text": "multitask learning"}, {"st": 138, "ed": 140, "text": "negative transfer"}]
[{"st": 8, "ed": 10, "text": "manually annotated"}, {"st": 12, "ed": 14, "text": "social media"}, {"st": 14, "ed": 16, "text": "sentiment analysis"}, {"st": 17, "ed": 19, "text": "related tasks"}, {"st": 40, "ed": 42, "text": "distant supervision"}, {"st": 48, "ed": 50, "text": "noisy labels"}, {"st": 81, "ed": 83, "text": "benchmark datasets"}, {"st": 106, "ed": 108, "text": "performance improvement"}, {"st": 110, "ed": 112, "text": "distant supervision"}]
[{"st": 6, "ed": 8, "text": "kernel regression"}, {"st": 50, "ed": 52, "text": "regularization parameter"}, {"st": 60, "ed": 62, "text": "confidence bounds"}, {"st": 79, "ed": 81, "text": "existing results"}, {"st": 82, "ed": 84, "text": "finite dimensional"}, {"st": 84, "ed": 86, "text": "linear regression"}, {"st": 98, "ed": 100, "text": "regularization parameter"}, {"st": 104, "ed": 106, "text": "measurable function"}, {"st": 190, "ed": 192, "text": "thompson sampling"}]
[{"st": 9, "ed": 11, "text": "prediction task"}, {"st": 12, "ed": 14, "text": "recommender systems"}, {"st": 15, "ed": 17, "text": "significantly outperforms"}, {"st": 35, "ed": 37, "text": "deep autoencoder"}, {"st": 42, "ed": 46, "text": "trained end to end"}, {"st": 48, "ed": 50, "text": "layer wise"}, {"st": 53, "ed": 55, "text": "empirically demonstrate"}, {"st": 57, "ed": 59, "text": "deep autoencoder"}, {"st": 69, "ed": 71, "text": "linear activation"}, {"st": 79, "ed": 81, "text": "deep models"}, {"st": 86, "ed": 88, "text": "regularization techniques"}, {"st": 131, "ed": 135, "text": "available at https github.com"}]
[{"st": 1, "ed": 3, "text": "contextual bandit"}, {"st": 15, "ed": 17, "text": "non stationary"}, {"st": 30, "ed": 32, "text": "contextual bandit"}, {"st": 34, "ed": 36, "text": "non stationary"}, {"st": 39, "ed": 41, "text": "existing methods"}, {"st": 67, "ed": 69, "text": "non stationary"}, {"st": 96, "ed": 98, "text": "mathcal o"}, {"st": 112, "ed": 114, "text": "mathcal o"}, {"st": 185, "ed": 187, "text": "parameter free"}]
[{"st": 4, "ed": 7, "text": "generative adversarial network"}, {"st": 27, "ed": 29, "text": "probabilistic model"}, {"st": 30, "ed": 33, "text": "gaussian mixture model"}, {"st": 38, "ed": 40, "text": "gan framework"}, {"st": 46, "ed": 48, "text": "loss function"}, {"st": 53, "ed": 55, "text": "classification loss"}, {"st": 83, "ed": 85, "text": "generate realistic"}, {"st": 121, "ed": 123, "text": "gan training"}]
[{"st": 20, "ed": 22, "text": "big data"}, {"st": 105, "ed": 107, "text": "error bounds"}, {"st": 137, "ed": 139, "text": "learning rates"}]
[{"st": 5, "ed": 8, "text": "a b testing"}, {"st": 10, "ed": 12, "text": "observed data"}, {"st": 71, "ed": 73, "text": "clinical trial"}]
[{"st": 9, "ed": 11, "text": "logistic regression"}, {"st": 17, "ed": 19, "text": "nonconvex optimization"}, {"st": 30, "ed": 32, "text": "convex function"}, {"st": 37, "ed": 39, "text": "ell 0"}, {"st": 51, "ed": 53, "text": "ell 1"}, {"st": 60, "ed": 62, "text": "sparsity inducing"}, {"st": 71, "ed": 73, "text": "logistic regression"}, {"st": 85, "ed": 87, "text": "regularization parameter"}, {"st": 98, "ed": 100, "text": "proximal gradient"}, {"st": 110, "ed": 112, "text": "logistic regression"}, {"st": 129, "ed": 131, "text": "convex function"}, {"st": 161, "ed": 163, "text": "numerical experiments"}, {"st": 165, "ed": 167, "text": "randomly generated"}]
[{"st": 3, "ed": 6, "text": "low rank matrix"}, {"st": 16, "ed": 18, "text": "low rank"}, {"st": 34, "ed": 36, "text": "low rank"}, {"st": 42, "ed": 44, "text": "gaussian distribution"}, {"st": 50, "ed": 52, "text": "precision matrix"}, {"st": 54, "ed": 56, "text": "wishart distribution"}, {"st": 79, "ed": 81, "text": "low rank"}, {"st": 90, "ed": 92, "text": "variational bayesian"}, {"st": 96, "ed": 98, "text": "matrix completion"}, {"st": 110, "ed": 112, "text": "variational bayesian"}, {"st": 126, "ed": 128, "text": "proposed method"}, {"st": 136, "ed": 138, "text": "matrix completion"}]
[{"st": 0, "ed": 2, "text": "generative modeling"}, {"st": 10, "ed": 12, "text": "notoriously difficult"}, {"st": 23, "ed": 25, "text": "generative model"}, {"st": 37, "ed": 41, "text": "generative adversarial networks gans"}, {"st": 66, "ed": 68, "text": "structured prediction"}, {"st": 73, "ed": 75, "text": "decision theory"}, {"st": 81, "ed": 83, "text": "recent advances"}, {"st": 84, "ed": 86, "text": "structured prediction"}, {"st": 128, "ed": 130, "text": "generative modeling"}]
[{"st": 1, "ed": 3, "text": "adversarial examples"}, {"st": 7, "ed": 9, "text": "adversarial training"}, {"st": 31, "ed": 33, "text": "adversarial images"}, {"st": 49, "ed": 51, "text": "adversarial training"}, {"st": 72, "ed": 74, "text": "adversarial images"}, {"st": 84, "ed": 86, "text": "adversarial images"}, {"st": 96, "ed": 98, "text": "embedding space"}, {"st": 102, "ed": 104, "text": "low level"}, {"st": 106, "ed": 108, "text": "similarity learning"}, {"st": 118, "ed": 120, "text": "adversarial images"}, {"st": 143, "ed": 145, "text": "adversarial training"}, {"st": 149, "ed": 151, "text": "low level"}, {"st": 151, "ed": 153, "text": "similarity learning"}, {"st": 184, "ed": 186, "text": "worst case"}]
[{"st": 0, "ed": 2, "text": "surrogate models"}, {"st": 4, "ed": 7, "text": "low computational cost"}, {"st": 16, "ed": 18, "text": "surrogate models"}, {"st": 144, "ed": 146, "text": "surrogate model"}, {"st": 160, "ed": 162, "text": "least squares"}, {"st": 193, "ed": 195, "text": "least squares"}, {"st": 207, "ed": 209, "text": "proposed method"}, {"st": 212, "ed": 214, "text": "surrogate models"}, {"st": 240, "ed": 242, "text": "proposed method"}, {"st": 247, "ed": 249, "text": "conventional methods"}, {"st": 260, "ed": 262, "text": "fixed budget"}, {"st": 280, "ed": 282, "text": "surrogate models"}, {"st": 289, "ed": 292, "text": "times faster than"}]
[{"st": 13, "ed": 15, "text": "machine learning"}, {"st": 21, "ed": 23, "text": "sydney australia"}, {"st": 31, "ed": 33, "text": "pang wei"}]
[{"st": 3, "ed": 5, "text": "non stationary"}, {"st": 6, "ed": 8, "text": "stochastic optimization"}, {"st": 13, "ed": 15, "text": "cost functions"}, {"st": 37, "ed": 39, "text": "local spatial"}, {"st": 62, "ed": 64, "text": "regret bounds"}, {"st": 67, "ed": 69, "text": "strongly convex"}, {"st": 73, "ed": 75, "text": "previous results"}, {"st": 95, "ed": 98, "text": "curse of dimensionality"}, {"st": 122, "ed": 124, "text": "convex functions"}]
[{"st": 45, "ed": 47, "text": "non trivial"}, {"st": 85, "ed": 87, "text": "machine learning"}, {"st": 109, "ed": 111, "text": "labeled examples"}, {"st": 114, "ed": 116, "text": "supervised learning"}, {"st": 134, "ed": 136, "text": "models including"}, {"st": 155, "ed": 157, "text": "anomaly detection"}, {"st": 186, "ed": 188, "text": "detection methods"}, {"st": 217, "ed": 219, "text": "anomaly detection"}, {"st": 252, "ed": 254, "text": "data stream"}]
[{"st": 10, "ed": 12, "text": "machine learning"}, {"st": 68, "ed": 70, "text": "large scale"}, {"st": 81, "ed": 83, "text": "machine learning"}]
[{"st": 8, "ed": 11, "text": "click through rate"}, {"st": 12, "ed": 14, "text": "prediction model"}, {"st": 24, "ed": 26, "text": "deep models"}, {"st": 51, "ed": 53, "text": "neural networks"}, {"st": 108, "ed": 110, "text": "loss functions"}, {"st": 136, "ed": 138, "text": "technique called"}, {"st": 154, "ed": 156, "text": "benchmark datasets"}, {"st": 157, "ed": 159, "text": "real life"}]
[{"st": 0, "ed": 2, "text": "recommender systems"}, {"st": 21, "ed": 23, "text": "collaborative filtering"}, {"st": 39, "ed": 41, "text": "model based"}, {"st": 48, "ed": 50, "text": "machine learning"}, {"st": 50, "ed": 52, "text": "data mining"}, {"st": 107, "ed": 109, "text": "auto encoder"}, {"st": 151, "ed": 153, "text": "neural network"}, {"st": 158, "ed": 160, "text": "proposed method"}, {"st": 196, "ed": 198, "text": "generative model"}]
[{"st": 0, "ed": 2, "text": "deep learning"}, {"st": 9, "ed": 12, "text": "supervised and unsupervised"}, {"st": 12, "ed": 14, "text": "machine learning"}, {"st": 21, "ed": 23, "text": "continuous optimization"}, {"st": 28, "ed": 30, "text": "deep learning"}, {"st": 56, "ed": 58, "text": "deep models"}, {"st": 72, "ed": 74, "text": "generative models"}, {"st": 181, "ed": 183, "text": "generative models"}]
[{"st": 16, "ed": 18, "text": "longitudinal data"}, {"st": 22, "ed": 24, "text": "main challenges"}, {"st": 67, "ed": 69, "text": "survival analysis"}, {"st": 88, "ed": 90, "text": "machine learning"}, {"st": 101, "ed": 103, "text": "challenging problems"}, {"st": 122, "ed": 124, "text": "statistical methods"}, {"st": 127, "ed": 129, "text": "machine learning"}, {"st": 132, "ed": 134, "text": "survival analysis"}, {"st": 150, "ed": 152, "text": "closely related"}, {"st": 153, "ed": 155, "text": "survival analysis"}, {"st": 158, "ed": 160, "text": "successful applications"}, {"st": 162, "ed": 165, "text": "real world application"}, {"st": 179, "ed": 181, "text": "recent advances"}, {"st": 182, "ed": 184, "text": "survival analysis"}]
[{"st": 0, "ed": 2, "text": "thompson sampling"}, {"st": 4, "ed": 6, "text": "empirical performance"}, {"st": 16, "ed": 18, "text": "thompson sampling"}, {"st": 54, "ed": 56, "text": "thompson sampling"}, {"st": 83, "ed": 85, "text": "thompson sampling"}, {"st": 87, "ed": 89, "text": "optimization problem"}, {"st": 101, "ed": 103, "text": "random variables"}]
[{"st": 4, "ed": 6, "text": "regularization scheme"}, {"st": 14, "ed": 16, "text": "neural networks"}, {"st": 22, "ed": 24, "text": "dynamic range"}, {"st": 30, "ed": 32, "text": "key idea"}, {"st": 36, "ed": 38, "text": "expressive power"}, {"st": 62, "ed": 65, "text": "end to end"}, {"st": 70, "ed": 72, "text": "classification loss"}, {"st": 79, "ed": 82, "text": "minimum description length"}, {"st": 113, "ed": 117, "text": "mnist and cifar 10"}, {"st": 134, "ed": 136, "text": "significantly smaller"}]
[{"st": 7, "ed": 9, "text": "computer science"}, {"st": 57, "ed": 59, "text": "streaming data"}, {"st": 69, "ed": 71, "text": "sampling methods"}, {"st": 175, "ed": 178, "text": "provide sufficient conditions"}]
[{"st": 7, "ed": 10, "text": "multi armed bandit"}, {"st": 27, "ed": 29, "text": "recommender systems"}, {"st": 68, "ed": 70, "text": "bandit algorithm"}]
[{"st": 0, "ed": 2, "text": "feature engineering"}, {"st": 17, "ed": 19, "text": "non trivial"}, {"st": 23, "ed": 25, "text": "feature engineering"}, {"st": 32, "ed": 34, "text": "automatically learn"}, {"st": 106, "ed": 108, "text": "feature engineering"}]
[{"st": 6, "ed": 8, "text": "contextual bandit"}, {"st": 19, "ed": 21, "text": "decision making"}, {"st": 41, "ed": 43, "text": "least square"}, {"st": 48, "ed": 50, "text": "expected reward"}, {"st": 72, "ed": 74, "text": "contextual bandit"}, {"st": 85, "ed": 87, "text": "ell 2"}, {"st": 93, "ed": 95, "text": "approximation error"}, {"st": 160, "ed": 162, "text": "ell 2"}, {"st": 187, "ed": 189, "text": "experiment results"}]
[{"st": 57, "ed": 59, "text": "large scale"}, {"st": 74, "ed": 76, "text": "parameter sharing"}, {"st": 86, "ed": 89, "text": "restricted boltzmann machine"}, {"st": 101, "ed": 103, "text": "structured sparsity"}, {"st": 104, "ed": 107, "text": "distance metric learning"}, {"st": 118, "ed": 120, "text": "applications including"}, {"st": 129, "ed": 131, "text": "representation learning"}, {"st": 143, "ed": 145, "text": "baseline methods"}]
[{"st": 7, "ed": 9, "text": "conditional generative"}, {"st": 12, "ed": 14, "text": "semi supervised"}, {"st": 28, "ed": 30, "text": "semi supervised"}, {"st": 43, "ed": 45, "text": "marginal distribution"}, {"st": 50, "ed": 52, "text": "conditional distribution"}, {"st": 61, "ed": 63, "text": "semi supervised"}, {"st": 65, "ed": 67, "text": "marginal distribution"}, {"st": 78, "ed": 80, "text": "unlabeled data"}, {"st": 82, "ed": 84, "text": "conditional distribution"}, {"st": 104, "ed": 106, "text": "semi supervised"}, {"st": 106, "ed": 108, "text": "conditional gan"}]
[{"st": 0, "ed": 3, "text": "support vector machines"}, {"st": 13, "ed": 15, "text": "statistical classification"}, {"st": 32, "ed": 34, "text": "training data"}, {"st": 43, "ed": 45, "text": "piecewise linear"}, {"st": 65, "ed": 67, "text": "linear classifiers"}, {"st": 78, "ed": 80, "text": "piecewise linear"}, {"st": 86, "ed": 88, "text": "kernel based"}, {"st": 97, "ed": 99, "text": "method works"}, {"st": 107, "ed": 109, "text": "conditional probabilities"}, {"st": 170, "ed": 172, "text": "linear classifiers"}, {"st": 185, "ed": 187, "text": "optimization procedure"}]
[{"st": 0, "ed": 2, "text": "anomaly detection"}, {"st": 3, "ed": 5, "text": "numerous applications"}, {"st": 44, "ed": 46, "text": "approach called"}, {"st": 131, "ed": 133, "text": "data mining"}, {"st": 138, "ed": 140, "text": "desired properties"}, {"st": 161, "ed": 163, "text": "high dimensional"}, {"st": 193, "ed": 195, "text": "scales linearly"}, {"st": 246, "ed": 248, "text": "real world"}]
[{"st": 0, "ed": 3, "text": "deep artificial neural"}, {"st": 9, "ed": 11, "text": "training data"}, {"st": 20, "ed": 22, "text": "training data"}, {"st": 36, "ed": 38, "text": "training set"}, {"st": 54, "ed": 58, "text": "convolutional neural network cnn"}, {"st": 96, "ed": 98, "text": "coarse grained"}, {"st": 110, "ed": 113, "text": "fold cross validation"}, {"st": 121, "ed": 123, "text": "top 5"}]
[{"st": 1, "ed": 4, "text": "artificial neural network"}, {"st": 68, "ed": 70, "text": "fuzzy logic"}]
[{"st": 6, "ed": 8, "text": "deep learning"}, {"st": 9, "ed": 11, "text": "machine learning"}, {"st": 13, "ed": 15, "text": "neural network"}, {"st": 17, "ed": 19, "text": "classification problems"}, {"st": 29, "ed": 31, "text": "computational efficiency"}, {"st": 48, "ed": 50, "text": "sample sizes"}, {"st": 62, "ed": 65, "text": "deep neural networks"}, {"st": 77, "ed": 79, "text": "faster convergence"}, {"st": 84, "ed": 87, "text": "deep neural network"}, {"st": 91, "ed": 93, "text": "classification problems"}, {"st": 100, "ed": 102, "text": "interpretable models"}, {"st": 124, "ed": 127, "text": "low computational cost"}, {"st": 129, "ed": 132, "text": "k nearest neighbor"}, {"st": 144, "ed": 146, "text": "consistently outperform"}, {"st": 146, "ed": 148, "text": "deep architectures"}, {"st": 160, "ed": 163, "text": "local and global"}]
[{"st": 8, "ed": 10, "text": "sum product"}, {"st": 10, "ed": 12, "text": "graphical model"}, {"st": 18, "ed": 22, "text": "sum product networks spns"}, {"st": 40, "ed": 42, "text": "context specific"}, {"st": 55, "ed": 57, "text": "conditional independence"}, {"st": 69, "ed": 71, "text": "probability distributions"}, {"st": 106, "ed": 108, "text": "empirical evaluation"}]
[{"st": 0, "ed": 2, "text": "tropical cyclone"}, {"st": 7, "ed": 9, "text": "challenging task"}, {"st": 24, "ed": 26, "text": "prediction models"}, {"st": 37, "ed": 40, "text": "spatial and temporal"}, {"st": 41, "ed": 43, "text": "transfer learning"}, {"st": 69, "ed": 71, "text": "ensemble learning"}, {"st": 81, "ed": 83, "text": "transfer learning"}, {"st": 127, "ed": 129, "text": "neural networks"}, {"st": 156, "ed": 158, "text": "transfer learning"}]
[{"st": 59, "ed": 61, "text": "toy model"}, {"st": 67, "ed": 69, "text": "linear combination"}, {"st": 85, "ed": 87, "text": "hidden units"}, {"st": 111, "ed": 113, "text": "linear combinations"}]
[{"st": 16, "ed": 18, "text": "recurrent network"}, {"st": 95, "ed": 97, "text": "approach achieves"}, {"st": 98, "ed": 100, "text": "relative improvement"}, {"st": 102, "ed": 104, "text": "speech recognition"}, {"st": 107, "ed": 109, "text": "significant improvement"}, {"st": 112, "ed": 114, "text": "caption generation"}]
[{"st": 1, "ed": 4, "text": "ability to learn"}, {"st": 16, "ed": 18, "text": "machine learning"}, {"st": 27, "ed": 31, "text": "amounts of training data"}, {"st": 39, "ed": 41, "text": "similar performance"}, {"st": 61, "ed": 63, "text": "promising performance"}, {"st": 67, "ed": 69, "text": "fixed size"}, {"st": 89, "ed": 92, "text": "few shot learning"}]
[{"st": 5, "ed": 8, "text": "online convex optimization"}, {"st": 18, "ed": 20, "text": "weight vector"}, {"st": 26, "ed": 28, "text": "scale invariance"}, {"st": 43, "ed": 45, "text": "linear transformation"}, {"st": 53, "ed": 55, "text": "online algorithms"}, {"st": 90, "ed": 92, "text": "optimal regret"}, {"st": 103, "ed": 105, "text": "scale invariant"}, {"st": 138, "ed": 140, "text": "scale invariant"}]
[{"st": 11, "ed": 13, "text": "feature selection"}, {"st": 15, "ed": 17, "text": "big data"}, {"st": 22, "ed": 24, "text": "sample size"}, {"st": 30, "ed": 32, "text": "big data"}, {"st": 44, "ed": 46, "text": "training examples"}, {"st": 60, "ed": 62, "text": "conditional independence"}, {"st": 64, "ed": 66, "text": "meta analysis"}, {"st": 107, "ed": 109, "text": "early stopping"}, {"st": 149, "ed": 151, "text": "empirical analysis"}, {"st": 161, "ed": 163, "text": "sample size"}]
[{"st": 5, "ed": 7, "text": "parameter estimation"}, {"st": 18, "ed": 20, "text": "hierarchical dirichlet"}, {"st": 24, "ed": 26, "text": "main result"}, {"st": 34, "ed": 36, "text": "parameter estimation"}, {"st": 45, "ed": 47, "text": "random forest"}, {"st": 89, "ed": 91, "text": "large datasets"}, {"st": 174, "ed": 176, "text": "classification tasks"}, {"st": 203, "ed": 205, "text": "hierarchical dirichlet"}, {"st": 214, "ed": 218, "text": "extensive set of experiments"}, {"st": 220, "ed": 222, "text": "standard datasets"}, {"st": 232, "ed": 234, "text": "random forest"}]
[{"st": 6, "ed": 8, "text": "machine learning"}, {"st": 10, "ed": 13, "text": "supervised machine learning"}, {"st": 20, "ed": 22, "text": "structured learning"}, {"st": 58, "ed": 61, "text": "conditional random field"}, {"st": 69, "ed": 71, "text": "open source"}, {"st": 76, "ed": 78, "text": "open source"}]
[{"st": 27, "ed": 29, "text": "active learning"}, {"st": 29, "ed": 31, "text": "based methods"}, {"st": 42, "ed": 44, "text": "prior knowledge"}, {"st": 104, "ed": 106, "text": "input space"}, {"st": 118, "ed": 120, "text": "decision boundary"}, {"st": 142, "ed": 145, "text": "number of iterations"}, {"st": 177, "ed": 179, "text": "sampling methods"}]
[{"st": 7, "ed": 9, "text": "e commerce"}, {"st": 27, "ed": 29, "text": "cash flow"}, {"st": 35, "ed": 37, "text": "supply chain"}, {"st": 44, "ed": 46, "text": "challenging problem"}, {"st": 60, "ed": 62, "text": "user preferences"}, {"st": 74, "ed": 76, "text": "predict future"}, {"st": 86, "ed": 88, "text": "based methods"}, {"st": 107, "ed": 109, "text": "feature engineering"}, {"st": 118, "ed": 120, "text": "time consuming"}, {"st": 130, "ed": 132, "text": "existing methods"}, {"st": 147, "ed": 149, "text": "structured data"}, {"st": 151, "ed": 155, "text": "convolutional neural network cnn"}, {"st": 186, "ed": 188, "text": "real world"}]
[{"st": 0, "ed": 3, "text": "wireless sensor networks"}, {"st": 68, "ed": 70, "text": "previous works"}, {"st": 115, "ed": 118, "text": "probability density function"}, {"st": 169, "ed": 171, "text": "confidence interval"}, {"st": 172, "ed": 174, "text": "false alarm"}, {"st": 205, "ed": 207, "text": "mixture distribution"}, {"st": 216, "ed": 218, "text": "real data"}, {"st": 264, "ed": 266, "text": "future research"}]
[{"st": 5, "ed": 8, "text": "support vector machines"}, {"st": 13, "ed": 15, "text": "multi class"}, {"st": 32, "ed": 34, "text": "generalization error"}, {"st": 40, "ed": 43, "text": "positive and negative"}, {"st": 97, "ed": 99, "text": "machine learning"}]
[{"st": 0, "ed": 3, "text": "classification and clustering"}, {"st": 26, "ed": 28, "text": "classification algorithms"}, {"st": 32, "ed": 34, "text": "clustering methods"}, {"st": 36, "ed": 38, "text": "class labels"}, {"st": 61, "ed": 63, "text": "clustering algorithms"}, {"st": 136, "ed": 139, "text": "classification and clustering"}, {"st": 147, "ed": 149, "text": "multi class"}, {"st": 162, "ed": 164, "text": "clustering methods"}, {"st": 182, "ed": 185, "text": "block coordinate descent"}, {"st": 202, "ed": 204, "text": "extensive experimental"}, {"st": 212, "ed": 214, "text": "baseline methods"}, {"st": 224, "ed": 226, "text": "existing methods"}, {"st": 228, "ed": 231, "text": "classification and clustering"}, {"st": 240, "ed": 242, "text": "methods outperform"}, {"st": 260, "ed": 263, "text": "times faster than"}, {"st": 271, "ed": 273, "text": "class imbalance"}]
[{"st": 0, "ed": 4, "text": "deep recurrent neural networks"}, {"st": 21, "ed": 24, "text": "difficult to train"}, {"st": 45, "ed": 47, "text": "improved results"}, {"st": 52, "ed": 54, "text": "deep lstm"}, {"st": 94, "ed": 96, "text": "word level"}]
[{"st": 9, "ed": 11, "text": "unsupervised clustering"}, {"st": 14, "ed": 16, "text": "binary data"}, {"st": 58, "ed": 60, "text": "distance based"}]
[{"st": 0, "ed": 2, "text": "machine learning"}, {"st": 8, "ed": 10, "text": "raman spectroscopy"}, {"st": 24, "ed": 26, "text": "non trivial"}, {"st": 48, "ed": 50, "text": "chemical species"}, {"st": 53, "ed": 56, "text": "convolutional neural network"}, {"st": 59, "ed": 61, "text": "automatically identify"}, {"st": 71, "ed": 73, "text": "ad hoc"}, {"st": 89, "ed": 91, "text": "classification performance"}, {"st": 98, "ed": 100, "text": "machine learning"}]
[{"st": 2, "ed": 4, "text": "powerful tool"}, {"st": 9, "ed": 11, "text": "point processes"}, {"st": 27, "ed": 29, "text": "point process"}, {"st": 41, "ed": 43, "text": "computer science"}, {"st": 77, "ed": 79, "text": "open source"}, {"st": 82, "ed": 84, "text": "learning algorithms"}, {"st": 85, "ed": 87, "text": "analysis tools"}, {"st": 122, "ed": 124, "text": "source code"}, {"st": 128, "ed": 130, "text": "https github.com"}]
[{"st": 5, "ed": 8, "text": "convolutional neural network"}, {"st": 32, "ed": 34, "text": "invariant representations"}, {"st": 46, "ed": 48, "text": "high level"}, {"st": 78, "ed": 80, "text": "efficient learning"}, {"st": 87, "ed": 89, "text": "cifar 10"}, {"st": 99, "ed": 101, "text": "front end"}, {"st": 108, "ed": 111, "text": "pre trained cnn"}, {"st": 122, "ed": 124, "text": "cifar 10"}]
[{"st": 8, "ed": 12, "text": "prediction with expert advice"}, {"st": 28, "ed": 32, "text": "prediction with expert advice"}, {"st": 37, "ed": 39, "text": "near optimal"}, {"st": 96, "ed": 98, "text": "near optimal"}, {"st": 116, "ed": 118, "text": "simple regret"}, {"st": 143, "ed": 145, "text": "regret bounds"}, {"st": 184, "ed": 186, "text": "highly effective"}]
[{"st": 0, "ed": 4, "text": "nonnegative matrix factorization nmf"}, {"st": 37, "ed": 39, "text": "time series"}, {"st": 63, "ed": 65, "text": "probabilistic framework"}, {"st": 94, "ed": 96, "text": "expectation maximization"}, {"st": 99, "ed": 101, "text": "maximum likelihood"}, {"st": 164, "ed": 166, "text": "proposed approach"}, {"st": 168, "ed": 170, "text": "numerical simulations"}, {"st": 173, "ed": 175, "text": "significantly outperforms"}, {"st": 187, "ed": 189, "text": "proposed approach"}, {"st": 199, "ed": 201, "text": "temporal dependencies"}, {"st": 204, "ed": 207, "text": "hidden markov model"}]
[{"st": 0, "ed": 2, "text": "support vector"}, {"st": 7, "ed": 9, "text": "machine learning"}, {"st": 15, "ed": 17, "text": "class classification"}, {"st": 30, "ed": 32, "text": "support vectors"}, {"st": 43, "ed": 45, "text": "large data"}, {"st": 59, "ed": 61, "text": "incremental learning"}, {"st": 77, "ed": 79, "text": "support vectors"}, {"st": 93, "ed": 95, "text": "higher dimensional"}, {"st": 95, "ed": 97, "text": "feature space"}, {"st": 101, "ed": 103, "text": "gaussian kernel"}, {"st": 110, "ed": 112, "text": "support vectors"}, {"st": 127, "ed": 129, "text": "support vectors"}, {"st": 132, "ed": 134, "text": "lagrange multiplier"}, {"st": 175, "ed": 177, "text": "real data"}, {"st": 182, "ed": 184, "text": "significant gains"}, {"st": 196, "ed": 198, "text": "objective function"}]
[{"st": 38, "ed": 40, "text": "adversarial training"}]
[{"st": 15, "ed": 17, "text": "low rank"}, {"st": 17, "ed": 19, "text": "latent factors"}, {"st": 21, "ed": 25, "text": "nonnegative matrix factorization nmf"}, {"st": 39, "ed": 41, "text": "latent factors"}, {"st": 79, "ed": 81, "text": "latent factors"}]
[{"st": 11, "ed": 13, "text": "real world"}, {"st": 35, "ed": 37, "text": "social network"}, {"st": 89, "ed": 91, "text": "sampling based"}, {"st": 154, "ed": 156, "text": "extensively evaluate"}, {"st": 158, "ed": 161, "text": "synthetic and real"}, {"st": 164, "ed": 166, "text": "achieves comparable"}]
[{"st": 36, "ed": 38, "text": "deep network"}, {"st": 45, "ed": 48, "text": "end to end"}, {"st": 56, "ed": 58, "text": "loss function"}, {"st": 136, "ed": 139, "text": "k nearest neighbour"}, {"st": 147, "ed": 150, "text": "undirected graphical models"}]
[{"st": 26, "ed": 28, "text": "latent factors"}, {"st": 103, "ed": 105, "text": "predictive accuracy"}, {"st": 118, "ed": 120, "text": "preliminary results"}]
[{"st": 22, "ed": 24, "text": "feature selection"}, {"st": 46, "ed": 48, "text": "feature selection"}, {"st": 73, "ed": 75, "text": "randomly selected"}, {"st": 81, "ed": 83, "text": "main contribution"}, {"st": 88, "ed": 90, "text": "theoretical analysis"}, {"st": 113, "ed": 115, "text": "convergence speed"}, {"st": 125, "ed": 127, "text": "empirical results"}]
[{"st": 0, "ed": 2, "text": "similarity based"}, {"st": 4, "ed": 7, "text": "semi supervised learning"}, {"st": 18, "ed": 20, "text": "pairwise similarity"}, {"st": 25, "ed": 27, "text": "pairwise similarity"}, {"st": 40, "ed": 42, "text": "similarity learning"}, {"st": 56, "ed": 58, "text": "proposed framework"}, {"st": 73, "ed": 75, "text": "generalization error"}, {"st": 95, "ed": 97, "text": "rademacher complexity"}, {"st": 98, "ed": 100, "text": "generalization error"}, {"st": 115, "ed": 117, "text": "pairwise similarity"}, {"st": 132, "ed": 134, "text": "pairwise similarity"}, {"st": 145, "ed": 148, "text": "semi supervised learning"}, {"st": 161, "ed": 163, "text": "squared error"}, {"st": 165, "ed": 167, "text": "kernel density"}, {"st": 183, "ed": 186, "text": "semi supervised learning"}]
[{"st": 7, "ed": 9, "text": "machine learning"}, {"st": 9, "ed": 11, "text": "computer vision"}, {"st": 80, "ed": 82, "text": "clustering technique"}, {"st": 105, "ed": 107, "text": "optimal solution"}, {"st": 122, "ed": 124, "text": "performance improvements"}, {"st": 128, "ed": 130, "text": "structure learning"}, {"st": 132, "ed": 134, "text": "subspace segmentation"}]
[{"st": 3, "ed": 5, "text": "multiple output"}, {"st": 5, "ed": 7, "text": "gaussian processes"}, {"st": 10, "ed": 12, "text": "linear combinations"}, {"st": 17, "ed": 19, "text": "gaussian processes"}, {"st": 53, "ed": 55, "text": "current approaches"}, {"st": 92, "ed": 94, "text": "complex valued"}, {"st": 115, "ed": 117, "text": "principled approach"}, {"st": 144, "ed": 146, "text": "previous methods"}, {"st": 158, "ed": 160, "text": "proposed method"}, {"st": 164, "ed": 166, "text": "synthetic data"}, {"st": 175, "ed": 177, "text": "real world"}]
[{"st": 5, "ed": 7, "text": "recently proposed"}, {"st": 7, "ed": 9, "text": "convex optimization"}, {"st": 11, "ed": 13, "text": "machine learning"}, {"st": 19, "ed": 21, "text": "big data"}, {"st": 46, "ed": 48, "text": "signal processing"}, {"st": 51, "ed": 53, "text": "highly scalable"}, {"st": 70, "ed": 76, "text": "alternating direction method of multipliers admm"}, {"st": 95, "ed": 97, "text": "powerful tool"}, {"st": 105, "ed": 108, "text": "a sufficient condition"}, {"st": 116, "ed": 118, "text": "network topology"}, {"st": 161, "ed": 163, "text": "machine learning"}]
[{"st": 16, "ed": 18, "text": "neural network"}, {"st": 28, "ed": 30, "text": "building blocks"}, {"st": 33, "ed": 35, "text": "forward pass"}, {"st": 37, "ed": 39, "text": "update rules"}]
[{"st": 1, "ed": 3, "text": "mixture models"}, {"st": 9, "ed": 11, "text": "machine learning"}, {"st": 12, "ed": 14, "text": "pattern recognition"}, {"st": 18, "ed": 20, "text": "dimensionality reduction"}, {"st": 37, "ed": 39, "text": "supervised learning"}, {"st": 56, "ed": 58, "text": "synthetic data"}, {"st": 72, "ed": 74, "text": "training data"}]
[{"st": 10, "ed": 12, "text": "domain knowledge"}, {"st": 25, "ed": 27, "text": "domain knowledge"}, {"st": 29, "ed": 31, "text": "n grams"}, {"st": 42, "ed": 44, "text": "neural networks"}, {"st": 61, "ed": 63, "text": "domain knowledge"}, {"st": 71, "ed": 73, "text": "portable executable"}, {"st": 81, "ed": 83, "text": "neural networks"}, {"st": 89, "ed": 91, "text": "explicit feature"}, {"st": 98, "ed": 100, "text": "domain knowledge"}]
[{"st": 4, "ed": 7, "text": "electronic health records"}, {"st": 26, "ed": 28, "text": "recent progress"}, {"st": 34, "ed": 36, "text": "deep learning"}, {"st": 38, "ed": 41, "text": "shown promising results"}, {"st": 68, "ed": 70, "text": "deep learning"}, {"st": 76, "ed": 78, "text": "risk prediction"}, {"st": 88, "ed": 91, "text": "generative adversarial network"}, {"st": 112, "ed": 115, "text": "semi supervised learning"}, {"st": 119, "ed": 121, "text": "generative model"}, {"st": 124, "ed": 128, "text": "convolutional neural network cnn"}, {"st": 129, "ed": 131, "text": "prediction model"}, {"st": 146, "ed": 148, "text": "proposed framework"}, {"st": 154, "ed": 156, "text": "significant improvements"}, {"st": 157, "ed": 159, "text": "classification tasks"}, {"st": 161, "ed": 163, "text": "generated data"}]
[{"st": 114, "ed": 116, "text": "strongly connected"}, {"st": 132, "ed": 134, "text": "at large"}]
[{"st": 21, "ed": 23, "text": "statistical models"}, {"st": 64, "ed": 66, "text": "linear regression"}, {"st": 83, "ed": 85, "text": "based method"}, {"st": 85, "ed": 87, "text": "outperforms previous"}, {"st": 92, "ed": 94, "text": "improved performance"}]
[{"st": 5, "ed": 7, "text": "variational autoencoder"}, {"st": 77, "ed": 81, "text": "extensive set of experiments"}]
[{"st": 7, "ed": 9, "text": "low rank"}, {"st": 32, "ed": 34, "text": "low rank"}]
[{"st": 1, "ed": 4, "text": "deep neural networks"}, {"st": 20, "ed": 22, "text": "training samples"}, {"st": 33, "ed": 35, "text": "neural networks"}, {"st": 36, "ed": 38, "text": "classification task"}, {"st": 40, "ed": 42, "text": "training samples"}, {"st": 54, "ed": 56, "text": "regularization term"}, {"st": 59, "ed": 61, "text": "hidden layers"}, {"st": 75, "ed": 77, "text": "invariant representations"}, {"st": 94, "ed": 96, "text": "numerical experiments"}, {"st": 110, "ed": 112, "text": "neural network"}, {"st": 121, "ed": 123, "text": "source code"}, {"st": 126, "ed": 128, "text": "https github.com"}, {"st": 131, "ed": 133, "text": "invariant features"}]
[{"st": 10, "ed": 12, "text": "gaussian processes"}, {"st": 23, "ed": 25, "text": "main contribution"}, {"st": 77, "ed": 81, "text": "mnist and cifar 10"}, {"st": 97, "ed": 99, "text": "marginal likelihood"}, {"st": 126, "ed": 128, "text": "marginal likelihood"}]
[{"st": 38, "ed": 41, "text": "batch and online"}, {"st": 56, "ed": 58, "text": "majority voting"}, {"st": 61, "ed": 63, "text": "majority voting"}, {"st": 79, "ed": 81, "text": "big data"}, {"st": 124, "ed": 126, "text": "accurate predictions"}, {"st": 153, "ed": 155, "text": "class labels"}, {"st": 188, "ed": 190, "text": "challenging task"}]
[{"st": 0, "ed": 2, "text": "residual network"}, {"st": 25, "ed": 27, "text": "weight initialization"}, {"st": 28, "ed": 30, "text": "neural network"}, {"st": 70, "ed": 72, "text": "batch normalization"}]
[{"st": 1, "ed": 3, "text": "feature sets"}, {"st": 10, "ed": 12, "text": "neural networks"}, {"st": 67, "ed": 69, "text": "learning representations"}]
[{"st": 0, "ed": 3, "text": "multivariate time series"}, {"st": 13, "ed": 15, "text": "traditional approaches"}, {"st": 24, "ed": 26, "text": "recent approaches"}, {"st": 29, "ed": 32, "text": "recurrent neural networks"}, {"st": 37, "ed": 39, "text": "time series"}, {"st": 42, "ed": 45, "text": "multivariate time series"}, {"st": 83, "ed": 85, "text": "predictive performance"}, {"st": 86, "ed": 88, "text": "complex models"}, {"st": 108, "ed": 110, "text": "time series"}, {"st": 113, "ed": 115, "text": "linear model"}, {"st": 130, "ed": 132, "text": "existing algorithms"}, {"st": 138, "ed": 140, "text": "extensive empirical"}, {"st": 143, "ed": 145, "text": "real world"}]
[{"st": 0, "ed": 2, "text": "semi supervised"}, {"st": 131, "ed": 134, "text": "k means clustering"}, {"st": 137, "ed": 139, "text": "sufficient conditions"}, {"st": 165, "ed": 167, "text": "sample complexity"}, {"st": 211, "ed": 213, "text": "synthetic data"}]
[{"st": 0, "ed": 2, "text": "deep learning"}, {"st": 12, "ed": 14, "text": "machine learning"}, {"st": 24, "ed": 26, "text": "deep learning"}, {"st": 28, "ed": 31, "text": "vulnerable to adversarial"}, {"st": 44, "ed": 46, "text": "adversarial perturbations"}, {"st": 69, "ed": 71, "text": "deep learning"}, {"st": 84, "ed": 86, "text": "ensemble methods"}, {"st": 117, "ed": 119, "text": "ensemble methods"}, {"st": 134, "ed": 136, "text": "cifar 10"}, {"st": 139, "ed": 141, "text": "ensemble methods"}, {"st": 147, "ed": 149, "text": "neural networks"}]
[{"st": 17, "ed": 19, "text": "small molecule"}, {"st": 28, "ed": 30, "text": "undirected graph"}, {"st": 43, "ed": 45, "text": "convolutional networks"}, {"st": 48, "ed": 50, "text": "neural networks"}, {"st": 62, "ed": 64, "text": "previous works"}, {"st": 67, "ed": 69, "text": "feature vectors"}, {"st": 79, "ed": 81, "text": "feature vector"}, {"st": 133, "ed": 136, "text": "classification and regression"}, {"st": 154, "ed": 156, "text": "class imbalance"}]
[{"st": 11, "ed": 13, "text": "real world"}, {"st": 40, "ed": 42, "text": "true labels"}, {"st": 75, "ed": 77, "text": "marginal distributions"}, {"st": 97, "ed": 99, "text": "true label"}, {"st": 101, "ed": 103, "text": "supervised learning"}, {"st": 140, "ed": 142, "text": "method works"}]
[{"st": 13, "ed": 15, "text": "mode collapse"}, {"st": 17, "ed": 21, "text": "generative adversarial network gan"}, {"st": 45, "ed": 48, "text": "kullback leibler kl"}, {"st": 55, "ed": 57, "text": "objective function"}, {"st": 62, "ed": 64, "text": "statistical properties"}, {"st": 83, "ed": 86, "text": "generative adversarial nets"}, {"st": 139, "ed": 141, "text": "theoretical analysis"}, {"st": 184, "ed": 187, "text": "conduct extensive experiments"}, {"st": 188, "ed": 192, "text": "synthetic and real world"}, {"st": 192, "ed": 194, "text": "large scale"}, {"st": 195, "ed": 198, "text": "mnist cifar 10"}, {"st": 224, "ed": 227, "text": "qualitative and quantitative"}]
[{"st": 8, "ed": 10, "text": "structure learning"}, {"st": 12, "ed": 15, "text": "probabilistic graphical models"}, {"st": 32, "ed": 35, "text": "partial differential equations"}, {"st": 53, "ed": 55, "text": "structure learning"}, {"st": 87, "ed": 89, "text": "finite sample"}, {"st": 99, "ed": 101, "text": "hyper parameters"}, {"st": 105, "ed": 107, "text": "sparsity inducing"}, {"st": 124, "ed": 126, "text": "structure learning"}, {"st": 170, "ed": 172, "text": "closed form"}, {"st": 186, "ed": 188, "text": "synthetic data"}, {"st": 190, "ed": 193, "text": "partial differential equations"}, {"st": 200, "ed": 202, "text": "real data"}, {"st": 236, "ed": 238, "text": "real data"}, {"st": 242, "ed": 244, "text": "underlying structure"}, {"st": 246, "ed": 248, "text": "atmospheric circulation"}]
[{"st": 84, "ed": 87, "text": "upper confidence bound"}, {"st": 93, "ed": 95, "text": "exploration exploitation"}, {"st": 142, "ed": 144, "text": "synthetic data"}, {"st": 145, "ed": 147, "text": "real world"}, {"st": 153, "ed": 155, "text": "significantly outperforms"}, {"st": 156, "ed": 158, "text": "bandit algorithms"}, {"st": 163, "ed": 165, "text": "thompson sampling"}]
[{"st": 1, "ed": 4, "text": "nonnegative matrix factorization"}, {"st": 9, "ed": 11, "text": "matrix decomposition"}, {"st": 17, "ed": 19, "text": "signal processing"}, {"st": 33, "ed": 35, "text": "multi objective"}, {"st": 35, "ed": 37, "text": "optimization problem"}]
[{"st": 0, "ed": 2, "text": "optimization algorithms"}, {"st": 4, "ed": 6, "text": "deep models"}, {"st": 10, "ed": 12, "text": "convergence rate"}, {"st": 16, "ed": 18, "text": "training process"}, {"st": 25, "ed": 27, "text": "generalization performance"}, {"st": 44, "ed": 47, "text": "stochastic gradient descent"}, {"st": 56, "ed": 58, "text": "generalization performance"}, {"st": 64, "ed": 67, "text": "deep neural networks"}, {"st": 114, "ed": 116, "text": "generalization performance"}, {"st": 124, "ed": 126, "text": "generalization performance"}, {"st": 127, "ed": 129, "text": "classification tasks"}, {"st": 145, "ed": 147, "text": "training process"}]
[{"st": 12, "ed": 15, "text": "semi supervised learning"}, {"st": 24, "ed": 26, "text": "learning framework"}, {"st": 43, "ed": 45, "text": "learning process"}, {"st": 78, "ed": 80, "text": "proposed framework"}, {"st": 91, "ed": 93, "text": "prediction accuracy"}, {"st": 109, "ed": 111, "text": "maximum principle"}]
[{"st": 20, "ed": 22, "text": "traffic flow"}, {"st": 22, "ed": 24, "text": "traditional methods"}, {"st": 33, "ed": 35, "text": "prediction tasks"}, {"st": 38, "ed": 41, "text": "spatial and temporal"}, {"st": 49, "ed": 51, "text": "deep learning"}, {"st": 52, "ed": 54, "text": "spatio temporal"}, {"st": 55, "ed": 57, "text": "convolutional networks"}, {"st": 61, "ed": 63, "text": "time series"}, {"st": 72, "ed": 75, "text": "convolutional and recurrent"}, {"st": 93, "ed": 95, "text": "faster training"}, {"st": 108, "ed": 110, "text": "spatio temporal"}, {"st": 113, "ed": 115, "text": "multi scale"}, {"st": 118, "ed": 120, "text": "consistently outperforms"}, {"st": 127, "ed": 129, "text": "real world"}]
[{"st": 0, "ed": 2, "text": "fully convolutional"}, {"st": 2, "ed": 4, "text": "neural networks"}, {"st": 17, "ed": 20, "text": "task of classifying"}, {"st": 20, "ed": 22, "text": "time series"}, {"st": 28, "ed": 30, "text": "fully convolutional"}, {"st": 33, "ed": 36, "text": "short term memory"}, {"st": 36, "ed": 39, "text": "recurrent neural network"}, {"st": 39, "ed": 41, "text": "lstm rnn"}, {"st": 44, "ed": 46, "text": "time series"}, {"st": 55, "ed": 57, "text": "fully convolutional"}, {"st": 75, "ed": 78, "text": "short term memory"}, {"st": 78, "ed": 81, "text": "fully convolutional network"}, {"st": 98, "ed": 100, "text": "attention mechanism"}, {"st": 102, "ed": 104, "text": "time series"}, {"st": 109, "ed": 112, "text": "short term memory"}, {"st": 112, "ed": 115, "text": "fully convolutional network"}, {"st": 121, "ed": 123, "text": "attention mechanism"}, {"st": 128, "ed": 130, "text": "decision process"}, {"st": 137, "ed": 139, "text": "fine tuning"}]
[{"st": 55, "ed": 57, "text": "classification task"}, {"st": 66, "ed": 69, "text": "machine learning models"}, {"st": 70, "ed": 72, "text": "logistic regression"}, {"st": 72, "ed": 75, "text": "support vector machine"}, {"st": 76, "ed": 78, "text": "neural networks"}, {"st": 122, "ed": 124, "text": "feature vectors"}, {"st": 132, "ed": 134, "text": "prediction models"}, {"st": 149, "ed": 151, "text": "evaluation shows"}, {"st": 153, "ed": 155, "text": "neural networks"}]
[{"st": 1, "ed": 4, "text": "latent dirichlet allocation"}, {"st": 12, "ed": 15, "text": "latent dirichlet allocation"}, {"st": 19, "ed": 21, "text": "multi label"}, {"st": 38, "ed": 40, "text": "multi label"}, {"st": 76, "ed": 79, "text": "hundreds of thousands"}, {"st": 92, "ed": 95, "text": "conduct extensive experiments"}, {"st": 107, "ed": 110, "text": "hundreds of thousands"}, {"st": 112, "ed": 114, "text": "proposed algorithm"}, {"st": 116, "ed": 118, "text": "previously proposed"}, {"st": 152, "ed": 154, "text": "competitive results"}, {"st": 158, "ed": 161, "text": "multi label classification"}]
[{"st": 4, "ed": 6, "text": "predictive models"}, {"st": 42, "ed": 44, "text": "boosting algorithms"}, {"st": 68, "ed": 70, "text": "random forest"}, {"st": 146, "ed": 148, "text": "real datasets"}]
[{"st": 0, "ed": 3, "text": "multi task learning"}, {"st": 46, "ed": 48, "text": "input data"}, {"st": 90, "ed": 92, "text": "convergence speed"}, {"st": 117, "ed": 119, "text": "sentiment analysis"}]
[{"st": 48, "ed": 50, "text": "clustering method"}, {"st": 61, "ed": 63, "text": "method called"}, {"st": 64, "ed": 66, "text": "least squares"}, {"st": 71, "ed": 73, "text": "multi output"}, {"st": 98, "ed": 100, "text": "data mining"}]
[{"st": 15, "ed": 17, "text": "contextual information"}, {"st": 20, "ed": 22, "text": "learning scheme"}, {"st": 26, "ed": 29, "text": "deep neural networks"}, {"st": 32, "ed": 35, "text": "ability to capture"}, {"st": 59, "ed": 62, "text": "variational lower bound"}, {"st": 72, "ed": 75, "text": "variational auto encoder"}, {"st": 76, "ed": 80, "text": "trained end to end"}, {"st": 89, "ed": 91, "text": "real world"}, {"st": 112, "ed": 114, "text": "multi dimensional"}, {"st": 121, "ed": 123, "text": "amazon rainforest"}, {"st": 138, "ed": 140, "text": "previous methods"}, {"st": 147, "ed": 150, "text": "taking advantage of"}, {"st": 151, "ed": 153, "text": "large datasets"}]
[{"st": 5, "ed": 8, "text": "end to end"}, {"st": 11, "ed": 13, "text": "hierarchical representations"}, {"st": 18, "ed": 20, "text": "fully convolutional"}, {"st": 20, "ed": 23, "text": "deep neural networks"}, {"st": 31, "ed": 34, "text": "deep neural network"}, {"st": 37, "ed": 39, "text": "feature extractor"}, {"st": 43, "ed": 46, "text": "end to end"}, {"st": 63, "ed": 65, "text": "multi channel"}, {"st": 77, "ed": 79, "text": "deep architecture"}, {"st": 106, "ed": 108, "text": "carefully designed"}, {"st": 112, "ed": 114, "text": "fully convolutional"}]
[{"st": 22, "ed": 24, "text": "prediction performance"}, {"st": 27, "ed": 29, "text": "cost effective"}, {"st": 42, "ed": 44, "text": "reinforcement learning"}, {"st": 51, "ed": 53, "text": "jointly learning"}, {"st": 81, "ed": 83, "text": "missing entries"}, {"st": 102, "ed": 104, "text": "joint learning"}, {"st": 111, "ed": 113, "text": "carefully designed"}, {"st": 124, "ed": 126, "text": "real datasets"}, {"st": 142, "ed": 144, "text": "prediction performance"}]
[{"st": 4, "ed": 6, "text": "neural network"}, {"st": 17, "ed": 19, "text": "computer vision"}, {"st": 20, "ed": 22, "text": "real world"}, {"st": 49, "ed": 51, "text": "pre defined"}, {"st": 67, "ed": 70, "text": "deep neural network"}, {"st": 92, "ed": 94, "text": "network architectures"}, {"st": 131, "ed": 133, "text": "policy network"}, {"st": 147, "ed": 149, "text": "policy network"}, {"st": 168, "ed": 170, "text": "score based"}, {"st": 182, "ed": 184, "text": "reward signal"}, {"st": 185, "ed": 187, "text": "policy gradients"}, {"st": 219, "ed": 221, "text": "similar performance"}, {"st": 231, "ed": 233, "text": "transfer learning"}, {"st": 240, "ed": 242, "text": "pre trained"}]
[]
[{"st": 3, "ed": 5, "text": "probabilistic framework"}, {"st": 30, "ed": 32, "text": "unified framework"}, {"st": 52, "ed": 54, "text": "neural networks"}, {"st": 55, "ed": 57, "text": "hidden units"}, {"st": 59, "ed": 61, "text": "random variables"}, {"st": 77, "ed": 79, "text": "extensive experiments"}, {"st": 81, "ed": 83, "text": "performance improvements"}, {"st": 87, "ed": 89, "text": "proposed framework"}, {"st": 93, "ed": 97, "text": "restricted boltzmann machine rbm"}, {"st": 103, "ed": 105, "text": "graphical model"}]
[{"st": 5, "ed": 8, "text": "dirichlet process mixture"}, {"st": 25, "ed": 27, "text": "bayesian nonparametric"}, {"st": 118, "ed": 122, "text": "real world data sets"}, {"st": 125, "ed": 127, "text": "proposed method"}, {"st": 128, "ed": 130, "text": "achieve high"}, {"st": 136, "ed": 138, "text": "without compromising"}]
[{"st": 1, "ed": 3, "text": "real world"}, {"st": 8, "ed": 10, "text": "optimization problem"}, {"st": 119, "ed": 121, "text": "gaussian processes"}]
[{"st": 2, "ed": 5, "text": "generative adversarial network"}, {"st": 10, "ed": 12, "text": "semi supervised"}, {"st": 12, "ed": 14, "text": "cross domain"}, {"st": 14, "ed": 16, "text": "joint distribution"}, {"st": 19, "ed": 21, "text": "training data"}, {"st": 45, "ed": 47, "text": "neural networks"}, {"st": 61, "ed": 63, "text": "conditional distributions"}, {"st": 81, "ed": 83, "text": "real data"}, {"st": 101, "ed": 104, "text": "under mild assumptions"}, {"st": 107, "ed": 109, "text": "joint distributions"}, {"st": 139, "ed": 141, "text": "semi supervised"}, {"st": 141, "ed": 143, "text": "image classification"}, {"st": 143, "ed": 146, "text": "image to image"}, {"st": 150, "ed": 152, "text": "image generation"}]
[{"st": 1, "ed": 3, "text": "recent years"}, {"st": 3, "ed": 5, "text": "significant progress"}, {"st": 10, "ed": 12, "text": "challenging problems"}, {"st": 16, "ed": 20, "text": "deep reinforcement learning rl"}, {"st": 47, "ed": 49, "text": "deep rl"}, {"st": 58, "ed": 60, "text": "standard benchmark"}, {"st": 137, "ed": 139, "text": "deep rl"}]
[{"st": 2, "ed": 4, "text": "early detection"}, {"st": 19, "ed": 21, "text": "based approaches"}, {"st": 54, "ed": 56, "text": "machine learning"}, {"st": 69, "ed": 71, "text": "explicitly modeling"}, {"st": 72, "ed": 74, "text": "temporal patterns"}, {"st": 139, "ed": 141, "text": "text mining"}, {"st": 162, "ed": 164, "text": "large scale"}, {"st": 164, "ed": 167, "text": "multivariate time series"}, {"st": 180, "ed": 182, "text": "benchmark datasets"}, {"st": 186, "ed": 188, "text": "proposed approach"}, {"st": 196, "ed": 198, "text": "prediction accuracy"}]
[{"st": 3, "ed": 5, "text": "deep models"}, {"st": 28, "ed": 30, "text": "piecewise linear"}, {"st": 68, "ed": 70, "text": "real world"}, {"st": 86, "ed": 89, "text": "classification and regression"}]
[{"st": 4, "ed": 6, "text": "technique called"}, {"st": 7, "ed": 10, "text": "principal component analysis"}, {"st": 16, "ed": 18, "text": "low dimensional"}, {"st": 46, "ed": 48, "text": "multiple datasets"}, {"st": 82, "ed": 84, "text": "wide variety"}, {"st": 111, "ed": 113, "text": "feature selection"}]
[{"st": 8, "ed": 11, "text": "armed bandit problem"}, {"st": 112, "ed": 114, "text": "previous works"}, {"st": 177, "ed": 179, "text": "worst case"}]
[{"st": 1, "ed": 6, "text": "deep convolutional neural network cnn"}, {"st": 12, "ed": 14, "text": "computer vision"}, {"st": 64, "ed": 66, "text": "method named"}, {"st": 75, "ed": 77, "text": "convolutional layers"}, {"st": 82, "ed": 84, "text": "unlike existing"}, {"st": 144, "ed": 146, "text": "top 5"}, {"st": 148, "ed": 150, "text": "vgg 16"}, {"st": 154, "ed": 156, "text": "top 5"}, {"st": 164, "ed": 166, "text": "directly applied"}, {"st": 181, "ed": 183, "text": "resnet 50"}, {"st": 188, "ed": 190, "text": "top 5"}, {"st": 202, "ed": 204, "text": "transfer learning"}]
[{"st": 7, "ed": 9, "text": "log likelihood"}, {"st": 14, "ed": 16, "text": "convex functions"}, {"st": 25, "ed": 27, "text": "convex functions"}, {"st": 32, "ed": 34, "text": "negative log"}, {"st": 38, "ed": 40, "text": "contrastive divergence"}, {"st": 43, "ed": 45, "text": "special case"}, {"st": 76, "ed": 78, "text": "computational budget"}, {"st": 79, "ed": 81, "text": "proposed algorithm"}, {"st": 86, "ed": 88, "text": "log likelihood"}, {"st": 94, "ed": 96, "text": "contrastive divergence"}, {"st": 113, "ed": 116, "text": "efficient and effective"}]
[{"st": 7, "ed": 9, "text": "latent variable"}, {"st": 15, "ed": 17, "text": "latent variable"}, {"st": 26, "ed": 28, "text": "observed data"}, {"st": 54, "ed": 56, "text": "latent variable"}, {"st": 61, "ed": 63, "text": "computationally efficient"}, {"st": 77, "ed": 80, "text": "online learning algorithm"}, {"st": 81, "ed": 83, "text": "latent variable"}, {"st": 93, "ed": 95, "text": "global optimum"}, {"st": 99, "ed": 101, "text": "o sqrt"}, {"st": 115, "ed": 118, "text": "bag of words"}, {"st": 134, "ed": 136, "text": "hyper parameters"}, {"st": 138, "ed": 142, "text": "synthetic and real world"}]
[{"st": 2, "ed": 4, "text": "variational inference"}, {"st": 17, "ed": 20, "text": "kullback leibler kl"}, {"st": 40, "ed": 42, "text": "marginal likelihood"}, {"st": 52, "ed": 54, "text": "bias variance"}, {"st": 64, "ed": 66, "text": "marginal likelihood"}, {"st": 81, "ed": 83, "text": "statistical physics"}, {"st": 110, "ed": 112, "text": "k 1"}, {"st": 117, "ed": 119, "text": "marginal likelihood"}, {"st": 140, "ed": 142, "text": "gaussian processes"}, {"st": 143, "ed": 145, "text": "variational autoencoders"}]
[{"st": 1, "ed": 4, "text": "empirical risk minimization"}, {"st": 9, "ed": 13, "text": "reproducing kernel hilbert spaces"}, {"st": 16, "ed": 18, "text": "important role"}, {"st": 53, "ed": 55, "text": "kernel learning"}, {"st": 57, "ed": 59, "text": "gaussian kernels"}, {"st": 61, "ed": 63, "text": "recently proposed"}, {"st": 76, "ed": 78, "text": "grid search"}, {"st": 105, "ed": 107, "text": "kernel based"}, {"st": 113, "ed": 115, "text": "loss function"}, {"st": 130, "ed": 132, "text": "regularization parameter"}, {"st": 165, "ed": 167, "text": "robust statistics"}, {"st": 173, "ed": 175, "text": "small perturbations"}]
[{"st": 27, "ed": 29, "text": "e commerce"}, {"st": 60, "ed": 63, "text": "orders of magnitude"}, {"st": 74, "ed": 76, "text": "real world"}, {"st": 79, "ed": 81, "text": "method outperforms"}, {"st": 81, "ed": 83, "text": "competing approaches"}]
[{"st": 0, "ed": 2, "text": "multi task"}, {"st": 2, "ed": 4, "text": "multi output"}, {"st": 32, "ed": 35, "text": "gaussian processes gps"}, {"st": 69, "ed": 71, "text": "input features"}, {"st": 84, "ed": 86, "text": "parameter learning"}, {"st": 101, "ed": 103, "text": "parameter learning"}, {"st": 108, "ed": 110, "text": "training data"}, {"st": 144, "ed": 146, "text": "substantial improvements"}]
[{"st": 41, "ed": 43, "text": "moving average"}, {"st": 49, "ed": 51, "text": "lstm networks"}, {"st": 66, "ed": 68, "text": "result shows"}, {"st": 83, "ed": 85, "text": "lstm networks"}, {"st": 87, "ed": 89, "text": "lstm networks"}]
[{"st": 0, "ed": 4, "text": "recurrent neural networks rnns"}, {"st": 54, "ed": 56, "text": "predictive state"}, {"st": 59, "ed": 61, "text": "latent state"}, {"st": 91, "ed": 93, "text": "sufficient statistics"}, {"st": 114, "ed": 117, "text": "recurrent neural networks"}, {"st": 118, "ed": 120, "text": "predictive state"}, {"st": 137, "ed": 139, "text": "predictive state"}, {"st": 170, "ed": 172, "text": "imitation learning"}]
[{"st": 29, "ed": 31, "text": "randomly generated"}, {"st": 43, "ed": 45, "text": "sample size"}]
[{"st": 3, "ed": 7, "text": "generative adversarial networks gans"}, {"st": 29, "ed": 31, "text": "wasserstein gans"}, {"st": 77, "ed": 79, "text": "neural network"}, {"st": 97, "ed": 99, "text": "regularization term"}, {"st": 127, "ed": 129, "text": "regularization term"}]
[{"st": 11, "ed": 14, "text": "deep neural networks"}, {"st": 17, "ed": 21, "text": "convolutional neural networks cnn"}, {"st": 33, "ed": 35, "text": "ensemble based"}, {"st": 66, "ed": 69, "text": "supervised and unsupervised"}, {"st": 87, "ed": 89, "text": "training speed"}]
[{"st": 0, "ed": 3, "text": "deep neural networks"}, {"st": 8, "ed": 10, "text": "machine learning"}, {"st": 13, "ed": 15, "text": "image classification"}, {"st": 44, "ed": 46, "text": "neural networks"}, {"st": 74, "ed": 77, "text": "deep neural network"}, {"st": 109, "ed": 111, "text": "local search"}, {"st": 112, "ed": 114, "text": "linear programming"}, {"st": 138, "ed": 140, "text": "recently proposed"}, {"st": 153, "ed": 155, "text": "local minima"}, {"st": 160, "ed": 162, "text": "global optimum"}, {"st": 166, "ed": 168, "text": "mixed integer"}, {"st": 180, "ed": 182, "text": "recently proposed"}, {"st": 193, "ed": 195, "text": "proposed approach"}]
[{"st": 0, "ed": 2, "text": "traditional medicine"}, {"st": 70, "ed": 72, "text": "detection method"}, {"st": 77, "ed": 79, "text": "convex optimization"}, {"st": 99, "ed": 101, "text": "african american"}, {"st": 129, "ed": 131, "text": "input variables"}, {"st": 136, "ed": 138, "text": "input variables"}]
[{"st": 0, "ed": 2, "text": "class imbalance"}, {"st": 33, "ed": 36, "text": "the minority class"}, {"st": 38, "ed": 40, "text": "majority class"}, {"st": 41, "ed": 44, "text": "the minority class"}, {"st": 51, "ed": 53, "text": "classifier performance"}, {"st": 58, "ed": 60, "text": "class imbalance"}, {"st": 75, "ed": 77, "text": "class imbalance"}, {"st": 78, "ed": 80, "text": "cost sensitive"}, {"st": 93, "ed": 96, "text": "deep belief networks"}, {"st": 108, "ed": 110, "text": "proposed method"}, {"st": 135, "ed": 137, "text": "financial transaction"}]
[{"st": 8, "ed": 10, "text": "l1 norm"}, {"st": 15, "ed": 17, "text": "kernel pca"}, {"st": 19, "ed": 21, "text": "widely studied"}, {"st": 27, "ed": 29, "text": "l1 norm"}, {"st": 66, "ed": 68, "text": "convergence analysis"}, {"st": 85, "ed": 87, "text": "optimal solution"}, {"st": 89, "ed": 91, "text": "l1 norm"}, {"st": 91, "ed": 93, "text": "kernel pca"}, {"st": 102, "ed": 104, "text": "principal components"}, {"st": 133, "ed": 135, "text": "l1 norm"}, {"st": 135, "ed": 137, "text": "kernel pca"}]
[{"st": 3, "ed": 5, "text": "empirical comparison"}, {"st": 17, "ed": 19, "text": "time series"}, {"st": 37, "ed": 40, "text": "takes into account"}]
[{"st": 1, "ed": 3, "text": "recent theoretical"}, {"st": 3, "ed": 5, "text": "analysis shows"}, {"st": 8, "ed": 13, "text": "non negative matrix factorization nmf"}, {"st": 14, "ed": 16, "text": "spectral clustering"}, {"st": 16, "ed": 18, "text": "based approach"}, {"st": 42, "ed": 44, "text": "kernel based"}, {"st": 46, "ed": 48, "text": "update rules"}, {"st": 51, "ed": 53, "text": "subspace clustering"}, {"st": 62, "ed": 64, "text": "subspace clustering"}, {"st": 66, "ed": 68, "text": "kernel based"}, {"st": 70, "ed": 72, "text": "subspace clustering"}, {"st": 110, "ed": 112, "text": "geometric structure"}, {"st": 122, "ed": 124, "text": "based approach"}, {"st": 125, "ed": 127, "text": "subspace clustering"}, {"st": 127, "ed": 130, "text": "takes into account"}, {"st": 141, "ed": 143, "text": "local geometry"}, {"st": 147, "ed": 149, "text": "clustering performance"}, {"st": 154, "ed": 156, "text": "recently proposed"}]
[{"st": 2, "ed": 5, "text": "stochastic gradient descent"}, {"st": 7, "ed": 9, "text": "large scale"}, {"st": 9, "ed": 11, "text": "machine learning"}, {"st": 13, "ed": 15, "text": "common practice"}, {"st": 22, "ed": 24, "text": "training data"}, {"st": 65, "ed": 67, "text": "training data"}, {"st": 76, "ed": 78, "text": "convergence properties"}, {"st": 105, "ed": 107, "text": "machine learning"}, {"st": 141, "ed": 143, "text": "convergence guarantee"}, {"st": 160, "ed": 162, "text": "deep learning"}, {"st": 177, "ed": 179, "text": "convergence analysis"}, {"st": 185, "ed": 187, "text": "convergence rate"}, {"st": 242, "ed": 244, "text": "theoretical results"}, {"st": 248, "ed": 250, "text": "large scale"}, {"st": 250, "ed": 252, "text": "machine learning"}, {"st": 264, "ed": 266, "text": "faster convergence"}, {"st": 270, "ed": 272, "text": "theoretical findings"}, {"st": 275, "ed": 277, "text": "extensive experiments"}, {"st": 278, "ed": 280, "text": "logistic regression"}]
[{"st": 4, "ed": 6, "text": "spoken language"}]
[{"st": 3, "ed": 6, "text": "deep learning based"}, {"st": 54, "ed": 57, "text": "stochastic gradient descent"}, {"st": 68, "ed": 70, "text": "problems including"}]
[{"st": 8, "ed": 11, "text": "deep neural network"}, {"st": 37, "ed": 39, "text": "deep networks"}, {"st": 46, "ed": 49, "text": "loss in accuracy"}, {"st": 64, "ed": 66, "text": "baseline models"}, {"st": 90, "ed": 94, "text": "number of hidden units"}, {"st": 126, "ed": 128, "text": "efficient inference"}, {"st": 129, "ed": 131, "text": "resource constrained"}, {"st": 192, "ed": 194, "text": "neural network"}, {"st": 195, "ed": 197, "text": "deep cnns"}, {"st": 201, "ed": 203, "text": "lstm models"}, {"st": 206, "ed": 208, "text": "sparse models"}, {"st": 209, "ed": 211, "text": "consistently outperform"}]
[{"st": 0, "ed": 2, "text": "neural networks"}, {"st": 8, "ed": 10, "text": "machine learning"}, {"st": 19, "ed": 21, "text": "neural networks"}, {"st": 47, "ed": 49, "text": "neural network"}, {"st": 57, "ed": 59, "text": "theoretical properties"}, {"st": 75, "ed": 77, "text": "neural networks"}, {"st": 87, "ed": 89, "text": "neural networks"}, {"st": 91, "ed": 93, "text": "weight vectors"}, {"st": 99, "ed": 101, "text": "finite set"}, {"st": 107, "ed": 109, "text": "local optima"}, {"st": 129, "ed": 132, "text": "theoretical and empirical"}, {"st": 137, "ed": 139, "text": "neural network"}]
[{"st": 14, "ed": 18, "text": "recurrent neural networks rnns"}, {"st": 23, "ed": 25, "text": "explicitly model"}, {"st": 26, "ed": 28, "text": "temporal dependencies"}, {"st": 45, "ed": 47, "text": "improve performance"}, {"st": 59, "ed": 61, "text": "hidden units"}, {"st": 67, "ed": 69, "text": "complex models"}, {"st": 87, "ed": 89, "text": "representation learning"}, {"st": 97, "ed": 101, "text": "restricted boltzmann machines rbms"}, {"st": 121, "ed": 123, "text": "conditional distribution"}, {"st": 137, "ed": 140, "text": "optical character recognition"}, {"st": 157, "ed": 160, "text": "optical character recognition"}, {"st": 160, "ed": 164, "text": "part of speech tagging"}, {"st": 174, "ed": 177, "text": "recurrent neural networks"}]
[{"st": 3, "ed": 5, "text": "time series"}, {"st": 40, "ed": 42, "text": "time series"}, {"st": 62, "ed": 65, "text": "free to play"}, {"st": 72, "ed": 74, "text": "time series"}, {"st": 76, "ed": 78, "text": "increasingly important"}, {"st": 98, "ed": 100, "text": "time series"}, {"st": 118, "ed": 120, "text": "similarity measures"}, {"st": 146, "ed": 149, "text": "free to play"}, {"st": 156, "ed": 158, "text": "temporal patterns"}]
[{"st": 21, "ed": 23, "text": "control problem"}, {"st": 30, "ed": 33, "text": "value function approximation"}, {"st": 48, "ed": 50, "text": "computationally efficient"}, {"st": 55, "ed": 57, "text": "step ahead"}, {"st": 75, "ed": 78, "text": "approximate dynamic programming"}]
[{"st": 51, "ed": 53, "text": "gaussian process"}, {"st": 63, "ed": 65, "text": "variational approximation"}, {"st": 84, "ed": 86, "text": "time series"}]
[{"st": 36, "ed": 38, "text": "prior information"}, {"st": 42, "ed": 44, "text": "soft margin"}, {"st": 58, "ed": 60, "text": "prior information"}, {"st": 83, "ed": 85, "text": "optimization problem"}, {"st": 88, "ed": 90, "text": "quadratic programming"}, {"st": 98, "ed": 100, "text": "prior information"}, {"st": 138, "ed": 140, "text": "blast furnace"}]
[{"st": 1, "ed": 3, "text": "real world"}, {"st": 6, "ed": 8, "text": "dynamical systems"}, {"st": 19, "ed": 21, "text": "network topology"}, {"st": 49, "ed": 51, "text": "real world"}, {"st": 53, "ed": 55, "text": "existing techniques"}, {"st": 89, "ed": 92, "text": "local and global"}, {"st": 108, "ed": 111, "text": "local and global"}, {"st": 144, "ed": 146, "text": "langevin dynamics"}, {"st": 169, "ed": 173, "text": "synthetic and real world"}]
[{"st": 24, "ed": 27, "text": "probabilistic graphical models"}, {"st": 47, "ed": 49, "text": "random variables"}, {"st": 62, "ed": 64, "text": "difficult task"}, {"st": 71, "ed": 73, "text": "deep architecture"}, {"st": 84, "ed": 88, "text": "sum product networks spns"}, {"st": 123, "ed": 125, "text": "random variables"}, {"st": 138, "ed": 140, "text": "efficient learning"}, {"st": 143, "ed": 145, "text": "empirical evidence"}, {"st": 154, "ed": 156, "text": "capture complex"}]
[{"st": 13, "ed": 15, "text": "unlabelled data"}, {"st": 19, "ed": 21, "text": "semi supervised"}, {"st": 35, "ed": 37, "text": "semi supervised"}, {"st": 60, "ed": 62, "text": "semi supervised"}, {"st": 62, "ed": 64, "text": "parameter learning"}, {"st": 65, "ed": 69, "text": "sum product networks spns"}, {"st": 73, "ed": 75, "text": "probabilistic models"}, {"st": 94, "ed": 97, "text": "generative and discriminative"}, {"st": 97, "ed": 100, "text": "semi supervised learning"}, {"st": 104, "ed": 106, "text": "unlabelled data"}, {"st": 117, "ed": 119, "text": "computationally efficient"}, {"st": 139, "ed": 142, "text": "semi supervised learning"}, {"st": 158, "ed": 161, "text": "generative and discriminative"}]
[{"st": 3, "ed": 6, "text": "simple yet effective"}, {"st": 9, "ed": 11, "text": "neural networks"}, {"st": 30, "ed": 32, "text": "logistic regression"}, {"st": 45, "ed": 47, "text": "optimization problem"}, {"st": 56, "ed": 58, "text": "theoretical analysis"}, {"st": 61, "ed": 63, "text": "matrix factorization"}, {"st": 65, "ed": 67, "text": "random variables"}, {"st": 93, "ed": 95, "text": "matrix factorization"}, {"st": 96, "ed": 98, "text": "theoretical understanding"}, {"st": 122, "ed": 124, "text": "matrix factorization"}, {"st": 147, "ed": 149, "text": "closely related"}, {"st": 155, "ed": 157, "text": "nuclear norm"}, {"st": 216, "ed": 218, "text": "synthetic experiments"}]
[{"st": 6, "ed": 9, "text": "online learning algorithm"}, {"st": 18, "ed": 21, "text": "stochastic gradient descent"}, {"st": 23, "ed": 26, "text": "a reproducing kernel"}, {"st": 33, "ed": 35, "text": "step size"}, {"st": 47, "ed": 49, "text": "generalization ability"}, {"st": 87, "ed": 89, "text": "convergence rate"}, {"st": 118, "ed": 120, "text": "step sizes"}, {"st": 128, "ed": 130, "text": "theoretical analysis"}, {"st": 137, "ed": 139, "text": "fine structure"}]
[{"st": 41, "ed": 43, "text": "linear model"}, {"st": 67, "ed": 69, "text": "least squares"}, {"st": 97, "ed": 99, "text": "random forest"}, {"st": 103, "ed": 107, "text": "synthetic and real world"}]
[{"st": 9, "ed": 11, "text": "learned models"}, {"st": 37, "ed": 39, "text": "training data"}, {"st": 46, "ed": 48, "text": "lifelong learning"}, {"st": 72, "ed": 74, "text": "lifelong learning"}, {"st": 108, "ed": 110, "text": "theoretical justification"}, {"st": 140, "ed": 142, "text": "accurately predict"}, {"st": 149, "ed": 152, "text": "zero shot learning"}, {"st": 161, "ed": 163, "text": "training data"}]
[{"st": 7, "ed": 10, "text": "mean square error"}, {"st": 24, "ed": 26, "text": "signal processing"}, {"st": 54, "ed": 56, "text": "computational complexity"}, {"st": 76, "ed": 78, "text": "large scale"}, {"st": 95, "ed": 97, "text": "computational burden"}, {"st": 138, "ed": 140, "text": "excellent performance"}]
[{"st": 12, "ed": 14, "text": "limited data"}, {"st": 38, "ed": 40, "text": "detection method"}, {"st": 42, "ed": 45, "text": "efficient and accurate"}, {"st": 45, "ed": 47, "text": "machine learning"}, {"st": 62, "ed": 65, "text": "kernel ridge regression"}, {"st": 68, "ed": 71, "text": "kernel ridge regression"}, {"st": 87, "ed": 89, "text": "reduction technique"}, {"st": 94, "ed": 97, "text": "kernel ridge regression"}, {"st": 101, "ed": 103, "text": "computational efficiency"}, {"st": 113, "ed": 116, "text": "numerical linear algebra"}, {"st": 118, "ed": 120, "text": "nystr om"}, {"st": 128, "ed": 130, "text": "feature space"}, {"st": 130, "ed": 132, "text": "without compromising"}, {"st": 142, "ed": 144, "text": "computational cost"}, {"st": 166, "ed": 168, "text": "detection method"}, {"st": 181, "ed": 183, "text": "numerical examples"}, {"st": 187, "ed": 189, "text": "detection method"}, {"st": 189, "ed": 191, "text": "significantly improves"}, {"st": 192, "ed": 194, "text": "computational efficiency"}, {"st": 203, "ed": 205, "text": "method yields"}, {"st": 221, "ed": 223, "text": "multi core"}]
[{"st": 2, "ed": 5, "text": "intelligent transportation systems"}, {"st": 11, "ed": 13, "text": "accurate prediction"}, {"st": 21, "ed": 23, "text": "customer experience"}, {"st": 48, "ed": 51, "text": "deep neural networks"}, {"st": 66, "ed": 68, "text": "spatio temporal"}, {"st": 68, "ed": 70, "text": "neural network"}, {"st": 115, "ed": 117, "text": "feature engineering"}, {"st": 150, "ed": 152, "text": "proposed approach"}, {"st": 166, "ed": 168, "text": "significantly reduces"}, {"st": 190, "ed": 192, "text": "proposed approach"}]
[{"st": 7, "ed": 9, "text": "deep learning"}, {"st": 20, "ed": 22, "text": "time series"}, {"st": 76, "ed": 78, "text": "web traffic"}, {"st": 78, "ed": 80, "text": "time series"}]
[{"st": 5, "ed": 7, "text": "machine learning"}, {"st": 8, "ed": 10, "text": "hyperparameter optimization"}, {"st": 83, "ed": 85, "text": "meta learning"}, {"st": 95, "ed": 97, "text": "meta data"}, {"st": 107, "ed": 110, "text": "support vector machines"}, {"st": 110, "ed": 112, "text": "random forests"}, {"st": 145, "ed": 147, "text": "experiments confirm"}]
[{"st": 8, "ed": 11, "text": "canonical correlation analysis"}, {"st": 66, "ed": 69, "text": "canonical correlation analysis"}, {"st": 81, "ed": 83, "text": "l 0"}, {"st": 85, "ed": 87, "text": "l 0"}, {"st": 95, "ed": 97, "text": "l 0"}, {"st": 99, "ed": 101, "text": "synthetic data"}, {"st": 102, "ed": 105, "text": "real world data"}, {"st": 132, "ed": 134, "text": "group lasso"}]
[{"st": 12, "ed": 14, "text": "large scale"}, {"st": 14, "ed": 16, "text": "kernel methods"}, {"st": 29, "ed": 31, "text": "regularization scheme"}, {"st": 50, "ed": 52, "text": "theoretical analysis"}, {"st": 55, "ed": 57, "text": "least square"}, {"st": 57, "ed": 59, "text": "regularization scheme"}, {"st": 77, "ed": 80, "text": "multi task learning"}, {"st": 86, "ed": 88, "text": "convergence rates"}, {"st": 110, "ed": 112, "text": "linear function"}, {"st": 142, "ed": 144, "text": "multi class"}, {"st": 144, "ed": 146, "text": "image classification"}, {"st": 153, "ed": 155, "text": "intrusion detection"}]
[{"st": 8, "ed": 10, "text": "structural information"}, {"st": 13, "ed": 15, "text": "multi class"}, {"st": 23, "ed": 25, "text": "convolutional network"}, {"st": 27, "ed": 29, "text": "neural network"}, {"st": 40, "ed": 42, "text": "proposed approach"}, {"st": 44, "ed": 46, "text": "approximate inference"}, {"st": 51, "ed": 54, "text": "conditional random field"}, {"st": 59, "ed": 61, "text": "proposed approach"}, {"st": 62, "ed": 64, "text": "document classification"}, {"st": 65, "ed": 67, "text": "object recognition"}, {"st": 86, "ed": 88, "text": "experiment results"}, {"st": 95, "ed": 97, "text": "baseline method"}]
[{"st": 3, "ed": 5, "text": "imitation learning"}, {"st": 13, "ed": 15, "text": "multi modal"}, {"st": 35, "ed": 37, "text": "supervised learning"}, {"st": 49, "ed": 51, "text": "multi modal"}, {"st": 51, "ed": 53, "text": "imitation learning"}, {"st": 68, "ed": 70, "text": "imitation learning"}, {"st": 88, "ed": 90, "text": "approach outperforms"}, {"st": 95, "ed": 97, "text": "mutual information"}, {"st": 119, "ed": 121, "text": "autonomous driving"}]
[{"st": 2, "ed": 4, "text": "matrix factorization"}, {"st": 21, "ed": 23, "text": "deep learning"}, {"st": 36, "ed": 38, "text": "empirical performance"}, {"st": 39, "ed": 41, "text": "theoretical properties"}, {"st": 60, "ed": 62, "text": "theoretical analysis"}, {"st": 68, "ed": 70, "text": "random variables"}, {"st": 127, "ed": 129, "text": "global minimum"}, {"st": 136, "ed": 138, "text": "nuclear norm"}, {"st": 151, "ed": 153, "text": "low rank"}]
[{"st": 73, "ed": 75, "text": "network structure"}, {"st": 124, "ed": 126, "text": "proposed algorithm"}]
[{"st": 76, "ed": 78, "text": "linear regression"}, {"st": 82, "ed": 84, "text": "neural network"}, {"st": 85, "ed": 88, "text": "convolutional neural network"}]
[{"st": 0, "ed": 2, "text": "recent advances"}, {"st": 3, "ed": 5, "text": "weakly supervised"}, {"st": 22, "ed": 24, "text": "classification methods"}, {"st": 24, "ed": 26, "text": "typically require"}, {"st": 32, "ed": 34, "text": "prior probability"}, {"st": 52, "ed": 55, "text": "principal component analysis"}, {"st": 74, "ed": 76, "text": "representation learning"}, {"st": 115, "ed": 118, "text": "deep neural networks"}]
[{"st": 5, "ed": 7, "text": "well calibrated"}, {"st": 65, "ed": 67, "text": "well calibrated"}, {"st": 99, "ed": 101, "text": "hyper parameter"}, {"st": 108, "ed": 110, "text": "hyper parameter"}, {"st": 148, "ed": 150, "text": "logistic regression"}, {"st": 170, "ed": 172, "text": "deep learning"}, {"st": 207, "ed": 209, "text": "deep learning"}, {"st": 214, "ed": 216, "text": "machine learning"}]
[{"st": 12, "ed": 14, "text": "convex geometry"}, {"st": 24, "ed": 26, "text": "convex polytope"}, {"st": 39, "ed": 41, "text": "generative models"}, {"st": 125, "ed": 127, "text": "method outperforms"}, {"st": 136, "ed": 138, "text": "low dimensional"}]
[{"st": 0, "ed": 2, "text": "recommender systems"}, {"st": 10, "ed": 12, "text": "learning algorithms"}, {"st": 15, "ed": 17, "text": "matrix factorization"}, {"st": 28, "ed": 30, "text": "inner product"}, {"st": 32, "ed": 34, "text": "latent features"}, {"st": 40, "ed": 42, "text": "widely studied"}, {"st": 58, "ed": 60, "text": "latent features"}, {"st": 69, "ed": 71, "text": "complex nonlinear"}, {"st": 76, "ed": 78, "text": "matrix factorization"}, {"st": 98, "ed": 102, "text": "nonnegative matrix factorization nmf"}, {"st": 124, "ed": 126, "text": "latent factors"}, {"st": 136, "ed": 138, "text": "multi layer"}, {"st": 155, "ed": 157, "text": "latent factors"}, {"st": 172, "ed": 174, "text": "low dimensional"}, {"st": 180, "ed": 182, "text": "recommender systems"}]
[{"st": 6, "ed": 8, "text": "data science"}, {"st": 23, "ed": 27, "text": "unsupervised and semi supervised"}, {"st": 74, "ed": 76, "text": "mathcal o"}, {"st": 97, "ed": 99, "text": "mathcal o"}, {"st": 115, "ed": 118, "text": "approximate nearest neighbor"}]
[{"st": 25, "ed": 27, "text": "latent space"}, {"st": 43, "ed": 46, "text": "variational auto encoder"}, {"st": 49, "ed": 51, "text": "unsupervised learning"}, {"st": 52, "ed": 54, "text": "sequential data"}, {"st": 57, "ed": 59, "text": "latent representations"}, {"st": 70, "ed": 72, "text": "latent state"}, {"st": 87, "ed": 89, "text": "missing data"}, {"st": 106, "ed": 110, "text": "trained end to end"}, {"st": 121, "ed": 123, "text": "competing methods"}, {"st": 126, "ed": 128, "text": "missing data"}]
[{"st": 0, "ed": 2, "text": "recent advances"}, {"st": 3, "ed": 5, "text": "policy gradient"}, {"st": 7, "ed": 9, "text": "deep learning"}, {"st": 15, "ed": 17, "text": "reinforcement learning"}, {"st": 53, "ed": 55, "text": "policy gradient"}, {"st": 56, "ed": 58, "text": "significantly improve"}, {"st": 68, "ed": 70, "text": "trust region"}, {"st": 71, "ed": 73, "text": "conjugate gradient"}, {"st": 83, "ed": 85, "text": "method achieves"}, {"st": 97, "ed": 99, "text": "policy gradient"}, {"st": 102, "ed": 104, "text": "continuous control"}, {"st": 106, "ed": 108, "text": "trust region"}, {"st": 108, "ed": 110, "text": "policy optimization"}]
[{"st": 0, "ed": 3, "text": "deep neural networks"}, {"st": 4, "ed": 8, "text": "vulnerable to adversarial examples"}, {"st": 21, "ed": 23, "text": "adversarial attacks"}, {"st": 33, "ed": 35, "text": "deep learning"}, {"st": 44, "ed": 46, "text": "adversarial attacks"}, {"st": 70, "ed": 72, "text": "iterative algorithms"}, {"st": 115, "ed": 117, "text": "black box"}, {"st": 121, "ed": 123, "text": "iterative algorithms"}, {"st": 133, "ed": 135, "text": "trained models"}, {"st": 145, "ed": 147, "text": "black box"}, {"st": 165, "ed": 167, "text": "deep models"}, {"st": 183, "ed": 185, "text": "adversarial attack"}, {"st": 187, "ed": 189, "text": "adversarial attack"}]
[{"st": 2, "ed": 4, "text": "parameter estimation"}, {"st": 6, "ed": 8, "text": "factor analysis"}, {"st": 11, "ed": 13, "text": "generative model"}, {"st": 70, "ed": 72, "text": "iterative optimization"}, {"st": 74, "ed": 77, "text": "stochastic variational inference"}, {"st": 117, "ed": 119, "text": "excellent results"}]
[{"st": 78, "ed": 80, "text": "input space"}, {"st": 103, "ed": 105, "text": "non stationary"}, {"st": 146, "ed": 148, "text": "gaussian process"}, {"st": 174, "ed": 176, "text": "gaussian process"}, {"st": 220, "ed": 222, "text": "time series"}, {"st": 251, "ed": 253, "text": "surrogate model"}]
[{"st": 0, "ed": 2, "text": "hyperparameter optimization"}, {"st": 21, "ed": 23, "text": "hyperparameter optimization"}, {"st": 29, "ed": 31, "text": "validation error"}, {"st": 52, "ed": 54, "text": "domain expertise"}, {"st": 94, "ed": 96, "text": "convolutional layers"}, {"st": 98, "ed": 100, "text": "bi directional"}, {"st": 147, "ed": 149, "text": "image datasets"}, {"st": 161, "ed": 164, "text": "deep residual networks"}]
[{"st": 8, "ed": 10, "text": "convex relaxations"}, {"st": 12, "ed": 14, "text": "penalty functions"}, {"st": 64, "ed": 66, "text": "sufficient conditions"}]
[{"st": 10, "ed": 12, "text": "optimal transport"}, {"st": 26, "ed": 28, "text": "convex optimization"}, {"st": 32, "ed": 34, "text": "efficiently solved"}, {"st": 84, "ed": 86, "text": "strongly convex"}, {"st": 108, "ed": 110, "text": "group lasso"}, {"st": 118, "ed": 120, "text": "group sparse"}, {"st": 129, "ed": 131, "text": "approximation error"}, {"st": 148, "ed": 150, "text": "approximation error"}, {"st": 165, "ed": 167, "text": "proposed framework"}]
[{"st": 2, "ed": 4, "text": "based methods"}, {"st": 10, "ed": 13, "text": "linear dimensionality reduction"}, {"st": 24, "ed": 26, "text": "streaming data"}, {"st": 36, "ed": 38, "text": "computational complexity"}, {"st": 48, "ed": 50, "text": "input data"}, {"st": 59, "ed": 61, "text": "high dimensional"}, {"st": 71, "ed": 73, "text": "observed data"}, {"st": 123, "ed": 125, "text": "proposed algorithm"}, {"st": 142, "ed": 144, "text": "input data"}]
[{"st": 0, "ed": 2, "text": "covariate shift"}, {"st": 2, "ed": 4, "text": "classification problems"}, {"st": 34, "ed": 36, "text": "cross validation"}, {"st": 94, "ed": 96, "text": "regularization parameter"}, {"st": 99, "ed": 101, "text": "training data"}]
[{"st": 15, "ed": 17, "text": "time series"}, {"st": 25, "ed": 27, "text": "sensory data"}, {"st": 41, "ed": 43, "text": "activity recognition"}, {"st": 48, "ed": 50, "text": "sensitive information"}, {"st": 60, "ed": 62, "text": "sensory data"}, {"st": 86, "ed": 88, "text": "time series"}, {"st": 110, "ed": 112, "text": "discriminative features"}, {"st": 144, "ed": 146, "text": "objective function"}, {"st": 195, "ed": 197, "text": "activity recognition"}, {"st": 204, "ed": 206, "text": "extensive experiments"}, {"st": 217, "ed": 219, "text": "recognition accuracy"}]
[{"st": 0, "ed": 3, "text": "deep neural networks"}, {"st": 6, "ed": 8, "text": "increasingly important"}, {"st": 12, "ed": 14, "text": "empirical performance"}, {"st": 69, "ed": 71, "text": "statistical learning"}, {"st": 77, "ed": 79, "text": "sampling based"}, {"st": 119, "ed": 121, "text": "stochastic optimization"}, {"st": 126, "ed": 128, "text": "regularization scheme"}, {"st": 133, "ed": 135, "text": "mathcal o"}, {"st": 136, "ed": 138, "text": "frac 1"}, {"st": 139, "ed": 141, "text": "generalization error"}, {"st": 148, "ed": 150, "text": "convex function"}, {"st": 159, "ed": 162, "text": "stochastic gradient descent"}, {"st": 168, "ed": 170, "text": "convex function"}, {"st": 179, "ed": 181, "text": "empirically validate"}, {"st": 182, "ed": 184, "text": "improved performance"}, {"st": 191, "ed": 193, "text": "convex function"}, {"st": 199, "ed": 201, "text": "real world"}]
[{"st": 2, "ed": 4, "text": "predictive analytics"}, {"st": 26, "ed": 29, "text": "training and test"}, {"st": 45, "ed": 47, "text": "concept drift"}, {"st": 94, "ed": 96, "text": "prediction performance"}, {"st": 138, "ed": 140, "text": "empirical studies"}]
[{"st": 0, "ed": 2, "text": "deep learning"}, {"st": 2, "ed": 4, "text": "typically requires"}, {"st": 15, "ed": 17, "text": "learning problems"}, {"st": 44, "ed": 46, "text": "meta learning"}, {"st": 57, "ed": 59, "text": "neural network"}, {"st": 98, "ed": 100, "text": "input data"}, {"st": 172, "ed": 174, "text": "meta learning"}, {"st": 180, "ed": 182, "text": "neural network"}, {"st": 186, "ed": 188, "text": "external memory"}, {"st": 203, "ed": 205, "text": "input data"}, {"st": 284, "ed": 286, "text": "meta learning"}, {"st": 300, "ed": 302, "text": "large margin"}, {"st": 306, "ed": 308, "text": "classification tasks"}]
[{"st": 4, "ed": 6, "text": "supervised learning"}, {"st": 13, "ed": 15, "text": "machine learning"}, {"st": 50, "ed": 52, "text": "class classification"}, {"st": 58, "ed": 60, "text": "positive class"}, {"st": 62, "ed": 64, "text": "related methods"}, {"st": 68, "ed": 70, "text": "hyper parameters"}, {"st": 72, "ed": 75, "text": "positive and negative"}, {"st": 82, "ed": 84, "text": "positive unlabeled"}, {"st": 93, "ed": 95, "text": "unlabeled data"}, {"st": 122, "ed": 124, "text": "classification problem"}, {"st": 128, "ed": 131, "text": "empirical risk minimization"}, {"st": 156, "ed": 158, "text": "estimation error"}, {"st": 169, "ed": 171, "text": "proposed method"}, {"st": 172, "ed": 175, "text": "deep neural networks"}]
[{"st": 6, "ed": 8, "text": "neural network"}, {"st": 9, "ed": 11, "text": "machine learning"}, {"st": 83, "ed": 85, "text": "global minimum"}, {"st": 87, "ed": 89, "text": "cost functions"}, {"st": 122, "ed": 124, "text": "deep learning"}, {"st": 148, "ed": 150, "text": "optimization problem"}, {"st": 152, "ed": 155, "text": "ordinary differential equation"}, {"st": 188, "ed": 190, "text": "back propagation"}]
[{"st": 1, "ed": 3, "text": "neural networks"}, {"st": 4, "ed": 6, "text": "latent variables"}, {"st": 12, "ed": 14, "text": "probabilistic models"}, {"st": 23, "ed": 25, "text": "network weights"}, {"st": 30, "ed": 32, "text": "latent variables"}, {"st": 34, "ed": 36, "text": "capture complex"}, {"st": 55, "ed": 57, "text": "decision making"}, {"st": 68, "ed": 70, "text": "active learning"}, {"st": 91, "ed": 93, "text": "reinforcement learning"}]
[{"st": 12, "ed": 14, "text": "machine learning"}, {"st": 21, "ed": 23, "text": "ensemble based"}, {"st": 23, "ed": 25, "text": "online learning"}, {"st": 72, "ed": 75, "text": "simulated and real"}, {"st": 79, "ed": 81, "text": "proposed method"}]
[{"st": 7, "ed": 9, "text": "approximate inference"}, {"st": 10, "ed": 13, "text": "gaussian process gp"}, {"st": 22, "ed": 25, "text": "stochastic variational inference"}, {"st": 36, "ed": 38, "text": "key idea"}, {"st": 48, "ed": 50, "text": "variational parameters"}, {"st": 80, "ed": 83, "text": "deep neural networks"}, {"st": 92, "ed": 94, "text": "neural network"}, {"st": 116, "ed": 118, "text": "neural network"}, {"st": 119, "ed": 122, "text": "end to end"}, {"st": 136, "ed": 138, "text": "proposed approach"}, {"st": 143, "ed": 145, "text": "benchmark datasets"}, {"st": 146, "ed": 149, "text": "mnist cifar 10"}]
[{"st": 7, "ed": 9, "text": "differentially private"}, {"st": 13, "ed": 15, "text": "differentially private"}, {"st": 35, "ed": 37, "text": "differentially private"}, {"st": 84, "ed": 86, "text": "differential privacy"}, {"st": 100, "ed": 102, "text": "differential privacy"}, {"st": 112, "ed": 114, "text": "excess risk"}, {"st": 142, "ed": 144, "text": "excess risk"}]
[{"st": 0, "ed": 2, "text": "transfer learning"}, {"st": 7, "ed": 10, "text": "deep neural networks"}, {"st": 11, "ed": 13, "text": "fine tuning"}, {"st": 72, "ed": 74, "text": "convolutional filters"}, {"st": 89, "ed": 91, "text": "recent advances"}, {"st": 108, "ed": 110, "text": "objective functions"}, {"st": 146, "ed": 148, "text": "transfer learning"}, {"st": 155, "ed": 157, "text": "standard benchmarks"}, {"st": 167, "ed": 169, "text": "transfer learning"}]
[{"st": 1, "ed": 3, "text": "common practice"}, {"st": 6, "ed": 8, "text": "deep convolutional"}, {"st": 8, "ed": 10, "text": "neural architectures"}, {"st": 13, "ed": 16, "text": "fully connected layers"}, {"st": 22, "ed": 25, "text": "cross entropy loss"}, {"st": 30, "ed": 32, "text": "recent studies"}, {"st": 43, "ed": 45, "text": "cost functions"}, {"st": 46, "ed": 49, "text": "support vector machines"}, {"st": 50, "ed": 53, "text": "linear discriminant analysis"}, {"st": 59, "ed": 61, "text": "classification performance"}, {"st": 91, "ed": 93, "text": "objective functions"}, {"st": 100, "ed": 102, "text": "loss functions"}, {"st": 121, "ed": 123, "text": "standard datasets"}, {"st": 138, "ed": 140, "text": "classification performance"}]
[{"st": 4, "ed": 7, "text": "gaussian process gp"}, {"st": 20, "ed": 22, "text": "real world"}, {"st": 26, "ed": 28, "text": "finite dimensional"}, {"st": 70, "ed": 74, "text": "markov chain monte carlo"}, {"st": 103, "ed": 105, "text": "real data"}, {"st": 112, "ed": 114, "text": "monte carlo"}]
[{"st": 51, "ed": 53, "text": "instance specific"}, {"st": 70, "ed": 73, "text": "naive bayes classifier"}, {"st": 74, "ed": 76, "text": "nearest neighbour"}, {"st": 145, "ed": 148, "text": "naive bayes classifier"}]
[{"st": 18, "ed": 20, "text": "learning theory"}, {"st": 23, "ed": 26, "text": "empirical risk minimization"}, {"st": 40, "ed": 42, "text": "rademacher complexity"}, {"st": 61, "ed": 63, "text": "pac bayesian"}, {"st": 81, "ed": 83, "text": "maximum likelihood"}, {"st": 88, "ed": 90, "text": "log loss"}, {"st": 95, "ed": 97, "text": "main result"}, {"st": 98, "ed": 100, "text": "excess risk"}, {"st": 108, "ed": 110, "text": "main result"}, {"st": 115, "ed": 117, "text": "rademacher complexity"}, {"st": 118, "ed": 120, "text": "l 2"}, {"st": 136, "ed": 138, "text": "log loss"}, {"st": 158, "ed": 160, "text": "rademacher complexity"}]
[{"st": 3, "ed": 5, "text": "computer vision"}, {"st": 9, "ed": 12, "text": "deep neural networks"}, {"st": 77, "ed": 79, "text": "neural networks"}, {"st": 121, "ed": 124, "text": "mnist cifar 10"}]
[{"st": 12, "ed": 14, "text": "machine teaching"}, {"st": 29, "ed": 31, "text": "feature representations"}, {"st": 60, "ed": 62, "text": "faster convergence"}, {"st": 95, "ed": 97, "text": "sample complexity"}]
[{"st": 11, "ed": 13, "text": "online algorithm"}, {"st": 18, "ed": 23, "text": "alternating direction method of multipliers"}, {"st": 50, "ed": 52, "text": "online algorithm"}, {"st": 66, "ed": 68, "text": "convergence rate"}, {"st": 69, "ed": 71, "text": "o sqrt"}, {"st": 72, "ed": 74, "text": "sqrt t"}, {"st": 108, "ed": 110, "text": "convergence rate"}, {"st": 111, "ed": 113, "text": "o sqrt"}, {"st": 117, "ed": 119, "text": "sqrt t"}, {"st": 128, "ed": 130, "text": "convergence analysis"}, {"st": 138, "ed": 140, "text": "signal processing"}]
[{"st": 3, "ed": 5, "text": "multi label"}, {"st": 8, "ed": 10, "text": "multi label"}, {"st": 17, "ed": 19, "text": "multi label"}, {"st": 45, "ed": 47, "text": "boosting algorithms"}, {"st": 49, "ed": 51, "text": "loss bounds"}, {"st": 52, "ed": 54, "text": "multi label"}, {"st": 107, "ed": 109, "text": "real data"}]
[{"st": 8, "ed": 13, "text": "computer vision and machine learning"}, {"st": 20, "ed": 22, "text": "feature selection"}, {"st": 40, "ed": 42, "text": "machine learning"}, {"st": 60, "ed": 62, "text": "feature selection"}, {"st": 67, "ed": 69, "text": "group lasso"}, {"st": 73, "ed": 75, "text": "feature selection"}, {"st": 81, "ed": 83, "text": "important features"}, {"st": 104, "ed": 106, "text": "feature selection"}, {"st": 113, "ed": 115, "text": "benchmark dataset"}, {"st": 118, "ed": 120, "text": "proposed method"}]
[{"st": 0, "ed": 4, "text": "generative adversarial networks gans"}, {"st": 8, "ed": 10, "text": "generative models"}, {"st": 20, "ed": 22, "text": "generative models"}, {"st": 34, "ed": 36, "text": "generative model"}, {"st": 56, "ed": 58, "text": "nash equilibrium"}, {"st": 88, "ed": 90, "text": "training distribution"}, {"st": 101, "ed": 103, "text": "recent research"}, {"st": 119, "ed": 121, "text": "learning process"}, {"st": 140, "ed": 142, "text": "gan training"}, {"st": 169, "ed": 171, "text": "gan training"}, {"st": 190, "ed": 193, "text": "point of view"}, {"st": 239, "ed": 241, "text": "gan training"}, {"st": 248, "ed": 250, "text": "nash equilibria"}]
[{"st": 9, "ed": 11, "text": "time series"}, {"st": 27, "ed": 29, "text": "time series"}, {"st": 39, "ed": 41, "text": "time series"}, {"st": 44, "ed": 47, "text": "cold start problem"}, {"st": 49, "ed": 51, "text": "recommender systems"}, {"st": 66, "ed": 68, "text": "time series"}, {"st": 96, "ed": 98, "text": "cold start"}, {"st": 105, "ed": 107, "text": "time series"}, {"st": 111, "ed": 113, "text": "step ahead"}, {"st": 126, "ed": 128, "text": "missing data"}, {"st": 132, "ed": 134, "text": "computationally efficient"}, {"st": 143, "ed": 145, "text": "matrix factorization"}, {"st": 188, "ed": 190, "text": "real world"}]
[{"st": 56, "ed": 60, "text": "convolutional neural network cnn"}, {"st": 135, "ed": 137, "text": "benchmark datasets"}]
[{"st": 4, "ed": 6, "text": "deep learning"}, {"st": 13, "ed": 15, "text": "real world"}, {"st": 42, "ed": 44, "text": "real world"}, {"st": 99, "ed": 101, "text": "machine learning"}]
[{"st": 0, "ed": 2, "text": "transfer learning"}, {"st": 3, "ed": 6, "text": "deep neural networks"}, {"st": 7, "ed": 9, "text": "feature extractors"}, {"st": 11, "ed": 13, "text": "increasingly popular"}, {"st": 34, "ed": 37, "text": "deep neural network"}, {"st": 74, "ed": 76, "text": "features extracted"}, {"st": 77, "ed": 80, "text": "deep neural networks"}, {"st": 111, "ed": 113, "text": "feature vectors"}, {"st": 127, "ed": 129, "text": "real datasets"}, {"st": 135, "ed": 138, "text": "deep neural network"}, {"st": 138, "ed": 140, "text": "feature extractors"}]
[{"st": 147, "ed": 149, "text": "multi label"}, {"st": 149, "ed": 151, "text": "evaluation measures"}, {"st": 182, "ed": 184, "text": "multi label"}, {"st": 201, "ed": 203, "text": "significantly outperforms"}]
[{"st": 1, "ed": 4, "text": "each data point"}, {"st": 21, "ed": 23, "text": "feature vectors"}, {"st": 57, "ed": 59, "text": "large networks"}, {"st": 97, "ed": 99, "text": "method yields"}, {"st": 99, "ed": 101, "text": "competitive results"}, {"st": 102, "ed": 104, "text": "classification tasks"}, {"st": 106, "ed": 108, "text": "autoimmune disease"}]
[{"st": 7, "ed": 9, "text": "predictive distributions"}, {"st": 17, "ed": 19, "text": "recent literature"}, {"st": 28, "ed": 30, "text": "predictive distributions"}, {"st": 31, "ed": 33, "text": "machine learning"}, {"st": 57, "ed": 59, "text": "predictive distributions"}, {"st": 60, "ed": 62, "text": "kernel methods"}]
[{"st": 1, "ed": 3, "text": "classification models"}, {"st": 24, "ed": 26, "text": "decision making"}, {"st": 30, "ed": 32, "text": "classification problem"}, {"st": 33, "ed": 35, "text": "real valued"}, {"st": 54, "ed": 57, "text": "support vector machines"}, {"st": 88, "ed": 90, "text": "extensive experiments"}, {"st": 137, "ed": 139, "text": "standard classification"}, {"st": 161, "ed": 163, "text": "negative log"}, {"st": 185, "ed": 188, "text": "support vector machine"}, {"st": 200, "ed": 203, "text": "deep neural network"}, {"st": 204, "ed": 207, "text": "support vector machine"}, {"st": 209, "ed": 211, "text": "decision tree"}, {"st": 242, "ed": 244, "text": "set size"}]
[{"st": 0, "ed": 3, "text": "deep neural networks"}, {"st": 24, "ed": 26, "text": "speech recognition"}, {"st": 26, "ed": 28, "text": "image classification"}, {"st": 29, "ed": 31, "text": "object tracking"}, {"st": 38, "ed": 40, "text": "fundamental problems"}, {"st": 53, "ed": 55, "text": "input output"}, {"st": 65, "ed": 67, "text": "adversarial examples"}, {"st": 90, "ed": 92, "text": "unlabeled data"}, {"st": 111, "ed": 113, "text": "provide theoretical"}]
[{"st": 1, "ed": 3, "text": "fisher information"}, {"st": 18, "ed": 20, "text": "local geometry"}, {"st": 24, "ed": 27, "text": "recurrent neural networks"}, {"st": 51, "ed": 53, "text": "image captioning"}, {"st": 61, "ed": 63, "text": "latent embedding"}, {"st": 80, "ed": 84, "text": "deep recurrent neural networks"}, {"st": 89, "ed": 91, "text": "latent embedding"}, {"st": 94, "ed": 96, "text": "recurrent network"}, {"st": 105, "ed": 108, "text": "gaussian mixture model"}, {"st": 133, "ed": 135, "text": "adjacency matrix"}, {"st": 152, "ed": 154, "text": "latent embedding"}]
[{"st": 1, "ed": 4, "text": "deep neural networks"}, {"st": 37, "ed": 39, "text": "neural network"}, {"st": 55, "ed": 57, "text": "neural network"}, {"st": 72, "ed": 76, "text": "cifar 10 cifar 100"}, {"st": 92, "ed": 94, "text": "neural network"}, {"st": 110, "ed": 112, "text": "adversarial examples"}]
[{"st": 10, "ed": 13, "text": "deep neural networks"}, {"st": 30, "ed": 32, "text": "neural networks"}, {"st": 50, "ed": 52, "text": "neural network"}, {"st": 67, "ed": 69, "text": "natural language"}, {"st": 113, "ed": 115, "text": "image classification"}, {"st": 128, "ed": 131, "text": "qualitatively and quantitatively"}]
[{"st": 6, "ed": 8, "text": "deep learning"}, {"st": 24, "ed": 26, "text": "control problem"}, {"st": 42, "ed": 44, "text": "maximum principle"}, {"st": 96, "ed": 98, "text": "gradient based"}, {"st": 117, "ed": 119, "text": "convergence rate"}, {"st": 119, "ed": 121, "text": "per iteration"}, {"st": 150, "ed": 152, "text": "deep learning"}, {"st": 161, "ed": 163, "text": "gradient based"}]
[{"st": 10, "ed": 12, "text": "generalization properties"}, {"st": 21, "ed": 23, "text": "worst case"}, {"st": 32, "ed": 34, "text": "machine learning"}, {"st": 35, "ed": 37, "text": "recent years"}, {"st": 43, "ed": 45, "text": "statistical mechanics"}, {"st": 57, "ed": 59, "text": "deep learning"}, {"st": 94, "ed": 96, "text": "effective temperature"}, {"st": 119, "ed": 121, "text": "statistical mechanics"}, {"st": 132, "ed": 134, "text": "empirical results"}, {"st": 138, "ed": 141, "text": "deep neural networks"}, {"st": 144, "ed": 146, "text": "training data"}, {"st": 153, "ed": 155, "text": "generalization properties"}, {"st": 156, "ed": 158, "text": "learning algorithms"}]
[{"st": 5, "ed": 7, "text": "classification tasks"}, {"st": 10, "ed": 12, "text": "experiments conducted"}, {"st": 26, "ed": 28, "text": "ensemble methods"}, {"st": 30, "ed": 32, "text": "random forests"}, {"st": 37, "ed": 39, "text": "ensemble methods"}, {"st": 55, "ed": 58, "text": "divide and conquer"}, {"st": 71, "ed": 73, "text": "decision tree"}, {"st": 80, "ed": 82, "text": "decision trees"}, {"st": 86, "ed": 88, "text": "class distributions"}, {"st": 115, "ed": 117, "text": "class labels"}, {"st": 124, "ed": 126, "text": "decision tree"}]
[{"st": 80, "ed": 82, "text": "base classifiers"}, {"st": 101, "ed": 103, "text": "proposed approach"}, {"st": 121, "ed": 123, "text": "conducted experiments"}, {"st": 126, "ed": 128, "text": "proposed approach"}, {"st": 137, "ed": 139, "text": "based approach"}, {"st": 148, "ed": 150, "text": "class label"}, {"st": 161, "ed": 163, "text": "method achieves"}]
[{"st": 16, "ed": 18, "text": "low complexity"}]
[{"st": 8, "ed": 10, "text": "kernel matrices"}, {"st": 11, "ed": 14, "text": "random fourier features"}, {"st": 16, "ed": 18, "text": "gaussian kernel"}, {"st": 22, "ed": 24, "text": "mathcal o"}, {"st": 35, "ed": 37, "text": "random features"}, {"st": 79, "ed": 82, "text": "random fourier features"}, {"st": 101, "ed": 104, "text": "kernel ridge regression"}]
[{"st": 0, "ed": 2, "text": "deep learning"}, {"st": 8, "ed": 10, "text": "reinforcement learning"}, {"st": 13, "ed": 16, "text": "deep neural networks"}, {"st": 17, "ed": 19, "text": "learning representations"}, {"st": 27, "ed": 31, "text": "deep neural network architecture"}, {"st": 34, "ed": 36, "text": "learning representations"}, {"st": 37, "ed": 39, "text": "multi task"}, {"st": 43, "ed": 45, "text": "multiple tasks"}, {"st": 60, "ed": 64, "text": "deep neural network architecture"}]
[{"st": 6, "ed": 8, "text": "kernel learning"}, {"st": 23, "ed": 25, "text": "method produces"}, {"st": 28, "ed": 30, "text": "feature maps"}, {"st": 56, "ed": 58, "text": "min max"}, {"st": 61, "ed": 66, "text": "synthetic and real world datasets"}, {"st": 73, "ed": 75, "text": "random features"}]
[{"st": 4, "ed": 8, "text": "generative adversarial networks gans"}, {"st": 19, "ed": 21, "text": "generated image"}, {"st": 45, "ed": 47, "text": "deep learning"}, {"st": 55, "ed": 57, "text": "training data"}, {"st": 60, "ed": 62, "text": "impressive results"}, {"st": 80, "ed": 83, "text": "semi supervised learning"}, {"st": 114, "ed": 116, "text": "training scheme"}, {"st": 126, "ed": 128, "text": "training scheme"}]
[{"st": 3, "ed": 5, "text": "neural network"}, {"st": 11, "ed": 13, "text": "hamming distance"}, {"st": 26, "ed": 28, "text": "neural network"}, {"st": 44, "ed": 46, "text": "batch normalization"}, {"st": 82, "ed": 84, "text": "batch normalization"}, {"st": 92, "ed": 94, "text": "hamming distance"}, {"st": 96, "ed": 99, "text": "rectified linear units"}, {"st": 107, "ed": 109, "text": "hamming distance"}, {"st": 178, "ed": 180, "text": "fuzzy logic"}, {"st": 185, "ed": 187, "text": "learning speed"}]
[{"st": 8, "ed": 10, "text": "logistic regression"}, {"st": 12, "ed": 14, "text": "linearly separable"}, {"st": 21, "ed": 23, "text": "max margin"}, {"st": 35, "ed": 37, "text": "loss functions"}, {"st": 43, "ed": 45, "text": "multi class"}, {"st": 54, "ed": 56, "text": "deep network"}, {"st": 92, "ed": 95, "text": "cross entropy loss"}, {"st": 98, "ed": 100, "text": "training error"}, {"st": 104, "ed": 106, "text": "training loss"}, {"st": 130, "ed": 132, "text": "complex models"}]
[{"st": 0, "ed": 2, "text": "spectral graph"}, {"st": 2, "ed": 6, "text": "convolutional neural networks cnns"}, {"st": 14, "ed": 16, "text": "computational complexity"}, {"st": 27, "ed": 29, "text": "convolutional network"}, {"st": 33, "ed": 35, "text": "convolutional network"}, {"st": 50, "ed": 52, "text": "fixed size"}, {"st": 93, "ed": 95, "text": "structured data"}]
[{"st": 4, "ed": 6, "text": "fully connected"}, {"st": 6, "ed": 8, "text": "neural networks"}, {"st": 26, "ed": 28, "text": "weight matrices"}, {"st": 41, "ed": 43, "text": "trainable parameters"}, {"st": 69, "ed": 71, "text": "neural networks"}, {"st": 78, "ed": 80, "text": "back propagation"}, {"st": 121, "ed": 123, "text": "neural networks"}, {"st": 127, "ed": 129, "text": "hidden layers"}]
[{"st": 5, "ed": 7, "text": "inference algorithm"}, {"st": 27, "ed": 29, "text": "graphical model"}, {"st": 50, "ed": 52, "text": "marginal distribution"}, {"st": 63, "ed": 65, "text": "approximation error"}, {"st": 97, "ed": 99, "text": "theoretical bounds"}, {"st": 107, "ed": 109, "text": "inference algorithm"}, {"st": 111, "ed": 114, "text": "fast and accurate"}]
[{"st": 11, "ed": 13, "text": "unsupervised learning"}, {"st": 15, "ed": 17, "text": "latent feature"}, {"st": 27, "ed": 30, "text": "restricted boltzmann machines"}, {"st": 69, "ed": 71, "text": "feature space"}, {"st": 107, "ed": 110, "text": "atlanta police department"}]
[{"st": 3, "ed": 5, "text": "machine learning"}, {"st": 24, "ed": 26, "text": "machine learning"}, {"st": 43, "ed": 45, "text": "malignant tumor"}, {"st": 98, "ed": 100, "text": "deep learning"}, {"st": 132, "ed": 134, "text": "feature importance"}, {"st": 158, "ed": 160, "text": "feature importance"}]
[{"st": 9, "ed": 11, "text": "convex function"}, {"st": 32, "ed": 34, "text": "feature selection"}, {"st": 51, "ed": 53, "text": "convergence rates"}, {"st": 65, "ed": 67, "text": "empirical results"}, {"st": 69, "ed": 71, "text": "theoretical findings"}, {"st": 82, "ed": 84, "text": "optimization methods"}]
[{"st": 1, "ed": 3, "text": "convolutional networks"}, {"st": 6, "ed": 9, "text": "deep neural networks"}, {"st": 27, "ed": 29, "text": "receptive field"}, {"st": 37, "ed": 39, "text": "previous attempts"}, {"st": 42, "ed": 44, "text": "receptive field"}, {"st": 52, "ed": 54, "text": "convergence guarantee"}, {"st": 56, "ed": 58, "text": "receptive field"}, {"st": 89, "ed": 91, "text": "theoretical guarantee"}, {"st": 98, "ed": 100, "text": "local optimum"}, {"st": 102, "ed": 104, "text": "empirical results"}]
[{"st": 0, "ed": 3, "text": "deep neural networks"}, {"st": 20, "ed": 22, "text": "computer vision"}, {"st": 22, "ed": 24, "text": "speech processing"}, {"st": 28, "ed": 30, "text": "recent advances"}, {"st": 79, "ed": 81, "text": "main challenges"}, {"st": 91, "ed": 93, "text": "exploding gradient"}, {"st": 102, "ed": 104, "text": "objective function"}, {"st": 119, "ed": 122, "text": "vanishing gradient problem"}, {"st": 140, "ed": 142, "text": "standard datasets"}, {"st": 178, "ed": 180, "text": "loss function"}]
[{"st": 0, "ed": 2, "text": "neural networks"}, {"st": 3, "ed": 7, "text": "vulnerable to adversarial examples"}, {"st": 24, "ed": 26, "text": "robust optimization"}, {"st": 53, "ed": 55, "text": "training procedure"}, {"st": 61, "ed": 63, "text": "worst case"}]
[{"st": 4, "ed": 6, "text": "continual learning"}, {"st": 13, "ed": 15, "text": "continual learning"}, {"st": 18, "ed": 20, "text": "variational inference"}, {"st": 22, "ed": 24, "text": "recent advances"}, {"st": 25, "ed": 27, "text": "monte carlo"}, {"st": 38, "ed": 40, "text": "discriminative models"}, {"st": 41, "ed": 44, "text": "deep generative models"}, {"st": 46, "ed": 48, "text": "continual learning"}, {"st": 65, "ed": 67, "text": "continual learning"}, {"st": 72, "ed": 74, "text": "continual learning"}, {"st": 81, "ed": 83, "text": "catastrophic forgetting"}, {"st": 85, "ed": 87, "text": "fully automatic"}]
[{"st": 12, "ed": 14, "text": "finite sample"}]
[{"st": 4, "ed": 6, "text": "deep learning"}, {"st": 28, "ed": 30, "text": "deep learning"}, {"st": 63, "ed": 65, "text": "deep learning"}, {"st": 67, "ed": 70, "text": "convolutional neural networks"}, {"st": 70, "ed": 72, "text": "residual networks"}, {"st": 73, "ed": 75, "text": "neural networks"}, {"st": 75, "ed": 78, "text": "recurrent neural networks"}, {"st": 118, "ed": 120, "text": "deep learning"}]
[{"st": 0, "ed": 3, "text": "approximate dynamic programming"}, {"st": 7, "ed": 9, "text": "value iteration"}, {"st": 11, "ed": 13, "text": "successfully applied"}, {"st": 16, "ed": 18, "text": "reinforcement learning"}, {"st": 22, "ed": 25, "text": "approximate dynamic programming"}, {"st": 34, "ed": 36, "text": "reinforcement learning"}, {"st": 47, "ed": 49, "text": "dynamic programming"}, {"st": 52, "ed": 54, "text": "value iteration"}, {"st": 64, "ed": 66, "text": "value iteration"}, {"st": 79, "ed": 81, "text": "performance guarantee"}, {"st": 83, "ed": 85, "text": "performance guarantees"}, {"st": 86, "ed": 88, "text": "existing algorithms"}, {"st": 96, "ed": 98, "text": "existing algorithms"}, {"st": 104, "ed": 106, "text": "numerical experiments"}]
[{"st": 10, "ed": 13, "text": "bag of words"}, {"st": 18, "ed": 20, "text": "knowledge graph"}, {"st": 27, "ed": 29, "text": "question answering"}, {"st": 30, "ed": 32, "text": "supervised classification"}, {"st": 40, "ed": 43, "text": "entities and relations"}]
[{"st": 7, "ed": 9, "text": "convergence rate"}, {"st": 11, "ed": 14, "text": "latent dirichlet allocation"}, {"st": 24, "ed": 27, "text": "maximum likelihood estimator"}, {"st": 39, "ed": 41, "text": "distance metric"}, {"st": 72, "ed": 74, "text": "previous works"}, {"st": 93, "ed": 95, "text": "convergence rate"}]
[{"st": 0, "ed": 2, "text": "policy gradient"}, {"st": 10, "ed": 12, "text": "reinforcement learning"}, {"st": 24, "ed": 26, "text": "policy gradient"}, {"st": 31, "ed": 33, "text": "sample efficiency"}, {"st": 49, "ed": 51, "text": "policy gradient"}, {"st": 70, "ed": 73, "text": "advantage actor critic"}, {"st": 81, "ed": 83, "text": "empirical studies"}, {"st": 87, "ed": 89, "text": "significantly improves"}, {"st": 90, "ed": 92, "text": "sample efficiency"}, {"st": 98, "ed": 100, "text": "policy gradient"}]
[{"st": 5, "ed": 7, "text": "deep learning"}, {"st": 13, "ed": 15, "text": "machine learning"}, {"st": 23, "ed": 25, "text": "loss functions"}, {"st": 27, "ed": 29, "text": "neural networks"}, {"st": 81, "ed": 83, "text": "loss functions"}, {"st": 103, "ed": 105, "text": "loss functions"}, {"st": 111, "ed": 113, "text": "sufficient conditions"}, {"st": 134, "ed": 136, "text": "loss functions"}, {"st": 146, "ed": 148, "text": "loss function"}, {"st": 154, "ed": 156, "text": "local minimum"}, {"st": 158, "ed": 160, "text": "loss function"}, {"st": 162, "ed": 164, "text": "hidden layer"}, {"st": 168, "ed": 170, "text": "activation function"}, {"st": 172, "ed": 174, "text": "local minimum"}]
[{"st": 7, "ed": 9, "text": "linear activation"}, {"st": 12, "ed": 14, "text": "neural network"}, {"st": 21, "ed": 23, "text": "machine learning"}, {"st": 27, "ed": 29, "text": "empirical analysis"}, {"st": 39, "ed": 41, "text": "classification task"}, {"st": 110, "ed": 112, "text": "neural network"}, {"st": 115, "ed": 117, "text": "impressive results"}]
[{"st": 23, "ed": 25, "text": "learning theory"}, {"st": 67, "ed": 69, "text": "main result"}, {"st": 132, "ed": 134, "text": "probability distributions"}]
[{"st": 0, "ed": 3, "text": "low rank tensor"}, {"st": 24, "ed": 26, "text": "gaussian processes"}, {"st": 30, "ed": 32, "text": "machine learning"}, {"st": 55, "ed": 58, "text": "low rank tensor"}, {"st": 67, "ed": 69, "text": "gaussian processes"}, {"st": 71, "ed": 73, "text": "low rank"}, {"st": 78, "ed": 80, "text": "bayesian inference"}, {"st": 91, "ed": 93, "text": "learning curve"}, {"st": 102, "ed": 105, "text": "low rank tensor"}, {"st": 110, "ed": 112, "text": "highly dependent"}]
[{"st": 80, "ed": 82, "text": "linear interpolation"}, {"st": 109, "ed": 111, "text": "standard benchmark"}, {"st": 111, "ed": 113, "text": "image datasets"}, {"st": 122, "ed": 124, "text": "generated samples"}]
[{"st": 6, "ed": 8, "text": "generative models"}, {"st": 9, "ed": 12, "text": "deep neural networks"}, {"st": 65, "ed": 67, "text": "generative models"}, {"st": 70, "ed": 72, "text": "latent structure"}]
[{"st": 3, "ed": 5, "text": "deep network"}, {"st": 6, "ed": 8, "text": "image classification"}, {"st": 16, "ed": 18, "text": "latent features"}, {"st": 36, "ed": 38, "text": "latent features"}, {"st": 88, "ed": 90, "text": "image quality"}, {"st": 291, "ed": 293, "text": "linear classification"}]
[{"st": 1, "ed": 4, "text": "boosted decision trees"}, {"st": 7, "ed": 9, "text": "machine learning"}, {"st": 31, "ed": 33, "text": "boosting algorithm"}, {"st": 90, "ed": 92, "text": "boosted trees"}, {"st": 108, "ed": 111, "text": "layer by layer"}, {"st": 126, "ed": 128, "text": "faster convergence"}, {"st": 141, "ed": 143, "text": "open source"}, {"st": 144, "ed": 146, "text": "boosted trees"}, {"st": 178, "ed": 180, "text": "embedded devices"}, {"st": 182, "ed": 184, "text": "fast inference"}]
[{"st": 1, "ed": 3, "text": "boosted trees"}, {"st": 13, "ed": 15, "text": "distributed training"}, {"st": 35, "ed": 38, "text": "layer by layer"}, {"st": 48, "ed": 50, "text": "multi class"}, {"st": 55, "ed": 57, "text": "regularization techniques"}]
[{"st": 32, "ed": 36, "text": "mean squared error mse"}, {"st": 59, "ed": 61, "text": "np hard"}, {"st": 63, "ed": 65, "text": "semidefinite programming"}, {"st": 79, "ed": 81, "text": "submodular function"}, {"st": 85, "ed": 87, "text": "greedy algorithm"}, {"st": 97, "ed": 99, "text": "worst case"}, {"st": 99, "ed": 101, "text": "performance guarantee"}, {"st": 108, "ed": 110, "text": "greedy algorithm"}, {"st": 112, "ed": 114, "text": "non stationary"}, {"st": 125, "ed": 127, "text": "numerical simulations"}, {"st": 128, "ed": 132, "text": "synthetic and real world"}, {"st": 136, "ed": 138, "text": "greedy algorithm"}]
[{"st": 9, "ed": 11, "text": "fully connected"}, {"st": 11, "ed": 13, "text": "neural network"}, {"st": 24, "ed": 26, "text": "gaussian process"}, {"st": 37, "ed": 39, "text": "exact bayesian"}, {"st": 43, "ed": 45, "text": "neural networks"}, {"st": 56, "ed": 58, "text": "kernel functions"}, {"st": 60, "ed": 62, "text": "multi layer"}, {"st": 63, "ed": 65, "text": "neural networks"}, {"st": 95, "ed": 97, "text": "fully bayesian"}, {"st": 114, "ed": 116, "text": "deep networks"}, {"st": 122, "ed": 124, "text": "computationally efficient"}, {"st": 145, "ed": 148, "text": "deep neural networks"}, {"st": 156, "ed": 159, "text": "trained neural network"}]
[{"st": 5, "ed": 7, "text": "variational inference"}, {"st": 15, "ed": 17, "text": "gaussian process"}, {"st": 44, "ed": 46, "text": "variational bayesian"}, {"st": 61, "ed": 63, "text": "variational parameters"}, {"st": 69, "ed": 72, "text": "variational lower bound"}, {"st": 93, "ed": 96, "text": "variational lower bound"}, {"st": 150, "ed": 152, "text": "per iteration"}, {"st": 159, "ed": 161, "text": "empirically evaluate"}, {"st": 165, "ed": 167, "text": "proposed framework"}, {"st": 169, "ed": 171, "text": "real world"}]
[{"st": 51, "ed": 53, "text": "unlabeled examples"}, {"st": 76, "ed": 78, "text": "error rate"}, {"st": 93, "ed": 95, "text": "frac 1"}, {"st": 98, "ed": 100, "text": "frac 1"}, {"st": 115, "ed": 117, "text": "frac 1"}, {"st": 191, "ed": 193, "text": "learning algorithm"}, {"st": 204, "ed": 206, "text": "unlabeled examples"}, {"st": 226, "ed": 228, "text": "entire dataset"}, {"st": 233, "ed": 236, "text": "k nearest neighbor"}, {"st": 251, "ed": 253, "text": "hyperparameter tuning"}]
[{"st": 0, "ed": 2, "text": "neural networks"}, {"st": 3, "ed": 5, "text": "low precision"}, {"st": 5, "ed": 8, "text": "weights and activations"}, {"st": 32, "ed": 34, "text": "forward pass"}, {"st": 47, "ed": 49, "text": "low precision"}, {"st": 49, "ed": 51, "text": "neural networks"}, {"st": 55, "ed": 57, "text": "adversarial attacks"}, {"st": 60, "ed": 62, "text": "worst case"}, {"st": 76, "ed": 78, "text": "low precision"}, {"st": 80, "ed": 83, "text": "weights and activations"}, {"st": 113, "ed": 115, "text": "neural networks"}]
[{"st": 3, "ed": 6, "text": "unsupervised representation learning"}, {"st": 12, "ed": 14, "text": "latent variable"}, {"st": 21, "ed": 23, "text": "marginal likelihood"}, {"st": 36, "ed": 39, "text": "evidence lower bound"}, {"st": 42, "ed": 44, "text": "maximum likelihood"}, {"st": 56, "ed": 58, "text": "latent representation"}, {"st": 76, "ed": 78, "text": "mutual information"}, {"st": 83, "ed": 85, "text": "latent variable"}, {"st": 121, "ed": 124, "text": "quantitative and qualitative"}, {"st": 136, "ed": 138, "text": "latent variable"}]
[{"st": 6, "ed": 9, "text": "multi class classification"}, {"st": 63, "ed": 66, "text": "maximum likelihood estimator"}, {"st": 117, "ed": 119, "text": "prediction accuracy"}, {"st": 141, "ed": 143, "text": "significant speedup"}, {"st": 147, "ed": 149, "text": "mathcal o"}]
[{"st": 8, "ed": 11, "text": "convex loss function"}, {"st": 23, "ed": 25, "text": "generic framework"}, {"st": 52, "ed": 55, "text": "dictionary learning algorithms"}, {"st": 82, "ed": 86, "text": "synthetic and real data"}, {"st": 110, "ed": 112, "text": "k svd"}]
[{"st": 9, "ed": 11, "text": "generative model"}, {"st": 29, "ed": 31, "text": "graphical model"}, {"st": 43, "ed": 45, "text": "neural network"}, {"st": 51, "ed": 53, "text": "marginal distributions"}, {"st": 67, "ed": 69, "text": "importance sampling"}]
[{"st": 7, "ed": 9, "text": "deep learning"}, {"st": 28, "ed": 30, "text": "linear classifiers"}, {"st": 33, "ed": 35, "text": "doesn t"}, {"st": 50, "ed": 52, "text": "linear transformation"}]
[{"st": 2, "ed": 4, "text": "class imbalanced"}, {"st": 11, "ed": 13, "text": "challenging problem"}, {"st": 14, "ed": 16, "text": "supervised learning"}, {"st": 17, "ed": 19, "text": "standard classification"}, {"st": 61, "ed": 63, "text": "training data"}, {"st": 70, "ed": 72, "text": "class imbalanced"}, {"st": 95, "ed": 98, "text": "simple and effective"}, {"st": 102, "ed": 105, "text": "k means clustering"}, {"st": 122, "ed": 124, "text": "empirical results"}, {"st": 125, "ed": 127, "text": "extensive experiments"}, {"st": 132, "ed": 134, "text": "training data"}, {"st": 137, "ed": 139, "text": "proposed method"}, {"st": 143, "ed": 145, "text": "k means"}, {"st": 146, "ed": 148, "text": "consistently outperforms"}]
[{"st": 24, "ed": 26, "text": "sampling distribution"}, {"st": 43, "ed": 45, "text": "approximate posterior"}, {"st": 89, "ed": 91, "text": "majority vote"}, {"st": 116, "ed": 118, "text": "numerical experiments"}]
[{"st": 36, "ed": 38, "text": "pre processing"}, {"st": 45, "ed": 47, "text": "input data"}]
[{"st": 14, "ed": 16, "text": "probability distributions"}, {"st": 23, "ed": 25, "text": "conditional independence"}, {"st": 35, "ed": 37, "text": "undirected graph"}, {"st": 38, "ed": 41, "text": "markov random field"}, {"st": 94, "ed": 96, "text": "sampling methods"}]
[{"st": 0, "ed": 2, "text": "genetic algorithms"}, {"st": 13, "ed": 15, "text": "natural selection"}, {"st": 38, "ed": 41, "text": "deep reinforcement learning"}, {"st": 57, "ed": 59, "text": "policy optimization"}, {"st": 62, "ed": 64, "text": "genetic algorithm"}, {"st": 65, "ed": 67, "text": "sample efficient"}, {"st": 72, "ed": 74, "text": "imitation learning"}, {"st": 83, "ed": 85, "text": "policy gradient"}, {"st": 98, "ed": 100, "text": "genetic algorithm"}, {"st": 112, "ed": 114, "text": "policy gradient"}, {"st": 116, "ed": 118, "text": "achieves comparable"}]
[{"st": 1, "ed": 3, "text": "variational approximations"}, {"st": 10, "ed": 13, "text": "gaussian process gp"}, {"st": 23, "ed": 25, "text": "generative model"}, {"st": 38, "ed": 40, "text": "predictive accuracy"}, {"st": 66, "ed": 68, "text": "variational inference"}, {"st": 74, "ed": 76, "text": "approximate posterior"}, {"st": 81, "ed": 83, "text": "latent variables"}, {"st": 85, "ed": 87, "text": "mean field"}, {"st": 90, "ed": 93, "text": "fast and accurate"}, {"st": 101, "ed": 103, "text": "inference algorithm"}]
[{"st": 4, "ed": 6, "text": "variational autoencoders"}, {"st": 8, "ed": 12, "text": "generative adversarial networks gans"}, {"st": 23, "ed": 25, "text": "latent space"}, {"st": 64, "ed": 66, "text": "latent space"}, {"st": 85, "ed": 87, "text": "latent space"}, {"st": 98, "ed": 100, "text": "riemannian geometry"}, {"st": 111, "ed": 113, "text": "shortest path"}, {"st": 115, "ed": 117, "text": "riemannian manifold"}, {"st": 122, "ed": 124, "text": "method yields"}, {"st": 126, "ed": 128, "text": "distance measure"}, {"st": 135, "ed": 138, "text": "deep generative models"}, {"st": 142, "ed": 144, "text": "linear interpolation"}, {"st": 158, "ed": 160, "text": "previously learned"}, {"st": 171, "ed": 173, "text": "ground truth"}, {"st": 181, "ed": 183, "text": "motion capture"}, {"st": 187, "ed": 189, "text": "generative model"}]
[{"st": 7, "ed": 9, "text": "variational inference"}, {"st": 23, "ed": 25, "text": "variational distribution"}, {"st": 32, "ed": 34, "text": "achieves comparable"}, {"st": 34, "ed": 36, "text": "predictive performance"}, {"st": 39, "ed": 41, "text": "classification task"}, {"st": 52, "ed": 54, "text": "maximum likelihood"}]
[{"st": 100, "ed": 102, "text": "cluster centers"}, {"st": 108, "ed": 110, "text": "theoretical analysis"}, {"st": 127, "ed": 129, "text": "covariate shift"}, {"st": 130, "ed": 132, "text": "transfer learning"}, {"st": 139, "ed": 141, "text": "extensive experiments"}, {"st": 142, "ed": 144, "text": "real world"}, {"st": 168, "ed": 170, "text": "insurance company"}]
[{"st": 4, "ed": 6, "text": "auto encoder"}, {"st": 13, "ed": 15, "text": "generative model"}, {"st": 26, "ed": 28, "text": "wasserstein distance"}, {"st": 48, "ed": 51, "text": "variational auto encoder"}, {"st": 58, "ed": 60, "text": "training distribution"}, {"st": 81, "ed": 83, "text": "auto encoders"}, {"st": 99, "ed": 102, "text": "encoder decoder architecture"}]
[{"st": 40, "ed": 42, "text": "proposed approach"}, {"st": 70, "ed": 72, "text": "feature learning"}, {"st": 74, "ed": 76, "text": "deep learning"}, {"st": 78, "ed": 81, "text": "convolutional neural network"}]
[{"st": 1, "ed": 3, "text": "vision based"}, {"st": 4, "ed": 6, "text": "gesture recognition"}, {"st": 19, "ed": 21, "text": "gesture recognition"}, {"st": 27, "ed": 29, "text": "deep learning"}, {"st": 31, "ed": 33, "text": "doppler radar"}, {"st": 75, "ed": 77, "text": "machine learning"}, {"st": 80, "ed": 82, "text": "deep convolutional"}, {"st": 94, "ed": 96, "text": "recognition rate"}]
[{"st": 2, "ed": 5, "text": "multi armed bandits"}, {"st": 8, "ed": 10, "text": "extensively studied"}, {"st": 17, "ed": 19, "text": "cognitive radio"}, {"st": 83, "ed": 85, "text": "outperform existing"}, {"st": 89, "ed": 92, "text": "strong theoretical guarantees"}, {"st": 139, "ed": 141, "text": "empirical performance"}]
[{"st": 43, "ed": 45, "text": "sampling scheme"}, {"st": 58, "ed": 60, "text": "sample sizes"}, {"st": 69, "ed": 72, "text": "central limit theorem"}, {"st": 99, "ed": 101, "text": "machine learning"}, {"st": 102, "ed": 104, "text": "online learning"}, {"st": 115, "ed": 117, "text": "theoretical guarantees"}, {"st": 131, "ed": 133, "text": "detection performance"}, {"st": 156, "ed": 158, "text": "case study"}, {"st": 170, "ed": 172, "text": "closely related"}]
[{"st": 0, "ed": 3, "text": "canonical correlation analysis"}, {"st": 8, "ed": 10, "text": "statistical methods"}, {"st": 21, "ed": 24, "text": "canonical correlation analysis"}, {"st": 39, "ed": 41, "text": "sample size"}, {"st": 75, "ed": 78, "text": "canonical correlation analysis"}, {"st": 98, "ed": 100, "text": "numerical examples"}, {"st": 123, "ed": 125, "text": "optimisation problems"}, {"st": 127, "ed": 129, "text": "statistical significance"}, {"st": 133, "ed": 135, "text": "canonical correlation"}, {"st": 154, "ed": 156, "text": "canonical correlation"}]
[{"st": 32, "ed": 34, "text": "sufficient statistics"}, {"st": 68, "ed": 70, "text": "mutual information"}, {"st": 106, "ed": 108, "text": "multivariate gaussian"}, {"st": 121, "ed": 123, "text": "closed form"}, {"st": 126, "ed": 130, "text": "canonical correlation analysis cca"}]
[{"st": 15, "ed": 17, "text": "linear combinations"}, {"st": 47, "ed": 49, "text": "exponentially large"}, {"st": 82, "ed": 84, "text": "loss minimization"}, {"st": 86, "ed": 88, "text": "key idea"}, {"st": 94, "ed": 96, "text": "loss minimization"}, {"st": 112, "ed": 114, "text": "benchmark datasets"}, {"st": 115, "ed": 117, "text": "empirically demonstrate"}, {"st": 124, "ed": 126, "text": "computational efficiency"}, {"st": 126, "ed": 128, "text": "prediction accuracy"}]
[{"st": 1, "ed": 3, "text": "key challenge"}, {"st": 4, "ed": 6, "text": "online learning"}, {"st": 18, "ed": 20, "text": "recent studies"}, {"st": 27, "ed": 30, "text": "online learning algorithm"}, {"st": 56, "ed": 58, "text": "meta algorithm"}, {"st": 63, "ed": 65, "text": "regret bound"}, {"st": 106, "ed": 108, "text": "regret bound"}, {"st": 121, "ed": 123, "text": "parameter free"}, {"st": 127, "ed": 130, "text": "with expert advice"}, {"st": 166, "ed": 168, "text": "empirical results"}, {"st": 181, "ed": 184, "text": "with expert advice"}, {"st": 185, "ed": 187, "text": "metric learning"}]
[{"st": 0, "ed": 2, "text": "encoder decoder"}, {"st": 61, "ed": 64, "text": "and vice versa"}, {"st": 99, "ed": 101, "text": "real life"}, {"st": 107, "ed": 109, "text": "gan training"}, {"st": 113, "ed": 115, "text": "mode collapse"}, {"st": 120, "ed": 122, "text": "near optimal"}, {"st": 145, "ed": 147, "text": "encoder decoder"}, {"st": 164, "ed": 166, "text": "training objective"}]
[{"st": 1, "ed": 5, "text": "problems in machine learning"}, {"st": 20, "ed": 23, "text": "learning and inference"}, {"st": 25, "ed": 27, "text": "undirected models"}, {"st": 30, "ed": 32, "text": "variational approximation"}, {"st": 34, "ed": 36, "text": "log likelihood"}, {"st": 49, "ed": 52, "text": "log partition function"}, {"st": 90, "ed": 92, "text": "undirected models"}, {"st": 95, "ed": 97, "text": "variational inference"}, {"st": 99, "ed": 101, "text": "empirically demonstrate"}, {"st": 109, "ed": 111, "text": "generative modeling"}]
[{"st": 0, "ed": 2, "text": "generative adversarial"}, {"st": 10, "ed": 12, "text": "moment matching"}, {"st": 37, "ed": 39, "text": "true distribution"}, {"st": 68, "ed": 70, "text": "linear span"}, {"st": 88, "ed": 90, "text": "neural networks"}, {"st": 97, "ed": 99, "text": "generalization bounds"}, {"st": 104, "ed": 106, "text": "true distribution"}]
[{"st": 3, "ed": 5, "text": "linear regression"}, {"st": 28, "ed": 30, "text": "message passing"}, {"st": 53, "ed": 55, "text": "spin glass"}, {"st": 57, "ed": 59, "text": "asymptotic analysis"}, {"st": 75, "ed": 77, "text": "numerical experiments"}, {"st": 81, "ed": 83, "text": "sufficiently large"}, {"st": 100, "ed": 102, "text": "phase transition"}, {"st": 116, "ed": 118, "text": "parameter space"}, {"st": 164, "ed": 166, "text": "based methods"}, {"st": 192, "ed": 195, "text": "coordinate descent algorithm"}]
[{"st": 2, "ed": 4, "text": "becoming increasingly"}, {"st": 7, "ed": 9, "text": "machine learning"}, {"st": 22, "ed": 24, "text": "adversarial examples"}, {"st": 24, "ed": 26, "text": "previous studies"}, {"st": 33, "ed": 35, "text": "neural networks"}, {"st": 54, "ed": 56, "text": "adversarial examples"}, {"st": 64, "ed": 66, "text": "neural networks"}, {"st": 74, "ed": 76, "text": "functional form"}, {"st": 118, "ed": 120, "text": "power law"}, {"st": 147, "ed": 149, "text": "models including"}, {"st": 153, "ed": 155, "text": "deep networks"}, {"st": 155, "ed": 157, "text": "linear models"}, {"st": 195, "ed": 197, "text": "network architectures"}, {"st": 205, "ed": 207, "text": "neural architecture"}, {"st": 209, "ed": 211, "text": "reinforcement learning"}, {"st": 228, "ed": 230, "text": "black box"}]
[{"st": 4, "ed": 6, "text": "latent state"}, {"st": 11, "ed": 13, "text": "latent state"}, {"st": 26, "ed": 28, "text": "sampling algorithm"}]
[{"st": 84, "ed": 86, "text": "expert knowledge"}, {"st": 100, "ed": 103, "text": "theoretical and empirical"}, {"st": 106, "ed": 108, "text": "proposed method"}, {"st": 121, "ed": 125, "text": "synthetic and real data"}, {"st": 146, "ed": 148, "text": "large scale"}]
[{"st": 146, "ed": 148, "text": "numerical experiments"}, {"st": 149, "ed": 151, "text": "time series"}]
[{"st": 211, "ed": 214, "text": "deep neural network"}]
[{"st": 0, "ed": 2, "text": "deep learning"}, {"st": 12, "ed": 15, "text": "visual object recognition"}, {"st": 56, "ed": 58, "text": "online learning"}, {"st": 86, "ed": 88, "text": "convergence speed"}, {"st": 110, "ed": 112, "text": "convergence speed"}]
[{"st": 0, "ed": 2, "text": "transfer learning"}, {"st": 19, "ed": 21, "text": "source domain"}, {"st": 31, "ed": 33, "text": "multiple domains"}, {"st": 37, "ed": 39, "text": "labeled data"}, {"st": 89, "ed": 91, "text": "approach called"}, {"st": 93, "ed": 95, "text": "transfer learning"}, {"st": 113, "ed": 115, "text": "latent factors"}, {"st": 131, "ed": 133, "text": "nonnegative matrix"}, {"st": 137, "ed": 139, "text": "proposed approach"}, {"st": 158, "ed": 160, "text": "learning algorithm"}, {"st": 165, "ed": 167, "text": "empirical study"}]
[{"st": 15, "ed": 18, "text": "semi supervised learning"}, {"st": 67, "ed": 69, "text": "theoretical analysis"}, {"st": 85, "ed": 88, "text": "gaussian mixture models"}, {"st": 90, "ed": 92, "text": "real datasets"}, {"st": 114, "ed": 117, "text": "significant performance gains"}]
[{"st": 2, "ed": 4, "text": "theoretical guarantees"}, {"st": 6, "ed": 8, "text": "alternating minimization"}, {"st": 13, "ed": 15, "text": "sparse coding"}, {"st": 41, "ed": 43, "text": "x 1"}, {"st": 52, "ed": 54, "text": "alternating minimization"}, {"st": 58, "ed": 60, "text": "ell 1"}, {"st": 71, "ed": 73, "text": "alternating minimization"}, {"st": 89, "ed": 91, "text": "theoretical analyses"}, {"st": 100, "ed": 102, "text": "operator norm"}, {"st": 137, "ed": 139, "text": "convergence rates"}, {"st": 157, "ed": 159, "text": "random initialization"}, {"st": 172, "ed": 174, "text": "generative model"}, {"st": 205, "ed": 207, "text": "sample complexity"}]
[{"st": 0, "ed": 2, "text": "sparse coding"}, {"st": 10, "ed": 12, "text": "signal processing"}, {"st": 12, "ed": 14, "text": "deep learning"}, {"st": 16, "ed": 18, "text": "machine learning"}, {"st": 38, "ed": 40, "text": "key challenge"}, {"st": 101, "ed": 103, "text": "sparse coding"}, {"st": 108, "ed": 110, "text": "efficient implementation"}, {"st": 115, "ed": 117, "text": "theoretically analyze"}, {"st": 122, "ed": 124, "text": "sample complexity"}, {"st": 143, "ed": 146, "text": "computationally efficient algorithm"}, {"st": 148, "ed": 150, "text": "sparse coding"}, {"st": 162, "ed": 164, "text": "numerical experiments"}, {"st": 165, "ed": 167, "text": "simulated data"}]
[{"st": 2, "ed": 4, "text": "neural networks"}, {"st": 10, "ed": 12, "text": "neural network"}, {"st": 15, "ed": 17, "text": "external memory"}, {"st": 27, "ed": 29, "text": "neural networks"}, {"st": 31, "ed": 35, "text": "recurrent neural networks rnns"}, {"st": 73, "ed": 75, "text": "deep models"}, {"st": 76, "ed": 78, "text": "embedded systems"}, {"st": 96, "ed": 98, "text": "recent developments"}, {"st": 100, "ed": 102, "text": "multi layer"}, {"st": 103, "ed": 105, "text": "convolutional networks"}, {"st": 164, "ed": 166, "text": "content based"}, {"st": 173, "ed": 175, "text": "performance degradation"}, {"st": 201, "ed": 203, "text": "fixed point"}, {"st": 209, "ed": 211, "text": "floating point"}, {"st": 228, "ed": 230, "text": "error rate"}, {"st": 237, "ed": 239, "text": "fixed point"}]
[{"st": 18, "ed": 21, "text": "service level agreement"}, {"st": 79, "ed": 83, "text": "deep recurrent neural networks"}, {"st": 115, "ed": 117, "text": "real world"}]
[{"st": 4, "ed": 7, "text": "electronic health records"}, {"st": 13, "ed": 15, "text": "predictive models"}, {"st": 21, "ed": 23, "text": "time series"}, {"st": 26, "ed": 30, "text": "recurrent neural network rnn"}, {"st": 32, "ed": 37, "text": "long short term memory lstm"}, {"st": 38, "ed": 41, "text": "deep neural networks"}, {"st": 80, "ed": 82, "text": "attention mechanisms"}, {"st": 84, "ed": 86, "text": "remarkable success"}, {"st": 104, "ed": 106, "text": "attention models"}, {"st": 108, "ed": 110, "text": "time series"}, {"st": 130, "ed": 132, "text": "attention mechanism"}, {"st": 148, "ed": 150, "text": "multi task"}, {"st": 167, "ed": 169, "text": "benchmark datasets"}, {"st": 173, "ed": 175, "text": "proposed approach"}, {"st": 185, "ed": 187, "text": "lstm models"}, {"st": 191, "ed": 193, "text": "hand engineered"}]
[{"st": 20, "ed": 22, "text": "multimodal data"}, {"st": 24, "ed": 26, "text": "application domains"}, {"st": 46, "ed": 48, "text": "machine learning"}, {"st": 50, "ed": 52, "text": "recent research"}, {"st": 53, "ed": 55, "text": "deep learning"}, {"st": 56, "ed": 58, "text": "reinforcement learning"}, {"st": 62, "ed": 64, "text": "deep rl"}, {"st": 72, "ed": 74, "text": "computational power"}, {"st": 121, "ed": 123, "text": "deep rl"}, {"st": 152, "ed": 154, "text": "challenging research"}]
[{"st": 7, "ed": 10, "text": "few shot learning"}, {"st": 17, "ed": 19, "text": "partially observed"}, {"st": 19, "ed": 21, "text": "graphical model"}, {"st": 26, "ed": 28, "text": "input images"}, {"st": 39, "ed": 41, "text": "message passing"}, {"st": 41, "ed": 43, "text": "inference algorithms"}, {"st": 45, "ed": 47, "text": "neural network"}, {"st": 52, "ed": 54, "text": "neural network"}, {"st": 60, "ed": 62, "text": "recently proposed"}, {"st": 62, "ed": 65, "text": "few shot learning"}, {"st": 79, "ed": 82, "text": "few shot learning"}, {"st": 84, "ed": 86, "text": "semi supervised"}, {"st": 87, "ed": 89, "text": "active learning"}]
[{"st": 0, "ed": 3, "text": "electronic health records"}, {"st": 40, "ed": 42, "text": "prediction performance"}, {"st": 43, "ed": 47, "text": "generative adversarial networks gans"}, {"st": 49, "ed": 51, "text": "learning algorithms"}, {"st": 104, "ed": 107, "text": "significantly outperforms existing"}, {"st": 116, "ed": 118, "text": "missing values"}, {"st": 119, "ed": 121, "text": "class imbalance"}]
[{"st": 10, "ed": 12, "text": "imagenet dataset"}, {"st": 20, "ed": 22, "text": "computer vision"}, {"st": 41, "ed": 44, "text": "deep artificial neural"}, {"st": 76, "ed": 78, "text": "training process"}, {"st": 89, "ed": 91, "text": "representation learning"}, {"st": 93, "ed": 95, "text": "deep networks"}, {"st": 111, "ed": 113, "text": "computational requirements"}, {"st": 141, "ed": 143, "text": "resnet 50"}, {"st": 145, "ed": 147, "text": "large scale"}, {"st": 202, "ed": 204, "text": "resnet 50"}, {"st": 235, "ed": 237, "text": "resnet 50"}, {"st": 245, "ed": 247, "text": "resnet 50"}]
[{"st": 9, "ed": 12, "text": "deep neural networks"}, {"st": 16, "ed": 19, "text": "semi supervised learning"}, {"st": 31, "ed": 33, "text": "approach outperforms"}, {"st": 50, "ed": 52, "text": "labeled examples"}]
[{"st": 8, "ed": 11, "text": "kullback leibler kl"}, {"st": 12, "ed": 14, "text": "variational inference"}, {"st": 18, "ed": 20, "text": "variational dropout"}, {"st": 25, "ed": 27, "text": "variational bayes"}, {"st": 37, "ed": 40, "text": "evidence lower bound"}, {"st": 83, "ed": 85, "text": "variational dropout"}, {"st": 97, "ed": 99, "text": "variational dropout"}, {"st": 101, "ed": 103, "text": "variational dropout"}, {"st": 130, "ed": 132, "text": "variational inference"}, {"st": 154, "ed": 156, "text": "training error"}, {"st": 164, "ed": 167, "text": "evidence lower bound"}]
[{"st": 6, "ed": 8, "text": "parameter estimation"}, {"st": 9, "ed": 11, "text": "finite mixture"}, {"st": 23, "ed": 25, "text": "proposed framework"}, {"st": 29, "ed": 31, "text": "clustering methods"}, {"st": 44, "ed": 46, "text": "expectation maximization"}, {"st": 51, "ed": 53, "text": "bregman divergences"}, {"st": 56, "ed": 58, "text": "exponential family"}]
[{"st": 17, "ed": 19, "text": "small perturbations"}, {"st": 35, "ed": 37, "text": "adversarial examples"}, {"st": 85, "ed": 87, "text": "neural network"}, {"st": 116, "ed": 118, "text": "optimization algorithms"}, {"st": 126, "ed": 128, "text": "worst case"}, {"st": 136, "ed": 138, "text": "based attacks"}, {"st": 142, "ed": 144, "text": "gradient based"}]
[{"st": 17, "ed": 20, "text": "recurrent neural networks"}, {"st": 49, "ed": 51, "text": "rnn based"}, {"st": 53, "ed": 55, "text": "context specific"}, {"st": 64, "ed": 66, "text": "latent representations"}, {"st": 68, "ed": 70, "text": "local models"}, {"st": 111, "ed": 113, "text": "real world"}]
[{"st": 5, "ed": 8, "text": "deep neural networks"}, {"st": 33, "ed": 35, "text": "weight pruning"}, {"st": 81, "ed": 83, "text": "neural networks"}]
[{"st": 10, "ed": 13, "text": "deep neural networks"}, {"st": 33, "ed": 35, "text": "input output"}, {"st": 55, "ed": 57, "text": "singular values"}, {"st": 71, "ed": 73, "text": "deep linear"}, {"st": 81, "ed": 83, "text": "weight initialization"}, {"st": 112, "ed": 114, "text": "powerful tools"}, {"st": 128, "ed": 130, "text": "deep network"}, {"st": 131, "ed": 133, "text": "input output"}, {"st": 150, "ed": 152, "text": "weight initialization"}, {"st": 185, "ed": 187, "text": "demonstrate empirically"}, {"st": 195, "ed": 198, "text": "orders of magnitude"}, {"st": 213, "ed": 215, "text": "consistently outperform"}, {"st": 220, "ed": 222, "text": "analysis reveals"}, {"st": 229, "ed": 231, "text": "singular values"}]
[{"st": 0, "ed": 4, "text": "generative adversarial networks gans"}, {"st": 9, "ed": 11, "text": "deep generative"}, {"st": 22, "ed": 26, "text": "trained end to end"}, {"st": 27, "ed": 29, "text": "real valued"}, {"st": 70, "ed": 73, "text": "difficult to train"}, {"st": 75, "ed": 77, "text": "discrete data"}, {"st": 93, "ed": 95, "text": "reinforcement learning"}, {"st": 112, "ed": 114, "text": "gan framework"}, {"st": 142, "ed": 145, "text": "temporal difference td"}]
[{"st": 7, "ed": 9, "text": "organic chemist"}, {"st": 52, "ed": 54, "text": "prediction task"}, {"st": 67, "ed": 71, "text": "trained end to end"}, {"st": 114, "ed": 116, "text": "approach achieves"}]
[{"st": 1, "ed": 3, "text": "recent years"}, {"st": 109, "ed": 111, "text": "convex relaxation"}, {"st": 117, "ed": 121, "text": "synthetic and real data"}, {"st": 122, "ed": 124, "text": "promising results"}]
[{"st": 119, "ed": 123, "text": "generative adversarial networks gans"}, {"st": 147, "ed": 149, "text": "competitive results"}, {"st": 150, "ed": 153, "text": "semi supervised learning"}, {"st": 154, "ed": 156, "text": "cifar 10"}]
[{"st": 8, "ed": 10, "text": "machine learning"}, {"st": 10, "ed": 12, "text": "based methods"}, {"st": 13, "ed": 15, "text": "computational biology"}, {"st": 40, "ed": 42, "text": "supervised learning"}, {"st": 45, "ed": 47, "text": "instance level"}, {"st": 51, "ed": 54, "text": "multiple instance learning"}, {"st": 66, "ed": 68, "text": "large margin"}, {"st": 68, "ed": 70, "text": "classification methods"}, {"st": 71, "ed": 74, "text": "multiple instance learning"}, {"st": 91, "ed": 93, "text": "large margin"}, {"st": 95, "ed": 97, "text": "multiple instance"}, {"st": 188, "ed": 190, "text": "improved accuracy"}]
[{"st": 47, "ed": 49, "text": "convergence rate"}, {"st": 66, "ed": 68, "text": "proposed algorithm"}, {"st": 78, "ed": 80, "text": "convergence rate"}, {"st": 129, "ed": 131, "text": "large scale"}]
[{"st": 6, "ed": 8, "text": "machine learning"}, {"st": 112, "ed": 114, "text": "neural network"}, {"st": 116, "ed": 119, "text": "real life data"}, {"st": 132, "ed": 134, "text": "logistic regression"}]
[{"st": 6, "ed": 8, "text": "elastic net"}, {"st": 9, "ed": 12, "text": "canonical correlation analysis"}, {"st": 17, "ed": 19, "text": "unsupervised learning"}, {"st": 30, "ed": 32, "text": "elastic net"}, {"st": 83, "ed": 85, "text": "theoretical guarantees"}, {"st": 86, "ed": 89, "text": "takes advantage of"}, {"st": 113, "ed": 115, "text": "learning paradigm"}, {"st": 125, "ed": 127, "text": "feature selection"}, {"st": 183, "ed": 185, "text": "optimization problem"}, {"st": 205, "ed": 207, "text": "large scale"}, {"st": 208, "ed": 210, "text": "streaming data"}]
[{"st": 5, "ed": 7, "text": "metric learning"}, {"st": 15, "ed": 17, "text": "deep learning"}, {"st": 17, "ed": 19, "text": "tasks including"}, {"st": 19, "ed": 21, "text": "face recognition"}, {"st": 39, "ed": 41, "text": "feature map"}, {"st": 63, "ed": 65, "text": "classification loss"}, {"st": 116, "ed": 118, "text": "mode collapse"}]
[{"st": 1, "ed": 3, "text": "inverse problems"}, {"st": 10, "ed": 12, "text": "radiative transfer"}, {"st": 24, "ed": 26, "text": "remote sensing"}, {"st": 117, "ed": 119, "text": "nonparametric regression"}, {"st": 133, "ed": 136, "text": "taking into account"}, {"st": 147, "ed": 149, "text": "gaussian process"}, {"st": 173, "ed": 176, "text": "simulated and real"}, {"st": 187, "ed": 189, "text": "hyper parameter"}, {"st": 192, "ed": 194, "text": "gp model"}, {"st": 235, "ed": 238, "text": "simulated and real"}, {"st": 255, "ed": 257, "text": "simulated data"}]
[{"st": 20, "ed": 23, "text": "empirical risk minimization"}, {"st": 43, "ed": 45, "text": "cross validation"}, {"st": 56, "ed": 58, "text": "training set"}, {"st": 63, "ed": 65, "text": "validation set"}, {"st": 83, "ed": 85, "text": "cross validation"}, {"st": 91, "ed": 93, "text": "cross validation"}, {"st": 147, "ed": 149, "text": "computationally efficient"}, {"st": 153, "ed": 156, "text": "provide theoretical guarantees"}, {"st": 166, "ed": 168, "text": "optimization algorithm"}, {"st": 174, "ed": 177, "text": "empirical risk minimization"}, {"st": 180, "ed": 182, "text": "numerical experiments"}, {"st": 185, "ed": 188, "text": "accuracy and efficiency"}, {"st": 194, "ed": 196, "text": "proposed framework"}]
[{"st": 3, "ed": 5, "text": "class imbalance"}, {"st": 21, "ed": 23, "text": "supervised learning"}, {"st": 35, "ed": 37, "text": "cost function"}, {"st": 45, "ed": 47, "text": "imbalanced datasets"}, {"st": 60, "ed": 62, "text": "cost sensitive"}, {"st": 64, "ed": 66, "text": "ensemble based"}, {"st": 90, "ed": 92, "text": "cost sensitive"}, {"st": 109, "ed": 111, "text": "based method"}, {"st": 134, "ed": 136, "text": "instance based"}, {"st": 141, "ed": 143, "text": "weight update"}, {"st": 158, "ed": 160, "text": "imbalanced datasets"}, {"st": 164, "ed": 166, "text": "significant improvement"}]
[{"st": 6, "ed": 8, "text": "parametric models"}, {"st": 16, "ed": 18, "text": "machine learning"}, {"st": 19, "ed": 21, "text": "kernel methods"}, {"st": 29, "ed": 32, "text": "a reproducing kernel"}, {"st": 38, "ed": 40, "text": "linear models"}, {"st": 57, "ed": 59, "text": "training data"}, {"st": 64, "ed": 66, "text": "prior knowledge"}, {"st": 80, "ed": 82, "text": "computational complexity"}, {"st": 87, "ed": 90, "text": "end to end"}, {"st": 93, "ed": 95, "text": "task specific"}, {"st": 100, "ed": 103, "text": "deep neural networks"}, {"st": 106, "ed": 108, "text": "de facto"}, {"st": 110, "ed": 113, "text": "end to end"}, {"st": 127, "ed": 129, "text": "deep architectures"}, {"st": 136, "ed": 138, "text": "computational efficiency"}, {"st": 139, "ed": 142, "text": "end to end"}, {"st": 153, "ed": 155, "text": "optimization framework"}, {"st": 168, "ed": 170, "text": "deep learning"}, {"st": 172, "ed": 174, "text": "task specific"}, {"st": 196, "ed": 198, "text": "linear subspaces"}, {"st": 206, "ed": 208, "text": "dropout regularization"}, {"st": 230, "ed": 232, "text": "pre trained"}, {"st": 233, "ed": 235, "text": "kernel machines"}, {"st": 242, "ed": 244, "text": "case studies"}, {"st": 246, "ed": 248, "text": "training data"}, {"st": 251, "ed": 253, "text": "explicit feature"}]
[{"st": 2, "ed": 4, "text": "social network"}, {"st": 46, "ed": 48, "text": "accurately estimate"}, {"st": 101, "ed": 103, "text": "proposed framework"}, {"st": 143, "ed": 146, "text": "gaussian graphical models"}, {"st": 153, "ed": 155, "text": "iterative optimization"}]
[{"st": 8, "ed": 10, "text": "latent variable"}, {"st": 16, "ed": 19, "text": "recurrent neural networks"}, {"st": 22, "ed": 24, "text": "recurrent models"}, {"st": 34, "ed": 36, "text": "sequential data"}, {"st": 44, "ed": 46, "text": "recently proposed"}, {"st": 61, "ed": 63, "text": "latent variable"}, {"st": 79, "ed": 81, "text": "variational inference"}, {"st": 83, "ed": 85, "text": "approximate posterior"}, {"st": 101, "ed": 104, "text": "variational lower bound"}, {"st": 109, "ed": 111, "text": "latent variables"}, {"st": 131, "ed": 133, "text": "latent variables"}, {"st": 162, "ed": 164, "text": "conceptually simple"}, {"st": 182, "ed": 184, "text": "competitive performance"}, {"st": 209, "ed": 211, "text": "source code"}, {"st": 212, "ed": 214, "text": "https github.com"}]
[{"st": 7, "ed": 9, "text": "kernel functions"}, {"st": 15, "ed": 17, "text": "positive semidefinite"}, {"st": 79, "ed": 81, "text": "l 2"}, {"st": 90, "ed": 92, "text": "soft margin"}, {"st": 113, "ed": 115, "text": "training data"}, {"st": 122, "ed": 124, "text": "significantly outperform"}, {"st": 125, "ed": 127, "text": "kernel learning"}, {"st": 137, "ed": 139, "text": "kernel learning"}, {"st": 153, "ed": 156, "text": "multiple kernel learning"}]
[{"st": 6, "ed": 8, "text": "base classifiers"}, {"st": 36, "ed": 38, "text": "generalization error"}, {"st": 54, "ed": 56, "text": "key idea"}, {"st": 68, "ed": 70, "text": "base classifiers"}, {"st": 80, "ed": 82, "text": "beta distribution"}, {"st": 103, "ed": 105, "text": "randomly chosen"}, {"st": 105, "ed": 107, "text": "base classifiers"}, {"st": 120, "ed": 122, "text": "empirical evidence"}, {"st": 173, "ed": 175, "text": "great potential"}]
[{"st": 11, "ed": 14, "text": "black box optimization"}, {"st": 23, "ed": 25, "text": "active learning"}, {"st": 49, "ed": 51, "text": "continuous optimization"}, {"st": 65, "ed": 67, "text": "existing methods"}, {"st": 71, "ed": 73, "text": "continuous optimization"}, {"st": 73, "ed": 75, "text": "variational inference"}, {"st": 88, "ed": 90, "text": "wide variety"}, {"st": 113, "ed": 115, "text": "active learning"}]
[{"st": 4, "ed": 6, "text": "semi supervised"}, {"st": 6, "ed": 8, "text": "machine learning"}, {"st": 24, "ed": 26, "text": "variational inference"}, {"st": 38, "ed": 40, "text": "variational distribution"}, {"st": 55, "ed": 57, "text": "large scale"}, {"st": 75, "ed": 77, "text": "mean field"}, {"st": 77, "ed": 79, "text": "variational inference"}, {"st": 81, "ed": 83, "text": "recent advances"}, {"st": 126, "ed": 128, "text": "mean field"}, {"st": 143, "ed": 145, "text": "latent variables"}]
[{"st": 0, "ed": 3, "text": "recurrent neural networks"}, {"st": 5, "ed": 8, "text": "short term memory"}, {"st": 84, "ed": 86, "text": "lstm architecture"}, {"st": 127, "ed": 130, "text": "variational lower bound"}]
[{"st": 25, "ed": 27, "text": "crop yield"}, {"st": 31, "ed": 33, "text": "decision making"}, {"st": 40, "ed": 42, "text": "machine learning"}, {"st": 45, "ed": 47, "text": "crop yield"}, {"st": 74, "ed": 76, "text": "decision making"}, {"st": 110, "ed": 112, "text": "prediction model"}, {"st": 164, "ed": 166, "text": "decision making"}]
[{"st": 6, "ed": 9, "text": "deep neural networks"}, {"st": 12, "ed": 14, "text": "challenging problem"}, {"st": 17, "ed": 19, "text": "increasing attention"}, {"st": 53, "ed": 55, "text": "empirical comparison"}, {"st": 90, "ed": 92, "text": "unified framework"}, {"st": 135, "ed": 138, "text": "image and text"}]
[{"st": 4, "ed": 7, "text": "unsupervised domain adaptation"}, {"st": 8, "ed": 10, "text": "neural networks"}, {"st": 17, "ed": 19, "text": "based regularization"}, {"st": 25, "ed": 27, "text": "based regularization"}, {"st": 29, "ed": 31, "text": "domain invariant"}, {"st": 31, "ed": 33, "text": "latent feature"}, {"st": 41, "ed": 43, "text": "domain specific"}, {"st": 79, "ed": 81, "text": "dual space"}, {"st": 99, "ed": 102, "text": "standard benchmark datasets"}, {"st": 103, "ed": 105, "text": "sentiment analysis"}, {"st": 106, "ed": 108, "text": "object recognition"}]
[{"st": 12, "ed": 14, "text": "deep models"}, {"st": 23, "ed": 25, "text": "deep models"}, {"st": 43, "ed": 45, "text": "time series"}, {"st": 59, "ed": 61, "text": "decision trees"}, {"st": 100, "ed": 102, "text": "without sacrificing"}]
[{"st": 87, "ed": 89, "text": "prohibitively expensive"}, {"st": 104, "ed": 106, "text": "developing countries"}, {"st": 124, "ed": 127, "text": "deep neural network"}, {"st": 130, "ed": 132, "text": "decision support"}, {"st": 154, "ed": 156, "text": "feature engineering"}, {"st": 226, "ed": 228, "text": "proposed approach"}, {"st": 243, "ed": 246, "text": "task of classifying"}, {"st": 256, "ed": 258, "text": "significantly higher"}]
[{"st": 3, "ed": 5, "text": "batch size"}, {"st": 12, "ed": 14, "text": "time consuming"}, {"st": 35, "ed": 38, "text": "multi armed bandit"}, {"st": 42, "ed": 44, "text": "in grid"}, {"st": 49, "ed": 51, "text": "batch size"}, {"st": 72, "ed": 74, "text": "batch size"}, {"st": 79, "ed": 81, "text": "batch size"}, {"st": 92, "ed": 94, "text": "batch size"}, {"st": 102, "ed": 104, "text": "batch size"}, {"st": 136, "ed": 138, "text": "learning process"}, {"st": 145, "ed": 147, "text": "batch size"}]
[{"st": 4, "ed": 7, "text": "artificial neural networks"}, {"st": 11, "ed": 14, "text": "partial differential equations"}, {"st": 31, "ed": 33, "text": "cost function"}, {"st": 37, "ed": 39, "text": "network parameters"}, {"st": 71, "ed": 74, "text": "feedforward neural networks"}, {"st": 77, "ed": 80, "text": "gradient based optimization"}, {"st": 86, "ed": 88, "text": "quasi newton"}, {"st": 98, "ed": 101, "text": "feedforward neural networks"}, {"st": 108, "ed": 111, "text": "deep neural network"}, {"st": 125, "ed": 127, "text": "neural networks"}]
[{"st": 13, "ed": 15, "text": "machine learning"}, {"st": 16, "ed": 18, "text": "deep learning"}, {"st": 26, "ed": 28, "text": "artificial intelligence"}]
[{"st": 9, "ed": 11, "text": "decision makers"}, {"st": 82, "ed": 84, "text": "learning framework"}, {"st": 94, "ed": 96, "text": "decision making"}, {"st": 100, "ed": 102, "text": "learning algorithm"}, {"st": 110, "ed": 112, "text": "decision makers"}, {"st": 117, "ed": 119, "text": "real world"}]
[{"st": 5, "ed": 9, "text": "recurrent neural network architecture"}, {"st": 10, "ed": 12, "text": "achieves comparable"}, {"st": 33, "ed": 35, "text": "efficient learning"}, {"st": 52, "ed": 54, "text": "update rule"}, {"st": 63, "ed": 65, "text": "learning dynamics"}, {"st": 70, "ed": 72, "text": "input output"}]
[{"st": 0, "ed": 2, "text": "additive models"}, {"st": 14, "ed": 17, "text": "classification and regression"}, {"st": 53, "ed": 55, "text": "technique called"}, {"st": 62, "ed": 64, "text": "decision tree"}, {"st": 101, "ed": 103, "text": "predictive performance"}, {"st": 114, "ed": 116, "text": "decision trees"}]
[{"st": 1, "ed": 3, "text": "multi class"}, {"st": 76, "ed": 78, "text": "analysis reveals"}, {"st": 91, "ed": 93, "text": "source code"}]
[{"st": 10, "ed": 12, "text": "neural network"}, {"st": 42, "ed": 44, "text": "previous approaches"}, {"st": 49, "ed": 51, "text": "large networks"}, {"st": 60, "ed": 62, "text": "floating point"}, {"st": 77, "ed": 79, "text": "network architectures"}, {"st": 81, "ed": 83, "text": "wide variety"}]
[{"st": 0, "ed": 2, "text": "deep learning"}, {"st": 10, "ed": 12, "text": "multiple layers"}, {"st": 25, "ed": 28, "text": "gaussian mixture models"}, {"st": 34, "ed": 37, "text": "gaussian mixture model"}, {"st": 42, "ed": 44, "text": "multiple layers"}, {"st": 45, "ed": 47, "text": "latent variables"}, {"st": 62, "ed": 64, "text": "mixture model"}, {"st": 72, "ed": 74, "text": "linear models"}]
[{"st": 7, "ed": 9, "text": "matrix factorization"}, {"st": 23, "ed": 25, "text": "gaussian processes"}, {"st": 33, "ed": 35, "text": "gaussian processes"}, {"st": 45, "ed": 47, "text": "kernel matrix"}, {"st": 52, "ed": 54, "text": "hyper parameters"}, {"st": 58, "ed": 60, "text": "main contributions"}, {"st": 77, "ed": 79, "text": "kernel matrix"}, {"st": 81, "ed": 83, "text": "matrix factorization"}, {"st": 98, "ed": 101, "text": "accuracy and computational"}, {"st": 104, "ed": 106, "text": "proposed method"}, {"st": 117, "ed": 119, "text": "matrix factorization"}, {"st": 128, "ed": 130, "text": "gaussian process"}, {"st": 132, "ed": 134, "text": "finite set"}, {"st": 148, "ed": 150, "text": "kernel functions"}]
[{"st": 9, "ed": 11, "text": "convergence analysis"}, {"st": 30, "ed": 32, "text": "regularization terms"}, {"st": 33, "ed": 35, "text": "ell 1"}, {"st": 36, "ed": 38, "text": "ell 2"}, {"st": 50, "ed": 52, "text": "loss function"}, {"st": 65, "ed": 67, "text": "weight vector"}, {"st": 71, "ed": 73, "text": "optimal solution"}, {"st": 80, "ed": 82, "text": "loss function"}, {"st": 113, "ed": 115, "text": "numerical experiments"}]
[{"st": 9, "ed": 11, "text": "error correction"}, {"st": 42, "ed": 44, "text": "neural networks"}]
[{"st": 1, "ed": 3, "text": "variational autoencoder"}, {"st": 18, "ed": 20, "text": "latent variables"}, {"st": 60, "ed": 62, "text": "sequential data"}, {"st": 70, "ed": 73, "text": "recurrent neural network"}, {"st": 134, "ed": 136, "text": "lstm models"}]
[{"st": 84, "ed": 86, "text": "learning algorithm"}, {"st": 166, "ed": 168, "text": "technical analysis"}, {"st": 187, "ed": 189, "text": "real world"}]
[{"st": 52, "ed": 54, "text": "multiple labels"}, {"st": 67, "ed": 69, "text": "log loss"}, {"st": 79, "ed": 82, "text": "cross entropy loss"}, {"st": 104, "ed": 107, "text": "multi task learning"}, {"st": 112, "ed": 114, "text": "supervised learning"}, {"st": 118, "ed": 120, "text": "prediction models"}, {"st": 122, "ed": 124, "text": "related tasks"}, {"st": 125, "ed": 127, "text": "learned jointly"}, {"st": 139, "ed": 141, "text": "iterative algorithm"}, {"st": 148, "ed": 150, "text": "global convergence"}, {"st": 178, "ed": 181, "text": "taking into account"}, {"st": 189, "ed": 191, "text": "empirical results"}, {"st": 202, "ed": 204, "text": "excess risk"}, {"st": 220, "ed": 222, "text": "text categorization"}]
[{"st": 5, "ed": 7, "text": "inference algorithm"}, {"st": 22, "ed": 24, "text": "dependency structure"}, {"st": 39, "ed": 41, "text": "kernel functions"}, {"st": 71, "ed": 73, "text": "theoretical analysis"}, {"st": 98, "ed": 100, "text": "conditional distributions"}, {"st": 108, "ed": 110, "text": "empirical results"}, {"st": 113, "ed": 115, "text": "method outperforms"}, {"st": 124, "ed": 126, "text": "message passing"}]
[{"st": 1, "ed": 3, "text": "machine learning"}, {"st": 22, "ed": 24, "text": "low dimensional"}, {"st": 32, "ed": 34, "text": "semi definite"}, {"st": 37, "ed": 39, "text": "kernel matrix"}, {"st": 44, "ed": 46, "text": "semi definite"}, {"st": 49, "ed": 51, "text": "low rank"}, {"st": 66, "ed": 68, "text": "infinite dimensional"}, {"st": 72, "ed": 74, "text": "semi definite"}, {"st": 106, "ed": 108, "text": "nystr om"}, {"st": 117, "ed": 119, "text": "kernel matrix"}, {"st": 128, "ed": 130, "text": "semi definite"}]
[{"st": 15, "ed": 17, "text": "semi supervised"}, {"st": 103, "ed": 107, "text": "synthetic and real data"}]
[{"st": 0, "ed": 4, "text": "conditional generative adversarial networks"}, {"st": 6, "ed": 8, "text": "generative models"}, {"st": 17, "ed": 19, "text": "latent variables"}, {"st": 40, "ed": 42, "text": "generation process"}, {"st": 57, "ed": 59, "text": "trained jointly"}]
[{"st": 0, "ed": 3, "text": "graph structured data"}, {"st": 8, "ed": 10, "text": "social networks"}, {"st": 21, "ed": 23, "text": "neural networks"}, {"st": 36, "ed": 38, "text": "neural network"}, {"st": 45, "ed": 47, "text": "existing works"}, {"st": 60, "ed": 64, "text": "recurrent neural networks rnns"}, {"st": 83, "ed": 86, "text": "convolutional neural network"}, {"st": 102, "ed": 104, "text": "learning problems"}, {"st": 110, "ed": 112, "text": "deep networks"}, {"st": 125, "ed": 127, "text": "multi layer"}, {"st": 164, "ed": 167, "text": "ability to learn"}, {"st": 187, "ed": 189, "text": "network science"}, {"st": 193, "ed": 195, "text": "semi supervised"}, {"st": 201, "ed": 203, "text": "numerical results"}]
[{"st": 0, "ed": 2, "text": "recent results"}, {"st": 36, "ed": 38, "text": "group level"}, {"st": 137, "ed": 139, "text": "dependency structure"}, {"st": 141, "ed": 143, "text": "statistically significant"}, {"st": 147, "ed": 149, "text": "theoretically analyze"}, {"st": 152, "ed": 154, "text": "error rate"}]
[{"st": 5, "ed": 7, "text": "learning framework"}, {"st": 13, "ed": 15, "text": "learning process"}, {"st": 33, "ed": 35, "text": "bandit problem"}, {"st": 38, "ed": 40, "text": "cost function"}, {"st": 58, "ed": 60, "text": "o sqrt"}, {"st": 63, "ed": 65, "text": "regret bound"}, {"st": 66, "ed": 68, "text": "strong convexity"}, {"st": 87, "ed": 89, "text": "convex optimization"}, {"st": 100, "ed": 102, "text": "convex optimization"}, {"st": 110, "ed": 112, "text": "convergence rate"}, {"st": 113, "ed": 115, "text": "convex optimization"}, {"st": 117, "ed": 119, "text": "optimal regret"}]
[{"st": 7, "ed": 9, "text": "machine learning"}, {"st": 15, "ed": 17, "text": "linear regression"}, {"st": 17, "ed": 19, "text": "multilayer perceptron"}, {"st": 20, "ed": 22, "text": "nearest neighbor"}, {"st": 27, "ed": 30, "text": "support vector machine"}, {"st": 35, "ed": 37, "text": "breast cancer"}, {"st": 96, "ed": 98, "text": "training phase"}, {"st": 105, "ed": 107, "text": "hyper parameters"}]
[{"st": 4, "ed": 6, "text": "computational chemistry"}, {"st": 30, "ed": 32, "text": "deep learning"}, {"st": 50, "ed": 52, "text": "latent space"}, {"st": 52, "ed": 55, "text": "and vice versa"}, {"st": 68, "ed": 70, "text": "latent space"}, {"st": 87, "ed": 89, "text": "latent space"}, {"st": 118, "ed": 120, "text": "training set"}]
[{"st": 6, "ed": 8, "text": "important role"}, {"st": 9, "ed": 11, "text": "machine learning"}, {"st": 16, "ed": 18, "text": "annotated data"}, {"st": 40, "ed": 42, "text": "missing values"}, {"st": 57, "ed": 59, "text": "missing values"}, {"st": 64, "ed": 66, "text": "feature space"}, {"st": 80, "ed": 82, "text": "temporal dynamics"}, {"st": 95, "ed": 97, "text": "significantly outperforms"}, {"st": 103, "ed": 105, "text": "air quality"}]
[{"st": 7, "ed": 9, "text": "pure exploration"}, {"st": 72, "ed": 74, "text": "fixed budget"}, {"st": 134, "ed": 136, "text": "linear programming"}]
[{"st": 6, "ed": 9, "text": "task of classifying"}, {"st": 11, "ed": 13, "text": "positive unlabeled"}, {"st": 17, "ed": 19, "text": "discriminative learning"}, {"st": 37, "ed": 39, "text": "decision boundary"}, {"st": 65, "ed": 69, "text": "generative adversarial networks gans"}, {"st": 72, "ed": 74, "text": "positive unlabeled"}, {"st": 99, "ed": 102, "text": "positive and negative"}, {"st": 149, "ed": 151, "text": "generated samples"}, {"st": 154, "ed": 157, "text": "deep neural networks"}, {"st": 169, "ed": 171, "text": "semi supervised"}]
[{"st": 0, "ed": 4, "text": "convolutional neural networks cnn"}, {"st": 20, "ed": 22, "text": "receptive fields"}, {"st": 30, "ed": 32, "text": "face verification"}, {"st": 32, "ed": 35, "text": "visual question answering"}, {"st": 49, "ed": 51, "text": "neural network"}, {"st": 56, "ed": 58, "text": "main idea"}, {"st": 62, "ed": 64, "text": "weight matrix"}, {"st": 86, "ed": 88, "text": "receptive fields"}, {"st": 107, "ed": 109, "text": "gaussian function"}, {"st": 127, "ed": 129, "text": "content information"}, {"st": 141, "ed": 143, "text": "gaussian function"}]
[{"st": 32, "ed": 34, "text": "differential geometry"}, {"st": 75, "ed": 78, "text": "semi supervised learning"}, {"st": 139, "ed": 142, "text": "semi supervised learning"}]
[{"st": 0, "ed": 2, "text": "deep learning"}, {"st": 4, "ed": 8, "text": "vulnerable to adversarial examples"}, {"st": 59, "ed": 61, "text": "neural networks"}, {"st": 88, "ed": 90, "text": "neural networks"}, {"st": 96, "ed": 98, "text": "adversarial perturbations"}, {"st": 114, "ed": 116, "text": "neural networks"}]
[{"st": 14, "ed": 16, "text": "user preferences"}, {"st": 20, "ed": 22, "text": "problems involving"}, {"st": 42, "ed": 45, "text": "constrained optimization problem"}, {"st": 70, "ed": 72, "text": "user preferences"}, {"st": 177, "ed": 179, "text": "empirical results"}]
[{"st": 4, "ed": 6, "text": "representation learning"}, {"st": 16, "ed": 18, "text": "low dimensional"}, {"st": 22, "ed": 24, "text": "representation learning"}, {"st": 31, "ed": 33, "text": "generative models"}, {"st": 43, "ed": 45, "text": "discriminative models"}, {"st": 66, "ed": 68, "text": "representation learning"}, {"st": 78, "ed": 80, "text": "generative model"}, {"st": 81, "ed": 83, "text": "discriminative model"}, {"st": 95, "ed": 97, "text": "generative model"}, {"st": 116, "ed": 118, "text": "discriminative model"}, {"st": 120, "ed": 122, "text": "discriminative model"}, {"st": 131, "ed": 133, "text": "ground truth"}, {"st": 162, "ed": 164, "text": "generative model"}, {"st": 183, "ed": 185, "text": "desirable properties"}, {"st": 194, "ed": 196, "text": "extensive experiments"}, {"st": 197, "ed": 199, "text": "real world"}, {"st": 211, "ed": 213, "text": "applications including"}, {"st": 213, "ed": 215, "text": "link prediction"}, {"st": 215, "ed": 217, "text": "node classification"}]
[{"st": 5, "ed": 8, "text": "supply chain management"}, {"st": 47, "ed": 49, "text": "prediction methods"}, {"st": 70, "ed": 72, "text": "extreme weather"}, {"st": 90, "ed": 92, "text": "accurately predict"}, {"st": 171, "ed": 173, "text": "extreme weather"}, {"st": 178, "ed": 180, "text": "time consuming"}, {"st": 222, "ed": 225, "text": "artificial neural network"}]
[{"st": 0, "ed": 2, "text": "variational inference"}, {"st": 3, "ed": 5, "text": "latent variable"}, {"st": 10, "ed": 12, "text": "machine learning"}, {"st": 18, "ed": 21, "text": "evidence lower bound"}, {"st": 39, "ed": 41, "text": "variational distribution"}, {"st": 76, "ed": 78, "text": "random variables"}, {"st": 80, "ed": 82, "text": "variational distribution"}, {"st": 95, "ed": 97, "text": "inference network"}, {"st": 107, "ed": 109, "text": "latent variables"}, {"st": 124, "ed": 126, "text": "maximum likelihood"}, {"st": 131, "ed": 133, "text": "proposed approach"}]
[{"st": 3, "ed": 5, "text": "las vegas"}, {"st": 7, "ed": 11, "text": "markov chain monte carlo"}, {"st": 14, "ed": 18, "text": "restricted boltzmann machines rbms"}, {"st": 25, "ed": 27, "text": "las vegas"}, {"st": 47, "ed": 49, "text": "training data"}, {"st": 84, "ed": 86, "text": "contrastive divergence"}, {"st": 99, "ed": 101, "text": "mnist dataset"}]
[{"st": 1, "ed": 3, "text": "autoregressive models"}, {"st": 18, "ed": 20, "text": "d dimensional"}, {"st": 31, "ed": 33, "text": "conditional distributions"}, {"st": 56, "ed": 58, "text": "partially observed"}, {"st": 66, "ed": 68, "text": "training procedure"}, {"st": 78, "ed": 80, "text": "partially observed"}, {"st": 80, "ed": 82, "text": "input vector"}, {"st": 109, "ed": 111, "text": "provide evidence"}, {"st": 116, "ed": 118, "text": "training procedure"}, {"st": 148, "ed": 150, "text": "conditional distributions"}, {"st": 159, "ed": 161, "text": "conditional distributions"}, {"st": 179, "ed": 181, "text": "prior knowledge"}, {"st": 200, "ed": 202, "text": "standard datasets"}]
[{"st": 0, "ed": 4, "text": "generative adversarial networks gans"}, {"st": 14, "ed": 17, "text": "difficult to train"}, {"st": 44, "ed": 46, "text": "latent space"}, {"st": 62, "ed": 65, "text": "generative adversarial networks"}, {"st": 69, "ed": 71, "text": "gan framework"}, {"st": 102, "ed": 104, "text": "latent space"}, {"st": 135, "ed": 137, "text": "image generation"}]
[{"st": 7, "ed": 9, "text": "latent variable"}, {"st": 119, "ed": 121, "text": "approximate posterior"}, {"st": 121, "ed": 123, "text": "inference algorithms"}, {"st": 125, "ed": 127, "text": "variational inference"}, {"st": 154, "ed": 156, "text": "bayesian nonparametric"}, {"st": 165, "ed": 167, "text": "sampling algorithm"}, {"st": 173, "ed": 175, "text": "monte carlo"}, {"st": 184, "ed": 187, "text": "mixture of experts"}, {"st": 190, "ed": 192, "text": "latent feature"}, {"st": 199, "ed": 202, "text": "effectiveness and efficiency"}]
[{"st": 9, "ed": 11, "text": "neural networks"}, {"st": 34, "ed": 36, "text": "fully connected"}, {"st": 40, "ed": 42, "text": "hidden layer"}, {"st": 44, "ed": 46, "text": "weight distribution"}, {"st": 47, "ed": 49, "text": "activation function"}, {"st": 111, "ed": 113, "text": "deep networks"}, {"st": 119, "ed": 121, "text": "fixed point"}, {"st": 141, "ed": 143, "text": "weight initialization"}, {"st": 148, "ed": 150, "text": "neural network"}]
[{"st": 38, "ed": 40, "text": "qualitative analysis"}]
[{"st": 12, "ed": 14, "text": "variational autoencoder"}]
[{"st": 15, "ed": 18, "text": "image and video"}, {"st": 29, "ed": 31, "text": "feature representations"}, {"st": 49, "ed": 52, "text": "factors of variation"}, {"st": 68, "ed": 70, "text": "supervised classification"}, {"st": 75, "ed": 77, "text": "labeled samples"}, {"st": 82, "ed": 84, "text": "unlabeled samples"}, {"st": 111, "ed": 114, "text": "end to end"}, {"st": 118, "ed": 120, "text": "variational autoencoders"}, {"st": 124, "ed": 126, "text": "experimentally demonstrate"}, {"st": 138, "ed": 140, "text": "classification performance"}, {"st": 141, "ed": 143, "text": "competitive results"}]
[{"st": 0, "ed": 2, "text": "feature selection"}, {"st": 9, "ed": 12, "text": "discrete random variables"}, {"st": 44, "ed": 47, "text": "gaussian mixture models"}, {"st": 50, "ed": 52, "text": "low order"}, {"st": 87, "ed": 90, "text": "goodness of fit"}, {"st": 97, "ed": 99, "text": "real data"}]
[{"st": 3, "ed": 7, "text": "deep reinforcement learning rl"}, {"st": 27, "ed": 29, "text": "rl agent"}, {"st": 101, "ed": 104, "text": "deep q network"}, {"st": 178, "ed": 180, "text": "supervised learning"}, {"st": 194, "ed": 196, "text": "prediction accuracy"}, {"st": 230, "ed": 232, "text": "rl agent"}, {"st": 247, "ed": 249, "text": "prediction accuracy"}]
[{"st": 7, "ed": 9, "text": "active learning"}, {"st": 41, "ed": 43, "text": "active learning"}, {"st": 49, "ed": 51, "text": "recent advances"}, {"st": 70, "ed": 72, "text": "multivariate data"}, {"st": 81, "ed": 83, "text": "recent works"}, {"st": 98, "ed": 100, "text": "near optimal"}, {"st": 102, "ed": 104, "text": "prior knowledge"}]
[{"st": 1, "ed": 3, "text": "representation learning"}, {"st": 8, "ed": 10, "text": "learned representations"}, {"st": 17, "ed": 19, "text": "training data"}, {"st": 43, "ed": 45, "text": "weight vectors"}, {"st": 72, "ed": 74, "text": "neural networks"}, {"st": 76, "ed": 78, "text": "sparse coding"}, {"st": 96, "ed": 98, "text": "weight vectors"}]
[{"st": 14, "ed": 16, "text": "network science"}, {"st": 46, "ed": 48, "text": "kernel based"}, {"st": 49, "ed": 51, "text": "kalman filter"}, {"st": 55, "ed": 57, "text": "spatio temporal"}, {"st": 60, "ed": 62, "text": "efficient online"}, {"st": 70, "ed": 72, "text": "kernel based"}, {"st": 72, "ed": 74, "text": "learning framework"}, {"st": 111, "ed": 113, "text": "multi kernel"}, {"st": 133, "ed": 135, "text": "linear span"}, {"st": 142, "ed": 144, "text": "multi kernel"}, {"st": 144, "ed": 146, "text": "learning algorithm"}, {"st": 151, "ed": 153, "text": "kernel matrices"}, {"st": 160, "ed": 164, "text": "synthetic and real data"}]
[{"st": 13, "ed": 15, "text": "training distribution"}, {"st": 29, "ed": 31, "text": "real world"}, {"st": 31, "ed": 33, "text": "machine learning"}, {"st": 39, "ed": 42, "text": "deep neural networks"}, {"st": 73, "ed": 75, "text": "pre trained"}, {"st": 81, "ed": 83, "text": "prior works"}, {"st": 113, "ed": 115, "text": "inference algorithms"}, {"st": 131, "ed": 133, "text": "cross entropy"}, {"st": 158, "ed": 160, "text": "training samples"}, {"st": 174, "ed": 176, "text": "neural networks"}, {"st": 185, "ed": 189, "text": "deep convolutional neural networks"}]
[{"st": 9, "ed": 11, "text": "genetic algorithm"}, {"st": 14, "ed": 17, "text": "support vector machine"}, {"st": 43, "ed": 45, "text": "motion features"}, {"st": 51, "ed": 53, "text": "human skeleton"}, {"st": 67, "ed": 69, "text": "highly efficient"}, {"st": 84, "ed": 86, "text": "human skeleton"}, {"st": 107, "ed": 109, "text": "classification accuracy"}, {"st": 119, "ed": 121, "text": "genetic algorithm"}, {"st": 147, "ed": 149, "text": "proposed approach"}]
[{"st": 6, "ed": 8, "text": "classification problem"}, {"st": 19, "ed": 21, "text": "true labels"}, {"st": 22, "ed": 24, "text": "complementary labels"}, {"st": 75, "ed": 77, "text": "complementary labels"}, {"st": 84, "ed": 86, "text": "complementary labels"}, {"st": 88, "ed": 90, "text": "transition probabilities"}, {"st": 112, "ed": 114, "text": "previous methods"}, {"st": 118, "ed": 120, "text": "transition probabilities"}, {"st": 154, "ed": 156, "text": "prairie dog"}, {"st": 158, "ed": 160, "text": "complementary labels"}, {"st": 179, "ed": 181, "text": "transition probabilities"}, {"st": 190, "ed": 192, "text": "fundamental problems"}, {"st": 204, "ed": 206, "text": "transition probabilities"}, {"st": 212, "ed": 214, "text": "loss functions"}, {"st": 217, "ed": 220, "text": "deep neural network"}, {"st": 225, "ed": 227, "text": "complementary labels"}, {"st": 235, "ed": 237, "text": "complementary labels"}, {"st": 239, "ed": 241, "text": "proposed method"}, {"st": 250, "ed": 252, "text": "true labels"}, {"st": 261, "ed": 263, "text": "empirically validate"}, {"st": 267, "ed": 269, "text": "proposed method"}]
[{"st": 1, "ed": 3, "text": "clustering algorithms"}, {"st": 4, "ed": 6, "text": "massive data"}, {"st": 68, "ed": 70, "text": "clustering problems"}, {"st": 92, "ed": 94, "text": "clustering problems"}]
[{"st": 0, "ed": 2, "text": "kernel methods"}, {"st": 20, "ed": 22, "text": "kernel methods"}, {"st": 37, "ed": 39, "text": "kernel methods"}, {"st": 42, "ed": 44, "text": "large scale"}, {"st": 45, "ed": 48, "text": "random fourier features"}, {"st": 55, "ed": 57, "text": "kernel method"}, {"st": 59, "ed": 61, "text": "large scale"}, {"st": 64, "ed": 66, "text": "kernel function"}, {"st": 95, "ed": 97, "text": "training data"}, {"st": 134, "ed": 137, "text": "scale to large"}, {"st": 144, "ed": 146, "text": "kernel approximation"}, {"st": 155, "ed": 158, "text": "random fourier features"}, {"st": 174, "ed": 176, "text": "proposed approach"}, {"st": 179, "ed": 182, "text": "classification and regression"}, {"st": 189, "ed": 191, "text": "random features"}]
[{"st": 3, "ed": 5, "text": "detection method"}, {"st": 14, "ed": 16, "text": "neural network"}, {"st": 24, "ed": 26, "text": "input space"}]
[{"st": 5, "ed": 7, "text": "public health"}, {"st": 11, "ed": 13, "text": "machine learning"}, {"st": 38, "ed": 41, "text": "electronic health records"}, {"st": 50, "ed": 52, "text": "neural network"}, {"st": 63, "ed": 65, "text": "binary classification"}, {"st": 111, "ed": 113, "text": "neural networks"}]
[{"st": 0, "ed": 2, "text": "large scale"}, {"st": 2, "ed": 4, "text": "gaussian process"}, {"st": 14, "ed": 16, "text": "space complexity"}, {"st": 25, "ed": 27, "text": "gaussian process"}, {"st": 33, "ed": 35, "text": "large scale"}, {"st": 57, "ed": 59, "text": "gaussian process"}, {"st": 87, "ed": 89, "text": "variational inference"}, {"st": 101, "ed": 103, "text": "space complexity"}, {"st": 130, "ed": 132, "text": "large scale"}, {"st": 133, "ed": 135, "text": "gaussian process"}, {"st": 151, "ed": 153, "text": "outperforms previous"}, {"st": 155, "ed": 157, "text": "gaussian process"}]
[{"st": 1, "ed": 3, "text": "training data"}, {"st": 10, "ed": 12, "text": "machine learning"}, {"st": 61, "ed": 63, "text": "ground truth"}, {"st": 67, "ed": 70, "text": "end to end"}, {"st": 73, "ed": 75, "text": "recently proposed"}, {"st": 75, "ed": 77, "text": "machine learning"}, {"st": 107, "ed": 109, "text": "user study"}, {"st": 118, "ed": 120, "text": "predictive performance"}, {"st": 161, "ed": 165, "text": "department of veterans affairs"}, {"st": 175, "ed": 177, "text": "open source"}, {"st": 177, "ed": 180, "text": "text and image"}, {"st": 192, "ed": 194, "text": "predictive performance"}, {"st": 206, "ed": 208, "text": "predictive performance"}]
[{"st": 9, "ed": 11, "text": "representation learning"}, {"st": 42, "ed": 44, "text": "deep learning"}, {"st": 94, "ed": 96, "text": "data model"}, {"st": 131, "ed": 134, "text": "recurrent neural network"}, {"st": 148, "ed": 150, "text": "prediction task"}, {"st": 180, "ed": 182, "text": "https github.com"}]
[{"st": 11, "ed": 13, "text": "material science"}, {"st": 19, "ed": 21, "text": "learning representations"}, {"st": 25, "ed": 27, "text": "semi supervised"}, {"st": 36, "ed": 38, "text": "feature extraction"}, {"st": 61, "ed": 65, "text": "unsupervised and semi supervised"}, {"st": 69, "ed": 71, "text": "recently proposed"}, {"st": 77, "ed": 79, "text": "message passing"}, {"st": 81, "ed": 83, "text": "hierarchical representations"}, {"st": 97, "ed": 99, "text": "outperforms existing"}, {"st": 110, "ed": 112, "text": "semi supervised"}, {"st": 114, "ed": 116, "text": "predictive performance"}]
[{"st": 2, "ed": 4, "text": "optimal policy"}, {"st": 6, "ed": 8, "text": "multi modal"}, {"st": 8, "ed": 10, "text": "reward function"}, {"st": 12, "ed": 14, "text": "challenging problem"}, {"st": 15, "ed": 17, "text": "reinforcement learning"}, {"st": 45, "ed": 47, "text": "reward function"}, {"st": 79, "ed": 81, "text": "real world"}, {"st": 100, "ed": 102, "text": "method called"}, {"st": 103, "ed": 105, "text": "policy search"}, {"st": 126, "ed": 128, "text": "proposed method"}, {"st": 151, "ed": 153, "text": "significantly reduces"}, {"st": 156, "ed": 158, "text": "hyper parameters"}, {"st": 183, "ed": 185, "text": "successfully applied"}, {"st": 188, "ed": 190, "text": "motion planning"}]
[{"st": 0, "ed": 3, "text": "generative adversarial networks"}, {"st": 43, "ed": 45, "text": "large scale"}, {"st": 45, "ed": 47, "text": "empirical study"}, {"st": 67, "ed": 69, "text": "hyperparameter optimization"}, {"st": 81, "ed": 83, "text": "computational budget"}, {"st": 106, "ed": 109, "text": "precision and recall"}, {"st": 142, "ed": 144, "text": "consistently outperforms"}]
[{"st": 48, "ed": 50, "text": "kernel based"}, {"st": 54, "ed": 56, "text": "inference problem"}, {"st": 69, "ed": 71, "text": "signal processing"}, {"st": 90, "ed": 92, "text": "real world"}, {"st": 103, "ed": 105, "text": "numerical examples"}]
[{"st": 32, "ed": 35, "text": "input and output"}, {"st": 62, "ed": 64, "text": "recently proposed"}, {"st": 78, "ed": 81, "text": "end to end"}, {"st": 92, "ed": 94, "text": "strong baseline"}, {"st": 95, "ed": 97, "text": "character level"}]
[{"st": 23, "ed": 25, "text": "statistical learning"}, {"st": 26, "ed": 28, "text": "data mining"}, {"st": 36, "ed": 38, "text": "outlier detection"}, {"st": 51, "ed": 53, "text": "becoming increasingly"}, {"st": 82, "ed": 84, "text": "feature selection"}, {"st": 200, "ed": 202, "text": "proposed framework"}]
[{"st": 48, "ed": 50, "text": "unsupervised learning"}, {"st": 62, "ed": 64, "text": "multi relational"}, {"st": 137, "ed": 139, "text": "machine learning"}, {"st": 141, "ed": 143, "text": "case study"}, {"st": 148, "ed": 150, "text": "mixture models"}]
[{"st": 5, "ed": 7, "text": "semi supervised"}, {"st": 7, "ed": 9, "text": "few shot"}, {"st": 13, "ed": 15, "text": "labeled samples"}, {"st": 18, "ed": 20, "text": "unlabeled data"}, {"st": 26, "ed": 28, "text": "prototypical networks"}, {"st": 40, "ed": 42, "text": "prototypical networks"}]
[{"st": 1, "ed": 3, "text": "machine learning"}, {"st": 9, "ed": 12, "text": "shown promising results"}, {"st": 18, "ed": 20, "text": "learning algorithms"}, {"st": 23, "ed": 25, "text": "decision making"}, {"st": 38, "ed": 40, "text": "causal relationships"}, {"st": 76, "ed": 78, "text": "causal relationships"}, {"st": 90, "ed": 92, "text": "decision making"}, {"st": 102, "ed": 104, "text": "post processing"}, {"st": 108, "ed": 110, "text": "causal relationships"}, {"st": 115, "ed": 117, "text": "prediction accuracy"}, {"st": 119, "ed": 121, "text": "multi view"}, {"st": 135, "ed": 137, "text": "causal relationships"}, {"st": 151, "ed": 153, "text": "prediction accuracy"}]
[{"st": 3, "ed": 5, "text": "real world"}, {"st": 13, "ed": 15, "text": "class imbalance"}, {"st": 24, "ed": 26, "text": "majority class"}, {"st": 34, "ed": 37, "text": "the minority class"}, {"st": 42, "ed": 44, "text": "machine learning"}, {"st": 46, "ed": 48, "text": "perform poorly"}, {"st": 73, "ed": 75, "text": "proposed algorithm"}, {"st": 79, "ed": 82, "text": "k nearest neighbor"}, {"st": 82, "ed": 84, "text": "k nn"}, {"st": 92, "ed": 94, "text": "k nn"}, {"st": 121, "ed": 123, "text": "decision making"}, {"st": 147, "ed": 149, "text": "training samples"}, {"st": 161, "ed": 164, "text": "local and global"}, {"st": 198, "ed": 200, "text": "proposed approach"}, {"st": 207, "ed": 209, "text": "imbalanced datasets"}]
[{"st": 5, "ed": 9, "text": "generative adversarial networks gans"}, {"st": 15, "ed": 17, "text": "approximate bayesian"}, {"st": 62, "ed": 64, "text": "deep networks"}, {"st": 99, "ed": 102, "text": "benchmark data sets"}]
[{"st": 1, "ed": 4, "text": "short term memory"}, {"st": 15, "ed": 17, "text": "strong performance"}, {"st": 58, "ed": 60, "text": "topic models"}, {"st": 85, "ed": 87, "text": "monte carlo"}, {"st": 106, "ed": 108, "text": "inference algorithm"}]
[{"st": 8, "ed": 12, "text": "convolutional neural networks cnns"}, {"st": 14, "ed": 17, "text": "weights and activations"}, {"st": 31, "ed": 34, "text": "weights and activations"}, {"st": 62, "ed": 64, "text": "previous works"}, {"st": 71, "ed": 73, "text": "prediction accuracy"}, {"st": 92, "ed": 94, "text": "linear combination"}, {"st": 136, "ed": 138, "text": "prediction accuracy"}]
[{"st": 12, "ed": 14, "text": "data science"}, {"st": 79, "ed": 81, "text": "convex optimization"}, {"st": 111, "ed": 113, "text": "experimentally demonstrate"}]
[{"st": 6, "ed": 8, "text": "generative models"}, {"st": 12, "ed": 14, "text": "sequential data"}, {"st": 15, "ed": 17, "text": "back propagation"}, {"st": 34, "ed": 36, "text": "generative modeling"}, {"st": 52, "ed": 54, "text": "cognitive science"}, {"st": 68, "ed": 70, "text": "neural coding"}, {"st": 73, "ed": 75, "text": "learning algorithm"}, {"st": 80, "ed": 82, "text": "generative model"}, {"st": 102, "ed": 104, "text": "local learning"}, {"st": 122, "ed": 124, "text": "proposed algorithm"}, {"st": 130, "ed": 132, "text": "generative modeling"}, {"st": 141, "ed": 144, "text": "strengths and weaknesses"}]
[{"st": 0, "ed": 2, "text": "deep cnns"}, {"st": 43, "ed": 45, "text": "adversarial examples"}, {"st": 69, "ed": 71, "text": "deep cnn"}, {"st": 136, "ed": 138, "text": "cifar 10"}, {"st": 233, "ed": 235, "text": "deep cnns"}, {"st": 246, "ed": 248, "text": "higher level"}]
[{"st": 11, "ed": 14, "text": "deep neural network"}, {"st": 16, "ed": 20, "text": "convolutional neural networks cnn"}, {"st": 24, "ed": 26, "text": "convolutional filters"}, {"st": 30, "ed": 32, "text": "random variable"}, {"st": 36, "ed": 38, "text": "object class"}, {"st": 84, "ed": 86, "text": "classification results"}, {"st": 91, "ed": 93, "text": "experiments demonstrate"}, {"st": 122, "ed": 124, "text": "magnetic resonance"}, {"st": 170, "ed": 172, "text": "visual object"}, {"st": 176, "ed": 178, "text": "transfer learning"}, {"st": 208, "ed": 210, "text": "previously unseen"}, {"st": 215, "ed": 217, "text": "theoretical analysis"}, {"st": 232, "ed": 235, "text": "future research directions"}]
[{"st": 39, "ed": 41, "text": "attention models"}, {"st": 49, "ed": 51, "text": "vascular disease"}, {"st": 67, "ed": 69, "text": "medical history"}, {"st": 81, "ed": 83, "text": "attention models"}, {"st": 87, "ed": 90, "text": "gated recurrent unit"}, {"st": 115, "ed": 117, "text": "precision recall"}, {"st": 132, "ed": 134, "text": "methods outperform"}, {"st": 134, "ed": 136, "text": "standard classification"}]
[{"st": 46, "ed": 48, "text": "probabilistic models"}, {"st": 63, "ed": 65, "text": "textual data"}, {"st": 72, "ed": 74, "text": "image data"}, {"st": 79, "ed": 82, "text": "spatial and temporal"}, {"st": 95, "ed": 97, "text": "machine learning"}, {"st": 113, "ed": 115, "text": "spatial information"}, {"st": 119, "ed": 121, "text": "topic model"}, {"st": 141, "ed": 143, "text": "topic models"}, {"st": 172, "ed": 174, "text": "scene understanding"}]
[{"st": 43, "ed": 45, "text": "ground truth"}, {"st": 45, "ed": 47, "text": "training data"}, {"st": 52, "ed": 54, "text": "prior information"}, {"st": 57, "ed": 59, "text": "latent space"}, {"st": 82, "ed": 84, "text": "latent variable"}, {"st": 112, "ed": 114, "text": "higher layers"}, {"st": 132, "ed": 134, "text": "sampling based"}, {"st": 136, "ed": 138, "text": "preliminary results"}]
[{"st": 0, "ed": 4, "text": "generative adversarial networks gans"}, {"st": 13, "ed": 15, "text": "neural networks"}, {"st": 20, "ed": 22, "text": "realistic images"}, {"st": 51, "ed": 54, "text": "electronic health records"}, {"st": 75, "ed": 77, "text": "time series"}, {"st": 87, "ed": 89, "text": "predictive power"}, {"st": 114, "ed": 116, "text": "representation learning"}]
[{"st": 25, "ed": 27, "text": "dependency structure"}, {"st": 29, "ed": 31, "text": "generative model"}, {"st": 44, "ed": 46, "text": "previous methods"}, {"st": 48, "ed": 50, "text": "dependency structure"}, {"st": 82, "ed": 84, "text": "graphical model"}, {"st": 132, "ed": 134, "text": "generative model"}]
[{"st": 1, "ed": 3, "text": "computer aided"}, {"st": 17, "ed": 19, "text": "deep learning"}, {"st": 29, "ed": 31, "text": "deep learning"}, {"st": 34, "ed": 36, "text": "successfully applied"}, {"st": 79, "ed": 81, "text": "machine learning"}, {"st": 148, "ed": 150, "text": "machine learning"}, {"st": 156, "ed": 158, "text": "multi instance"}]
[{"st": 104, "ed": 106, "text": "group sparse"}, {"st": 107, "ed": 109, "text": "learning algorithm"}, {"st": 122, "ed": 124, "text": "dynamical systems"}, {"st": 127, "ed": 129, "text": "expectation maximization"}, {"st": 137, "ed": 139, "text": "proposed algorithm"}, {"st": 147, "ed": 151, "text": "synthetic and real world"}]
[{"st": 0, "ed": 2, "text": "deep learning"}, {"st": 15, "ed": 17, "text": "training data"}, {"st": 27, "ed": 29, "text": "training sets"}, {"st": 41, "ed": 43, "text": "application domains"}, {"st": 54, "ed": 56, "text": "training set"}, {"st": 74, "ed": 76, "text": "large scale"}, {"st": 79, "ed": 81, "text": "generalization error"}, {"st": 86, "ed": 88, "text": "training sets"}, {"st": 99, "ed": 101, "text": "machine learning"}, {"st": 102, "ed": 104, "text": "machine translation"}, {"st": 106, "ed": 108, "text": "image processing"}, {"st": 112, "ed": 114, "text": "empirical results"}, {"st": 115, "ed": 117, "text": "power law"}, {"st": 117, "ed": 119, "text": "generalization error"}, {"st": 127, "ed": 129, "text": "power law"}, {"st": 134, "ed": 136, "text": "learning curve"}, {"st": 157, "ed": 159, "text": "power law"}, {"st": 178, "ed": 180, "text": "deep learning"}]
[{"st": 9, "ed": 11, "text": "deep learning"}, {"st": 51, "ed": 56, "text": "convolutional neural network cnn architecture"}, {"st": 79, "ed": 82, "text": "deep neural network"}, {"st": 85, "ed": 87, "text": "higher accuracy"}, {"st": 129, "ed": 131, "text": "convolutional layers"}, {"st": 152, "ed": 154, "text": "recently introduced"}, {"st": 156, "ed": 158, "text": "residual networks"}, {"st": 167, "ed": 169, "text": "achieve high"}, {"st": 185, "ed": 188, "text": "deep neural network"}]
[{"st": 28, "ed": 30, "text": "riemannian manifold"}, {"st": 66, "ed": 68, "text": "labeled data"}, {"st": 82, "ed": 84, "text": "proposed method"}, {"st": 105, "ed": 108, "text": "false positive rate"}]
[{"st": 4, "ed": 6, "text": "topic models"}, {"st": 7, "ed": 9, "text": "low dimensional"}, {"st": 25, "ed": 28, "text": "latent dirichlet allocation"}, {"st": 40, "ed": 42, "text": "accurate prediction"}, {"st": 46, "ed": 48, "text": "existing approaches"}, {"st": 100, "ed": 102, "text": "case study"}, {"st": 107, "ed": 110, "text": "electronic health records"}, {"st": 118, "ed": 120, "text": "topic models"}, {"st": 123, "ed": 125, "text": "logistic regression"}]
[{"st": 0, "ed": 2, "text": "multiple modalities"}, {"st": 25, "ed": 27, "text": "machine learning"}, {"st": 54, "ed": 56, "text": "based approaches"}, {"st": 77, "ed": 80, "text": "mild cognitive impairment"}, {"st": 89, "ed": 91, "text": "local feature"}, {"st": 105, "ed": 108, "text": "computer aided diagnosis"}, {"st": 116, "ed": 118, "text": "feature extraction"}, {"st": 119, "ed": 122, "text": "taking advantage of"}, {"st": 134, "ed": 136, "text": "significantly improve"}, {"st": 143, "ed": 145, "text": "local feature"}]
[{"st": 4, "ed": 7, "text": "deep neural networks"}, {"st": 25, "ed": 27, "text": "detection methods"}, {"st": 69, "ed": 72, "text": "deep neural networks"}, {"st": 81, "ed": 83, "text": "recent advances"}, {"st": 84, "ed": 86, "text": "interpretable models"}, {"st": 109, "ed": 111, "text": "extensive experiments"}, {"st": 112, "ed": 114, "text": "benchmark datasets"}]
[{"st": 1, "ed": 3, "text": "supervised hashing"}, {"st": 5, "ed": 8, "text": "attracted much attention"}, {"st": 26, "ed": 28, "text": "np hard"}, {"st": 52, "ed": 54, "text": "supervised hashing"}, {"st": 73, "ed": 76, "text": "conditional random field"}, {"st": 112, "ed": 114, "text": "local optimum"}, {"st": 116, "ed": 119, "text": "variational lower bound"}, {"st": 130, "ed": 132, "text": "linear approximation"}, {"st": 134, "ed": 136, "text": "sigmoid function"}, {"st": 147, "ed": 149, "text": "closed form"}]
[{"st": 13, "ed": 15, "text": "machine learning"}, {"st": 31, "ed": 33, "text": "classification tasks"}, {"st": 45, "ed": 47, "text": "machine learning"}, {"st": 80, "ed": 82, "text": "machine learning"}, {"st": 117, "ed": 119, "text": "machine learning"}, {"st": 136, "ed": 138, "text": "feature set"}, {"st": 156, "ed": 158, "text": "decision making"}]
[{"st": 0, "ed": 2, "text": "statistical learning"}, {"st": 25, "ed": 28, "text": "point of view"}, {"st": 56, "ed": 58, "text": "generative models"}, {"st": 116, "ed": 118, "text": "proposed method"}, {"st": 153, "ed": 155, "text": "transfer learning"}]
[{"st": 1, "ed": 3, "text": "computationally efficient"}, {"st": 6, "ed": 8, "text": "deep learning"}, {"st": 10, "ed": 12, "text": "continuous optimization"}, {"st": 36, "ed": 38, "text": "variational inference"}, {"st": 62, "ed": 64, "text": "variational inference"}, {"st": 73, "ed": 75, "text": "variational inference"}, {"st": 84, "ed": 86, "text": "natural gradient"}, {"st": 99, "ed": 101, "text": "computationally efficient"}, {"st": 102, "ed": 105, "text": "easy to implement"}]
[{"st": 1, "ed": 3, "text": "recent years"}, {"st": 3, "ed": 6, "text": "deep neural networks"}, {"st": 20, "ed": 22, "text": "performance gain"}, {"st": 29, "ed": 31, "text": "computational costs"}, {"st": 43, "ed": 45, "text": "deep model"}, {"st": 71, "ed": 73, "text": "optimization framework"}, {"st": 74, "ed": 76, "text": "deep model"}, {"st": 103, "ed": 105, "text": "optimization process"}, {"st": 141, "ed": 143, "text": "algorithm outperforms"}, {"st": 145, "ed": 147, "text": "optimization methods"}]
[{"st": 3, "ed": 5, "text": "maximum likelihood"}, {"st": 7, "ed": 9, "text": "neural networks"}, {"st": 54, "ed": 56, "text": "empirical results"}, {"st": 57, "ed": 60, "text": "regression and classification"}]
[{"st": 2, "ed": 4, "text": "human brain"}, {"st": 6, "ed": 8, "text": "learning agents"}, {"st": 10, "ed": 12, "text": "real world"}, {"st": 14, "ed": 16, "text": "complex environments"}, {"st": 32, "ed": 34, "text": "hierarchical bayesian"}, {"st": 51, "ed": 53, "text": "off line"}, {"st": 54, "ed": 57, "text": "batch and online"}, {"st": 102, "ed": 104, "text": "sufficient statistics"}, {"st": 131, "ed": 133, "text": "episodic memory"}, {"st": 138, "ed": 140, "text": "limited memory"}, {"st": 191, "ed": 193, "text": "episodic memory"}]
[{"st": 6, "ed": 9, "text": "low rank tensor"}, {"st": 15, "ed": 17, "text": "trace norm"}, {"st": 19, "ed": 21, "text": "low rank"}, {"st": 26, "ed": 28, "text": "existing works"}, {"st": 48, "ed": 50, "text": "trace norm"}, {"st": 71, "ed": 73, "text": "trace norm"}, {"st": 74, "ed": 77, "text": "low rank tensor"}, {"st": 121, "ed": 123, "text": "optimization framework"}, {"st": 125, "ed": 127, "text": "computationally efficient"}, {"st": 127, "ed": 129, "text": "trust region"}, {"st": 142, "ed": 145, "text": "real world data"}]
[{"st": 6, "ed": 8, "text": "l 0"}, {"st": 8, "ed": 10, "text": "norm regularization"}, {"st": 11, "ed": 13, "text": "neural networks"}, {"st": 36, "ed": 39, "text": "training and inference"}, {"st": 54, "ed": 56, "text": "special cases"}, {"st": 57, "ed": 59, "text": "l 0"}, {"st": 63, "ed": 65, "text": "l 0"}, {"st": 78, "ed": 80, "text": "regularization term"}, {"st": 121, "ed": 123, "text": "l 0"}, {"st": 193, "ed": 195, "text": "efficient learning"}, {"st": 199, "ed": 202, "text": "stochastic gradient descent"}]
[{"st": 15, "ed": 17, "text": "mathbb r"}, {"st": 25, "ed": 27, "text": "gaussian process"}, {"st": 47, "ed": 49, "text": "cumulative regret"}, {"st": 58, "ed": 60, "text": "existing methods"}, {"st": 62, "ed": 64, "text": "exhaustive search"}, {"st": 77, "ed": 79, "text": "proposed algorithm"}, {"st": 90, "ed": 92, "text": "computational complexity"}, {"st": 111, "ed": 113, "text": "sufficient conditions"}, {"st": 118, "ed": 120, "text": "regret bounds"}, {"st": 139, "ed": 141, "text": "contextual bandits"}]
[{"st": 2, "ed": 4, "text": "deep linear"}, {"st": 21, "ed": 23, "text": "local minima"}, {"st": 28, "ed": 30, "text": "hidden layer"}]
[{"st": 8, "ed": 10, "text": "facial expressions"}, {"st": 23, "ed": 27, "text": "facial action coding system"}, {"st": 33, "ed": 35, "text": "facial expression"}, {"st": 62, "ed": 64, "text": "facial expression"}, {"st": 114, "ed": 116, "text": "labeled data"}, {"st": 148, "ed": 150, "text": "frame level"}, {"st": 170, "ed": 172, "text": "frame level"}, {"st": 187, "ed": 189, "text": "weakly supervised"}, {"st": 192, "ed": 196, "text": "multiple instance learning mil"}, {"st": 221, "ed": 223, "text": "recognition accuracy"}, {"st": 243, "ed": 245, "text": "lung cancer"}]
[{"st": 0, "ed": 3, "text": "deep generative models"}, {"st": 26, "ed": 28, "text": "deep models"}, {"st": 42, "ed": 44, "text": "generative models"}, {"st": 132, "ed": 134, "text": "reinforcement learning"}, {"st": 159, "ed": 161, "text": "generative models"}, {"st": 181, "ed": 183, "text": "source code"}, {"st": 192, "ed": 194, "text": "variational autoencoder"}]
[{"st": 0, "ed": 2, "text": "large data"}, {"st": 8, "ed": 10, "text": "neural networks"}, {"st": 12, "ed": 14, "text": "sensitive information"}, {"st": 26, "ed": 28, "text": "training data"}, {"st": 75, "ed": 77, "text": "neural networks"}, {"st": 102, "ed": 104, "text": "differential privacy"}, {"st": 106, "ed": 108, "text": "differential privacy"}]
[{"st": 0, "ed": 2, "text": "deep learning"}, {"st": 6, "ed": 9, "text": "artificial neural network"}, {"st": 72, "ed": 75, "text": "high energy physics"}, {"st": 79, "ed": 81, "text": "remote sensing"}, {"st": 111, "ed": 113, "text": "regularization techniques"}, {"st": 118, "ed": 120, "text": "deep networks"}, {"st": 135, "ed": 137, "text": "generalization performance"}, {"st": 182, "ed": 184, "text": "interpretable models"}, {"st": 189, "ed": 191, "text": "back propagation"}, {"st": 204, "ed": 206, "text": "decision process"}, {"st": 207, "ed": 209, "text": "deep networks"}, {"st": 221, "ed": 223, "text": "ad hoc"}]
[{"st": 1, "ed": 5, "text": "generative adversarial networks gans"}, {"st": 7, "ed": 9, "text": "impressive performance"}, {"st": 21, "ed": 23, "text": "computer vision"}, {"st": 35, "ed": 38, "text": "difficult to train"}, {"st": 42, "ed": 44, "text": "time consuming"}, {"st": 56, "ed": 58, "text": "training process"}, {"st": 129, "ed": 131, "text": "local minimum"}, {"st": 191, "ed": 193, "text": "real world"}, {"st": 197, "ed": 199, "text": "approach outperforms"}, {"st": 206, "ed": 208, "text": "mode collapse"}, {"st": 217, "ed": 219, "text": "convergence speed"}]
[{"st": 0, "ed": 2, "text": "variational bayesian"}, {"st": 2, "ed": 4, "text": "neural nets"}, {"st": 8, "ed": 10, "text": "deep learning"}, {"st": 37, "ed": 39, "text": "natural gradient"}, {"st": 52, "ed": 55, "text": "evidence lower bound"}, {"st": 68, "ed": 70, "text": "matrix variate"}, {"st": 77, "ed": 79, "text": "natural gradient"}, {"st": 109, "ed": 111, "text": "monte carlo"}, {"st": 120, "ed": 122, "text": "uncertainty estimates"}, {"st": 125, "ed": 127, "text": "efficient exploration"}, {"st": 128, "ed": 130, "text": "active learning"}, {"st": 131, "ed": 133, "text": "intrinsic motivation"}]
[{"st": 17, "ed": 19, "text": "large scale"}, {"st": 31, "ed": 33, "text": "machine learning"}, {"st": 36, "ed": 38, "text": "machine learning"}, {"st": 55, "ed": 58, "text": "false positive rate"}, {"st": 60, "ed": 62, "text": "false negative"}, {"st": 67, "ed": 69, "text": "cost sensitive"}, {"st": 74, "ed": 76, "text": "hyper parameters"}, {"st": 82, "ed": 84, "text": "variational autoencoders"}, {"st": 98, "ed": 100, "text": "proposed approach"}]
[{"st": 17, "ed": 19, "text": "open question"}, {"st": 27, "ed": 29, "text": "spatio temporal"}, {"st": 39, "ed": 41, "text": "spatio temporal"}]
[{"st": 0, "ed": 3, "text": "deep neural networks"}, {"st": 41, "ed": 43, "text": "regularization technique"}, {"st": 50, "ed": 52, "text": "large data"}, {"st": 55, "ed": 57, "text": "neural networks"}, {"st": 58, "ed": 60, "text": "sensitive information"}, {"st": 72, "ed": 74, "text": "training data"}, {"st": 83, "ed": 85, "text": "recently proposed"}, {"st": 85, "ed": 87, "text": "variational dropout"}, {"st": 104, "ed": 106, "text": "variational dropout"}, {"st": 121, "ed": 123, "text": "neural networks"}, {"st": 148, "ed": 150, "text": "differential privacy"}, {"st": 152, "ed": 154, "text": "differential privacy"}, {"st": 171, "ed": 173, "text": "variational dropout"}]
[{"st": 1, "ed": 3, "text": "distributed training"}, {"st": 4, "ed": 7, "text": "deep neural networks"}, {"st": 44, "ed": 46, "text": "wide variety"}, {"st": 50, "ed": 53, "text": "deep neural networks"}, {"st": 58, "ed": 60, "text": "network architectures"}, {"st": 102, "ed": 104, "text": "excellent results"}, {"st": 113, "ed": 115, "text": "deep learning"}, {"st": 117, "ed": 119, "text": "multiple domains"}, {"st": 134, "ed": 136, "text": "network parameters"}, {"st": 150, "ed": 153, "text": "end to end"}, {"st": 158, "ed": 160, "text": "fully connected"}, {"st": 161, "ed": 163, "text": "recurrent layers"}, {"st": 166, "ed": 168, "text": "convolutional layers"}]
[{"st": 30, "ed": 32, "text": "deep learning"}, {"st": 37, "ed": 39, "text": "deep networks"}, {"st": 40, "ed": 43, "text": "first order logic"}, {"st": 87, "ed": 89, "text": "neural networks"}, {"st": 92, "ed": 94, "text": "hidden layers"}, {"st": 96, "ed": 98, "text": "logistic regression"}, {"st": 102, "ed": 104, "text": "logistic regression"}, {"st": 117, "ed": 119, "text": "back propagation"}, {"st": 127, "ed": 129, "text": "layer wise"}, {"st": 136, "ed": 138, "text": "deep learning"}, {"st": 149, "ed": 151, "text": "real world"}]
[{"st": 1, "ed": 4, "text": "multi armed bandit"}, {"st": 12, "ed": 14, "text": "exploration exploitation"}, {"st": 33, "ed": 35, "text": "multi armed"}, {"st": 35, "ed": 37, "text": "slot machine"}, {"st": 50, "ed": 52, "text": "reward distributions"}, {"st": 79, "ed": 81, "text": "real applications"}, {"st": 95, "ed": 97, "text": "epsilon greedy"}, {"st": 98, "ed": 101, "text": "upper confidence bound"}, {"st": 153, "ed": 155, "text": "epsilon greedy"}, {"st": 174, "ed": 176, "text": "prior knowledge"}]
[{"st": 2, "ed": 4, "text": "deep learning"}, {"st": 27, "ed": 29, "text": "diminishing returns"}, {"st": 45, "ed": 47, "text": "stochastic optimization"}, {"st": 69, "ed": 71, "text": "computational resources"}, {"st": 118, "ed": 120, "text": "resnet 50"}, {"st": 127, "ed": 129, "text": "mini batch"}, {"st": 138, "ed": 140, "text": "validation error"}, {"st": 149, "ed": 151, "text": "total number"}, {"st": 155, "ed": 157, "text": "mini batch"}, {"st": 162, "ed": 164, "text": "validation error"}, {"st": 203, "ed": 205, "text": "learning rate"}]
[{"st": 12, "ed": 14, "text": "batch size"}, {"st": 15, "ed": 18, "text": "stochastic gradient descent"}, {"st": 23, "ed": 25, "text": "batch size"}, {"st": 51, "ed": 53, "text": "batch size"}, {"st": 86, "ed": 88, "text": "proposed approach"}, {"st": 92, "ed": 94, "text": "related methods"}, {"st": 110, "ed": 112, "text": "barcelona spain"}]
[{"st": 1, "ed": 3, "text": "recent years"}, {"st": 3, "ed": 7, "text": "convolutional neural networks cnn"}, {"st": 10, "ed": 12, "text": "important role"}, {"st": 29, "ed": 31, "text": "classification tasks"}, {"st": 131, "ed": 133, "text": "neural network"}]
[{"st": 0, "ed": 4, "text": "convolutional neural networks cnns"}, {"st": 23, "ed": 25, "text": "increasing attention"}, {"st": 65, "ed": 67, "text": "structural information"}, {"st": 89, "ed": 92, "text": "convolutional neural network"}, {"st": 112, "ed": 114, "text": "convolutional layer"}, {"st": 121, "ed": 123, "text": "gaussian function"}, {"st": 157, "ed": 159, "text": "optimization process"}, {"st": 161, "ed": 163, "text": "convolutional layer"}, {"st": 168, "ed": 170, "text": "feature learning"}, {"st": 191, "ed": 193, "text": "neural network"}, {"st": 208, "ed": 210, "text": "receptive fields"}, {"st": 233, "ed": 235, "text": "proposed method"}]
[{"st": 29, "ed": 32, "text": "deep neural network"}, {"st": 94, "ed": 96, "text": "image classification"}, {"st": 104, "ed": 106, "text": "natural language"}]
[{"st": 5, "ed": 7, "text": "et al"}, {"st": 16, "ed": 18, "text": "catastrophic forgetting"}, {"st": 32, "ed": 34, "text": "et al"}, {"st": 46, "ed": 48, "text": "et al"}]
[{"st": 1, "ed": 3, "text": "latent variable"}, {"st": 7, "ed": 9, "text": "joint distribution"}, {"st": 53, "ed": 55, "text": "latent variable"}, {"st": 74, "ed": 76, "text": "iterative procedure"}, {"st": 79, "ed": 81, "text": "gibbs sampling"}, {"st": 91, "ed": 93, "text": "joint distribution"}, {"st": 105, "ed": 107, "text": "joint distribution"}, {"st": 112, "ed": 114, "text": "latent code"}, {"st": 119, "ed": 121, "text": "iterative procedure"}, {"st": 125, "ed": 127, "text": "joint distribution"}, {"st": 161, "ed": 163, "text": "latent variable"}, {"st": 176, "ed": 178, "text": "global minimum"}, {"st": 199, "ed": 201, "text": "latent variable"}, {"st": 221, "ed": 223, "text": "class conditional"}]
[{"st": 23, "ed": 25, "text": "supervised learning"}, {"st": 39, "ed": 41, "text": "anomaly detection"}, {"st": 55, "ed": 57, "text": "limited data"}, {"st": 77, "ed": 80, "text": "large data sets"}, {"st": 119, "ed": 121, "text": "prior knowledge"}, {"st": 149, "ed": 151, "text": "ensemble method"}, {"st": 152, "ed": 154, "text": "unsupervised clustering"}, {"st": 168, "ed": 170, "text": "data point"}]
[{"st": 1, "ed": 3, "text": "feature map"}, {"st": 6, "ed": 8, "text": "denoising autoencoder"}, {"st": 32, "ed": 35, "text": "deep neural networks"}, {"st": 40, "ed": 42, "text": "feature maps"}, {"st": 67, "ed": 69, "text": "feature map"}, {"st": 76, "ed": 78, "text": "feature map"}, {"st": 82, "ed": 85, "text": "input and output"}, {"st": 98, "ed": 101, "text": "input and output"}, {"st": 107, "ed": 109, "text": "high dimensional"}, {"st": 165, "ed": 168, "text": "deep neural networks"}, {"st": 210, "ed": 213, "text": "deep neural networks"}]
[{"st": 95, "ed": 97, "text": "existing methods"}, {"st": 114, "ed": 116, "text": "neural networks"}, {"st": 133, "ed": 135, "text": "random forests"}, {"st": 148, "ed": 150, "text": "time series"}]
[{"st": 6, "ed": 8, "text": "real world"}, {"st": 52, "ed": 55, "text": "simple linear regression"}, {"st": 60, "ed": 62, "text": "deep learning"}, {"st": 69, "ed": 71, "text": "recent years"}, {"st": 73, "ed": 75, "text": "deep learning"}, {"st": 82, "ed": 84, "text": "supervised learning"}, {"st": 87, "ed": 89, "text": "image classification"}, {"st": 98, "ed": 100, "text": "deep learning"}, {"st": 146, "ed": 148, "text": "level features"}, {"st": 181, "ed": 183, "text": "real world"}, {"st": 196, "ed": 198, "text": "network structure"}]
[{"st": 0, "ed": 2, "text": "class imbalance"}, {"st": 5, "ed": 7, "text": "challenging research"}, {"st": 9, "ed": 11, "text": "data mining"}, {"st": 12, "ed": 14, "text": "machine learning"}, {"st": 18, "ed": 20, "text": "real life"}, {"st": 27, "ed": 29, "text": "learning algorithms"}, {"st": 31, "ed": 33, "text": "classification accuracy"}, {"st": 37, "ed": 39, "text": "majority class"}, {"st": 45, "ed": 48, "text": "the minority class"}, {"st": 58, "ed": 60, "text": "majority class"}, {"st": 62, "ed": 64, "text": "real life"}, {"st": 70, "ed": 72, "text": "sampling methods"}, {"st": 76, "ed": 78, "text": "majority class"}, {"st": 81, "ed": 84, "text": "the minority class"}, {"st": 84, "ed": 86, "text": "cost sensitive"}, {"st": 89, "ed": 91, "text": "ensemble learning"}, {"st": 124, "ed": 126, "text": "proposed algorithm"}, {"st": 161, "ed": 163, "text": "ensemble learning"}, {"st": 172, "ed": 174, "text": "multi class"}]
[{"st": 17, "ed": 20, "text": "deep neural networks"}, {"st": 23, "ed": 25, "text": "distributed memory"}, {"st": 28, "ed": 31, "text": "stochastic gradient descent"}, {"st": 45, "ed": 47, "text": "batch size"}, {"st": 107, "ed": 109, "text": "parallel algorithm"}, {"st": 137, "ed": 139, "text": "pure data"}]
[{"st": 39, "ed": 41, "text": "sample efficiency"}, {"st": 77, "ed": 79, "text": "empirical evaluation"}, {"st": 80, "ed": 82, "text": "bayesian optimization"}, {"st": 89, "ed": 91, "text": "empirical evidence"}, {"st": 113, "ed": 115, "text": "gaussian process"}]
[{"st": 3, "ed": 5, "text": "computer vision"}, {"st": 6, "ed": 8, "text": "recommender systems"}, {"st": 9, "ed": 11, "text": "low rank"}, {"st": 27, "ed": 29, "text": "low rank"}, {"st": 37, "ed": 39, "text": "learning agent"}, {"st": 42, "ed": 45, "text": "row and column"}, {"st": 148, "ed": 150, "text": "problem specific"}]
[{"st": 16, "ed": 18, "text": "false positives"}, {"st": 23, "ed": 25, "text": "meta algorithm"}, {"st": 26, "ed": 28, "text": "stability selection"}, {"st": 34, "ed": 36, "text": "finite sample"}, {"st": 56, "ed": 58, "text": "matching pursuit"}, {"st": 67, "ed": 69, "text": "stability selection"}, {"st": 71, "ed": 73, "text": "selection algorithms"}, {"st": 74, "ed": 76, "text": "group lasso"}, {"st": 79, "ed": 81, "text": "input output"}, {"st": 86, "ed": 88, "text": "stability selection"}, {"st": 116, "ed": 118, "text": "tuning parameters"}, {"st": 125, "ed": 127, "text": "stability selection"}, {"st": 130, "ed": 133, "text": "strengths and weaknesses"}, {"st": 135, "ed": 137, "text": "competing methods"}]
[{"st": 2, "ed": 4, "text": "binary classification"}, {"st": 6, "ed": 8, "text": "positive definite"}, {"st": 15, "ed": 17, "text": "convergence rates"}, {"st": 29, "ed": 31, "text": "squared loss"}, {"st": 49, "ed": 51, "text": "classification error"}, {"st": 55, "ed": 57, "text": "low noise"}]
[{"st": 14, "ed": 16, "text": "practical scenarios"}, {"st": 69, "ed": 71, "text": "machine learning"}, {"st": 116, "ed": 119, "text": "convex optimization problem"}, {"st": 135, "ed": 137, "text": "real world"}, {"st": 162, "ed": 164, "text": "true labels"}, {"st": 194, "ed": 196, "text": "machine learning"}, {"st": 213, "ed": 215, "text": "machine learning"}, {"st": 218, "ed": 220, "text": "cost effective"}]
[{"st": 0, "ed": 2, "text": "deep learning"}, {"st": 8, "ed": 10, "text": "application domains"}, {"st": 20, "ed": 22, "text": "deep learning"}, {"st": 22, "ed": 25, "text": "deep neural networks"}, {"st": 28, "ed": 30, "text": "multiple layers"}, {"st": 46, "ed": 48, "text": "deep learning"}, {"st": 93, "ed": 96, "text": "fast fourier transform"}, {"st": 98, "ed": 100, "text": "dnn training"}, {"st": 126, "ed": 129, "text": "training and inference"}]
[{"st": 9, "ed": 11, "text": "multi modal"}, {"st": 24, "ed": 27, "text": "spatial and temporal"}, {"st": 59, "ed": 62, "text": "convolutional neural network"}, {"st": 78, "ed": 80, "text": "large scale"}, {"st": 101, "ed": 103, "text": "automatically learn"}, {"st": 117, "ed": 119, "text": "previous studies"}, {"st": 167, "ed": 169, "text": "distance matrix"}, {"st": 196, "ed": 199, "text": "support vector regression"}, {"st": 208, "ed": 210, "text": "new york"}, {"st": 235, "ed": 238, "text": "mean square error"}]
[{"st": 1, "ed": 3, "text": "recent literature"}, {"st": 4, "ed": 6, "text": "deep learning"}, {"st": 34, "ed": 36, "text": "prior distribution"}, {"st": 37, "ed": 39, "text": "neural network"}, {"st": 45, "ed": 47, "text": "variational bayes"}, {"st": 57, "ed": 59, "text": "training examples"}, {"st": 77, "ed": 79, "text": "training data"}]
[{"st": 0, "ed": 4, "text": "recurrent neural networks rnns"}, {"st": 38, "ed": 40, "text": "computer vision"}, {"st": 81, "ed": 83, "text": "low rank"}, {"st": 126, "ed": 128, "text": "challenging tasks"}, {"st": 133, "ed": 135, "text": "image captioning"}, {"st": 136, "ed": 138, "text": "image generation"}, {"st": 151, "ed": 153, "text": "prediction accuracy"}, {"st": 163, "ed": 165, "text": "fewer parameters"}, {"st": 178, "ed": 180, "text": "action recognition"}]
[{"st": 4, "ed": 6, "text": "logistic regression"}, {"st": 32, "ed": 34, "text": "synthetic data"}, {"st": 38, "ed": 40, "text": "l1 regularized"}, {"st": 70, "ed": 72, "text": "open source"}]
[{"st": 29, "ed": 31, "text": "gaussian process"}, {"st": 50, "ed": 52, "text": "closed form"}]
[{"st": 1, "ed": 4, "text": "deep neural networks"}, {"st": 14, "ed": 16, "text": "classification methods"}, {"st": 58, "ed": 60, "text": "fewer parameters"}, {"st": 73, "ed": 75, "text": "promising results"}, {"st": 83, "ed": 85, "text": "existing methods"}, {"st": 93, "ed": 95, "text": "deep network"}, {"st": 99, "ed": 101, "text": "fine tuning"}, {"st": 128, "ed": 131, "text": "fully connected layers"}, {"st": 154, "ed": 156, "text": "low dimensional"}, {"st": 172, "ed": 175, "text": "conduct extensive experiments"}, {"st": 176, "ed": 178, "text": "benchmark datasets"}, {"st": 188, "ed": 190, "text": "compression ratio"}, {"st": 218, "ed": 220, "text": "fewer parameters"}]
[{"st": 10, "ed": 12, "text": "computational cost"}, {"st": 13, "ed": 15, "text": "deep learning"}, {"st": 19, "ed": 22, "text": "efficient and accurate"}, {"st": 49, "ed": 51, "text": "floating point"}, {"st": 63, "ed": 65, "text": "training procedure"}, {"st": 67, "ed": 70, "text": "end to end"}, {"st": 109, "ed": 111, "text": "imagenet classification"}]
[{"st": 22, "ed": 24, "text": "wasserstein gans"}, {"st": 41, "ed": 43, "text": "learning speed"}, {"st": 49, "ed": 51, "text": "wasserstein distance"}, {"st": 75, "ed": 77, "text": "source code"}]
[{"st": 0, "ed": 2, "text": "optimal transport"}, {"st": 3, "ed": 5, "text": "recently gained"}, {"st": 7, "ed": 9, "text": "machine learning"}, {"st": 13, "ed": 15, "text": "domain adaptation"}, {"st": 22, "ed": 25, "text": "ability to capture"}, {"st": 44, "ed": 46, "text": "optimal transport"}, {"st": 86, "ed": 88, "text": "domain adaptation"}, {"st": 89, "ed": 91, "text": "natural language"}]
[{"st": 2, "ed": 4, "text": "patient specific"}, {"st": 13, "ed": 16, "text": "intensive care unit"}, {"st": 63, "ed": 66, "text": "mean squared error"}, {"st": 89, "ed": 92, "text": "recurrent neural network"}, {"st": 112, "ed": 114, "text": "los angeles"}, {"st": 157, "ed": 159, "text": "main results"}, {"st": 204, "ed": 206, "text": "rnn model"}, {"st": 283, "ed": 285, "text": "rnn model"}]
[{"st": 5, "ed": 7, "text": "information technology"}, {"st": 12, "ed": 14, "text": "multi view"}, {"st": 21, "ed": 23, "text": "machine learning"}, {"st": 24, "ed": 26, "text": "data mining"}, {"st": 32, "ed": 34, "text": "multi view"}, {"st": 34, "ed": 37, "text": "semi supervised learning"}, {"st": 49, "ed": 51, "text": "multi view"}, {"st": 80, "ed": 82, "text": "multiple views"}, {"st": 103, "ed": 105, "text": "multi view"}, {"st": 108, "ed": 110, "text": "multi task"}, {"st": 111, "ed": 113, "text": "multi view"}, {"st": 115, "ed": 117, "text": "multi view"}, {"st": 122, "ed": 124, "text": "real world"}, {"st": 138, "ed": 140, "text": "open problems"}]
[{"st": 30, "ed": 32, "text": "machine learning"}, {"st": 35, "ed": 37, "text": "brain imaging"}, {"st": 51, "ed": 54, "text": "end to end"}, {"st": 66, "ed": 68, "text": "feature extraction"}, {"st": 99, "ed": 101, "text": "deep neural"}, {"st": 101, "ed": 103, "text": "generative model"}, {"st": 106, "ed": 110, "text": "functional magnetic resonance imaging"}, {"st": 128, "ed": 130, "text": "posterior probability"}, {"st": 137, "ed": 139, "text": "imaging data"}, {"st": 160, "ed": 162, "text": "large margin"}, {"st": 167, "ed": 170, "text": "support vector machine"}, {"st": 170, "ed": 172, "text": "logistic regression"}, {"st": 173, "ed": 175, "text": "multilayer perceptron"}, {"st": 179, "ed": 181, "text": "feature extractors"}]
[{"st": 7, "ed": 9, "text": "optimization problems"}, {"st": 41, "ed": 43, "text": "hyperparameter optimization"}, {"st": 44, "ed": 46, "text": "meta learning"}, {"st": 70, "ed": 72, "text": "supervised learning"}, {"st": 83, "ed": 85, "text": "recently proposed"}, {"st": 117, "ed": 119, "text": "preliminary experiments"}]
[{"st": 0, "ed": 3, "text": "stochastic gradient descent"}, {"st": 14, "ed": 16, "text": "large scale"}, {"st": 28, "ed": 30, "text": "theoretical results"}, {"st": 35, "ed": 37, "text": "convergence rates"}, {"st": 82, "ed": 84, "text": "empirical loss"}, {"st": 109, "ed": 111, "text": "fast convergence"}, {"st": 116, "ed": 119, "text": "number of iterations"}, {"st": 130, "ed": 132, "text": "objective function"}, {"st": 135, "ed": 137, "text": "objective function"}, {"st": 154, "ed": 156, "text": "convergence rate"}, {"st": 165, "ed": 167, "text": "step size"}, {"st": 172, "ed": 174, "text": "batch size"}, {"st": 180, "ed": 182, "text": "m 1"}, {"st": 201, "ed": 203, "text": "mini batch"}, {"st": 211, "ed": 213, "text": "batch size"}, {"st": 223, "ed": 225, "text": "batch size"}, {"st": 246, "ed": 248, "text": "mini batch"}, {"st": 280, "ed": 282, "text": "theoretical analyses"}, {"st": 300, "ed": 303, "text": "deep neural networks"}]
[{"st": 0, "ed": 2, "text": "class imbalance"}, {"st": 6, "ed": 8, "text": "challenging research"}, {"st": 13, "ed": 15, "text": "machine learning"}, {"st": 20, "ed": 22, "text": "real life"}, {"st": 27, "ed": 29, "text": "machine learning"}, {"st": 39, "ed": 41, "text": "majority class"}, {"st": 51, "ed": 54, "text": "the minority class"}, {"st": 66, "ed": 68, "text": "cost sensitive"}, {"st": 93, "ed": 95, "text": "boosting algorithm"}, {"st": 102, "ed": 104, "text": "weak learners"}, {"st": 119, "ed": 121, "text": "existing techniques"}, {"st": 137, "ed": 139, "text": "imbalanced datasets"}, {"st": 144, "ed": 146, "text": "ensemble methods"}, {"st": 156, "ed": 158, "text": "significant improvement"}, {"st": 189, "ed": 191, "text": "https github.com"}]
[{"st": 4, "ed": 6, "text": "collected data"}, {"st": 51, "ed": 53, "text": "linear regression"}, {"st": 60, "ed": 62, "text": "coarse grained"}, {"st": 85, "ed": 87, "text": "finite sample"}, {"st": 87, "ed": 90, "text": "bias and variance"}, {"st": 98, "ed": 100, "text": "confidence intervals"}, {"st": 127, "ed": 130, "text": "multi armed bandits"}, {"st": 132, "ed": 134, "text": "time series"}]
[{"st": 9, "ed": 11, "text": "large scale"}, {"st": 11, "ed": 13, "text": "kernel approximation"}, {"st": 21, "ed": 23, "text": "random features"}, {"st": 71, "ed": 73, "text": "supervised learning"}, {"st": 83, "ed": 85, "text": "random features"}, {"st": 92, "ed": 94, "text": "score function"}, {"st": 111, "ed": 113, "text": "score function"}, {"st": 128, "ed": 130, "text": "empirical results"}, {"st": 132, "ed": 134, "text": "benchmark datasets"}, {"st": 143, "ed": 145, "text": "random features"}, {"st": 149, "ed": 151, "text": "generalization error"}, {"st": 161, "ed": 163, "text": "pre processing"}]
[{"st": 24, "ed": 26, "text": "capture complex"}, {"st": 70, "ed": 73, "text": "degrees of freedom"}, {"st": 85, "ed": 87, "text": "linear combinations"}, {"st": 90, "ed": 92, "text": "non trivial"}, {"st": 221, "ed": 223, "text": "case studies"}]
[{"st": 11, "ed": 13, "text": "maximum likelihood"}, {"st": 17, "ed": 19, "text": "maximum likelihood"}, {"st": 25, "ed": 27, "text": "sufficient statistic"}, {"st": 36, "ed": 38, "text": "theoretical properties"}, {"st": 84, "ed": 86, "text": "symmetric group"}, {"st": 99, "ed": 101, "text": "approximate solution"}, {"st": 104, "ed": 106, "text": "empirical performance"}]
[{"st": 9, "ed": 11, "text": "classification problem"}, {"st": 13, "ed": 15, "text": "multi instance"}, {"st": 17, "ed": 19, "text": "real world"}, {"st": 19, "ed": 21, "text": "sequential data"}, {"st": 39, "ed": 41, "text": "growing season"}, {"st": 89, "ed": 91, "text": "sequential data"}, {"st": 97, "ed": 99, "text": "classification methods"}, {"st": 123, "ed": 125, "text": "multi instance"}, {"st": 138, "ed": 140, "text": "multi instance"}, {"st": 152, "ed": 154, "text": "extensively evaluate"}, {"st": 158, "ed": 160, "text": "real world"}, {"st": 168, "ed": 170, "text": "experiments demonstrate"}, {"st": 174, "ed": 176, "text": "proposed method"}, {"st": 178, "ed": 180, "text": "classification performance"}]
[{"st": 15, "ed": 17, "text": "error rate"}, {"st": 29, "ed": 31, "text": "training data"}, {"st": 35, "ed": 38, "text": "empirical risk minimization"}, {"st": 46, "ed": 48, "text": "hinge loss"}, {"st": 79, "ed": 81, "text": "training data"}, {"st": 135, "ed": 138, "text": "named entity recognition"}]
[{"st": 24, "ed": 26, "text": "crucial step"}, {"st": 29, "ed": 31, "text": "based methods"}, {"st": 55, "ed": 57, "text": "m 1"}, {"st": 86, "ed": 88, "text": "based method"}, {"st": 95, "ed": 97, "text": "m 1"}, {"st": 100, "ed": 102, "text": "learning algorithm"}, {"st": 152, "ed": 154, "text": "proposed algorithm"}, {"st": 155, "ed": 158, "text": "deep neural networks"}, {"st": 165, "ed": 167, "text": "learning algorithm"}, {"st": 171, "ed": 173, "text": "without compromising"}]
[{"st": 1, "ed": 3, "text": "significantly faster"}, {"st": 21, "ed": 23, "text": "mean shift"}, {"st": 29, "ed": 31, "text": "high dimensional"}, {"st": 44, "ed": 46, "text": "computational effort"}, {"st": 50, "ed": 52, "text": "multi scale"}, {"st": 91, "ed": 93, "text": "space complexity"}, {"st": 133, "ed": 135, "text": "active learning"}, {"st": 165, "ed": 167, "text": "classification results"}, {"st": 178, "ed": 180, "text": "nearest neighbor"}, {"st": 183, "ed": 185, "text": "support vector"}]
[{"st": 0, "ed": 2, "text": "unsupervised clustering"}, {"st": 24, "ed": 26, "text": "low dimensional"}, {"st": 86, "ed": 88, "text": "neural network"}]
[{"st": 4, "ed": 6, "text": "classification task"}, {"st": 7, "ed": 9, "text": "highly dependent"}, {"st": 11, "ed": 13, "text": "feature space"}, {"st": 40, "ed": 42, "text": "machine learning"}, {"st": 56, "ed": 58, "text": "real life"}, {"st": 67, "ed": 70, "text": "static and dynamic"}, {"st": 86, "ed": 88, "text": "generative models"}, {"st": 90, "ed": 93, "text": "hidden markov models"}, {"st": 96, "ed": 99, "text": "short term memory"}, {"st": 100, "ed": 103, "text": "artificial neural networks"}, {"st": 108, "ed": 110, "text": "temporal information"}, {"st": 137, "ed": 139, "text": "existing techniques"}]
[{"st": 9, "ed": 11, "text": "machine learning"}, {"st": 13, "ed": 15, "text": "recently gained"}, {"st": 18, "ed": 21, "text": "the research community"}, {"st": 41, "ed": 43, "text": "decision trees"}, {"st": 98, "ed": 100, "text": "tree based"}]
[{"st": 1, "ed": 3, "text": "classification algorithm"}, {"st": 24, "ed": 26, "text": "feature space"}, {"st": 64, "ed": 66, "text": "linear program"}, {"st": 73, "ed": 75, "text": "linear programming"}, {"st": 91, "ed": 93, "text": "kernel functions"}, {"st": 95, "ed": 97, "text": "linear classification"}, {"st": 104, "ed": 106, "text": "standard classification"}, {"st": 107, "ed": 110, "text": "support vector machine"}, {"st": 111, "ed": 114, "text": "linear discriminant analysis"}, {"st": 120, "ed": 122, "text": "classification methods"}, {"st": 128, "ed": 130, "text": "standard classification"}, {"st": 179, "ed": 181, "text": "linear classification"}]
[{"st": 7, "ed": 9, "text": "e commerce"}, {"st": 38, "ed": 40, "text": "based methods"}, {"st": 130, "ed": 132, "text": "low dimensional"}, {"st": 136, "ed": 138, "text": "learned representations"}, {"st": 174, "ed": 176, "text": "proposed approach"}]
[{"st": 8, "ed": 11, "text": "support vector machines"}, {"st": 96, "ed": 98, "text": "rule based"}, {"st": 109, "ed": 111, "text": "performance measure"}, {"st": 129, "ed": 131, "text": "class imbalanced"}, {"st": 165, "ed": 167, "text": "proposed method"}]
[{"st": 4, "ed": 7, "text": "conditional random fields"}, {"st": 18, "ed": 21, "text": "linear convergence rate"}, {"st": 24, "ed": 26, "text": "empirical performance"}, {"st": 53, "ed": 55, "text": "unlike previous"}, {"st": 73, "ed": 75, "text": "uniform sampling"}, {"st": 77, "ed": 79, "text": "preliminary experiments"}]
[{"st": 0, "ed": 2, "text": "reinforcement learning"}, {"st": 15, "ed": 17, "text": "impressive results"}, {"st": 22, "ed": 24, "text": "sample complexity"}, {"st": 46, "ed": 48, "text": "least squares"}, {"st": 48, "ed": 50, "text": "temporal difference"}]
[{"st": 0, "ed": 3, "text": "deep neural networks"}, {"st": 84, "ed": 87, "text": "simple yet effective"}, {"st": 92, "ed": 95, "text": "compares favorably to"}, {"st": 99, "ed": 101, "text": "deep learning"}, {"st": 123, "ed": 125, "text": "time series"}]
[{"st": 1, "ed": 4, "text": "short term memory"}, {"st": 10, "ed": 12, "text": "back propagation"}, {"st": 14, "ed": 16, "text": "great success"}, {"st": 44, "ed": 46, "text": "exact bayesian"}, {"st": 46, "ed": 48, "text": "neural network"}, {"st": 55, "ed": 57, "text": "real world"}, {"st": 72, "ed": 74, "text": "kalman filter"}, {"st": 99, "ed": 101, "text": "maximum likelihood"}, {"st": 105, "ed": 107, "text": "proposed algorithm"}, {"st": 115, "ed": 117, "text": "real world"}]
[{"st": 0, "ed": 3, "text": "artificial neural network"}, {"st": 33, "ed": 35, "text": "raw data"}, {"st": 52, "ed": 54, "text": "raw data"}, {"st": 69, "ed": 71, "text": "raw data"}, {"st": 77, "ed": 80, "text": "component analysis pca"}, {"st": 107, "ed": 109, "text": "classification problems"}, {"st": 113, "ed": 115, "text": "active learning"}, {"st": 123, "ed": 125, "text": "empirical results"}, {"st": 132, "ed": 134, "text": "proposed method"}, {"st": 135, "ed": 137, "text": "significantly improve"}]
[{"st": 3, "ed": 5, "text": "optimization problem"}, {"st": 10, "ed": 12, "text": "neural networks"}, {"st": 15, "ed": 17, "text": "mathbf x"}, {"st": 28, "ed": 30, "text": "mathbf x"}, {"st": 79, "ed": 81, "text": "local minima"}, {"st": 112, "ed": 114, "text": "conduct experiments"}, {"st": 122, "ed": 124, "text": "local minima"}, {"st": 145, "ed": 147, "text": "local minima"}]
[{"st": 0, "ed": 2, "text": "kernel regression"}, {"st": 38, "ed": 40, "text": "weighted average"}, {"st": 52, "ed": 54, "text": "distance based"}, {"st": 54, "ed": 56, "text": "kernel function"}, {"st": 76, "ed": 78, "text": "metric learning"}, {"st": 84, "ed": 86, "text": "kernel regression"}, {"st": 89, "ed": 91, "text": "metric learning"}, {"st": 94, "ed": 96, "text": "kernel regression"}, {"st": 98, "ed": 100, "text": "metric learning"}, {"st": 106, "ed": 108, "text": "kernel regression"}, {"st": 117, "ed": 119, "text": "norm regularization"}, {"st": 126, "ed": 128, "text": "mahalanobis distance"}, {"st": 138, "ed": 140, "text": "dimensionality reduction"}, {"st": 153, "ed": 155, "text": "kernel regression"}, {"st": 165, "ed": 167, "text": "proposed method"}, {"st": 185, "ed": 187, "text": "practical problems"}, {"st": 199, "ed": 201, "text": "proposed method"}, {"st": 205, "ed": 207, "text": "kernel regression"}, {"st": 221, "ed": 223, "text": "proposed method"}]
[{"st": 1, "ed": 3, "text": "distributed stochastic"}, {"st": 5, "ed": 7, "text": "t sne"}, {"st": 11, "ed": 13, "text": "dimensionality reduction"}, {"st": 26, "ed": 28, "text": "t sne"}, {"st": 37, "ed": 40, "text": "hundreds of thousands"}, {"st": 49, "ed": 52, "text": "fast fourier transform"}, {"st": 55, "ed": 57, "text": "t sne"}, {"st": 69, "ed": 71, "text": "time consuming"}, {"st": 73, "ed": 75, "text": "t sne"}, {"st": 91, "ed": 94, "text": "fast fourier transform"}, {"st": 121, "ed": 123, "text": "t sne"}, {"st": 134, "ed": 136, "text": "t sne"}, {"st": 153, "ed": 156, "text": "principal component analysis"}, {"st": 161, "ed": 163, "text": "principal components"}, {"st": 178, "ed": 180, "text": "t sne"}, {"st": 181, "ed": 183, "text": "large datasets"}]
[{"st": 8, "ed": 10, "text": "machine learning"}, {"st": 11, "ed": 13, "text": "real world"}, {"st": 24, "ed": 26, "text": "training data"}, {"st": 29, "ed": 31, "text": "data security"}, {"st": 34, "ed": 36, "text": "differential privacy"}, {"st": 44, "ed": 46, "text": "machine learning"}, {"st": 58, "ed": 60, "text": "differential privacy"}, {"st": 82, "ed": 86, "text": "markov chain monte carlo"}, {"st": 86, "ed": 88, "text": "sg mcmc"}, {"st": 92, "ed": 94, "text": "bayesian posterior"}, {"st": 100, "ed": 102, "text": "differential privacy"}, {"st": 116, "ed": 118, "text": "differentially private"}, {"st": 118, "ed": 120, "text": "sg mcmc"}, {"st": 122, "ed": 124, "text": "conduct experiments"}, {"st": 133, "ed": 135, "text": "sg mcmc"}]
[{"st": 13, "ed": 15, "text": "learning algorithm"}, {"st": 17, "ed": 19, "text": "pac bayes"}, {"st": 59, "ed": 61, "text": "pac bayes"}, {"st": 80, "ed": 82, "text": "training error"}, {"st": 99, "ed": 101, "text": "generalization bound"}, {"st": 115, "ed": 117, "text": "langevin dynamics"}, {"st": 120, "ed": 122, "text": "pac bayes"}, {"st": 139, "ed": 142, "text": "mnist and cifar10"}, {"st": 168, "ed": 170, "text": "generalization bounds"}]
[{"st": 6, "ed": 8, "text": "training data"}, {"st": 12, "ed": 14, "text": "deep networks"}, {"st": 18, "ed": 20, "text": "training data"}, {"st": 24, "ed": 26, "text": "noisy labels"}, {"st": 35, "ed": 37, "text": "deep networks"}, {"st": 44, "ed": 46, "text": "network architecture"}, {"st": 51, "ed": 53, "text": "true labels"}, {"st": 64, "ed": 66, "text": "loss functions"}, {"st": 76, "ed": 78, "text": "theoretical results"}, {"st": 79, "ed": 81, "text": "loss functions"}, {"st": 93, "ed": 95, "text": "sufficient conditions"}, {"st": 97, "ed": 99, "text": "loss function"}, {"st": 105, "ed": 107, "text": "loss function"}, {"st": 122, "ed": 124, "text": "existing results"}, {"st": 127, "ed": 129, "text": "loss functions"}, {"st": 139, "ed": 141, "text": "loss functions"}, {"st": 142, "ed": 144, "text": "deep networks"}, {"st": 148, "ed": 150, "text": "loss function"}, {"st": 165, "ed": 167, "text": "back propagation"}, {"st": 189, "ed": 191, "text": "loss functions"}]
[{"st": 3, "ed": 5, "text": "achieve high"}, {"st": 12, "ed": 14, "text": "multilayer perceptrons"}, {"st": 25, "ed": 27, "text": "low rank"}, {"st": 48, "ed": 51, "text": "low rank tensor"}, {"st": 56, "ed": 58, "text": "multi modal"}, {"st": 66, "ed": 68, "text": "low rank"}, {"st": 72, "ed": 74, "text": "theoretical analysis"}, {"st": 94, "ed": 96, "text": "deep convolutional"}, {"st": 98, "ed": 100, "text": "cifar 10"}]
[{"st": 1, "ed": 3, "text": "generative models"}, {"st": 11, "ed": 13, "text": "tasks involving"}, {"st": 32, "ed": 36, "text": "recurrent neural network rnn"}, {"st": 38, "ed": 40, "text": "conditional distribution"}, {"st": 95, "ed": 97, "text": "reinforcement learning"}, {"st": 110, "ed": 112, "text": "generative model"}, {"st": 134, "ed": 136, "text": "log likelihood"}, {"st": 138, "ed": 140, "text": "cifar 10"}, {"st": 157, "ed": 161, "text": "available at https github.com"}]
[{"st": 0, "ed": 2, "text": "kernel based"}, {"st": 24, "ed": 26, "text": "task specific"}, {"st": 35, "ed": 37, "text": "multi kernel"}, {"st": 70, "ed": 72, "text": "multi kernel"}, {"st": 72, "ed": 74, "text": "learning scheme"}, {"st": 99, "ed": 101, "text": "multi kernel"}, {"st": 101, "ed": 103, "text": "learning scheme"}, {"st": 143, "ed": 146, "text": "static and dynamic"}, {"st": 167, "ed": 170, "text": "synthetic and real"}]
[{"st": 0, "ed": 2, "text": "covariate shift"}, {"st": 15, "ed": 18, "text": "training and testing"}, {"st": 25, "ed": 27, "text": "covariate shift"}, {"st": 34, "ed": 37, "text": "training and testing"}, {"st": 43, "ed": 45, "text": "poor performance"}, {"st": 56, "ed": 58, "text": "recently developed"}, {"st": 73, "ed": 75, "text": "covariate shift"}, {"st": 103, "ed": 105, "text": "loss functions"}, {"st": 120, "ed": 122, "text": "covariate shift"}, {"st": 125, "ed": 127, "text": "feature based"}, {"st": 132, "ed": 134, "text": "input variables"}, {"st": 141, "ed": 143, "text": "covariate shift"}, {"st": 156, "ed": 158, "text": "covariate shift"}]
[{"st": 4, "ed": 8, "text": "convolutional neural network cnn"}, {"st": 85, "ed": 87, "text": "multi layered"}, {"st": 134, "ed": 136, "text": "prediction accuracy"}]
[{"st": 4, "ed": 6, "text": "algorithmic framework"}, {"st": 10, "ed": 12, "text": "online learning"}, {"st": 15, "ed": 17, "text": "parameter free"}, {"st": 42, "ed": 44, "text": "meta algorithm"}, {"st": 60, "ed": 62, "text": "computationally efficient"}, {"st": 62, "ed": 64, "text": "parameter free"}, {"st": 69, "ed": 71, "text": "banach spaces"}, {"st": 75, "ed": 77, "text": "previous results"}, {"st": 96, "ed": 98, "text": "mathbb r"}, {"st": 119, "ed": 121, "text": "supervised learning"}, {"st": 130, "ed": 132, "text": "meta algorithm"}, {"st": 136, "ed": 138, "text": "multi scale"}, {"st": 140, "ed": 144, "text": "prediction with expert advice"}]
[{"st": 5, "ed": 8, "text": "deep neural networks"}, {"st": 13, "ed": 15, "text": "pac bayesian"}]
[{"st": 0, "ed": 2, "text": "recommender systems"}, {"st": 25, "ed": 27, "text": "recommender systems"}, {"st": 77, "ed": 80, "text": "markov decision process"}, {"st": 83, "ed": 85, "text": "reinforcement learning"}, {"st": 87, "ed": 89, "text": "automatically learn"}, {"st": 94, "ed": 97, "text": "trial and error"}, {"st": 113, "ed": 115, "text": "user agent"}, {"st": 120, "ed": 122, "text": "pre train"}, {"st": 158, "ed": 160, "text": "proposed framework"}, {"st": 171, "ed": 173, "text": "real world"}, {"st": 173, "ed": 175, "text": "e commerce"}]
[{"st": 6, "ed": 8, "text": "approximate posterior"}, {"st": 16, "ed": 18, "text": "experiment results"}, {"st": 26, "ed": 29, "text": "deep neural network"}, {"st": 33, "ed": 35, "text": "joint probability"}, {"st": 47, "ed": 49, "text": "posterior probability"}, {"st": 61, "ed": 63, "text": "sampling algorithm"}, {"st": 70, "ed": 72, "text": "higher accuracy"}, {"st": 73, "ed": 75, "text": "medium sized"}, {"st": 115, "ed": 117, "text": "result shows"}, {"st": 125, "ed": 127, "text": "training examples"}, {"st": 130, "ed": 132, "text": "don t"}, {"st": 134, "ed": 136, "text": "training examples"}, {"st": 151, "ed": 153, "text": "discriminative model"}, {"st": 154, "ed": 157, "text": "deep neural network"}, {"st": 159, "ed": 161, "text": "generative model"}]
[{"st": 113, "ed": 116, "text": "restricted boltzmann machines"}, {"st": 121, "ed": 123, "text": "extensive experiments"}, {"st": 127, "ed": 130, "text": "publicly available datasets"}]
[{"st": 0, "ed": 2, "text": "recent advances"}, {"st": 10, "ed": 12, "text": "global optimal"}, {"st": 21, "ed": 23, "text": "local optima"}, {"st": 34, "ed": 36, "text": "https github.com"}, {"st": 82, "ed": 84, "text": "optimization problems"}, {"st": 85, "ed": 87, "text": "machine learning"}, {"st": 92, "ed": 94, "text": "large scale"}, {"st": 106, "ed": 108, "text": "real world"}, {"st": 108, "ed": 110, "text": "machine learning"}]
[{"st": 0, "ed": 3, "text": "sparse subspace clustering"}, {"st": 8, "ed": 10, "text": "machine learning"}, {"st": 21, "ed": 23, "text": "low dimensional"}, {"st": 23, "ed": 25, "text": "linear subspaces"}, {"st": 28, "ed": 30, "text": "numerous applications"}, {"st": 31, "ed": 33, "text": "pattern recognition"}, {"st": 55, "ed": 57, "text": "theoretical properties"}, {"st": 69, "ed": 71, "text": "theoretical guarantees"}, {"st": 74, "ed": 76, "text": "incomplete data"}, {"st": 97, "ed": 99, "text": "substantial improvement"}, {"st": 117, "ed": 119, "text": "missing entries"}]
[{"st": 71, "ed": 73, "text": "meta algorithm"}, {"st": 108, "ed": 110, "text": "real data"}, {"st": 117, "ed": 119, "text": "prediction performance"}]
[{"st": 0, "ed": 2, "text": "traffic flow"}, {"st": 13, "ed": 16, "text": "intelligent transportation systems"}, {"st": 45, "ed": 47, "text": "multi link"}, {"st": 49, "ed": 51, "text": "traffic flow"}, {"st": 56, "ed": 58, "text": "prediction models"}, {"st": 75, "ed": 77, "text": "multi link"}, {"st": 79, "ed": 82, "text": "multi task learning"}, {"st": 96, "ed": 98, "text": "multi link"}, {"st": 107, "ed": 109, "text": "neural network"}, {"st": 119, "ed": 121, "text": "problems involving"}, {"st": 125, "ed": 127, "text": "l1 regularization"}, {"st": 131, "ed": 133, "text": "graphical model"}, {"st": 138, "ed": 140, "text": "inverse covariance"}, {"st": 143, "ed": 145, "text": "gaussian process"}, {"st": 170, "ed": 172, "text": "traffic flow"}, {"st": 180, "ed": 182, "text": "traffic flow"}]
[{"st": 6, "ed": 8, "text": "gaussian processes"}, {"st": 79, "ed": 81, "text": "proposed approach"}, {"st": 94, "ed": 96, "text": "brownian motion"}, {"st": 104, "ed": 106, "text": "computationally feasible"}, {"st": 120, "ed": 122, "text": "existing approaches"}]
[{"st": 7, "ed": 10, "text": "generative adversarial networks"}, {"st": 12, "ed": 16, "text": "maximum mean discrepancy mmd"}, {"st": 34, "ed": 36, "text": "loss functions"}, {"st": 48, "ed": 50, "text": "optimization process"}, {"st": 55, "ed": 57, "text": "wasserstein gans"}, {"st": 112, "ed": 114, "text": "recently developed"}, {"st": 132, "ed": 134, "text": "wasserstein gan"}, {"st": 139, "ed": 141, "text": "faster training"}, {"st": 167, "ed": 169, "text": "learning rates"}]
[{"st": 3, "ed": 5, "text": "large datasets"}, {"st": 12, "ed": 14, "text": "missing entries"}, {"st": 20, "ed": 22, "text": "machine learning"}, {"st": 40, "ed": 42, "text": "clustering algorithm"}, {"st": 59, "ed": 61, "text": "ell 0"}, {"st": 63, "ed": 65, "text": "based optimization"}, {"st": 71, "ed": 73, "text": "theoretically analyze"}, {"st": 106, "ed": 109, "text": "simulated and real"}]
[{"st": 0, "ed": 2, "text": "spectral clustering"}, {"st": 37, "ed": 39, "text": "deep learning"}, {"st": 41, "ed": 43, "text": "spectral clustering"}, {"st": 59, "ed": 61, "text": "input data"}, {"st": 91, "ed": 94, "text": "scale to large"}, {"st": 105, "ed": 107, "text": "output layer"}, {"st": 127, "ed": 129, "text": "unseen data"}, {"st": 149, "ed": 151, "text": "unlabeled data"}, {"st": 173, "ed": 176, "text": "end to end"}, {"st": 202, "ed": 204, "text": "clustering results"}, {"st": 214, "ed": 218, "text": "available at https github.com"}]
[{"st": 4, "ed": 7, "text": "armed bandit problem"}, {"st": 32, "ed": 34, "text": "arm identification"}, {"st": 43, "ed": 45, "text": "frac 1"}, {"st": 58, "ed": 60, "text": "ucb algorithm"}, {"st": 65, "ed": 67, "text": "k nn"}, {"st": 73, "ed": 75, "text": "intrinsic dimension"}, {"st": 117, "ed": 120, "text": "multi armed bandit"}]
[{"st": 12, "ed": 14, "text": "matrix factorization"}, {"st": 46, "ed": 48, "text": "marginal likelihood"}, {"st": 95, "ed": 97, "text": "monte carlo"}, {"st": 97, "ed": 100, "text": "expectation maximization algorithm"}]
[{"st": 0, "ed": 2, "text": "cluster analysis"}, {"st": 61, "ed": 63, "text": "cluster analysis"}, {"st": 65, "ed": 67, "text": "detection problem"}, {"st": 98, "ed": 100, "text": "objective function"}, {"st": 121, "ed": 123, "text": "objective function"}, {"st": 132, "ed": 134, "text": "k means"}, {"st": 155, "ed": 157, "text": "challenging problem"}, {"st": 160, "ed": 162, "text": "k means"}, {"st": 165, "ed": 168, "text": "extensive experimental results"}, {"st": 177, "ed": 180, "text": "effectiveness and efficiency"}, {"st": 187, "ed": 189, "text": "k means"}, {"st": 195, "ed": 197, "text": "outlier detection"}, {"st": 233, "ed": 235, "text": "real world"}]
[{"st": 2, "ed": 4, "text": "neural networks"}, {"st": 8, "ed": 10, "text": "input vector"}, {"st": 22, "ed": 24, "text": "training objective"}, {"st": 34, "ed": 37, "text": "takes into account"}, {"st": 76, "ed": 78, "text": "multi layered"}, {"st": 85, "ed": 87, "text": "low dimensional"}]
[{"st": 0, "ed": 2, "text": "machine learning"}, {"st": 8, "ed": 12, "text": "vulnerable to adversarial examples"}, {"st": 36, "ed": 38, "text": "adversarial examples"}, {"st": 45, "ed": 47, "text": "prior probability"}, {"st": 59, "ed": 61, "text": "logistic regression"}, {"st": 64, "ed": 66, "text": "closed form"}, {"st": 72, "ed": 74, "text": "adversarial perturbation"}, {"st": 125, "ed": 127, "text": "real world"}]
[{"st": 0, "ed": 3, "text": "deep generative models"}, {"st": 4, "ed": 6, "text": "powerful tools"}, {"st": 9, "ed": 11, "text": "impressive results"}, {"st": 43, "ed": 45, "text": "inception score"}, {"st": 46, "ed": 48, "text": "recently proposed"}, {"st": 54, "ed": 56, "text": "generative models"}, {"st": 96, "ed": 98, "text": "generative models"}]
[{"st": 6, "ed": 9, "text": "inverse reinforcement learning"}, {"st": 15, "ed": 17, "text": "expert demonstrations"}, {"st": 25, "ed": 27, "text": "previous works"}, {"st": 43, "ed": 45, "text": "objective function"}, {"st": 50, "ed": 52, "text": "nash equilibrium"}, {"st": 62, "ed": 64, "text": "reward function"}, {"st": 68, "ed": 71, "text": "inverse reinforcement learning"}, {"st": 72, "ed": 75, "text": "deep neural networks"}, {"st": 94, "ed": 96, "text": "nash equilibrium"}, {"st": 97, "ed": 99, "text": "large scale"}, {"st": 104, "ed": 106, "text": "adversarial training"}, {"st": 121, "ed": 123, "text": "local optima"}, {"st": 129, "ed": 131, "text": "numerical experiments"}, {"st": 135, "ed": 137, "text": "nash equilibrium"}, {"st": 138, "ed": 141, "text": "inverse reinforcement learning"}, {"st": 149, "ed": 151, "text": "previous approaches"}, {"st": 158, "ed": 160, "text": "expert demonstrations"}, {"st": 164, "ed": 166, "text": "reward functions"}]
[{"st": 6, "ed": 8, "text": "discriminative learning"}, {"st": 9, "ed": 11, "text": "distance metric"}, {"st": 13, "ed": 15, "text": "pattern recognition"}, {"st": 21, "ed": 23, "text": "based methods"}, {"st": 41, "ed": 43, "text": "generalization performance"}, {"st": 46, "ed": 48, "text": "based methods"}, {"st": 73, "ed": 75, "text": "metric learning"}, {"st": 171, "ed": 173, "text": "pattern recognition"}, {"st": 176, "ed": 178, "text": "metric learning"}, {"st": 184, "ed": 186, "text": "metric learning"}, {"st": 197, "ed": 199, "text": "metric learning"}]
[{"st": 1, "ed": 3, "text": "recent years"}, {"st": 3, "ed": 6, "text": "multi label classification"}, {"st": 30, "ed": 32, "text": "supervised learning"}, {"st": 41, "ed": 43, "text": "learning algorithms"}, {"st": 44, "ed": 46, "text": "base level"}, {"st": 59, "ed": 61, "text": "ensemble learning"}, {"st": 65, "ed": 67, "text": "base level"}, {"st": 89, "ed": 91, "text": "base level"}, {"st": 96, "ed": 98, "text": "ensemble learning"}, {"st": 103, "ed": 106, "text": "multi label classification"}, {"st": 116, "ed": 118, "text": "base level"}, {"st": 123, "ed": 125, "text": "experiment results"}, {"st": 127, "ed": 129, "text": "proposed approach"}, {"st": 137, "ed": 140, "text": "multi label classification"}]
[{"st": 19, "ed": 21, "text": "deep models"}, {"st": 35, "ed": 37, "text": "deep network"}, {"st": 44, "ed": 46, "text": "finite dimensional"}, {"st": 69, "ed": 71, "text": "convergence speed"}, {"st": 77, "ed": 79, "text": "representational power"}, {"st": 103, "ed": 105, "text": "infinite dimensional"}, {"st": 113, "ed": 115, "text": "infinite dimensional"}, {"st": 126, "ed": 128, "text": "finite dimensional"}, {"st": 138, "ed": 140, "text": "local optima"}, {"st": 141, "ed": 143, "text": "finite dimensional"}, {"st": 148, "ed": 150, "text": "global optimal"}, {"st": 172, "ed": 174, "text": "optimization procedure"}, {"st": 216, "ed": 218, "text": "numerical experiments"}]
[{"st": 7, "ed": 10, "text": "simple yet effective"}, {"st": 24, "ed": 27, "text": "mnist and cifar10"}, {"st": 49, "ed": 52, "text": "deep neural networks"}, {"st": 55, "ed": 57, "text": "building blocks"}, {"st": 118, "ed": 121, "text": "each data point"}, {"st": 128, "ed": 132, "text": "signal to noise ratio"}]
[{"st": 1, "ed": 3, "text": "recent years"}, {"st": 6, "ed": 8, "text": "machine learning"}, {"st": 31, "ed": 33, "text": "recognition tasks"}, {"st": 43, "ed": 45, "text": "training samples"}, {"st": 70, "ed": 72, "text": "deep learning"}, {"st": 84, "ed": 87, "text": "deep neural network"}, {"st": 89, "ed": 91, "text": "deep features"}, {"st": 99, "ed": 101, "text": "experiments demonstrate"}, {"st": 107, "ed": 110, "text": "effective and efficient"}]
[{"st": 93, "ed": 95, "text": "theoretical analysis"}]
[{"st": 0, "ed": 2, "text": "cluster analysis"}, {"st": 8, "ed": 10, "text": "unlabeled data"}, {"st": 21, "ed": 23, "text": "cluster analysis"}, {"st": 59, "ed": 61, "text": "key challenge"}, {"st": 106, "ed": 108, "text": "internal validity"}, {"st": 116, "ed": 118, "text": "clustering algorithms"}, {"st": 148, "ed": 150, "text": "distress signal"}, {"st": 159, "ed": 161, "text": "numerical examples"}]
[{"st": 4, "ed": 6, "text": "larger datasets"}, {"st": 14, "ed": 16, "text": "wide variety"}, {"st": 22, "ed": 24, "text": "cluster analysis"}, {"st": 30, "ed": 33, "text": "exploratory data analysis"}, {"st": 49, "ed": 51, "text": "k means"}, {"st": 65, "ed": 67, "text": "initial conditions"}, {"st": 94, "ed": 96, "text": "k means"}, {"st": 127, "ed": 129, "text": "entire dataset"}, {"st": 164, "ed": 166, "text": "theoretical properties"}, {"st": 178, "ed": 180, "text": "method outperforms"}]
[{"st": 55, "ed": 57, "text": "feature representation"}, {"st": 59, "ed": 61, "text": "feature learning"}, {"st": 62, "ed": 64, "text": "word embedding"}, {"st": 69, "ed": 71, "text": "deep architectures"}, {"st": 72, "ed": 74, "text": "feature representation"}, {"st": 75, "ed": 77, "text": "higher level"}, {"st": 90, "ed": 92, "text": "prediction models"}, {"st": 104, "ed": 106, "text": "unlabeled data"}, {"st": 118, "ed": 120, "text": "representation learning"}, {"st": 166, "ed": 168, "text": "african americans"}, {"st": 173, "ed": 175, "text": "large datasets"}, {"st": 201, "ed": 203, "text": "comparative study"}]
[{"st": 1, "ed": 4, "text": "convolutional neural networks"}, {"st": 40, "ed": 42, "text": "real data"}, {"st": 85, "ed": 87, "text": "efficiently learn"}, {"st": 90, "ed": 93, "text": "distance metric learning"}, {"st": 95, "ed": 97, "text": "extensive experiments"}, {"st": 106, "ed": 108, "text": "performance improvement"}, {"st": 110, "ed": 112, "text": "convergence speed"}]
[{"st": 10, "ed": 14, "text": "multi armed bandit problem"}, {"st": 20, "ed": 22, "text": "bandit problem"}, {"st": 33, "ed": 35, "text": "regret bounds"}, {"st": 38, "ed": 40, "text": "examples include"}, {"st": 42, "ed": 44, "text": "regret bound"}, {"st": 55, "ed": 57, "text": "regret bound"}, {"st": 71, "ed": 73, "text": "regret bound"}, {"st": 96, "ed": 98, "text": "faster convergence"}, {"st": 109, "ed": 111, "text": "regret bound"}, {"st": 132, "ed": 134, "text": "expected loss"}, {"st": 167, "ed": 169, "text": "main idea"}, {"st": 222, "ed": 224, "text": "learning rate"}]
[{"st": 5, "ed": 7, "text": "weakly supervised"}, {"st": 18, "ed": 20, "text": "detection task"}, {"st": 24, "ed": 26, "text": "unseen classes"}, {"st": 85, "ed": 87, "text": "neural network"}, {"st": 99, "ed": 101, "text": "latent representation"}, {"st": 105, "ed": 107, "text": "cosine similarity"}, {"st": 111, "ed": 113, "text": "attention mechanism"}, {"st": 127, "ed": 129, "text": "detection task"}, {"st": 144, "ed": 146, "text": "domains including"}, {"st": 146, "ed": 148, "text": "computer vision"}, {"st": 160, "ed": 162, "text": "detection task"}, {"st": 197, "ed": 199, "text": "computer vision"}, {"st": 199, "ed": 201, "text": "object detection"}]
[{"st": 0, "ed": 2, "text": "positive definite"}, {"st": 6, "ed": 10, "text": "reproducing kernel hilbert spaces"}, {"st": 28, "ed": 30, "text": "approximation theory"}, {"st": 30, "ed": 33, "text": "point of view"}, {"st": 65, "ed": 67, "text": "kernel space"}, {"st": 114, "ed": 116, "text": "kernel space"}, {"st": 182, "ed": 184, "text": "kernel matrices"}, {"st": 186, "ed": 188, "text": "exponential decay"}]
[{"st": 3, "ed": 5, "text": "latent variable"}, {"st": 11, "ed": 14, "text": "scale to large"}, {"st": 18, "ed": 20, "text": "approximate inference"}, {"st": 30, "ed": 32, "text": "variational distribution"}, {"st": 48, "ed": 50, "text": "variational parameters"}, {"st": 55, "ed": 57, "text": "approximate inference"}, {"st": 58, "ed": 60, "text": "variational autoencoders"}]
[{"st": 0, "ed": 2, "text": "latent variable"}, {"st": 10, "ed": 12, "text": "missing data"}, {"st": 14, "ed": 16, "text": "variational autoencoder"}, {"st": 29, "ed": 31, "text": "encoder network"}, {"st": 34, "ed": 36, "text": "latent variables"}, {"st": 48, "ed": 50, "text": "missing data"}, {"st": 62, "ed": 64, "text": "posterior distribution"}, {"st": 66, "ed": 68, "text": "factor analysis"}, {"st": 74, "ed": 76, "text": "missing data"}, {"st": 83, "ed": 85, "text": "non trivial"}]
[{"st": 8, "ed": 12, "text": "expectation maximization em algorithm"}, {"st": 14, "ed": 16, "text": "expectation maximization"}, {"st": 46, "ed": 48, "text": "em algorithm"}, {"st": 67, "ed": 69, "text": "expectation maximization"}, {"st": 96, "ed": 99, "text": "gaussian mixture model"}, {"st": 123, "ed": 125, "text": "additive noise"}, {"st": 127, "ed": 129, "text": "sample size"}]
[{"st": 0, "ed": 2, "text": "recent research"}, {"st": 4, "ed": 7, "text": "deep neural networks"}, {"st": 13, "ed": 15, "text": "adversarial perturbations"}, {"st": 21, "ed": 23, "text": "input data"}, {"st": 28, "ed": 30, "text": "machine learning"}, {"st": 32, "ed": 34, "text": "classification models"}, {"st": 35, "ed": 37, "text": "deep learning"}, {"st": 40, "ed": 43, "text": "vulnerable to adversarial"}, {"st": 56, "ed": 59, "text": "deep neural networks"}, {"st": 79, "ed": 81, "text": "hidden layer"}, {"st": 90, "ed": 92, "text": "preliminary experiments"}, {"st": 106, "ed": 108, "text": "adversarial training"}]
[{"st": 6, "ed": 8, "text": "mutual information"}, {"st": 12, "ed": 14, "text": "random variables"}, {"st": 26, "ed": 28, "text": "mutual information"}, {"st": 41, "ed": 43, "text": "sample size"}, {"st": 91, "ed": 93, "text": "supervised classification"}, {"st": 96, "ed": 98, "text": "substantial improvement"}]
[{"st": 4, "ed": 6, "text": "fully connected"}, {"st": 6, "ed": 8, "text": "neural network"}, {"st": 47, "ed": 49, "text": "experimentally demonstrate"}, {"st": 63, "ed": 65, "text": "sampling methods"}, {"st": 71, "ed": 74, "text": "gaussian mixture models"}, {"st": 110, "ed": 112, "text": "output distribution"}]
[{"st": 1, "ed": 4, "text": "the past decade"}, {"st": 4, "ed": 7, "text": "multivariate time series"}, {"st": 21, "ed": 23, "text": "time series"}, {"st": 23, "ed": 25, "text": "classification models"}, {"st": 64, "ed": 67, "text": "multivariate time series"}, {"st": 67, "ed": 69, "text": "classification tasks"}, {"st": 71, "ed": 73, "text": "activity recognition"}, {"st": 81, "ed": 83, "text": "highly efficient"}]
[{"st": 1, "ed": 3, "text": "policy gradient"}, {"st": 5, "ed": 7, "text": "reinforcement learning"}, {"st": 8, "ed": 10, "text": "continuous control"}, {"st": 23, "ed": 26, "text": "easy to implement"}, {"st": 27, "ed": 29, "text": "explicit knowledge"}, {"st": 37, "ed": 40, "text": "end to end"}, {"st": 41, "ed": 43, "text": "directly optimizing"}, {"st": 44, "ed": 46, "text": "performance metric"}, {"st": 66, "ed": 68, "text": "continuous control"}, {"st": 79, "ed": 82, "text": "non convex optimization"}, {"st": 105, "ed": 107, "text": "optimal control"}, {"st": 136, "ed": 138, "text": "policy gradient"}, {"st": 143, "ed": 145, "text": "optimal solution"}]
[{"st": 15, "ed": 17, "text": "batch normalization"}]
[{"st": 31, "ed": 33, "text": "multi view"}, {"st": 33, "ed": 35, "text": "time series"}, {"st": 48, "ed": 50, "text": "correlation analysis"}, {"st": 55, "ed": 57, "text": "neural networks"}, {"st": 80, "ed": 82, "text": "canonical correlation"}, {"st": 86, "ed": 90, "text": "feed forward neural networks"}]
[{"st": 0, "ed": 2, "text": "survival analysis"}, {"st": 49, "ed": 51, "text": "multi task"}, {"st": 51, "ed": 53, "text": "logistic regression"}, {"st": 60, "ed": 62, "text": "deep learning"}, {"st": 77, "ed": 79, "text": "method outperforms"}]
[{"st": 0, "ed": 3, "text": "generative adversarial networks"}, {"st": 63, "ed": 65, "text": "theoretical foundation"}, {"st": 66, "ed": 68, "text": "generative adversarial"}, {"st": 129, "ed": 131, "text": "probability distributions"}, {"st": 132, "ed": 134, "text": "real data"}, {"st": 135, "ed": 137, "text": "generated data"}, {"st": 151, "ed": 154, "text": "point of view"}, {"st": 161, "ed": 163, "text": "generative models"}, {"st": 188, "ed": 190, "text": "empirical results"}, {"st": 193, "ed": 195, "text": "image generation"}]
[{"st": 0, "ed": 2, "text": "imitation learning"}]
[{"st": 7, "ed": 9, "text": "primal dual"}, {"st": 21, "ed": 23, "text": "minimization problems"}, {"st": 25, "ed": 27, "text": "regularization term"}, {"st": 47, "ed": 49, "text": "closed form"}, {"st": 57, "ed": 59, "text": "regularization term"}, {"st": 70, "ed": 72, "text": "per iteration"}, {"st": 81, "ed": 83, "text": "objective function"}, {"st": 84, "ed": 86, "text": "extremely high"}, {"st": 90, "ed": 92, "text": "input data"}, {"st": 109, "ed": 111, "text": "regularization term"}, {"st": 159, "ed": 161, "text": "numerical experiments"}]
[{"st": 1, "ed": 4, "text": "a b testing"}, {"st": 65, "ed": 67, "text": "off policy"}, {"st": 77, "ed": 79, "text": "importance sampling"}, {"st": 81, "ed": 83, "text": "importance sampling"}, {"st": 88, "ed": 90, "text": "bias variance"}, {"st": 120, "ed": 122, "text": "real world"}]
[{"st": 0, "ed": 2, "text": "recent studies"}, {"st": 6, "ed": 8, "text": "prediction models"}, {"st": 9, "ed": 11, "text": "prediction accuracy"}, {"st": 13, "ed": 15, "text": "random forest"}, {"st": 42, "ed": 44, "text": "random forest"}, {"st": 64, "ed": 66, "text": "exhaustive search"}, {"st": 73, "ed": 75, "text": "random forest"}, {"st": 94, "ed": 96, "text": "random forest"}, {"st": 157, "ed": 159, "text": "web application"}]
[{"st": 86, "ed": 88, "text": "partially observable"}, {"st": 157, "ed": 159, "text": "convergence speed"}, {"st": 160, "ed": 162, "text": "optimization problems"}]
[{"st": 8, "ed": 10, "text": "imitation learning"}, {"st": 26, "ed": 28, "text": "policy optimization"}, {"st": 32, "ed": 34, "text": "online learning"}, {"st": 112, "ed": 114, "text": "theoretical insights"}]
[{"st": 6, "ed": 8, "text": "predictive model"}, {"st": 14, "ed": 16, "text": "regularization technique"}, {"st": 28, "ed": 30, "text": "training points"}, {"st": 50, "ed": 52, "text": "sampling based"}]
[{"st": 0, "ed": 2, "text": "recent advances"}, {"st": 6, "ed": 9, "text": "linear discriminant analysis"}, {"st": 15, "ed": 17, "text": "dimensionality reduction"}, {"st": 97, "ed": 99, "text": "regularization term"}, {"st": 102, "ed": 104, "text": "generalization performance"}, {"st": 119, "ed": 121, "text": "convex problems"}]
[{"st": 2, "ed": 6, "text": "problems in machine learning"}, {"st": 11, "ed": 13, "text": "deep learning"}, {"st": 25, "ed": 27, "text": "convolutional networks"}, {"st": 45, "ed": 48, "text": "compares favorably with"}, {"st": 67, "ed": 69, "text": "labeled data"}, {"st": 156, "ed": 158, "text": "significantly improve"}, {"st": 174, "ed": 176, "text": "extensive experiments"}]
[{"st": 28, "ed": 31, "text": "learning to rank"}, {"st": 77, "ed": 80, "text": "learning to rank"}, {"st": 169, "ed": 171, "text": "significantly outperforms"}]
[{"st": 1, "ed": 3, "text": "recent years"}, {"st": 6, "ed": 8, "text": "deep learning"}, {"st": 20, "ed": 22, "text": "gesture recognition"}, {"st": 23, "ed": 25, "text": "deep learning"}, {"st": 69, "ed": 72, "text": "tens of thousands"}, {"st": 92, "ed": 94, "text": "low cost"}, {"st": 112, "ed": 114, "text": "pre training"}, {"st": 128, "ed": 130, "text": "convolutional network"}, {"st": 134, "ed": 136, "text": "transfer learning"}, {"st": 160, "ed": 162, "text": "training data"}, {"st": 168, "ed": 170, "text": "transfer learning"}, {"st": 182, "ed": 184, "text": "gesture recognition"}, {"st": 203, "ed": 205, "text": "case study"}]
[{"st": 0, "ed": 2, "text": "labeled data"}, {"st": 5, "ed": 7, "text": "activity recognition"}, {"st": 28, "ed": 30, "text": "real world"}, {"st": 32, "ed": 35, "text": "semi supervised learning"}, {"st": 36, "ed": 38, "text": "labeled examples"}, {"st": 39, "ed": 41, "text": "unlabeled examples"}, {"st": 48, "ed": 50, "text": "semi supervised"}, {"st": 54, "ed": 56, "text": "activity recognition"}, {"st": 59, "ed": 61, "text": "feature engineering"}, {"st": 74, "ed": 76, "text": "semi supervised"}, {"st": 79, "ed": 83, "text": "convolutional neural networks cnns"}, {"st": 89, "ed": 91, "text": "semi supervised"}, {"st": 95, "ed": 98, "text": "labeled and unlabeled"}, {"st": 102, "ed": 104, "text": "feature learning"}, {"st": 112, "ed": 114, "text": "real world"}, {"st": 121, "ed": 123, "text": "supervised methods"}, {"st": 125, "ed": 128, "text": "semi supervised learning"}]
[{"st": 68, "ed": 70, "text": "theoretical understanding"}, {"st": 79, "ed": 81, "text": "spectral graph"}, {"st": 84, "ed": 86, "text": "kernel based"}, {"st": 95, "ed": 97, "text": "kernel method"}, {"st": 111, "ed": 113, "text": "fully connected"}, {"st": 115, "ed": 117, "text": "edge weights"}, {"st": 131, "ed": 133, "text": "anomaly detection"}, {"st": 138, "ed": 140, "text": "achieves higher"}, {"st": 145, "ed": 147, "text": "anomaly detection"}]
[{"st": 0, "ed": 2, "text": "training set"}, {"st": 14, "ed": 16, "text": "training set"}, {"st": 63, "ed": 65, "text": "training set"}, {"st": 81, "ed": 83, "text": "training set"}, {"st": 92, "ed": 94, "text": "training set"}, {"st": 134, "ed": 136, "text": "optimization problem"}, {"st": 143, "ed": 145, "text": "continuous optimization"}, {"st": 151, "ed": 153, "text": "real data"}, {"st": 159, "ed": 161, "text": "training set"}]
[{"st": 2, "ed": 4, "text": "recommender systems"}, {"st": 33, "ed": 35, "text": "existing methods"}, {"st": 105, "ed": 107, "text": "knowledge graph"}, {"st": 114, "ed": 116, "text": "content based"}, {"st": 120, "ed": 123, "text": "click through rate"}, {"st": 131, "ed": 133, "text": "multi channel"}, {"st": 139, "ed": 142, "text": "convolutional neural network"}, {"st": 198, "ed": 200, "text": "extensive experiments"}]
[{"st": 3, "ed": 5, "text": "originally designed"}, {"st": 36, "ed": 38, "text": "fully connected"}, {"st": 38, "ed": 40, "text": "neural networks"}, {"st": 57, "ed": 59, "text": "multilayer perceptrons"}, {"st": 66, "ed": 68, "text": "models 1"}, {"st": 71, "ed": 73, "text": "additive models"}, {"st": 82, "ed": 84, "text": "boosted trees"}]
[{"st": 6, "ed": 9, "text": "task of classifying"}, {"st": 46, "ed": 50, "text": "deep neural network architecture"}, {"st": 55, "ed": 58, "text": "fully convolutional network"}, {"st": 61, "ed": 63, "text": "primal dual"}, {"st": 67, "ed": 71, "text": "trained end to end"}, {"st": 92, "ed": 94, "text": "energy minimization"}, {"st": 96, "ed": 98, "text": "fully convolutional"}, {"st": 98, "ed": 100, "text": "neural network"}, {"st": 141, "ed": 143, "text": "primal dual"}, {"st": 160, "ed": 162, "text": "primal dual"}, {"st": 177, "ed": 179, "text": "primal dual"}, {"st": 196, "ed": 198, "text": "hyperparameter tuning"}, {"st": 209, "ed": 211, "text": "pre trained"}]
[{"st": 2, "ed": 5, "text": "deep generative models"}, {"st": 8, "ed": 10, "text": "multiple modalities"}, {"st": 43, "ed": 45, "text": "joint representation"}, {"st": 66, "ed": 68, "text": "variational autoencoders"}, {"st": 75, "ed": 77, "text": "variational autoencoder"}, {"st": 98, "ed": 100, "text": "joint representation"}, {"st": 142, "ed": 144, "text": "experiments demonstrate"}, {"st": 183, "ed": 185, "text": "joint representation"}, {"st": 199, "ed": 201, "text": "joint representation"}]
[{"st": 1, "ed": 3, "text": "recent years"}, {"st": 3, "ed": 6, "text": "generative adversarial networks"}, {"st": 19, "ed": 21, "text": "latent spaces"}, {"st": 26, "ed": 28, "text": "high dimensional"}, {"st": 55, "ed": 57, "text": "spatio temporal"}, {"st": 66, "ed": 68, "text": "main goal"}, {"st": 71, "ed": 73, "text": "temporal data"}, {"st": 77, "ed": 80, "text": "low dimensional latent"}, {"st": 87, "ed": 89, "text": "spatio temporal"}, {"st": 98, "ed": 100, "text": "wasserstein gan"}, {"st": 107, "ed": 110, "text": "semi supervised learning"}, {"st": 154, "ed": 156, "text": "initial results"}, {"st": 158, "ed": 160, "text": "classification performance"}, {"st": 163, "ed": 165, "text": "latent representations"}]
[{"st": 11, "ed": 13, "text": "finite mixture"}, {"st": 26, "ed": 28, "text": "mixture models"}]
[{"st": 120, "ed": 122, "text": "multi class"}, {"st": 122, "ed": 125, "text": "linear discriminant analysis"}, {"st": 160, "ed": 162, "text": "numerical stability"}, {"st": 170, "ed": 172, "text": "statistical significance"}]
[{"st": 7, "ed": 9, "text": "signal processing"}, {"st": 10, "ed": 12, "text": "machine learning"}, {"st": 25, "ed": 27, "text": "time series"}, {"st": 40, "ed": 42, "text": "dimensionality reduction"}, {"st": 66, "ed": 68, "text": "dimensionality reduction"}, {"st": 88, "ed": 90, "text": "dimensionality reduction"}, {"st": 92, "ed": 94, "text": "special cases"}, {"st": 114, "ed": 117, "text": "taking into account"}, {"st": 125, "ed": 127, "text": "closed form"}, {"st": 133, "ed": 135, "text": "synthetic datasets"}]
[{"st": 0, "ed": 2, "text": "recurrent models"}, {"st": 20, "ed": 22, "text": "remains challenging"}, {"st": 37, "ed": 39, "text": "hierarchical structure"}, {"st": 65, "ed": 67, "text": "current approaches"}]
[{"st": 8, "ed": 11, "text": "artificial neural networks"}, {"st": 35, "ed": 37, "text": "neural network"}, {"st": 62, "ed": 64, "text": "classification problems"}, {"st": 107, "ed": 109, "text": "extensive experiments"}, {"st": 120, "ed": 122, "text": "numerical results"}, {"st": 127, "ed": 129, "text": "significant reduction"}, {"st": 138, "ed": 140, "text": "input signals"}]
[{"st": 6, "ed": 8, "text": "efficiently compute"}, {"st": 11, "ed": 13, "text": "steady state"}, {"st": 29, "ed": 31, "text": "differential equation"}, {"st": 36, "ed": 40, "text": "feed forward neural network"}]
[{"st": 45, "ed": 47, "text": "clustering algorithms"}, {"st": 60, "ed": 62, "text": "successfully applied"}, {"st": 63, "ed": 65, "text": "embedding vectors"}, {"st": 67, "ed": 69, "text": "face images"}]
[{"st": 0, "ed": 4, "text": "convolutional neural networks cnns"}, {"st": 11, "ed": 13, "text": "learning problems"}, {"st": 36, "ed": 38, "text": "examples include"}, {"st": 59, "ed": 61, "text": "convolutional networks"}, {"st": 86, "ed": 88, "text": "weight sharing"}, {"st": 95, "ed": 97, "text": "building blocks"}, {"st": 108, "ed": 110, "text": "cross correlation"}, {"st": 137, "ed": 140, "text": "fast fourier transform"}, {"st": 145, "ed": 147, "text": "computational efficiency"}]
[{"st": 91, "ed": 93, "text": "deep learning"}, {"st": 106, "ed": 108, "text": "deep learning"}, {"st": 167, "ed": 171, "text": "convolutional neural networks cnns"}, {"st": 178, "ed": 180, "text": "deep learning"}]
[{"st": 12, "ed": 14, "text": "low dimensional"}, {"st": 25, "ed": 27, "text": "low rank"}, {"st": 31, "ed": 33, "text": "latent features"}, {"st": 48, "ed": 50, "text": "convex optimization"}, {"st": 51, "ed": 53, "text": "side information"}, {"st": 100, "ed": 102, "text": "convex optimization"}, {"st": 106, "ed": 108, "text": "low dimensional"}, {"st": 122, "ed": 125, "text": "singular value decomposition"}]
[{"st": 0, "ed": 3, "text": "gaussian processes gps"}, {"st": 8, "ed": 10, "text": "capture complex"}, {"st": 12, "ed": 14, "text": "large scale"}, {"st": 27, "ed": 29, "text": "real world"}, {"st": 35, "ed": 38, "text": "high computational cost"}, {"st": 53, "ed": 55, "text": "kernel matrix"}, {"st": 63, "ed": 65, "text": "deep learning"}, {"st": 73, "ed": 75, "text": "kernel matrix"}, {"st": 92, "ed": 94, "text": "low rank"}, {"st": 102, "ed": 104, "text": "kernel matrix"}, {"st": 106, "ed": 108, "text": "mathcal o"}, {"st": 140, "ed": 142, "text": "demonstrate empirically"}, {"st": 154, "ed": 156, "text": "predictive performance"}]
[{"st": 5, "ed": 8, "text": "learning to rank"}, {"st": 12, "ed": 14, "text": "multi view"}, {"st": 16, "ed": 19, "text": "learning to rank"}, {"st": 30, "ed": 33, "text": "multi view learning"}, {"st": 70, "ed": 72, "text": "multi objective"}, {"st": 81, "ed": 83, "text": "feature mapping"}, {"st": 100, "ed": 103, "text": "end to end"}, {"st": 115, "ed": 117, "text": "ranking loss"}, {"st": 133, "ed": 135, "text": "proposed method"}, {"st": 139, "ed": 141, "text": "wide variety"}, {"st": 143, "ed": 145, "text": "problems including"}, {"st": 147, "ed": 149, "text": "multi view"}, {"st": 153, "ed": 155, "text": "image data"}]
[{"st": 2, "ed": 4, "text": "traffic congestion"}, {"st": 29, "ed": 31, "text": "traffic congestion"}, {"st": 39, "ed": 41, "text": "nashville tn"}, {"st": 44, "ed": 46, "text": "multi layered"}, {"st": 75, "ed": 77, "text": "traditional approaches"}, {"st": 81, "ed": 83, "text": "machine learning"}, {"st": 94, "ed": 96, "text": "traffic congestion"}, {"st": 116, "ed": 118, "text": "image data"}]
[{"st": 7, "ed": 9, "text": "transfer learning"}, {"st": 11, "ed": 13, "text": "pre trained"}, {"st": 13, "ed": 16, "text": "deep neural network"}, {"st": 20, "ed": 22, "text": "image dataset"}, {"st": 57, "ed": 59, "text": "a 20"}]
[{"st": 14, "ed": 16, "text": "streaming data"}, {"st": 33, "ed": 35, "text": "kernel pca"}, {"st": 45, "ed": 47, "text": "kernel matrix"}, {"st": 50, "ed": 52, "text": "computationally efficient"}, {"st": 65, "ed": 67, "text": "nystr om"}, {"st": 70, "ed": 72, "text": "kernel matrix"}, {"st": 81, "ed": 83, "text": "nystr om"}, {"st": 94, "ed": 96, "text": "empirical evaluation"}]
[{"st": 10, "ed": 13, "text": "deep neural networks"}, {"st": 19, "ed": 21, "text": "task specific"}, {"st": 21, "ed": 23, "text": "performance measures"}, {"st": 26, "ed": 28, "text": "f measure"}, {"st": 30, "ed": 33, "text": "kullback leibler divergence"}, {"st": 45, "ed": 47, "text": "deep learning"}, {"st": 53, "ed": 56, "text": "cross entropy loss"}, {"st": 70, "ed": 72, "text": "task specific"}, {"st": 72, "ed": 74, "text": "loss functions"}, {"st": 100, "ed": 102, "text": "stationary points"}, {"st": 105, "ed": 107, "text": "objective functions"}, {"st": 111, "ed": 113, "text": "training samples"}, {"st": 148, "ed": 150, "text": "deep architectures"}, {"st": 151, "ed": 153, "text": "multi layer"}, {"st": 155, "ed": 158, "text": "recurrent neural networks"}, {"st": 167, "ed": 169, "text": "real data"}, {"st": 172, "ed": 174, "text": "outperform traditional"}, {"st": 177, "ed": 179, "text": "deep networks"}, {"st": 183, "ed": 185, "text": "recent approaches"}, {"st": 186, "ed": 188, "text": "task specific"}]
[{"st": 0, "ed": 2, "text": "sigma pi"}, {"st": 3, "ed": 5, "text": "neural networks"}, {"st": 12, "ed": 14, "text": "neural networks"}, {"st": 23, "ed": 26, "text": "feedforward neural networks"}, {"st": 28, "ed": 30, "text": "neural networks"}, {"st": 33, "ed": 35, "text": "existing literature"}, {"st": 119, "ed": 121, "text": "sigma pi"}, {"st": 122, "ed": 124, "text": "neural network"}, {"st": 154, "ed": 156, "text": "regularization technique"}, {"st": 158, "ed": 160, "text": "learning process"}, {"st": 199, "ed": 201, "text": "numerical experiments"}, {"st": 203, "ed": 205, "text": "benchmark problems"}]
[{"st": 0, "ed": 3, "text": "recurrent neural networks"}, {"st": 5, "ed": 7, "text": "excellent performance"}, {"st": 31, "ed": 33, "text": "large scale"}, {"st": 60, "ed": 63, "text": "weights and activations"}, {"st": 97, "ed": 100, "text": "binary search tree"}, {"st": 100, "ed": 102, "text": "alternating minimization"}, {"st": 116, "ed": 119, "text": "short term memory"}, {"st": 121, "ed": 125, "text": "gated recurrent unit gru"}, {"st": 205, "ed": 207, "text": "image classification"}, {"st": 212, "ed": 215, "text": "feedforward neural networks"}]
[{"st": 0, "ed": 2, "text": "intrusion detection"}, {"st": 22, "ed": 24, "text": "important role"}, {"st": 54, "ed": 56, "text": "anomaly detection"}, {"st": 73, "ed": 75, "text": "anomaly detection"}, {"st": 80, "ed": 82, "text": "classification problem"}, {"st": 107, "ed": 109, "text": "current approaches"}, {"st": 110, "ed": 112, "text": "anomaly detection"}, {"st": 150, "ed": 152, "text": "anomaly detection"}, {"st": 155, "ed": 157, "text": "neural network"}, {"st": 161, "ed": 164, "text": "short term memory"}, {"st": 164, "ed": 167, "text": "recurrent neural network"}, {"st": 167, "ed": 169, "text": "lstm rnn"}, {"st": 183, "ed": 185, "text": "steps ahead"}, {"st": 192, "ed": 194, "text": "lstm rnn"}, {"st": 198, "ed": 200, "text": "time series"}, {"st": 219, "ed": 221, "text": "prediction errors"}, {"st": 240, "ed": 242, "text": "prediction errors"}, {"st": 265, "ed": 267, "text": "time series"}, {"st": 274, "ed": 276, "text": "experiments demonstrate"}]
[{"st": 6, "ed": 8, "text": "multi view"}, {"st": 16, "ed": 18, "text": "sequential data"}, {"st": 51, "ed": 53, "text": "neural network"}, {"st": 216, "ed": 218, "text": "competitive performance"}, {"st": 223, "ed": 225, "text": "deep learning"}, {"st": 228, "ed": 230, "text": "multi view"}]
[{"st": 2, "ed": 4, "text": "deep learning"}, {"st": 9, "ed": 12, "text": "multi label classification"}, {"st": 16, "ed": 18, "text": "representation learning"}, {"st": 21, "ed": 24, "text": "end to end"}, {"st": 27, "ed": 30, "text": "deep neural networks"}, {"st": 31, "ed": 34, "text": "multi label classification"}, {"st": 42, "ed": 44, "text": "neural network"}, {"st": 48, "ed": 50, "text": "neural network"}, {"st": 86, "ed": 88, "text": "multi class"}, {"st": 90, "ed": 92, "text": "multi label"}, {"st": 101, "ed": 103, "text": "neural network"}, {"st": 139, "ed": 141, "text": "neural network"}, {"st": 155, "ed": 157, "text": "cross entropy"}, {"st": 162, "ed": 164, "text": "multi label"}, {"st": 184, "ed": 186, "text": "benchmark dataset"}, {"st": 199, "ed": 201, "text": "multi label"}, {"st": 202, "ed": 204, "text": "classification methods"}, {"st": 206, "ed": 208, "text": "classification performance"}]
[{"st": 3, "ed": 6, "text": "multi task learning"}, {"st": 27, "ed": 29, "text": "reinforcement learning"}, {"st": 30, "ed": 33, "text": "multi task learning"}, {"st": 52, "ed": 54, "text": "reinforcement learning"}, {"st": 56, "ed": 59, "text": "multi task learning"}, {"st": 82, "ed": 85, "text": "multi task learning"}, {"st": 87, "ed": 89, "text": "continuous action"}, {"st": 107, "ed": 110, "text": "multi task learning"}]
[{"st": 0, "ed": 2, "text": "unsupervised learning"}, {"st": 3, "ed": 5, "text": "time series"}, {"st": 13, "ed": 15, "text": "challenging problem"}, {"st": 31, "ed": 33, "text": "dimensionality reduction"}, {"st": 39, "ed": 42, "text": "end to end"}, {"st": 42, "ed": 44, "text": "learning framework"}, {"st": 53, "ed": 55, "text": "dimensionality reduction"}, {"st": 73, "ed": 75, "text": "dimensionality reduction"}, {"st": 108, "ed": 110, "text": "gain insight"}, {"st": 124, "ed": 126, "text": "visualization method"}, {"st": 145, "ed": 147, "text": "time series"}, {"st": 165, "ed": 167, "text": "proposed algorithm"}, {"st": 180, "ed": 182, "text": "dimensionality reduction"}]
[{"st": 5, "ed": 7, "text": "generative model"}, {"st": 34, "ed": 36, "text": "hierarchical structure"}, {"st": 48, "ed": 50, "text": "semantically meaningful"}, {"st": 69, "ed": 71, "text": "inference network"}, {"st": 80, "ed": 82, "text": "semantically meaningful"}, {"st": 82, "ed": 84, "text": "hierarchical latent"}, {"st": 113, "ed": 115, "text": "extracted features"}, {"st": 123, "ed": 125, "text": "supervised approaches"}, {"st": 128, "ed": 130, "text": "prediction task"}, {"st": 138, "ed": 140, "text": "inference network"}, {"st": 149, "ed": 151, "text": "semi supervised"}, {"st": 155, "ed": 157, "text": "digit classification"}]
[{"st": 1, "ed": 3, "text": "recent years"}, {"st": 3, "ed": 5, "text": "neural network"}, {"st": 8, "ed": 10, "text": "widely adopted"}, {"st": 11, "ed": 13, "text": "machine learning"}, {"st": 22, "ed": 24, "text": "generative models"}, {"st": 26, "ed": 28, "text": "neural networks"}, {"st": 30, "ed": 32, "text": "successfully applied"}, {"st": 37, "ed": 40, "text": "low dimensional latent"}, {"st": 46, "ed": 50, "text": "generative adversarial networks gans"}, {"st": 54, "ed": 56, "text": "compressed sensing"}, {"st": 101, "ed": 103, "text": "latent space"}, {"st": 116, "ed": 118, "text": "input features"}]
[{"st": 0, "ed": 2, "text": "generalization performance"}, {"st": 5, "ed": 7, "text": "deep learning"}, {"st": 18, "ed": 20, "text": "deep models"}, {"st": 23, "ed": 25, "text": "training data"}, {"st": 48, "ed": 50, "text": "strong performance"}, {"st": 62, "ed": 64, "text": "real world"}, {"st": 65, "ed": 67, "text": "synthetic datasets"}, {"st": 76, "ed": 78, "text": "classification error"}, {"st": 118, "ed": 120, "text": "generalization bounds"}, {"st": 178, "ed": 180, "text": "noisy data"}, {"st": 215, "ed": 217, "text": "kernel function"}, {"st": 229, "ed": 231, "text": "deep learning"}, {"st": 235, "ed": 237, "text": "kernel methods"}, {"st": 247, "ed": 249, "text": "deep learning"}, {"st": 257, "ed": 259, "text": "kernel methods"}, {"st": 265, "ed": 267, "text": "theoretical results"}]
[{"st": 65, "ed": 67, "text": "deep learning"}, {"st": 83, "ed": 85, "text": "neural networks"}, {"st": 95, "ed": 97, "text": "don t"}]
[{"st": 0, "ed": 3, "text": "multi label classification"}, {"st": 12, "ed": 14, "text": "learning algorithm"}, {"st": 40, "ed": 42, "text": "deep learning"}, {"st": 104, "ed": 106, "text": "real world"}, {"st": 112, "ed": 115, "text": "real world data"}, {"st": 127, "ed": 129, "text": "evaluation criteria"}]
[{"st": 6, "ed": 10, "text": "generative adversarial networks gans"}, {"st": 13, "ed": 15, "text": "saddle points"}, {"st": 21, "ed": 23, "text": "convex optimization"}, {"st": 52, "ed": 54, "text": "gan training"}, {"st": 57, "ed": 59, "text": "primal dual"}, {"st": 87, "ed": 89, "text": "objective function"}, {"st": 93, "ed": 95, "text": "objective function"}, {"st": 110, "ed": 112, "text": "primal dual"}, {"st": 120, "ed": 122, "text": "proposed method"}, {"st": 126, "ed": 128, "text": "mode collapse"}, {"st": 145, "ed": 147, "text": "gaussian mixture"}, {"st": 147, "ed": 149, "text": "synthetic data"}, {"st": 150, "ed": 152, "text": "real world"}, {"st": 152, "ed": 154, "text": "image datasets"}, {"st": 159, "ed": 161, "text": "proposed method"}]
[{"st": 2, "ed": 4, "text": "learning parameters"}, {"st": 6, "ed": 9, "text": "hidden markov models"}, {"st": 15, "ed": 17, "text": "dna methylation"}, {"st": 28, "ed": 30, "text": "computationally expensive"}, {"st": 39, "ed": 41, "text": "recently developed"}, {"st": 47, "ed": 49, "text": "latent variable"}, {"st": 55, "ed": 57, "text": "highly efficient"}, {"st": 95, "ed": 97, "text": "feature map"}, {"st": 97, "ed": 99, "text": "based approach"}, {"st": 107, "ed": 109, "text": "provide theoretical"}, {"st": 109, "ed": 111, "text": "performance guarantees"}, {"st": 119, "ed": 121, "text": "dna methylation"}]
[{"st": 67, "ed": 69, "text": "discriminative model"}, {"st": 70, "ed": 72, "text": "simultaneously learns"}, {"st": 78, "ed": 80, "text": "sparsity inducing"}, {"st": 90, "ed": 92, "text": "feature selection"}, {"st": 108, "ed": 110, "text": "step size"}, {"st": 110, "ed": 113, "text": "stochastic gradient descent"}, {"st": 144, "ed": 146, "text": "significantly outperforms"}, {"st": 161, "ed": 163, "text": "case study"}]
[{"st": 1, "ed": 3, "text": "adversarial networks"}, {"st": 7, "ed": 9, "text": "domain adaptation"}, {"st": 9, "ed": 11, "text": "machine learning"}, {"st": 18, "ed": 20, "text": "conditional probability"}, {"st": 23, "ed": 25, "text": "input data"}, {"st": 38, "ed": 40, "text": "labelled data"}, {"st": 43, "ed": 45, "text": "source domain"}, {"st": 50, "ed": 52, "text": "conditional probability"}, {"st": 61, "ed": 63, "text": "prior distribution"}, {"st": 74, "ed": 76, "text": "adversarial training"}, {"st": 91, "ed": 94, "text": "source and target"}, {"st": 108, "ed": 111, "text": "source and target"}, {"st": 115, "ed": 117, "text": "probability distributions"}, {"st": 127, "ed": 131, "text": "source and target domains"}, {"st": 133, "ed": 135, "text": "domain adaptation"}, {"st": 148, "ed": 152, "text": "source and target domains"}, {"st": 190, "ed": 192, "text": "highly complex"}, {"st": 193, "ed": 195, "text": "theoretical analysis"}]
[{"st": 1, "ed": 4, "text": "support vector machines"}, {"st": 10, "ed": 12, "text": "large scale"}, {"st": 12, "ed": 14, "text": "classification problems"}, {"st": 61, "ed": 63, "text": "training data"}, {"st": 86, "ed": 88, "text": "data processing"}, {"st": 90, "ed": 92, "text": "nash equilibrium"}, {"st": 100, "ed": 102, "text": "learning algorithms"}, {"st": 111, "ed": 113, "text": "machine learning"}, {"st": 133, "ed": 135, "text": "training data"}, {"st": 138, "ed": 140, "text": "numerical experiments"}, {"st": 149, "ed": 151, "text": "network topology"}, {"st": 153, "ed": 155, "text": "important role"}]
[{"st": 20, "ed": 23, "text": "discrete wavelet transform"}, {"st": 86, "ed": 88, "text": "neural network"}]
[{"st": 27, "ed": 29, "text": "mathbf x"}, {"st": 38, "ed": 40, "text": "mathbb r"}, {"st": 47, "ed": 49, "text": "mathbf x"}, {"st": 50, "ed": 52, "text": "mathbb r"}, {"st": 60, "ed": 62, "text": "total variation"}, {"st": 79, "ed": 81, "text": "mathbb r"}, {"st": 83, "ed": 85, "text": "analysis shows"}, {"st": 94, "ed": 96, "text": "rademacher complexity"}, {"st": 97, "ed": 99, "text": "o sqrt"}, {"st": 125, "ed": 127, "text": "generalization error"}]
[{"st": 5, "ed": 7, "text": "active learning"}, {"st": 14, "ed": 16, "text": "e commerce"}, {"st": 58, "ed": 60, "text": "historical data"}, {"st": 86, "ed": 88, "text": "active learning"}, {"st": 91, "ed": 93, "text": "thompson sampling"}, {"st": 95, "ed": 97, "text": "efficiently learn"}, {"st": 111, "ed": 113, "text": "e commerce"}]
[{"st": 7, "ed": 9, "text": "unsupervised clustering"}, {"st": 30, "ed": 32, "text": "class label"}, {"st": 42, "ed": 44, "text": "domain specific"}, {"st": 61, "ed": 63, "text": "neural network"}, {"st": 66, "ed": 68, "text": "output layer"}, {"st": 109, "ed": 111, "text": "domain specific"}, {"st": 120, "ed": 122, "text": "k means"}, {"st": 157, "ed": 159, "text": "unsupervised clustering"}, {"st": 161, "ed": 163, "text": "mnist svhn"}]
[{"st": 0, "ed": 2, "text": "feature extraction"}, {"st": 3, "ed": 5, "text": "increasingly important"}, {"st": 13, "ed": 15, "text": "neural network"}, {"st": 16, "ed": 18, "text": "feature extraction"}, {"st": 18, "ed": 20, "text": "method achieves"}, {"st": 20, "ed": 22, "text": "great success"}, {"st": 77, "ed": 79, "text": "models including"}, {"st": 81, "ed": 83, "text": "denoising autoencoder"}, {"st": 97, "ed": 99, "text": "benchmark datasets"}, {"st": 121, "ed": 123, "text": "error rate"}]
[{"st": 12, "ed": 14, "text": "loss functions"}, {"st": 18, "ed": 20, "text": "metric learning"}, {"st": 24, "ed": 27, "text": "shown promising results"}, {"st": 39, "ed": 41, "text": "trained models"}, {"st": 58, "ed": 60, "text": "training set"}, {"st": 69, "ed": 71, "text": "metric learning"}, {"st": 192, "ed": 194, "text": "optimization framework"}, {"st": 197, "ed": 199, "text": "global convergence"}, {"st": 224, "ed": 226, "text": "metric learning"}]
[{"st": 68, "ed": 70, "text": "suffix tree"}, {"st": 85, "ed": 87, "text": "synthetic datasets"}, {"st": 91, "ed": 93, "text": "real world"}, {"st": 104, "ed": 106, "text": "predictive performance"}]
[{"st": 15, "ed": 17, "text": "latent factors"}, {"st": 32, "ed": 34, "text": "latent factors"}, {"st": 83, "ed": 85, "text": "latent factors"}]
[{"st": 0, "ed": 3, "text": "deep neural networks"}, {"st": 7, "ed": 9, "text": "computational models"}, {"st": 44, "ed": 46, "text": "large datasets"}, {"st": 60, "ed": 62, "text": "longitudinal data"}, {"st": 126, "ed": 128, "text": "deep learning"}, {"st": 134, "ed": 136, "text": "longitudinal data"}, {"st": 169, "ed": 171, "text": "proposed method"}, {"st": 189, "ed": 191, "text": "complex tasks"}, {"st": 203, "ed": 206, "text": "deep artificial neural"}, {"st": 212, "ed": 214, "text": "longitudinal data"}, {"st": 216, "ed": 219, "text": "low dimensional manifold"}, {"st": 238, "ed": 241, "text": "deep artificial neural"}, {"st": 247, "ed": 249, "text": "longitudinal data"}]
[{"st": 52, "ed": 54, "text": "continuous variables"}, {"st": 87, "ed": 89, "text": "prior distribution"}, {"st": 127, "ed": 131, "text": "markov chain monte carlo"}]
[{"st": 6, "ed": 8, "text": "spatio temporal"}, {"st": 10, "ed": 12, "text": "temporal patterns"}, {"st": 15, "ed": 17, "text": "spatial regions"}, {"st": 38, "ed": 40, "text": "spatio temporal"}, {"st": 82, "ed": 84, "text": "proposed method"}, {"st": 88, "ed": 91, "text": "synthetic and real"}, {"st": 98, "ed": 100, "text": "accurately predict"}, {"st": 103, "ed": 105, "text": "spatial regions"}]
[{"st": 3, "ed": 5, "text": "faster learning"}, {"st": 14, "ed": 16, "text": "training examples"}, {"st": 29, "ed": 31, "text": "text data"}, {"st": 37, "ed": 39, "text": "naive bayes"}, {"st": 53, "ed": 55, "text": "mutual information"}]
[{"st": 4, "ed": 6, "text": "large data"}, {"st": 26, "ed": 29, "text": "modern machine learning"}, {"st": 32, "ed": 34, "text": "large data"}, {"st": 117, "ed": 119, "text": "random forest"}, {"st": 131, "ed": 133, "text": "random forests"}]
[{"st": 1, "ed": 3, "text": "important issue"}, {"st": 4, "ed": 6, "text": "neural network"}, {"st": 51, "ed": 53, "text": "hidden layers"}, {"st": 79, "ed": 81, "text": "activation function"}, {"st": 129, "ed": 131, "text": "rectified linear"}, {"st": 131, "ed": 133, "text": "activation function"}, {"st": 154, "ed": 156, "text": "activation functions"}, {"st": 159, "ed": 161, "text": "empirically validate"}]
[{"st": 5, "ed": 7, "text": "higher order"}, {"st": 18, "ed": 20, "text": "molecular dynamics"}, {"st": 21, "ed": 23, "text": "monte carlo"}, {"st": 24, "ed": 26, "text": "previous works"}, {"st": 46, "ed": 48, "text": "neural network"}, {"st": 58, "ed": 60, "text": "continuous state"}, {"st": 112, "ed": 116, "text": "markov chain monte carlo"}, {"st": 137, "ed": 139, "text": "hidden layers"}, {"st": 148, "ed": 150, "text": "feature extraction"}]
[{"st": 0, "ed": 2, "text": "machine learning"}, {"st": 20, "ed": 22, "text": "classification problem"}, {"st": 26, "ed": 28, "text": "machine learning"}, {"st": 35, "ed": 37, "text": "machine learning"}, {"st": 47, "ed": 49, "text": "machine learning"}, {"st": 128, "ed": 130, "text": "base learner"}, {"st": 136, "ed": 139, "text": "number of iterations"}, {"st": 157, "ed": 159, "text": "base learner"}, {"st": 163, "ed": 165, "text": "tuning parameters"}, {"st": 169, "ed": 171, "text": "base learner"}, {"st": 179, "ed": 181, "text": "error rate"}]
[{"st": 8, "ed": 10, "text": "optimization problems"}, {"st": 32, "ed": 34, "text": "machine learning"}, {"st": 41, "ed": 43, "text": "validation error"}, {"st": 53, "ed": 55, "text": "proposed algorithm"}, {"st": 56, "ed": 58, "text": "bayesian optimization"}, {"st": 83, "ed": 85, "text": "machine learning"}, {"st": 85, "ed": 87, "text": "hyperparameter tuning"}]
[{"st": 24, "ed": 26, "text": "error rate"}, {"st": 117, "ed": 119, "text": "random forests"}]
[{"st": 59, "ed": 61, "text": "multi class"}, {"st": 76, "ed": 78, "text": "takes place"}, {"st": 137, "ed": 139, "text": "image content"}, {"st": 182, "ed": 184, "text": "multi class"}]
[{"st": 3, "ed": 5, "text": "approximation algorithm"}, {"st": 10, "ed": 12, "text": "time series"}, {"st": 21, "ed": 23, "text": "neural network"}, {"st": 25, "ed": 27, "text": "time series"}, {"st": 29, "ed": 31, "text": "low dimensional"}, {"st": 40, "ed": 42, "text": "time series"}, {"st": 58, "ed": 60, "text": "time series"}, {"st": 68, "ed": 70, "text": "embedding space"}, {"st": 76, "ed": 78, "text": "theoretical analysis"}, {"st": 93, "ed": 95, "text": "conduct experiments"}, {"st": 96, "ed": 98, "text": "real world"}]
[{"st": 62, "ed": 64, "text": "optimal transport"}, {"st": 71, "ed": 73, "text": "optimal transport"}, {"st": 91, "ed": 94, "text": "non convex optimization"}, {"st": 116, "ed": 118, "text": "wasserstein distance"}, {"st": 135, "ed": 137, "text": "wide applicability"}, {"st": 140, "ed": 142, "text": "online dating"}, {"st": 154, "ed": 156, "text": "synthetic data"}, {"st": 157, "ed": 159, "text": "real world"}]
[{"st": 16, "ed": 18, "text": "deep network"}, {"st": 57, "ed": 59, "text": "recently shown"}, {"st": 62, "ed": 64, "text": "test error"}, {"st": 82, "ed": 84, "text": "deep network"}, {"st": 109, "ed": 111, "text": "approximate inference"}, {"st": 120, "ed": 123, "text": "kullback leibler divergence"}, {"st": 129, "ed": 131, "text": "exponential family"}, {"st": 137, "ed": 139, "text": "exponential family"}, {"st": 145, "ed": 148, "text": "real world data"}, {"st": 163, "ed": 165, "text": "extensively evaluate"}]
[{"st": 0, "ed": 2, "text": "dynamic programming"}, {"st": 37, "ed": 39, "text": "neural networks"}, {"st": 55, "ed": 57, "text": "dynamic programming"}, {"st": 60, "ed": 62, "text": "strongly convex"}, {"st": 120, "ed": 122, "text": "viterbi algorithm"}, {"st": 131, "ed": 133, "text": "time series"}, {"st": 140, "ed": 142, "text": "structured prediction"}]
[{"st": 3, "ed": 6, "text": "rates of convergence"}, {"st": 13, "ed": 15, "text": "bayes optimal"}, {"st": 28, "ed": 30, "text": "surrogate loss"}, {"st": 38, "ed": 41, "text": "rate of convergence"}, {"st": 49, "ed": 51, "text": "bayes optimal"}, {"st": 65, "ed": 67, "text": "main result"}, {"st": 77, "ed": 79, "text": "hinge loss"}, {"st": 84, "ed": 86, "text": "exponential loss"}, {"st": 97, "ed": 99, "text": "bayes optimal"}, {"st": 105, "ed": 107, "text": "surrogate loss"}, {"st": 114, "ed": 116, "text": "bayes optimal"}]
[{"st": 22, "ed": 24, "text": "sequence prediction"}, {"st": 31, "ed": 33, "text": "medical history"}, {"st": 74, "ed": 76, "text": "neural network"}, {"st": 93, "ed": 95, "text": "external memory"}, {"st": 144, "ed": 146, "text": "medical history"}, {"st": 184, "ed": 186, "text": "neural network"}, {"st": 204, "ed": 206, "text": "improved performance"}, {"st": 209, "ed": 212, "text": "bag of words"}]
[{"st": 0, "ed": 3, "text": "convolutional neural networks"}, {"st": 9, "ed": 11, "text": "image recognition"}, {"st": 30, "ed": 32, "text": "domains including"}, {"st": 52, "ed": 54, "text": "neural networks"}, {"st": 68, "ed": 70, "text": "main result"}, {"st": 106, "ed": 108, "text": "representation theory"}]
[{"st": 0, "ed": 3, "text": "multi armed bandit"}, {"st": 8, "ed": 10, "text": "online learning"}, {"st": 13, "ed": 15, "text": "learning agent"}, {"st": 20, "ed": 22, "text": "cumulative reward"}, {"st": 43, "ed": 45, "text": "reward distributions"}, {"st": 58, "ed": 60, "text": "change detection"}, {"st": 70, "ed": 72, "text": "learning algorithm"}, {"st": 90, "ed": 92, "text": "o sqrt"}, {"st": 95, "ed": 97, "text": "regret bound"}, {"st": 111, "ed": 114, "text": "number of arms"}]
[{"st": 16, "ed": 19, "text": "support vector machine"}, {"st": 23, "ed": 25, "text": "boosting algorithms"}, {"st": 38, "ed": 40, "text": "missing data"}, {"st": 48, "ed": 50, "text": "missing values"}, {"st": 63, "ed": 65, "text": "imbalanced data"}, {"st": 100, "ed": 103, "text": "principal component analysis"}, {"st": 104, "ed": 106, "text": "based approaches"}, {"st": 116, "ed": 118, "text": "missing values"}, {"st": 152, "ed": 155, "text": "k means clustering"}, {"st": 161, "ed": 164, "text": "mean square error"}, {"st": 178, "ed": 180, "text": "cost sensitive"}]
[{"st": 7, "ed": 9, "text": "activation functions"}, {"st": 13, "ed": 15, "text": "neural network"}, {"st": 24, "ed": 26, "text": "max pooling"}, {"st": 31, "ed": 33, "text": "fully connected"}, {"st": 33, "ed": 35, "text": "neural networks"}, {"st": 37, "ed": 41, "text": "convolutional neural networks cnns"}, {"st": 80, "ed": 82, "text": "loss functions"}, {"st": 124, "ed": 126, "text": "significantly smaller"}, {"st": 138, "ed": 140, "text": "loss function"}, {"st": 193, "ed": 195, "text": "computational complexity"}, {"st": 199, "ed": 201, "text": "back propagation"}, {"st": 231, "ed": 233, "text": "significantly improve"}, {"st": 249, "ed": 252, "text": "stochastic gradient descent"}]
[{"st": 5, "ed": 7, "text": "latent space"}, {"st": 10, "ed": 12, "text": "auto encoders"}, {"st": 17, "ed": 20, "text": "synthetic and real"}, {"st": 39, "ed": 41, "text": "representation learning"}, {"st": 42, "ed": 44, "text": "promising results"}]
[{"st": 7, "ed": 10, "text": "multi task learning"}, {"st": 51, "ed": 53, "text": "related tasks"}]
[{"st": 6, "ed": 8, "text": "kernel approximation"}, {"st": 16, "ed": 18, "text": "monte carlo"}, {"st": 23, "ed": 25, "text": "kernel functions"}, {"st": 28, "ed": 30, "text": "kernel methods"}, {"st": 39, "ed": 41, "text": "numerical integration"}, {"st": 82, "ed": 84, "text": "extensive empirical"}]
[{"st": 33, "ed": 36, "text": "end to end"}, {"st": 37, "ed": 40, "text": "stochastic gradient descent"}, {"st": 58, "ed": 60, "text": "min max"}, {"st": 68, "ed": 70, "text": "network architectures"}, {"st": 103, "ed": 105, "text": "machine learning"}, {"st": 124, "ed": 126, "text": "random forests"}, {"st": 131, "ed": 133, "text": "deep neural"}]
[{"st": 4, "ed": 6, "text": "ensemble methods"}, {"st": 12, "ed": 14, "text": "random forests"}, {"st": 156, "ed": 158, "text": "user study"}, {"st": 164, "ed": 166, "text": "clustering performance"}]
[{"st": 26, "ed": 28, "text": "web applications"}, {"st": 38, "ed": 40, "text": "anomaly detection"}, {"st": 66, "ed": 68, "text": "anomaly detection"}, {"st": 88, "ed": 90, "text": "ensemble approach"}, {"st": 133, "ed": 135, "text": "anomaly detection"}]
[]
[{"st": 0, "ed": 2, "text": "machine learning"}, {"st": 5, "ed": 7, "text": "important role"}, {"st": 15, "ed": 17, "text": "search engines"}, {"st": 41, "ed": 43, "text": "multi label"}, {"st": 55, "ed": 57, "text": "extremely large"}, {"st": 77, "ed": 80, "text": "multi label classification"}, {"st": 96, "ed": 98, "text": "nearest neighbor"}, {"st": 117, "ed": 119, "text": "linear classifiers"}, {"st": 153, "ed": 156, "text": "vector space model"}, {"st": 166, "ed": 168, "text": "nearest neighbor"}]
[{"st": 8, "ed": 10, "text": "complex systems"}, {"st": 28, "ed": 30, "text": "latent factors"}, {"st": 43, "ed": 45, "text": "latent variable"}, {"st": 53, "ed": 56, "text": "multivariate time series"}, {"st": 74, "ed": 76, "text": "latent factors"}, {"st": 86, "ed": 88, "text": "low rank"}, {"st": 151, "ed": 156, "text": "alternating direction method of multipliers"}, {"st": 161, "ed": 163, "text": "efficient implementation"}, {"st": 175, "ed": 177, "text": "synthetic data"}, {"st": 184, "ed": 186, "text": "structure learning"}, {"st": 191, "ed": 193, "text": "ground truth"}, {"st": 211, "ed": 213, "text": "case studies"}, {"st": 230, "ed": 233, "text": "multivariate time series"}]
[{"st": 8, "ed": 10, "text": "efficient exploration"}, {"st": 16, "ed": 19, "text": "markov decision process"}, {"st": 53, "ed": 55, "text": "regret bound"}, {"st": 62, "ed": 64, "text": "significantly improves"}, {"st": 65, "ed": 67, "text": "existing algorithms"}, {"st": 73, "ed": 75, "text": "scales linearly"}, {"st": 128, "ed": 130, "text": "optimization problem"}, {"st": 145, "ed": 148, "text": "computationally efficient algorithm"}, {"st": 154, "ed": 156, "text": "numerical simulations"}, {"st": 158, "ed": 160, "text": "theoretical findings"}, {"st": 164, "ed": 166, "text": "significantly outperforms"}]
[{"st": 5, "ed": 7, "text": "contextual bandit"}, {"st": 28, "ed": 30, "text": "open problems"}, {"st": 83, "ed": 85, "text": "thompson sampling"}, {"st": 114, "ed": 116, "text": "contextual bandit"}]
[{"st": 11, "ed": 13, "text": "market capitalization"}, {"st": 61, "ed": 63, "text": "united states"}, {"st": 92, "ed": 94, "text": "mixture model"}, {"st": 114, "ed": 116, "text": "machine learning"}, {"st": 117, "ed": 119, "text": "time series"}, {"st": 130, "ed": 132, "text": "mixture model"}]
[{"st": 14, "ed": 16, "text": "feature engineering"}, {"st": 19, "ed": 21, "text": "recent research"}, {"st": 65, "ed": 67, "text": "denoising autoencoders"}, {"st": 68, "ed": 70, "text": "current account"}]
[{"st": 2, "ed": 4, "text": "image retrieval"}, {"st": 56, "ed": 58, "text": "classification task"}, {"st": 79, "ed": 81, "text": "active learning"}, {"st": 82, "ed": 87, "text": "graph based semi supervised learning"}, {"st": 92, "ed": 94, "text": "active learning"}, {"st": 98, "ed": 100, "text": "user interactions"}, {"st": 115, "ed": 117, "text": "unlabeled data"}, {"st": 121, "ed": 123, "text": "labeled data"}, {"st": 138, "ed": 140, "text": "sampling based"}, {"st": 151, "ed": 153, "text": "decision boundary"}, {"st": 159, "ed": 161, "text": "decision boundary"}, {"st": 172, "ed": 174, "text": "unlabeled data"}, {"st": 179, "ed": 181, "text": "based method"}, {"st": 206, "ed": 208, "text": "noisy labels"}, {"st": 218, "ed": 220, "text": "multiple modalities"}, {"st": 222, "ed": 224, "text": "visual information"}, {"st": 226, "ed": 228, "text": "deep learning"}]
[{"st": 5, "ed": 7, "text": "machine learning"}, {"st": 39, "ed": 41, "text": "computationally expensive"}, {"st": 44, "ed": 46, "text": "scales linearly"}, {"st": 74, "ed": 76, "text": "latent variable"}, {"st": 88, "ed": 90, "text": "marginal likelihood"}, {"st": 93, "ed": 95, "text": "unlike existing"}, {"st": 119, "ed": 121, "text": "large scale"}, {"st": 121, "ed": 123, "text": "classification problems"}, {"st": 134, "ed": 136, "text": "marginal likelihood"}, {"st": 139, "ed": 141, "text": "predictive performance"}]
[{"st": 0, "ed": 2, "text": "wasserstein distance"}, {"st": 3, "ed": 5, "text": "increasingly important"}, {"st": 7, "ed": 9, "text": "machine learning"}, {"st": 24, "ed": 26, "text": "computational complexity"}, {"st": 44, "ed": 46, "text": "machine learning"}, {"st": 53, "ed": 55, "text": "wasserstein distance"}, {"st": 60, "ed": 62, "text": "regularization parameter"}, {"st": 66, "ed": 68, "text": "numerical stability"}, {"st": 85, "ed": 87, "text": "optimal transport"}, {"st": 110, "ed": 112, "text": "generative models"}, {"st": 114, "ed": 116, "text": "optimal transport"}, {"st": 133, "ed": 135, "text": "convergence analysis"}, {"st": 142, "ed": 144, "text": "methods outperform"}]
[{"st": 12, "ed": 14, "text": "local geometry"}, {"st": 16, "ed": 18, "text": "large scale"}, {"st": 88, "ed": 90, "text": "real world"}, {"st": 90, "ed": 92, "text": "benchmark problems"}]
[{"st": 1, "ed": 3, "text": "machine learning"}, {"st": 5, "ed": 7, "text": "received increasing"}, {"st": 9, "ed": 11, "text": "recent years"}, {"st": 20, "ed": 22, "text": "decision making"}, {"st": 29, "ed": 31, "text": "task performance"}, {"st": 94, "ed": 96, "text": "decision making"}, {"st": 131, "ed": 133, "text": "objective function"}, {"st": 135, "ed": 137, "text": "predictive accuracy"}, {"st": 213, "ed": 215, "text": "search algorithm"}]
[{"st": 3, "ed": 5, "text": "sample complexity"}, {"st": 63, "ed": 65, "text": "rademacher complexity"}, {"st": 89, "ed": 91, "text": "linear prediction"}, {"st": 93, "ed": 95, "text": "neural networks"}]
[{"st": 13, "ed": 15, "text": "metric learning"}, {"st": 35, "ed": 37, "text": "few shot"}, {"st": 48, "ed": 50, "text": "metric learning"}, {"st": 107, "ed": 109, "text": "fine grained"}]
[{"st": 12, "ed": 14, "text": "low dimensional"}, {"st": 59, "ed": 61, "text": "real world"}, {"st": 91, "ed": 93, "text": "compact representation"}, {"st": 106, "ed": 108, "text": "latent representation"}, {"st": 113, "ed": 115, "text": "prior distribution"}, {"st": 117, "ed": 119, "text": "adversarial training"}, {"st": 147, "ed": 149, "text": "real world"}, {"st": 165, "ed": 167, "text": "link prediction"}]
[{"st": 8, "ed": 10, "text": "deep learning"}, {"st": 14, "ed": 17, "text": "stochastic gradient descent"}, {"st": 20, "ed": 22, "text": "network architecture"}, {"st": 28, "ed": 30, "text": "generalization performance"}, {"st": 49, "ed": 51, "text": "linear classifiers"}, {"st": 63, "ed": 65, "text": "convolutional layer"}, {"st": 117, "ed": 119, "text": "generalization performance"}, {"st": 134, "ed": 136, "text": "optimization algorithm"}]
[{"st": 10, "ed": 12, "text": "anomaly detection"}, {"st": 43, "ed": 45, "text": "expert knowledge"}, {"st": 62, "ed": 65, "text": "short term memory"}, {"st": 70, "ed": 74, "text": "recurrent neural network rnn"}, {"st": 94, "ed": 97, "text": "mars science laboratory"}, {"st": 118, "ed": 120, "text": "anomaly detection"}, {"st": 125, "ed": 127, "text": "false positive"}]
[{"st": 5, "ed": 7, "text": "adversarial examples"}, {"st": 8, "ed": 10, "text": "deep learning"}, {"st": 71, "ed": 73, "text": "adversarial examples"}, {"st": 108, "ed": 110, "text": "generalization performance"}, {"st": 112, "ed": 115, "text": "training and test"}, {"st": 131, "ed": 133, "text": "adversarial examples"}]
[{"st": 7, "ed": 9, "text": "method called"}]
[{"st": 1, "ed": 3, "text": "provide theoretical"}, {"st": 4, "ed": 6, "text": "empirical evidence"}, {"st": 22, "ed": 24, "text": "inference network"}, {"st": 27, "ed": 31, "text": "signal to noise ratio"}, {"st": 53, "ed": 56, "text": "learning and inference"}, {"st": 71, "ed": 73, "text": "auto encoder"}, {"st": 78, "ed": 80, "text": "auto encoder"}, {"st": 86, "ed": 88, "text": "auto encoder"}, {"st": 97, "ed": 99, "text": "auto encoder"}, {"st": 135, "ed": 137, "text": "inference network"}, {"st": 138, "ed": 140, "text": "generative network"}]
[{"st": 56, "ed": 58, "text": "theoretical framework"}, {"st": 97, "ed": 99, "text": "wasserstein distance"}, {"st": 133, "ed": 135, "text": "image generation"}]
[{"st": 2, "ed": 4, "text": "convex objective"}, {"st": 4, "ed": 6, "text": "loss functions"}, {"st": 11, "ed": 13, "text": "machine learning"}, {"st": 19, "ed": 22, "text": "convex loss functions"}, {"st": 31, "ed": 33, "text": "desirable properties"}, {"st": 38, "ed": 40, "text": "classification accuracy"}, {"st": 91, "ed": 93, "text": "global optimal"}, {"st": 110, "ed": 112, "text": "convergence analysis"}]
[{"st": 1, "ed": 3, "text": "recent years"}, {"st": 6, "ed": 8, "text": "deep learning"}, {"st": 12, "ed": 14, "text": "pattern recognition"}, {"st": 14, "ed": 16, "text": "computer vision"}, {"st": 17, "ed": 19, "text": "artificial intelligence"}, {"st": 31, "ed": 33, "text": "deep learning"}, {"st": 50, "ed": 53, "text": "training and testing"}, {"st": 54, "ed": 57, "text": "deep neural networks"}, {"st": 66, "ed": 68, "text": "open source"}, {"st": 75, "ed": 77, "text": "deep learning"}, {"st": 101, "ed": 103, "text": "hyper parameters"}, {"st": 108, "ed": 110, "text": "error prone"}, {"st": 116, "ed": 119, "text": "graphical user interface"}, {"st": 129, "ed": 132, "text": "end to end"}]
[{"st": 0, "ed": 2, "text": "learning algorithms"}, {"st": 10, "ed": 12, "text": "base classifiers"}, {"st": 12, "ed": 14, "text": "consistently outperform"}, {"st": 25, "ed": 27, "text": "supervised setting"}, {"st": 32, "ed": 34, "text": "base classifier"}, {"st": 51, "ed": 53, "text": "labeled data"}, {"st": 57, "ed": 59, "text": "ensemble methods"}, {"st": 61, "ed": 63, "text": "real world"}, {"st": 65, "ed": 67, "text": "labeled data"}, {"st": 78, "ed": 80, "text": "theoretical framework"}, {"st": 96, "ed": 98, "text": "base classifiers"}, {"st": 103, "ed": 105, "text": "ensemble learning"}]
[{"st": 7, "ed": 9, "text": "dynamical systems"}, {"st": 75, "ed": 78, "text": "variational auto encoder"}, {"st": 81, "ed": 83, "text": "latent code"}, {"st": 112, "ed": 114, "text": "ground truth"}, {"st": 131, "ed": 133, "text": "complex dynamics"}, {"st": 135, "ed": 137, "text": "motion capture"}]
[{"st": 0, "ed": 4, "text": "multiple instance learning mil"}, {"st": 8, "ed": 10, "text": "supervised learning"}, {"st": 13, "ed": 15, "text": "class label"}, {"st": 54, "ed": 56, "text": "neural network"}, {"st": 73, "ed": 75, "text": "attention based"}, {"st": 93, "ed": 95, "text": "approach achieves"}, {"st": 95, "ed": 97, "text": "comparable performance"}, {"st": 119, "ed": 121, "text": "real life"}, {"st": 123, "ed": 125, "text": "without sacrificing"}]
[{"st": 1, "ed": 3, "text": "stochastic optimization"}, {"st": 7, "ed": 9, "text": "uniform sampling"}, {"st": 41, "ed": 43, "text": "importance sampling"}, {"st": 59, "ed": 61, "text": "recently proposed"}, {"st": 68, "ed": 70, "text": "online optimization"}, {"st": 89, "ed": 91, "text": "importance sampling"}]
[{"st": 4, "ed": 6, "text": "service providers"}, {"st": 21, "ed": 23, "text": "machine learning"}, {"st": 66, "ed": 68, "text": "proposed method"}, {"st": 75, "ed": 78, "text": "accuracy and efficiency"}]
[{"st": 15, "ed": 17, "text": "machine learning"}, {"st": 106, "ed": 110, "text": "synthetic and real world"}, {"st": 114, "ed": 116, "text": "proposed framework"}, {"st": 118, "ed": 120, "text": "accurate estimates"}]
[{"st": 1, "ed": 3, "text": "neural networks"}, {"st": 6, "ed": 8, "text": "predictive models"}, {"st": 21, "ed": 23, "text": "closely related"}, {"st": 70, "ed": 72, "text": "neural networks"}, {"st": 96, "ed": 98, "text": "recently proposed"}, {"st": 107, "ed": 109, "text": "output distribution"}]
[{"st": 35, "ed": 37, "text": "link prediction"}, {"st": 50, "ed": 52, "text": "knowledge graph"}, {"st": 64, "ed": 66, "text": "link prediction"}, {"st": 112, "ed": 114, "text": "expert knowledge"}]
[{"st": 9, "ed": 11, "text": "mutual information"}, {"st": 19, "ed": 21, "text": "latent variable"}, {"st": 21, "ed": 23, "text": "generative model"}, {"st": 42, "ed": 44, "text": "latent variable"}]
[{"st": 5, "ed": 7, "text": "batch normalization"}, {"st": 16, "ed": 18, "text": "probabilistic model"}, {"st": 21, "ed": 23, "text": "batch normalization"}, {"st": 37, "ed": 39, "text": "probabilistic model"}, {"st": 59, "ed": 61, "text": "computational cost"}, {"st": 64, "ed": 66, "text": "batch normalization"}, {"st": 89, "ed": 91, "text": "batch normalization"}, {"st": 93, "ed": 95, "text": "architectures including"}, {"st": 95, "ed": 97, "text": "deep convolutional"}, {"st": 103, "ed": 107, "text": "mnist and cifar 10"}]
[{"st": 57, "ed": 59, "text": "compressive sensing"}, {"st": 61, "ed": 63, "text": "radio astronomy"}, {"st": 76, "ed": 78, "text": "input data"}, {"st": 79, "ed": 81, "text": "recovery guarantees"}, {"st": 90, "ed": 92, "text": "theoretical analysis"}, {"st": 101, "ed": 103, "text": "input data"}, {"st": 133, "ed": 135, "text": "low precision"}, {"st": 155, "ed": 157, "text": "radio astronomy"}]
[{"st": 0, "ed": 2, "text": "inverse classification"}, {"st": 30, "ed": 32, "text": "instance specific"}, {"st": 54, "ed": 56, "text": "inverse classification"}, {"st": 73, "ed": 76, "text": "a long standing"}, {"st": 97, "ed": 99, "text": "inverse classification"}, {"st": 129, "ed": 131, "text": "gaussian processes"}]
[{"st": 3, "ed": 5, "text": "latent variable"}, {"st": 6, "ed": 8, "text": "remains challenging"}, {"st": 73, "ed": 75, "text": "generative model"}, {"st": 102, "ed": 104, "text": "latent variables"}, {"st": 105, "ed": 107, "text": "gumbel softmax"}, {"st": 117, "ed": 119, "text": "variational autoencoders"}]
[{"st": 1, "ed": 3, "text": "convolutional network"}, {"st": 7, "ed": 11, "text": "convolutional neural network cnn"}, {"st": 19, "ed": 21, "text": "adjacency matrix"}, {"st": 30, "ed": 32, "text": "attention mechanism"}, {"st": 59, "ed": 61, "text": "multiple views"}, {"st": 146, "ed": 148, "text": "machine learning"}, {"st": 175, "ed": 177, "text": "attention based"}, {"st": 177, "ed": 179, "text": "multi relational"}, {"st": 181, "ed": 183, "text": "jointly learns"}, {"st": 183, "ed": 185, "text": "attention weights"}, {"st": 196, "ed": 198, "text": "real valued"}]
[{"st": 2, "ed": 4, "text": "machine learning"}, {"st": 18, "ed": 20, "text": "distance measure"}, {"st": 64, "ed": 66, "text": "dissimilarity measure"}, {"st": 89, "ed": 91, "text": "positive definite"}, {"st": 95, "ed": 97, "text": "dissimilarity measure"}, {"st": 108, "ed": 110, "text": "special case"}, {"st": 131, "ed": 135, "text": "reproducing kernel hilbert space"}, {"st": 137, "ed": 139, "text": "lipschitz continuous"}, {"st": 164, "ed": 166, "text": "nearest neighbor"}, {"st": 174, "ed": 176, "text": "random features"}, {"st": 180, "ed": 182, "text": "feature maps"}, {"st": 193, "ed": 195, "text": "feature map"}, {"st": 212, "ed": 214, "text": "time series"}, {"st": 220, "ed": 222, "text": "proposed framework"}, {"st": 222, "ed": 225, "text": "compares favorably to"}, {"st": 226, "ed": 228, "text": "distance based"}, {"st": 232, "ed": 235, "text": "k nearest neighbors"}]
[{"st": 50, "ed": 52, "text": "clustering problems"}, {"st": 56, "ed": 58, "text": "cross domain"}, {"st": 76, "ed": 78, "text": "optimization problem"}, {"st": 108, "ed": 110, "text": "large scale"}, {"st": 115, "ed": 120, "text": "synthetic and real world datasets"}, {"st": 133, "ed": 135, "text": "cross domain"}, {"st": 135, "ed": 138, "text": "real world problems"}, {"st": 141, "ed": 143, "text": "text mining"}, {"st": 172, "ed": 174, "text": "clustering algorithm"}, {"st": 175, "ed": 177, "text": "cross domain"}]
[{"st": 15, "ed": 17, "text": "loss function"}, {"st": 51, "ed": 53, "text": "consistently outperform"}, {"st": 74, "ed": 76, "text": "architectures including"}]
[{"st": 0, "ed": 3, "text": "deep convolutional network"}, {"st": 13, "ed": 15, "text": "wide variety"}, {"st": 46, "ed": 48, "text": "convolutional networks"}, {"st": 84, "ed": 86, "text": "image recognition"}, {"st": 87, "ed": 89, "text": "text analysis"}, {"st": 92, "ed": 94, "text": "convolutional networks"}, {"st": 158, "ed": 160, "text": "carefully designed"}, {"st": 166, "ed": 169, "text": "strengths and weaknesses"}, {"st": 170, "ed": 172, "text": "convolutional network"}, {"st": 184, "ed": 186, "text": "deep architectures"}, {"st": 193, "ed": 195, "text": "convolutional networks"}]
[{"st": 14, "ed": 16, "text": "multi stage"}, {"st": 37, "ed": 40, "text": "a b testing"}, {"st": 104, "ed": 106, "text": "non stationary"}, {"st": 111, "ed": 114, "text": "simple and efficient"}, {"st": 128, "ed": 133, "text": "synthetic and real world datasets"}, {"st": 137, "ed": 139, "text": "approach outperforms"}]
[{"st": 0, "ed": 2, "text": "multiple modalities"}, {"st": 11, "ed": 13, "text": "joint representation"}, {"st": 27, "ed": 29, "text": "generative models"}, {"st": 31, "ed": 33, "text": "multi modal"}, {"st": 42, "ed": 44, "text": "joint distribution"}, {"st": 58, "ed": 60, "text": "variational autoencoder"}, {"st": 66, "ed": 68, "text": "inference network"}, {"st": 77, "ed": 79, "text": "multi modal"}, {"st": 87, "ed": 89, "text": "efficiently learn"}, {"st": 97, "ed": 99, "text": "weakly supervised"}, {"st": 125, "ed": 127, "text": "approach yields"}, {"st": 128, "ed": 130, "text": "weakly supervised"}, {"st": 135, "ed": 137, "text": "case study"}, {"st": 141, "ed": 143, "text": "edge detection"}, {"st": 144, "ed": 146, "text": "facial landmark"}]
[{"st": 4, "ed": 6, "text": "representation learning"}, {"st": 12, "ed": 15, "text": "factors of variation"}, {"st": 30, "ed": 32, "text": "training data"}, {"st": 62, "ed": 64, "text": "generalization ability"}, {"st": 88, "ed": 91, "text": "cross entropy loss"}, {"st": 103, "ed": 105, "text": "mutual information"}, {"st": 117, "ed": 121, "text": "multi layer neural networks"}, {"st": 124, "ed": 128, "text": "mnist and cifar 10"}]
[{"st": 13, "ed": 15, "text": "low quality"}, {"st": 16, "ed": 18, "text": "training data"}, {"st": 166, "ed": 168, "text": "optimization method"}, {"st": 185, "ed": 187, "text": "proposed approach"}, {"st": 192, "ed": 194, "text": "theoretical analysis"}]
[{"st": 34, "ed": 36, "text": "distance measure"}, {"st": 74, "ed": 76, "text": "low dimensional"}, {"st": 82, "ed": 84, "text": "numerical results"}, {"st": 95, "ed": 97, "text": "dimensionality reduction"}]
[{"st": 76, "ed": 78, "text": "small sample"}]
[{"st": 0, "ed": 4, "text": "deep convolutional neural networks"}, {"st": 5, "ed": 8, "text": "achieved great success"}, {"st": 19, "ed": 21, "text": "specific task"}, {"st": 28, "ed": 30, "text": "prior knowledge"}, {"st": 35, "ed": 37, "text": "network architecture"}, {"st": 38, "ed": 41, "text": "trial and error"}, {"st": 51, "ed": 53, "text": "labeled data"}, {"st": 70, "ed": 72, "text": "pre trained"}, {"st": 83, "ed": 85, "text": "pre trained"}, {"st": 88, "ed": 90, "text": "fine tuned"}, {"st": 118, "ed": 120, "text": "feature representation"}, {"st": 135, "ed": 137, "text": "pre trained"}, {"st": 149, "ed": 151, "text": "network architecture"}, {"st": 165, "ed": 167, "text": "significantly reduced"}, {"st": 173, "ed": 175, "text": "multiple datasets"}, {"st": 177, "ed": 179, "text": "pre trained"}, {"st": 183, "ed": 185, "text": "proposed approach"}, {"st": 187, "ed": 189, "text": "cost effective"}]
[{"st": 1, "ed": 5, "text": "generative adversarial networks gans"}, {"st": 32, "ed": 35, "text": "maximum mean discrepancy"}, {"st": 96, "ed": 98, "text": "proposed framework"}]
[{"st": 2, "ed": 4, "text": "convergence guarantees"}, {"st": 5, "ed": 7, "text": "wasserstein distance"}, {"st": 39, "ed": 41, "text": "posterior distribution"}, {"st": 46, "ed": 48, "text": "strongly convex"}, {"st": 73, "ed": 75, "text": "theoretical bounds"}, {"st": 97, "ed": 99, "text": "real world"}]
[{"st": 37, "ed": 39, "text": "bandit mab"}, {"st": 48, "ed": 51, "text": "number of arms"}, {"st": 117, "ed": 119, "text": "optimal regret"}, {"st": 123, "ed": 125, "text": "optimal regret"}]
[{"st": 29, "ed": 31, "text": "clustering problem"}, {"st": 102, "ed": 104, "text": "clustering problem"}, {"st": 127, "ed": 129, "text": "np hard"}, {"st": 134, "ed": 136, "text": "approximation algorithms"}, {"st": 150, "ed": 152, "text": "real world"}]
[{"st": 2, "ed": 4, "text": "deep learning"}, {"st": 24, "ed": 26, "text": "faster training"}, {"st": 91, "ed": 93, "text": "existing methods"}, {"st": 115, "ed": 117, "text": "building code"}, {"st": 139, "ed": 141, "text": "open source"}, {"st": 172, "ed": 174, "text": "distributed training"}, {"st": 185, "ed": 187, "text": "https github.com"}]
[{"st": 1, "ed": 3, "text": "convex optimization"}, {"st": 11, "ed": 13, "text": "machine learning"}, {"st": 15, "ed": 17, "text": "large scale"}, {"st": 21, "ed": 23, "text": "training process"}, {"st": 36, "ed": 39, "text": "stochastic gradient descent"}, {"st": 57, "ed": 59, "text": "distributed stochastic"}, {"st": 59, "ed": 61, "text": "optimization method"}, {"st": 64, "ed": 66, "text": "step sizes"}, {"st": 67, "ed": 69, "text": "variance reduction"}, {"st": 93, "ed": 95, "text": "dataset size"}, {"st": 122, "ed": 124, "text": "significant progress"}, {"st": 147, "ed": 149, "text": "performance gains"}, {"st": 150, "ed": 152, "text": "large scale"}, {"st": 152, "ed": 154, "text": "logistic regression"}]
[{"st": 0, "ed": 3, "text": "low rank matrix"}, {"st": 6, "ed": 9, "text": "achieved great success"}, {"st": 11, "ed": 14, "text": "real world data"}, {"st": 16, "ed": 18, "text": "latent feature"}, {"st": 26, "ed": 28, "text": "prediction performance"}, {"st": 31, "ed": 33, "text": "latent variables"}, {"st": 43, "ed": 45, "text": "matrix factorization"}, {"st": 81, "ed": 83, "text": "matrix completion"}, {"st": 87, "ed": 89, "text": "optimization framework"}, {"st": 112, "ed": 114, "text": "optimization problem"}, {"st": 125, "ed": 127, "text": "latent variables"}, {"st": 148, "ed": 151, "text": "maximum likelihood estimator"}, {"st": 153, "ed": 155, "text": "special case"}, {"st": 162, "ed": 164, "text": "error bound"}, {"st": 169, "ed": 171, "text": "trace norm"}, {"st": 175, "ed": 178, "text": "conduct extensive experiments"}, {"st": 180, "ed": 183, "text": "synthetic and real"}]
[{"st": 2, "ed": 4, "text": "unsupervised learning"}, {"st": 37, "ed": 39, "text": "representation learning"}, {"st": 41, "ed": 43, "text": "mutual information"}, {"st": 59, "ed": 61, "text": "unsupervised learning"}, {"st": 81, "ed": 84, "text": "variational lower bound"}, {"st": 99, "ed": 101, "text": "variational autoencoders"}]
[{"st": 41, "ed": 43, "text": "validation set"}, {"st": 86, "ed": 88, "text": "model selection"}, {"st": 106, "ed": 110, "text": "mnist and cifar 10"}, {"st": 141, "ed": 143, "text": "model selection"}]
[{"st": 53, "ed": 55, "text": "time series"}, {"st": 65, "ed": 67, "text": "time series"}, {"st": 81, "ed": 83, "text": "time series"}, {"st": 103, "ed": 105, "text": "machine learning"}, {"st": 116, "ed": 118, "text": "proposed approach"}, {"st": 118, "ed": 120, "text": "shows significant"}]
[{"st": 25, "ed": 27, "text": "significantly reduces"}, {"st": 30, "ed": 32, "text": "weight parameters"}, {"st": 36, "ed": 38, "text": "training samples"}, {"st": 64, "ed": 67, "text": "feedforward neural network"}, {"st": 84, "ed": 86, "text": "learning algorithm"}, {"st": 117, "ed": 119, "text": "classification methods"}, {"st": 120, "ed": 122, "text": "deep learning"}]
[{"st": 7, "ed": 9, "text": "unsupervised learning"}, {"st": 10, "ed": 12, "text": "disentangled representations"}]
[{"st": 0, "ed": 3, "text": "distance metric learning"}, {"st": 7, "ed": 9, "text": "distance metric"}, {"st": 48, "ed": 50, "text": "comparable performance"}, {"st": 81, "ed": 83, "text": "promising results"}, {"st": 92, "ed": 95, "text": "non convex optimization"}, {"st": 99, "ed": 101, "text": "global optimal"}, {"st": 108, "ed": 110, "text": "theoretical understanding"}, {"st": 119, "ed": 121, "text": "generalization error"}, {"st": 141, "ed": 143, "text": "convex relaxations"}, {"st": 151, "ed": 153, "text": "global optimal"}, {"st": 173, "ed": 175, "text": "theoretical analysis"}]
[{"st": 5, "ed": 7, "text": "policy evaluation"}, {"st": 11, "ed": 13, "text": "contextual bandit"}, {"st": 55, "ed": 57, "text": "classification problem"}, {"st": 94, "ed": 96, "text": "kernel function"}, {"st": 165, "ed": 167, "text": "case study"}]
[{"st": 2, "ed": 4, "text": "generative adversarial"}, {"st": 18, "ed": 22, "text": "convolutional neural networks cnns"}, {"st": 26, "ed": 30, "text": "generative adversarial network gan"}, {"st": 46, "ed": 48, "text": "objective function"}, {"st": 67, "ed": 69, "text": "image data"}, {"st": 71, "ed": 75, "text": "mnist and cifar 10"}, {"st": 79, "ed": 81, "text": "generative adversarial"}, {"st": 84, "ed": 86, "text": "semi supervised"}]
[{"st": 0, "ed": 4, "text": "generative adversarial networks gans"}, {"st": 13, "ed": 16, "text": "real world data"}, {"st": 36, "ed": 38, "text": "anomaly detection"}, {"st": 41, "ed": 43, "text": "recently developed"}, {"st": 46, "ed": 48, "text": "anomaly detection"}]
[{"st": 10, "ed": 12, "text": "time series"}, {"st": 39, "ed": 41, "text": "proposed method"}, {"st": 71, "ed": 73, "text": "time series"}, {"st": 75, "ed": 77, "text": "proposed approach"}]
[{"st": 6, "ed": 8, "text": "representation learning"}, {"st": 22, "ed": 24, "text": "learned representations"}, {"st": 39, "ed": 41, "text": "representation learning"}, {"st": 61, "ed": 63, "text": "equal opportunity"}, {"st": 68, "ed": 70, "text": "worst case"}, {"st": 70, "ed": 72, "text": "theoretical guarantees"}, {"st": 99, "ed": 101, "text": "transfer learning"}, {"st": 105, "ed": 107, "text": "learned representations"}]
[{"st": 23, "ed": 25, "text": "closed form"}, {"st": 34, "ed": 36, "text": "real world"}, {"st": 52, "ed": 55, "text": "orders of magnitude"}]
[{"st": 7, "ed": 9, "text": "random forests"}, {"st": 10, "ed": 12, "text": "large datasets"}, {"st": 32, "ed": 35, "text": "simple yet effective"}, {"st": 53, "ed": 55, "text": "training instances"}, {"st": 70, "ed": 72, "text": "multi level"}, {"st": 91, "ed": 93, "text": "training instances"}, {"st": 103, "ed": 105, "text": "conceptually simple"}]
[{"st": 3, "ed": 5, "text": "deep learning"}, {"st": 32, "ed": 35, "text": "deep neural networks"}, {"st": 41, "ed": 43, "text": "optimization framework"}, {"st": 68, "ed": 70, "text": "fine grained"}, {"st": 80, "ed": 82, "text": "fully connected"}, {"st": 83, "ed": 85, "text": "convolutional layers"}, {"st": 98, "ed": 100, "text": "proposed algorithm"}, {"st": 101, "ed": 103, "text": "computational complexity"}, {"st": 106, "ed": 109, "text": "o n 2"}, {"st": 118, "ed": 121, "text": "o n 2"}, {"st": 134, "ed": 136, "text": "highly efficient"}, {"st": 136, "ed": 140, "text": "field programmable gate array"}, {"st": 146, "ed": 148, "text": "batch processing"}, {"st": 161, "ed": 163, "text": "proposed framework"}]
[{"st": 2, "ed": 4, "text": "machine learning"}, {"st": 36, "ed": 38, "text": "predictive models"}, {"st": 91, "ed": 93, "text": "predictive models"}, {"st": 115, "ed": 117, "text": "prediction performance"}, {"st": 141, "ed": 143, "text": "real world"}]
[{"st": 2, "ed": 4, "text": "neural networks"}, {"st": 9, "ed": 11, "text": "neural networks"}, {"st": 16, "ed": 18, "text": "multi dimensional"}]
[{"st": 16, "ed": 18, "text": "multi channel"}, {"st": 33, "ed": 37, "text": "recurrent neural networks rnns"}, {"st": 46, "ed": 48, "text": "time series"}, {"st": 50, "ed": 52, "text": "perform poorly"}, {"st": 66, "ed": 68, "text": "existing solutions"}, {"st": 74, "ed": 76, "text": "discriminative models"}, {"st": 93, "ed": 95, "text": "generative modeling"}, {"st": 119, "ed": 121, "text": "latent representation"}, {"st": 140, "ed": 142, "text": "metric spaces"}]
[{"st": 3, "ed": 5, "text": "local geometry"}, {"st": 8, "ed": 10, "text": "hidden layer"}, {"st": 10, "ed": 12, "text": "fully connected"}, {"st": 12, "ed": 14, "text": "neural network"}, {"st": 16, "ed": 18, "text": "training samples"}, {"st": 24, "ed": 26, "text": "logistic regression"}, {"st": 34, "ed": 36, "text": "empirical risk"}, {"st": 38, "ed": 40, "text": "quadratic loss"}, {"st": 41, "ed": 43, "text": "strong convexity"}, {"st": 48, "ed": 50, "text": "local neighborhood"}, {"st": 52, "ed": 54, "text": "ground truth"}, {"st": 59, "ed": 61, "text": "activation functions"}, {"st": 72, "ed": 74, "text": "sample complexity"}, {"st": 91, "ed": 93, "text": "critical point"}, {"st": 99, "ed": 101, "text": "ground truth"}, {"st": 112, "ed": 114, "text": "significantly improves"}, {"st": 120, "ed": 122, "text": "neural networks"}, {"st": 135, "ed": 137, "text": "global convergence"}, {"st": 140, "ed": 142, "text": "hidden layer"}, {"st": 142, "ed": 144, "text": "neural networks"}, {"st": 149, "ed": 151, "text": "empirical risk"}, {"st": 156, "ed": 158, "text": "near optimal"}]
[{"st": 9, "ed": 11, "text": "powerful tool"}, {"st": 15, "ed": 17, "text": "causal structure"}, {"st": 18, "ed": 21, "text": "real world data"}, {"st": 36, "ed": 38, "text": "computational complexity"}, {"st": 47, "ed": 49, "text": "variational inference"}, {"st": 53, "ed": 55, "text": "computational efficiency"}, {"st": 60, "ed": 62, "text": "gibbs sampling"}, {"st": 80, "ed": 82, "text": "gibbs sampling"}, {"st": 141, "ed": 143, "text": "gibbs sampler"}, {"st": 163, "ed": 166, "text": "real life data"}, {"st": 170, "ed": 172, "text": "significantly outperforms"}, {"st": 173, "ed": 175, "text": "baseline methods"}]
[{"st": 10, "ed": 13, "text": "deep neural network"}, {"st": 38, "ed": 40, "text": "conditional distribution"}, {"st": 52, "ed": 54, "text": "bayes classifier"}, {"st": 58, "ed": 60, "text": "naive bayes"}, {"st": 62, "ed": 65, "text": "deep generative models"}, {"st": 79, "ed": 81, "text": "detection method"}, {"st": 82, "ed": 84, "text": "adversarial examples"}, {"st": 91, "ed": 93, "text": "initial results"}, {"st": 113, "ed": 115, "text": "detection method"}]
[{"st": 7, "ed": 9, "text": "training samples"}, {"st": 15, "ed": 17, "text": "ensemble based"}, {"st": 19, "ed": 21, "text": "random forest"}, {"st": 24, "ed": 27, "text": "boosted decision trees"}, {"st": 61, "ed": 63, "text": "parametric models"}, {"st": 70, "ed": 72, "text": "computationally efficient"}, {"st": 140, "ed": 142, "text": "training samples"}]
[{"st": 0, "ed": 2, "text": "variational autoencoder"}, {"st": 9, "ed": 11, "text": "generative models"}, {"st": 23, "ed": 25, "text": "increasing complexity"}, {"st": 27, "ed": 29, "text": "raw data"}, {"st": 33, "ed": 35, "text": "deep networks"}, {"st": 66, "ed": 68, "text": "latent representations"}, {"st": 84, "ed": 86, "text": "latent representations"}, {"st": 90, "ed": 93, "text": "encoder and decoder"}, {"st": 97, "ed": 99, "text": "latent representation"}, {"st": 113, "ed": 115, "text": "fisher information"}, {"st": 131, "ed": 133, "text": "feed forward"}, {"st": 152, "ed": 154, "text": "skip connections"}, {"st": 166, "ed": 168, "text": "skip connections"}, {"st": 185, "ed": 187, "text": "promising performance"}]
[{"st": 0, "ed": 3, "text": "deep generative models"}, {"st": 6, "ed": 8, "text": "encouraging results"}, {"st": 25, "ed": 27, "text": "generative models"}, {"st": 38, "ed": 40, "text": "observed variables"}, {"st": 58, "ed": 60, "text": "latent factor"}, {"st": 90, "ed": 92, "text": "complex nonlinear"}, {"st": 108, "ed": 110, "text": "sparsity inducing"}, {"st": 125, "ed": 127, "text": "motion capture"}]
[{"st": 16, "ed": 18, "text": "linear transformations"}, {"st": 31, "ed": 34, "text": "low dimensional manifold"}, {"st": 93, "ed": 95, "text": "proposed method"}, {"st": 96, "ed": 98, "text": "successfully applied"}, {"st": 121, "ed": 123, "text": "low dimensional"}]
[{"st": 14, "ed": 16, "text": "computational efficiency"}, {"st": 52, "ed": 54, "text": "conditional distributions"}, {"st": 98, "ed": 100, "text": "distribution matching"}]
[{"st": 4, "ed": 6, "text": "sparse coding"}, {"st": 10, "ed": 12, "text": "sparse representation"}, {"st": 15, "ed": 17, "text": "proposed framework"}, {"st": 29, "ed": 32, "text": "each data point"}, {"st": 56, "ed": 58, "text": "component analysis"}, {"st": 76, "ed": 78, "text": "sparse representation"}, {"st": 90, "ed": 92, "text": "ell 1"}, {"st": 93, "ed": 95, "text": "optimization problems"}, {"st": 108, "ed": 110, "text": "sparse codes"}, {"st": 113, "ed": 115, "text": "deep learning"}, {"st": 132, "ed": 134, "text": "special case"}, {"st": 136, "ed": 142, "text": "alternating direction method of multipliers admm"}, {"st": 144, "ed": 146, "text": "empirically validate"}, {"st": 153, "ed": 155, "text": "vision tasks"}, {"st": 175, "ed": 177, "text": "sparse codes"}]
[{"st": 5, "ed": 7, "text": "prediction error"}, {"st": 10, "ed": 12, "text": "message passing"}, {"st": 34, "ed": 37, "text": "degrees of freedom"}, {"st": 50, "ed": 52, "text": "prediction error"}, {"st": 118, "ed": 120, "text": "real data"}]
[{"st": 8, "ed": 10, "text": "neural networks"}, {"st": 31, "ed": 34, "text": "deep neural network"}, {"st": 34, "ed": 36, "text": "weight matrices"}, {"st": 75, "ed": 77, "text": "low rank"}, {"st": 86, "ed": 88, "text": "low rank"}, {"st": 91, "ed": 93, "text": "rank factorization"}, {"st": 148, "ed": 150, "text": "rank factorization"}, {"st": 155, "ed": 157, "text": "hand tuned"}, {"st": 166, "ed": 168, "text": "error rates"}, {"st": 170, "ed": 172, "text": "competing methods"}, {"st": 183, "ed": 185, "text": "relative improvement"}, {"st": 186, "ed": 188, "text": "rank factorization"}, {"st": 193, "ed": 195, "text": "hand tuned"}, {"st": 202, "ed": 204, "text": "computationally expensive"}, {"st": 225, "ed": 227, "text": "rank factorization"}, {"st": 234, "ed": 236, "text": "hand tuned"}, {"st": 259, "ed": 261, "text": "compression ratio"}]
[{"st": 4, "ed": 6, "text": "challenging task"}, {"st": 37, "ed": 39, "text": "deep learning"}, {"st": 45, "ed": 48, "text": "short term memory"}, {"st": 48, "ed": 50, "text": "neural network"}, {"st": 84, "ed": 86, "text": "proposed framework"}, {"st": 130, "ed": 132, "text": "training process"}, {"st": 141, "ed": 143, "text": "lstm network"}, {"st": 149, "ed": 151, "text": "spatio temporal"}, {"st": 159, "ed": 161, "text": "consistently outperforms"}, {"st": 165, "ed": 167, "text": "baseline methods"}, {"st": 170, "ed": 172, "text": "real world"}, {"st": 183, "ed": 185, "text": "proposed framework"}, {"st": 194, "ed": 196, "text": "real world"}]
[{"st": 1, "ed": 3, "text": "practical applications"}, {"st": 4, "ed": 6, "text": "machine learning"}, {"st": 71, "ed": 73, "text": "performance metric"}, {"st": 82, "ed": 84, "text": "precision recall"}, {"st": 94, "ed": 96, "text": "deep learning"}, {"st": 99, "ed": 101, "text": "posterior probability"}, {"st": 102, "ed": 104, "text": "uncertainty estimates"}, {"st": 113, "ed": 115, "text": "computer vision"}, {"st": 115, "ed": 117, "text": "natural language"}, {"st": 152, "ed": 154, "text": "training data"}]
[{"st": 14, "ed": 16, "text": "applications including"}, {"st": 16, "ed": 18, "text": "parameter tuning"}, {"st": 19, "ed": 21, "text": "environmental monitoring"}, {"st": 45, "ed": 47, "text": "open problem"}, {"st": 100, "ed": 102, "text": "additive models"}, {"st": 119, "ed": 121, "text": "message passing"}, {"st": 144, "ed": 146, "text": "empirically demonstrate"}, {"st": 153, "ed": 157, "text": "synthetic and real world"}]
[{"st": 37, "ed": 39, "text": "learning theory"}, {"st": 66, "ed": 68, "text": "closely related"}, {"st": 138, "ed": 140, "text": "learning algorithm"}, {"st": 183, "ed": 185, "text": "learning algorithm"}]
[{"st": 66, "ed": 68, "text": "bayes rule"}, {"st": 138, "ed": 141, "text": "takes into account"}, {"st": 171, "ed": 173, "text": "main results"}]
[{"st": 10, "ed": 13, "text": "deep neural networks"}, {"st": 55, "ed": 57, "text": "input space"}, {"st": 67, "ed": 69, "text": "input space"}, {"st": 80, "ed": 82, "text": "neural network"}]
[{"st": 35, "ed": 37, "text": "strong assumptions"}, {"st": 63, "ed": 65, "text": "latent factor"}, {"st": 127, "ed": 129, "text": "multi stage"}, {"st": 129, "ed": 131, "text": "matrix factorization"}]
[{"st": 38, "ed": 40, "text": "social science"}, {"st": 58, "ed": 60, "text": "significantly reduce"}, {"st": 79, "ed": 81, "text": "computationally efficient"}, {"st": 112, "ed": 115, "text": "real world data"}, {"st": 125, "ed": 127, "text": "baseline methods"}]
[{"st": 42, "ed": 44, "text": "machine learning"}, {"st": 60, "ed": 62, "text": "statistical model"}, {"st": 99, "ed": 101, "text": "hyper parameters"}, {"st": 102, "ed": 104, "text": "machine learning"}, {"st": 112, "ed": 114, "text": "deep learning"}, {"st": 121, "ed": 123, "text": "neural network"}, {"st": 133, "ed": 135, "text": "gaussian process"}, {"st": 139, "ed": 141, "text": "neural architecture"}, {"st": 148, "ed": 150, "text": "distance metric"}, {"st": 154, "ed": 156, "text": "neural network"}, {"st": 164, "ed": 166, "text": "optimal transport"}, {"st": 176, "ed": 178, "text": "deep learning"}, {"st": 199, "ed": 201, "text": "cross validation"}, {"st": 206, "ed": 208, "text": "multi layer"}]
[{"st": 21, "ed": 23, "text": "machine learning"}, {"st": 37, "ed": 39, "text": "predictive modeling"}, {"st": 62, "ed": 64, "text": "low dimensional"}, {"st": 93, "ed": 95, "text": "gaussian process"}, {"st": 114, "ed": 116, "text": "meta learning"}]
[{"st": 7, "ed": 9, "text": "machine learning"}, {"st": 52, "ed": 54, "text": "machine learning"}, {"st": 72, "ed": 74, "text": "objective function"}, {"st": 103, "ed": 105, "text": "objective function"}, {"st": 116, "ed": 118, "text": "feature selection"}, {"st": 152, "ed": 154, "text": "regularization term"}, {"st": 158, "ed": 160, "text": "objective function"}]
[{"st": 2, "ed": 4, "text": "machine learning"}, {"st": 15, "ed": 18, "text": "deep neural networks"}, {"st": 37, "ed": 39, "text": "previously learned"}, {"st": 48, "ed": 51, "text": "deep neural networks"}]
[{"st": 0, "ed": 2, "text": "echo state"}, {"st": 36, "ed": 38, "text": "echo state"}, {"st": 58, "ed": 60, "text": "echo state"}]
[{"st": 2, "ed": 4, "text": "generative model"}, {"st": 11, "ed": 14, "text": "generative adversarial networks"}, {"st": 61, "ed": 63, "text": "takes place"}]
[{"st": 4, "ed": 6, "text": "online learning"}, {"st": 50, "ed": 52, "text": "regret bounds"}, {"st": 56, "ed": 58, "text": "special case"}, {"st": 100, "ed": 102, "text": "bregman divergences"}, {"st": 128, "ed": 130, "text": "strongly convex"}, {"st": 157, "ed": 159, "text": "closely related"}, {"st": 191, "ed": 193, "text": "posterior distribution"}]
[{"st": 7, "ed": 9, "text": "training objective"}, {"st": 22, "ed": 24, "text": "predict future"}, {"st": 25, "ed": 27, "text": "image pixels"}, {"st": 113, "ed": 115, "text": "mutual information"}, {"st": 126, "ed": 128, "text": "mutual information"}, {"st": 135, "ed": 137, "text": "mutual information"}, {"st": 162, "ed": 164, "text": "training objective"}]
[{"st": 12, "ed": 14, "text": "deep learning"}, {"st": 55, "ed": 58, "text": "deep neural networks"}, {"st": 96, "ed": 98, "text": "predictive performance"}, {"st": 141, "ed": 143, "text": "training set"}, {"st": 158, "ed": 160, "text": "multi layer"}, {"st": 162, "ed": 165, "text": "convolutional neural networks"}, {"st": 167, "ed": 169, "text": "image data"}, {"st": 192, "ed": 194, "text": "training epochs"}, {"st": 214, "ed": 216, "text": "gaussian noise"}]
[{"st": 33, "ed": 35, "text": "machine learning"}, {"st": 37, "ed": 39, "text": "big data"}, {"st": 80, "ed": 83, "text": "electronic health records"}, {"st": 128, "ed": 130, "text": "machine language"}, {"st": 142, "ed": 144, "text": "blood transfusion"}, {"st": 152, "ed": 154, "text": "blood transfusion"}, {"st": 165, "ed": 167, "text": "scikit learn"}, {"st": 167, "ed": 169, "text": "machine learning"}, {"st": 172, "ed": 175, "text": "support vector machines"}, {"st": 178, "ed": 180, "text": "support vector"}, {"st": 184, "ed": 186, "text": "linear model"}, {"st": 196, "ed": 198, "text": "decision tree"}]
[{"st": 7, "ed": 11, "text": "multi armed bandit problem"}, {"st": 13, "ed": 15, "text": "expected reward"}, {"st": 106, "ed": 108, "text": "bandit problem"}, {"st": 111, "ed": 114, "text": "upper confidence bound"}, {"st": 122, "ed": 124, "text": "parameter free"}, {"st": 159, "ed": 161, "text": "non stationary"}]
[{"st": 30, "ed": 32, "text": "weighted average"}, {"st": 44, "ed": 46, "text": "least squares"}, {"st": 71, "ed": 73, "text": "finite sample"}, {"st": 76, "ed": 78, "text": "proposed approach"}]
[{"st": 0, "ed": 2, "text": "topic models"}, {"st": 11, "ed": 13, "text": "machine learning"}, {"st": 30, "ed": 32, "text": "pre trained"}, {"st": 32, "ed": 34, "text": "word embedding"}, {"st": 56, "ed": 58, "text": "approximate inference"}, {"st": 65, "ed": 67, "text": "topic models"}, {"st": 92, "ed": 94, "text": "topic models"}, {"st": 138, "ed": 140, "text": "pre trained"}, {"st": 140, "ed": 142, "text": "word embedding"}, {"st": 171, "ed": 173, "text": "semi supervised"}, {"st": 192, "ed": 194, "text": "prediction performance"}, {"st": 206, "ed": 208, "text": "approach outperforms"}, {"st": 213, "ed": 216, "text": "latent dirichlet allocation"}, {"st": 221, "ed": 223, "text": "document classification"}]
[{"st": 6, "ed": 8, "text": "optimization methods"}, {"st": 11, "ed": 13, "text": "natural gradient"}, {"st": 27, "ed": 29, "text": "linear regression"}, {"st": 31, "ed": 33, "text": "linear classification"}, {"st": 41, "ed": 43, "text": "global minimum"}, {"st": 51, "ed": 53, "text": "optimization algorithms"}, {"st": 70, "ed": 72, "text": "step size"}]
[{"st": 0, "ed": 2, "text": "lifelong learning"}, {"st": 5, "ed": 7, "text": "machine learning"}, {"st": 50, "ed": 52, "text": "catastrophic forgetting"}, {"st": 76, "ed": 79, "text": "convolutional neural networks"}, {"st": 83, "ed": 85, "text": "experiment results"}, {"st": 118, "ed": 120, "text": "neural network"}]
[{"st": 3, "ed": 5, "text": "non stationary"}, {"st": 7, "ed": 9, "text": "bandit mab"}, {"st": 9, "ed": 12, "text": "problem and propose"}, {"st": 17, "ed": 19, "text": "limited memory"}, {"st": 22, "ed": 25, "text": "exploration and exploitation"}, {"st": 29, "ed": 31, "text": "sliding window"}, {"st": 31, "ed": 34, "text": "upper confidence bound"}, {"st": 58, "ed": 60, "text": "cumulative regret"}]
[{"st": 1, "ed": 3, "text": "recent works"}, {"st": 5, "ed": 7, "text": "based methods"}, {"st": 11, "ed": 13, "text": "based methods"}, {"st": 40, "ed": 42, "text": "based methods"}, {"st": 56, "ed": 58, "text": "hand crafted"}, {"st": 64, "ed": 68, "text": "generative adversarial network gan"}, {"st": 71, "ed": 73, "text": "inverse problems"}, {"st": 95, "ed": 97, "text": "inverse problems"}, {"st": 99, "ed": 102, "text": "provide theoretical guarantees"}, {"st": 104, "ed": 107, "text": "rate of convergence"}]
[{"st": 3, "ed": 5, "text": "positive definite"}, {"st": 110, "ed": 112, "text": "higher order"}, {"st": 132, "ed": 134, "text": "higher order"}]
[{"st": 26, "ed": 29, "text": "empirical risk minimization"}, {"st": 77, "ed": 79, "text": "kernel methods"}, {"st": 102, "ed": 104, "text": "linear models"}]
[{"st": 5, "ed": 7, "text": "building blocks"}, {"st": 11, "ed": 13, "text": "latent variable"}, {"st": 51, "ed": 54, "text": "end to end"}, {"st": 79, "ed": 82, "text": "easy to implement"}, {"st": 100, "ed": 102, "text": "gumbel softmax"}]
[{"st": 6, "ed": 8, "text": "building block"}, {"st": 10, "ed": 13, "text": "intelligent transportation systems"}, {"st": 18, "ed": 20, "text": "accurate prediction"}, {"st": 68, "ed": 70, "text": "large scale"}, {"st": 78, "ed": 80, "text": "big data"}, {"st": 90, "ed": 92, "text": "real world"}, {"st": 95, "ed": 97, "text": "prediction methods"}, {"st": 100, "ed": 102, "text": "time series"}, {"st": 112, "ed": 115, "text": "spatial and temporal"}, {"st": 116, "ed": 118, "text": "recent advances"}, {"st": 119, "ed": 121, "text": "deep learning"}, {"st": 127, "ed": 129, "text": "challenging tasks"}, {"st": 131, "ed": 133, "text": "image classification"}, {"st": 141, "ed": 143, "text": "large scale"}, {"st": 151, "ed": 153, "text": "deep learning"}, {"st": 159, "ed": 161, "text": "existing methods"}, {"st": 183, "ed": 185, "text": "multi view"}, {"st": 185, "ed": 187, "text": "spatial temporal"}, {"st": 189, "ed": 191, "text": "net framework"}, {"st": 194, "ed": 197, "text": "spatial and temporal"}, {"st": 223, "ed": 225, "text": "local spatial"}, {"st": 238, "ed": 240, "text": "temporal patterns"}, {"st": 243, "ed": 245, "text": "large scale"}]
[{"st": 10, "ed": 13, "text": "linear dimensionality reduction"}, {"st": 31, "ed": 33, "text": "computational complexity"}, {"st": 46, "ed": 48, "text": "time series"}, {"st": 62, "ed": 64, "text": "nystr om"}]
[{"st": 3, "ed": 5, "text": "et al"}, {"st": 14, "ed": 16, "text": "singular values"}, {"st": 32, "ed": 34, "text": "singular values"}, {"st": 41, "ed": 45, "text": "generative adversarial networks gans"}, {"st": 81, "ed": 83, "text": "ad hoc"}, {"st": 92, "ed": 94, "text": "inception score"}, {"st": 113, "ed": 115, "text": "regularization technique"}, {"st": 122, "ed": 124, "text": "condition number"}, {"st": 133, "ed": 135, "text": "inception score"}]
[{"st": 2, "ed": 5, "text": "stochastic gradient descent"}, {"st": 6, "ed": 8, "text": "based optimization"}, {"st": 10, "ed": 13, "text": "deep neural networks"}, {"st": 145, "ed": 147, "text": "batch sizes"}, {"st": 149, "ed": 151, "text": "learning rates"}, {"st": 164, "ed": 166, "text": "learning rate"}, {"st": 176, "ed": 178, "text": "batch size"}]
[{"st": 5, "ed": 7, "text": "decision tree"}, {"st": 7, "ed": 9, "text": "learning algorithm"}, {"st": 37, "ed": 39, "text": "decision tree"}, {"st": 80, "ed": 82, "text": "concept drift"}, {"st": 88, "ed": 90, "text": "higher accuracy"}]
[{"st": 6, "ed": 8, "text": "gaussian processes"}, {"st": 92, "ed": 94, "text": "multi task"}]
[]
[{"st": 0, "ed": 2, "text": "residual networks"}, {"st": 11, "ed": 13, "text": "deep learning"}, {"st": 96, "ed": 98, "text": "method called"}, {"st": 100, "ed": 102, "text": "classification tasks"}, {"st": 142, "ed": 144, "text": "proposed method"}]
[{"st": 9, "ed": 11, "text": "time series"}, {"st": 21, "ed": 23, "text": "main contribution"}, {"st": 34, "ed": 36, "text": "time series"}, {"st": 46, "ed": 48, "text": "missing values"}, {"st": 55, "ed": 57, "text": "linear regression"}, {"st": 84, "ed": 86, "text": "non stationary"}, {"st": 103, "ed": 105, "text": "hidden state"}, {"st": 110, "ed": 112, "text": "noisy observations"}, {"st": 116, "ed": 119, "text": "hidden markov model"}, {"st": 131, "ed": 136, "text": "synthetic and real world datasets"}, {"st": 150, "ed": 152, "text": "missing data"}, {"st": 178, "ed": 180, "text": "finite sample"}]
[{"st": 2, "ed": 4, "text": "active learning"}, {"st": 8, "ed": 10, "text": "labeled examples"}, {"st": 102, "ed": 104, "text": "active learning"}, {"st": 109, "ed": 111, "text": "active learning"}]
[{"st": 0, "ed": 2, "text": "recent advances"}, {"st": 3, "ed": 6, "text": "deep reinforcement learning"}, {"st": 26, "ed": 29, "text": "exploration and exploitation"}, {"st": 35, "ed": 37, "text": "thompson sampling"}, {"st": 41, "ed": 43, "text": "reinforcement learning"}, {"st": 65, "ed": 67, "text": "approximate bayesian"}, {"st": 74, "ed": 76, "text": "neural network"}, {"st": 84, "ed": 86, "text": "approximate bayesian"}, {"st": 86, "ed": 88, "text": "neural networks"}, {"st": 90, "ed": 92, "text": "thompson sampling"}, {"st": 100, "ed": 102, "text": "approximate posterior"}, {"st": 103, "ed": 105, "text": "thompson sampling"}, {"st": 110, "ed": 112, "text": "recently developed"}, {"st": 114, "ed": 116, "text": "approximate posterior"}, {"st": 119, "ed": 121, "text": "thompson sampling"}, {"st": 125, "ed": 127, "text": "contextual bandit"}, {"st": 139, "ed": 141, "text": "supervised learning"}, {"st": 145, "ed": 148, "text": "sequential decision making"}, {"st": 159, "ed": 161, "text": "uncertainty estimates"}]
[{"st": 5, "ed": 7, "text": "activation functions"}, {"st": 9, "ed": 12, "text": "deep neural network"}, {"st": 30, "ed": 32, "text": "total variation"}, {"st": 43, "ed": 46, "text": "deep neural networks"}, {"st": 67, "ed": 69, "text": "activation functions"}, {"st": 134, "ed": 136, "text": "ell 1"}]
[{"st": 3, "ed": 5, "text": "machine learning"}, {"st": 10, "ed": 12, "text": "feature engineering"}, {"st": 20, "ed": 22, "text": "machine learning"}]
[{"st": 17, "ed": 19, "text": "variable selection"}, {"st": 26, "ed": 28, "text": "theoretical guarantee"}, {"st": 30, "ed": 32, "text": "existing algorithms"}, {"st": 46, "ed": 48, "text": "variable selection"}, {"st": 52, "ed": 54, "text": "kernel based"}, {"st": 94, "ed": 96, "text": "proposed algorithm"}, {"st": 101, "ed": 105, "text": "reproducing kernel hilbert space"}, {"st": 108, "ed": 110, "text": "kernel functions"}, {"st": 121, "ed": 123, "text": "computational cost"}, {"st": 142, "ed": 144, "text": "proposed algorithm"}, {"st": 149, "ed": 152, "text": "under mild conditions"}, {"st": 155, "ed": 157, "text": "gaussian kernels"}, {"st": 169, "ed": 172, "text": "simulated and real"}]
[{"st": 22, "ed": 24, "text": "open question"}, {"st": 142, "ed": 144, "text": "objective function"}]
[{"st": 2, "ed": 4, "text": "time series"}, {"st": 24, "ed": 26, "text": "missing data"}, {"st": 50, "ed": 52, "text": "estimation problem"}, {"st": 67, "ed": 69, "text": "partially observed"}, {"st": 79, "ed": 81, "text": "transition matrix"}, {"st": 104, "ed": 106, "text": "transition matrix"}, {"st": 109, "ed": 111, "text": "autoregressive models"}, {"st": 116, "ed": 118, "text": "missing data"}, {"st": 124, "ed": 126, "text": "transition matrix"}, {"st": 132, "ed": 135, "text": "high dimensional sparse"}, {"st": 135, "ed": 137, "text": "linear regression"}, {"st": 141, "ed": 143, "text": "highly dependent"}, {"st": 144, "ed": 146, "text": "existing results"}, {"st": 150, "ed": 152, "text": "missing data"}]
[{"st": 4, "ed": 6, "text": "pac bayes"}, {"st": 14, "ed": 16, "text": "learning algorithm"}, {"st": 28, "ed": 30, "text": "generalization bounds"}, {"st": 55, "ed": 57, "text": "differentially private"}, {"st": 63, "ed": 65, "text": "pac bayes"}, {"st": 79, "ed": 81, "text": "generalization bound"}, {"st": 106, "ed": 108, "text": "langevin dynamics"}, {"st": 117, "ed": 119, "text": "pac bayes"}, {"st": 128, "ed": 130, "text": "differentially private"}, {"st": 143, "ed": 145, "text": "synthetic data"}, {"st": 147, "ed": 149, "text": "neural network"}, {"st": 164, "ed": 166, "text": "pac bayes"}]
[{"st": 7, "ed": 9, "text": "optimal policy"}, {"st": 11, "ed": 14, "text": "markov decision process"}, {"st": 22, "ed": 24, "text": "convex hull"}, {"st": 68, "ed": 70, "text": "np hard"}, {"st": 119, "ed": 121, "text": "convex combination"}, {"st": 130, "ed": 132, "text": "proposed algorithm"}, {"st": 152, "ed": 154, "text": "efficient implementation"}, {"st": 161, "ed": 163, "text": "policy gradient"}, {"st": 165, "ed": 167, "text": "proposed approach"}, {"st": 183, "ed": 185, "text": "iterative method"}, {"st": 200, "ed": 202, "text": "complex systems"}, {"st": 212, "ed": 214, "text": "time consuming"}]
[{"st": 0, "ed": 2, "text": "traditional methods"}, {"st": 3, "ed": 5, "text": "link prediction"}, {"st": 14, "ed": 16, "text": "feature based"}, {"st": 16, "ed": 18, "text": "latent feature"}, {"st": 20, "ed": 22, "text": "explicit feature"}, {"st": 42, "ed": 44, "text": "latent feature"}, {"st": 58, "ed": 60, "text": "explicit feature"}, {"st": 63, "ed": 65, "text": "machine learning"}, {"st": 95, "ed": 97, "text": "link prediction"}, {"st": 101, "ed": 103, "text": "link prediction"}, {"st": 116, "ed": 118, "text": "neural network"}, {"st": 126, "ed": 128, "text": "neural network"}, {"st": 176, "ed": 178, "text": "key feature"}, {"st": 185, "ed": 187, "text": "latent features"}, {"st": 197, "ed": 199, "text": "latent features"}, {"st": 224, "ed": 226, "text": "extensive experiments"}, {"st": 229, "ed": 231, "text": "strong performance"}, {"st": 236, "ed": 238, "text": "baseline methods"}, {"st": 240, "ed": 242, "text": "link prediction"}, {"st": 244, "ed": 246, "text": "network embedding"}]
[{"st": 2, "ed": 4, "text": "deep learning"}, {"st": 10, "ed": 12, "text": "recently gained"}, {"st": 22, "ed": 24, "text": "generative adversarial"}, {"st": 71, "ed": 74, "text": "theoretically and empirically"}]
[{"st": 7, "ed": 9, "text": "linear bandits"}, {"st": 15, "ed": 17, "text": "kernel functions"}, {"st": 20, "ed": 24, "text": "reproducing kernel hilbert space"}, {"st": 71, "ed": 73, "text": "mathcal o"}, {"st": 87, "ed": 89, "text": "mathcal o"}, {"st": 104, "ed": 106, "text": "mathcal o"}, {"st": 122, "ed": 124, "text": "mathcal o"}, {"st": 144, "ed": 146, "text": "kernel functions"}]
[{"st": 0, "ed": 3, "text": "stochastic gradient descent"}, {"st": 5, "ed": 8, "text": "achieved great success"}, {"st": 10, "ed": 13, "text": "deep neural network"}, {"st": 42, "ed": 45, "text": "deep neural network"}, {"st": 48, "ed": 50, "text": "learning rate"}, {"st": 86, "ed": 88, "text": "least squares"}, {"st": 116, "ed": 118, "text": "layer wise"}, {"st": 119, "ed": 121, "text": "learning rate"}, {"st": 132, "ed": 134, "text": "machine learning"}, {"st": 146, "ed": 149, "text": "deep neural networks"}]
[{"st": 5, "ed": 9, "text": "convolutional neural networks cnns"}, {"st": 11, "ed": 13, "text": "structured data"}]
[{"st": 3, "ed": 5, "text": "fully automatic"}, {"st": 16, "ed": 18, "text": "multi user"}, {"st": 47, "ed": 50, "text": "deep neural networks"}, {"st": 66, "ed": 68, "text": "fixed length"}, {"st": 94, "ed": 96, "text": "recurrent networks"}, {"st": 98, "ed": 100, "text": "hierarchical representations"}, {"st": 111, "ed": 113, "text": "proposed method"}, {"st": 141, "ed": 143, "text": "multi touch"}, {"st": 159, "ed": 161, "text": "gesture recognition"}]
[{"st": 0, "ed": 2, "text": "network pruning"}, {"st": 9, "ed": 11, "text": "neural network"}, {"st": 44, "ed": 46, "text": "network pruning"}, {"st": 90, "ed": 92, "text": "attention mechanism"}, {"st": 100, "ed": 102, "text": "network pruning"}, {"st": 104, "ed": 106, "text": "important information"}, {"st": 114, "ed": 116, "text": "cifar 10"}, {"st": 117, "ed": 119, "text": "proposed method"}, {"st": 122, "ed": 124, "text": "baseline method"}]
[{"st": 12, "ed": 14, "text": "singular values"}, {"st": 16, "ed": 18, "text": "deep network"}, {"st": 19, "ed": 21, "text": "input output"}, {"st": 47, "ed": 49, "text": "theoretical understanding"}, {"st": 61, "ed": 63, "text": "powerful tools"}, {"st": 76, "ed": 78, "text": "deep network"}]
[{"st": 0, "ed": 2, "text": "policy gradient"}, {"st": 11, "ed": 13, "text": "reinforcement learning"}, {"st": 44, "ed": 46, "text": "significantly reduces"}, {"st": 49, "ed": 51, "text": "sample efficiency"}, {"st": 69, "ed": 71, "text": "policy gradient"}, {"st": 105, "ed": 107, "text": "open source"}, {"st": 161, "ed": 163, "text": "significantly improve"}]
[{"st": 3, "ed": 5, "text": "recent advances"}, {"st": 6, "ed": 8, "text": "big data"}, {"st": 10, "ed": 12, "text": "prediction tasks"}, {"st": 12, "ed": 14, "text": "variational bayesian"}, {"st": 33, "ed": 35, "text": "approximate inference"}, {"st": 36, "ed": 39, "text": "stochastic variational inference"}, {"st": 43, "ed": 45, "text": "variational inference"}, {"st": 47, "ed": 49, "text": "stochastic optimization"}, {"st": 62, "ed": 64, "text": "massive data"}, {"st": 67, "ed": 69, "text": "classification tasks"}, {"st": 98, "ed": 103, "text": "alternating direction method of multipliers"}, {"st": 119, "ed": 121, "text": "inference algorithms"}, {"st": 186, "ed": 188, "text": "deep learning"}, {"st": 197, "ed": 199, "text": "topic modeling"}, {"st": 210, "ed": 213, "text": "latent dirichlet allocation"}, {"st": 214, "ed": 216, "text": "topic model"}, {"st": 218, "ed": 220, "text": "document classification"}, {"st": 228, "ed": 230, "text": "numerical experiments"}]
[{"st": 0, "ed": 3, "text": "deep neural networks"}, {"st": 28, "ed": 31, "text": "labeled training data"}, {"st": 46, "ed": 49, "text": "semi supervised learning"}, {"st": 60, "ed": 63, "text": "labeled and unlabeled"}, {"st": 72, "ed": 74, "text": "loss function"}, {"st": 84, "ed": 86, "text": "semi supervised"}, {"st": 95, "ed": 97, "text": "semi supervised"}, {"st": 144, "ed": 147, "text": "convolutional neural network"}]
[{"st": 8, "ed": 10, "text": "mathbf x"}, {"st": 20, "ed": 22, "text": "mathbf x"}, {"st": 46, "ed": 48, "text": "mathbf x"}, {"st": 61, "ed": 63, "text": "multidimensional scaling"}, {"st": 80, "ed": 82, "text": "theoretical bounds"}, {"st": 90, "ed": 92, "text": "mathbf x"}, {"st": 160, "ed": 162, "text": "significant gains"}]
[{"st": 0, "ed": 2, "text": "sparse representations"}, {"st": 10, "ed": 12, "text": "powerful tool"}, {"st": 14, "ed": 16, "text": "signal processing"}, {"st": 20, "ed": 22, "text": "super resolution"}, {"st": 86, "ed": 88, "text": "learning objective"}, {"st": 94, "ed": 96, "text": "image patches"}, {"st": 110, "ed": 112, "text": "class labels"}, {"st": 113, "ed": 115, "text": "image pixels"}, {"st": 118, "ed": 120, "text": "sparse representations"}, {"st": 152, "ed": 154, "text": "supervised classification"}, {"st": 156, "ed": 158, "text": "competitive results"}]
[{"st": 69, "ed": 71, "text": "prior distribution"}, {"st": 99, "ed": 101, "text": "mnist dataset"}, {"st": 117, "ed": 119, "text": "unlike previous"}]
[{"st": 0, "ed": 3, "text": "deep neural networks"}, {"st": 18, "ed": 20, "text": "neural networks"}, {"st": 37, "ed": 39, "text": "training distribution"}, {"st": 57, "ed": 59, "text": "training distribution"}, {"st": 90, "ed": 92, "text": "learning rates"}, {"st": 125, "ed": 127, "text": "neural networks"}, {"st": 129, "ed": 131, "text": "catastrophic forgetting"}, {"st": 141, "ed": 143, "text": "class labels"}, {"st": 155, "ed": 157, "text": "supervised tasks"}, {"st": 157, "ed": 159, "text": "large scale"}, {"st": 159, "ed": 161, "text": "image classification"}]
[{"st": 3, "ed": 5, "text": "high dimensional"}, {"st": 120, "ed": 122, "text": "key feature"}]
[{"st": 4, "ed": 6, "text": "method called"}, {"st": 8, "ed": 11, "text": "principal component analysis"}, {"st": 20, "ed": 22, "text": "sparse matrix"}, {"st": 36, "ed": 38, "text": "low dimensional"}, {"st": 38, "ed": 40, "text": "latent variable"}, {"st": 53, "ed": 55, "text": "subspace clustering"}, {"st": 70, "ed": 73, "text": "provide theoretical guarantees"}, {"st": 92, "ed": 94, "text": "nonconvex optimization"}, {"st": 97, "ed": 102, "text": "alternating direction method of multipliers"}, {"st": 117, "ed": 119, "text": "synthetic data"}, {"st": 133, "ed": 135, "text": "robust subspace"}]
[{"st": 27, "ed": 29, "text": "unlabeled data"}, {"st": 33, "ed": 35, "text": "statistical modeling"}, {"st": 41, "ed": 43, "text": "deep learning"}, {"st": 45, "ed": 47, "text": "unsupervised learning"}, {"st": 73, "ed": 75, "text": "joint distributions"}, {"st": 144, "ed": 146, "text": "result shows"}, {"st": 156, "ed": 158, "text": "activity tracker"}]
[{"st": 2, "ed": 4, "text": "machine learning"}, {"st": 5, "ed": 7, "text": "classification accuracy"}, {"st": 18, "ed": 20, "text": "class imbalance"}, {"st": 31, "ed": 33, "text": "precision recall"}, {"st": 51, "ed": 54, "text": "constrained optimization problem"}, {"st": 84, "ed": 86, "text": "positive rate"}, {"st": 97, "ed": 99, "text": "explicitly model"}, {"st": 113, "ed": 115, "text": "surrogate loss"}, {"st": 146, "ed": 148, "text": "competitive performance"}]
[{"st": 16, "ed": 18, "text": "generative model"}, {"st": 36, "ed": 38, "text": "machine learning"}, {"st": 54, "ed": 56, "text": "dimensionality reduction"}]
[{"st": 1, "ed": 3, "text": "recent years"}, {"st": 8, "ed": 10, "text": "machine learning"}, {"st": 23, "ed": 25, "text": "gaussian noise"}, {"st": 28, "ed": 30, "text": "theoretical understanding"}, {"st": 35, "ed": 37, "text": "statistical learning"}, {"st": 46, "ed": 48, "text": "statistical learning"}, {"st": 59, "ed": 61, "text": "gaussian noise"}, {"st": 76, "ed": 78, "text": "gaussian noise"}, {"st": 84, "ed": 86, "text": "gaussian noise"}, {"st": 87, "ed": 89, "text": "special cases"}, {"st": 92, "ed": 94, "text": "gaussian noise"}, {"st": 135, "ed": 137, "text": "learning rates"}, {"st": 147, "ed": 149, "text": "mathcal o"}, {"st": 167, "ed": 169, "text": "statistical learning"}, {"st": 178, "ed": 180, "text": "statistical learning"}]
[{"st": 3, "ed": 5, "text": "neural networks"}, {"st": 52, "ed": 54, "text": "sg mcmc"}, {"st": 55, "ed": 57, "text": "group sparse"}, {"st": 66, "ed": 68, "text": "posterior distribution"}, {"st": 77, "ed": 79, "text": "weight pruning"}, {"st": 98, "ed": 100, "text": "sg mcmc"}, {"st": 105, "ed": 107, "text": "achieve high"}, {"st": 107, "ed": 109, "text": "prediction accuracy"}, {"st": 110, "ed": 112, "text": "sg mcmc"}, {"st": 117, "ed": 119, "text": "parameter space"}, {"st": 124, "ed": 126, "text": "computation cost"}, {"st": 129, "ed": 132, "text": "training and testing"}, {"st": 174, "ed": 176, "text": "lstm models"}, {"st": 210, "ed": 212, "text": "empirical study"}, {"st": 214, "ed": 216, "text": "sg mcmc"}, {"st": 216, "ed": 218, "text": "group sparse"}, {"st": 220, "ed": 222, "text": "network pruning"}]
[{"st": 4, "ed": 6, "text": "deep learning"}, {"st": 14, "ed": 16, "text": "learning algorithms"}, {"st": 18, "ed": 20, "text": "important role"}, {"st": 21, "ed": 23, "text": "generalization performance"}, {"st": 25, "ed": 28, "text": "stochastic gradient descent"}, {"st": 56, "ed": 58, "text": "empirical analysis"}, {"st": 112, "ed": 114, "text": "langevin dynamics"}]
[{"st": 0, "ed": 2, "text": "phase retrieval"}, {"st": 92, "ed": 95, "text": "convolutional neural networks"}, {"st": 96, "ed": 98, "text": "powerful tool"}, {"st": 99, "ed": 101, "text": "machine learning"}, {"st": 103, "ed": 105, "text": "phase retrieval"}]
[{"st": 3, "ed": 5, "text": "machine learning"}, {"st": 6, "ed": 8, "text": "missing values"}, {"st": 23, "ed": 25, "text": "missing values"}, {"st": 30, "ed": 32, "text": "missing entries"}, {"st": 46, "ed": 48, "text": "prediction model"}, {"st": 64, "ed": 66, "text": "missing entries"}, {"st": 99, "ed": 101, "text": "probabilistic model"}, {"st": 107, "ed": 109, "text": "missing entries"}, {"st": 111, "ed": 113, "text": "computational cost"}, {"st": 130, "ed": 132, "text": "approach called"}, {"st": 148, "ed": 150, "text": "missing entries"}, {"st": 162, "ed": 164, "text": "prediction results"}, {"st": 167, "ed": 169, "text": "training sets"}, {"st": 191, "ed": 193, "text": "learning algorithms"}, {"st": 195, "ed": 197, "text": "least squares"}, {"st": 198, "ed": 201, "text": "support vector machine"}]
[{"st": 4, "ed": 6, "text": "distance based"}, {"st": 60, "ed": 62, "text": "learning problems"}, {"st": 62, "ed": 64, "text": "wasserstein distance"}, {"st": 89, "ed": 91, "text": "proposed approach"}, {"st": 119, "ed": 121, "text": "wasserstein distance"}, {"st": 130, "ed": 132, "text": "wasserstein distance"}, {"st": 139, "ed": 142, "text": "kullback leibler divergence"}]
[{"st": 4, "ed": 6, "text": "nearest neighbour"}, {"st": 8, "ed": 10, "text": "cost sensitive"}, {"st": 12, "ed": 14, "text": "low dimensional"}, {"st": 26, "ed": 28, "text": "learning rates"}, {"st": 36, "ed": 38, "text": "cost sensitive"}, {"st": 67, "ed": 69, "text": "nearest neighbour"}, {"st": 78, "ed": 80, "text": "low dimensional"}]
[{"st": 8, "ed": 11, "text": "k nearest neighbour"}, {"st": 11, "ed": 13, "text": "ucb algorithm"}, {"st": 14, "ed": 17, "text": "multi armed bandits"}, {"st": 31, "ed": 33, "text": "metric space"}, {"st": 35, "ed": 37, "text": "intrinsic dimension"}, {"st": 52, "ed": 54, "text": "conceptually simple"}, {"st": 59, "ed": 62, "text": "k nearest neighbour"}, {"st": 62, "ed": 64, "text": "ucb algorithm"}, {"st": 67, "ed": 69, "text": "prior knowledge"}, {"st": 73, "ed": 75, "text": "intrinsic dimension"}, {"st": 77, "ed": 79, "text": "marginal distribution"}, {"st": 86, "ed": 88, "text": "regret bound"}, {"st": 90, "ed": 93, "text": "k nearest neighbour"}, {"st": 93, "ed": 95, "text": "ucb algorithm"}, {"st": 108, "ed": 111, "text": "takes advantage of"}, {"st": 113, "ed": 115, "text": "intrinsic dimensionality"}, {"st": 117, "ed": 119, "text": "marginal distribution"}, {"st": 123, "ed": 125, "text": "low noise"}, {"st": 145, "ed": 147, "text": "regret bounds"}, {"st": 149, "ed": 152, "text": "k nearest neighbour"}, {"st": 153, "ed": 155, "text": "ucb algorithm"}, {"st": 162, "ed": 164, "text": "ucb algorithm"}, {"st": 169, "ed": 172, "text": "multi armed bandits"}, {"st": 177, "ed": 179, "text": "empirical results"}, {"st": 186, "ed": 189, "text": "k nearest neighbour"}, {"st": 191, "ed": 194, "text": "k nearest neighbour"}]
[{"st": 27, "ed": 29, "text": "recent studies"}, {"st": 45, "ed": 47, "text": "important information"}, {"st": 52, "ed": 54, "text": "web search"}, {"st": 58, "ed": 60, "text": "knowledge graph"}, {"st": 121, "ed": 123, "text": "missing information"}, {"st": 141, "ed": 143, "text": "training data"}, {"st": 145, "ed": 147, "text": "missing information"}, {"st": 170, "ed": 172, "text": "experiments conducted"}]
[{"st": 6, "ed": 8, "text": "theoretical framework"}, {"st": 14, "ed": 16, "text": "building block"}, {"st": 18, "ed": 20, "text": "deep learning"}, {"st": 77, "ed": 79, "text": "inner product"}, {"st": 109, "ed": 111, "text": "training process"}, {"st": 112, "ed": 114, "text": "matrix factorization"}, {"st": 120, "ed": 122, "text": "bias variance"}, {"st": 131, "ed": 133, "text": "stability theory"}, {"st": 160, "ed": 163, "text": "sheds light on"}, {"st": 182, "ed": 184, "text": "bias variance"}, {"st": 192, "ed": 194, "text": "open problem"}]
[{"st": 7, "ed": 9, "text": "sample efficient"}, {"st": 10, "ed": 12, "text": "reinforcement learning"}, {"st": 22, "ed": 24, "text": "sample efficient"}, {"st": 29, "ed": 31, "text": "hidden state"}, {"st": 39, "ed": 41, "text": "computationally efficient"}, {"st": 58, "ed": 60, "text": "statistically efficient"}, {"st": 69, "ed": 71, "text": "np hard"}, {"st": 92, "ed": 94, "text": "reinforcement learning"}]
[{"st": 1, "ed": 3, "text": "patient data"}, {"st": 84, "ed": 86, "text": "patient data"}, {"st": 111, "ed": 113, "text": "similarity based"}, {"st": 153, "ed": 155, "text": "longitudinal data"}, {"st": 184, "ed": 186, "text": "approach outperforms"}]
[{"st": 2, "ed": 4, "text": "linear models"}, {"st": 8, "ed": 10, "text": "x 1"}]
[{"st": 4, "ed": 6, "text": "random sampling"}, {"st": 24, "ed": 26, "text": "previous studies"}, {"st": 28, "ed": 30, "text": "random sampling"}, {"st": 35, "ed": 37, "text": "input data"}, {"st": 41, "ed": 43, "text": "response variable"}, {"st": 45, "ed": 47, "text": "response variable"}, {"st": 72, "ed": 74, "text": "input data"}, {"st": 81, "ed": 83, "text": "least square"}, {"st": 91, "ed": 93, "text": "random sampling"}, {"st": 126, "ed": 128, "text": "sample size"}, {"st": 140, "ed": 142, "text": "error bound"}, {"st": 146, "ed": 148, "text": "importance sampling"}, {"st": 160, "ed": 162, "text": "improved performance"}, {"st": 170, "ed": 174, "text": "synthetic and real data"}, {"st": 191, "ed": 193, "text": "sampling methods"}, {"st": 197, "ed": 199, "text": "statistical efficiency"}]
[{"st": 13, "ed": 15, "text": "robust subspace"}, {"st": 16, "ed": 18, "text": "robust subspace"}, {"st": 23, "ed": 25, "text": "low dimensional"}, {"st": 59, "ed": 62, "text": "advantages and disadvantages"}]
[{"st": 16, "ed": 18, "text": "low dimensional"}, {"st": 55, "ed": 57, "text": "recent works"}, {"st": 58, "ed": 60, "text": "neural networks"}, {"st": 73, "ed": 75, "text": "realistic images"}, {"st": 82, "ed": 84, "text": "probability density"}, {"st": 205, "ed": 207, "text": "low dimensional"}, {"st": 207, "ed": 209, "text": "generator network"}]
[{"st": 4, "ed": 6, "text": "contextual bandits"}, {"st": 35, "ed": 37, "text": "based approaches"}, {"st": 99, "ed": 101, "text": "extensive empirical"}]
[{"st": 0, "ed": 2, "text": "distributed stochastic"}, {"st": 63, "ed": 65, "text": "trained model"}, {"st": 109, "ed": 111, "text": "convergence analysis"}]
[{"st": 4, "ed": 6, "text": "matrix completion"}, {"st": 52, "ed": 55, "text": "non convex optimization"}, {"st": 68, "ed": 70, "text": "sample complexity"}, {"st": 101, "ed": 103, "text": "sample complexity"}, {"st": 117, "ed": 121, "text": "synthetic and real world"}]
[{"st": 0, "ed": 2, "text": "optimal transport"}, {"st": 9, "ed": 11, "text": "machine learning"}, {"st": 12, "ed": 14, "text": "computer vision"}, {"st": 20, "ed": 22, "text": "larger scale"}, {"st": 46, "ed": 48, "text": "optimal transport"}, {"st": 57, "ed": 59, "text": "recently developed"}, {"st": 107, "ed": 109, "text": "numerical experiments"}]
[{"st": 0, "ed": 3, "text": "deep neural networks"}, {"st": 17, "ed": 19, "text": "regularization techniques"}, {"st": 106, "ed": 108, "text": "sparsity inducing"}, {"st": 177, "ed": 179, "text": "variational inference"}, {"st": 188, "ed": 190, "text": "existing approaches"}, {"st": 193, "ed": 195, "text": "extensive experimental"}]
[{"st": 8, "ed": 10, "text": "multivariate data"}, {"st": 52, "ed": 54, "text": "sample complexity"}, {"st": 64, "ed": 66, "text": "sample complexity"}, {"st": 82, "ed": 84, "text": "substantially improve"}, {"st": 118, "ed": 120, "text": "estimation problem"}]
[{"st": 51, "ed": 53, "text": "recent research"}, {"st": 127, "ed": 129, "text": "hierarchical bayesian"}, {"st": 162, "ed": 164, "text": "latent variables"}, {"st": 173, "ed": 175, "text": "theoretical guarantees"}, {"st": 179, "ed": 181, "text": "nonparametric models"}, {"st": 199, "ed": 201, "text": "predictive performance"}, {"st": 202, "ed": 204, "text": "real world"}]
[{"st": 0, "ed": 2, "text": "neural networks"}, {"st": 14, "ed": 16, "text": "real images"}, {"st": 27, "ed": 29, "text": "deep learning"}, {"st": 36, "ed": 38, "text": "adversarial examples"}, {"st": 51, "ed": 54, "text": "zero sum game"}, {"st": 125, "ed": 127, "text": "trained models"}, {"st": 128, "ed": 130, "text": "fine tuning"}, {"st": 135, "ed": 137, "text": "experiments demonstrate"}]
[{"st": 23, "ed": 25, "text": "latent distribution"}, {"st": 27, "ed": 29, "text": "multiple classes"}, {"st": 57, "ed": 59, "text": "generated samples"}, {"st": 75, "ed": 77, "text": "gan training"}, {"st": 100, "ed": 102, "text": "training samples"}, {"st": 126, "ed": 128, "text": "generated samples"}, {"st": 137, "ed": 140, "text": "end to end"}, {"st": 163, "ed": 165, "text": "multiple datasets"}]
[{"st": 4, "ed": 7, "text": "multi label classification"}, {"st": 26, "ed": 28, "text": "extremely large"}, {"st": 56, "ed": 58, "text": "learning task"}, {"st": 79, "ed": 81, "text": "robust optimization"}, {"st": 114, "ed": 116, "text": "proximal gradient"}, {"st": 116, "ed": 118, "text": "based optimization"}, {"st": 138, "ed": 140, "text": "relative improvement"}, {"st": 144, "ed": 146, "text": "tree based"}, {"st": 149, "ed": 151, "text": "relative improvement"}, {"st": 176, "ed": 178, "text": "large scale"}, {"st": 178, "ed": 180, "text": "linear classification"}]
[{"st": 1, "ed": 3, "text": "machine learning"}, {"st": 36, "ed": 38, "text": "kernel methods"}, {"st": 54, "ed": 56, "text": "predictive performance"}, {"st": 58, "ed": 60, "text": "theoretical analysis"}, {"st": 68, "ed": 70, "text": "machine learning"}, {"st": 79, "ed": 81, "text": "kernel based"}, {"st": 105, "ed": 107, "text": "closed form"}, {"st": 119, "ed": 122, "text": "kernel ridge regression"}, {"st": 124, "ed": 127, "text": "kernel ridge regression"}, {"st": 136, "ed": 138, "text": "special case"}, {"st": 140, "ed": 143, "text": "kernel ridge regression"}, {"st": 164, "ed": 166, "text": "theoretical results"}]
[{"st": 78, "ed": 81, "text": "variational auto encoders"}, {"st": 97, "ed": 99, "text": "conditional generative"}, {"st": 103, "ed": 105, "text": "unified framework"}, {"st": 114, "ed": 117, "text": "takes into account"}, {"st": 139, "ed": 141, "text": "large corpora"}, {"st": 157, "ed": 159, "text": "nearest neighbors"}, {"st": 169, "ed": 173, "text": "simulated and real world"}, {"st": 177, "ed": 179, "text": "recommender systems"}]
[{"st": 85, "ed": 87, "text": "exponentially large"}, {"st": 106, "ed": 108, "text": "demonstrate empirically"}, {"st": 112, "ed": 114, "text": "theoretical results"}]
[{"st": 4, "ed": 6, "text": "submodular functions"}, {"st": 13, "ed": 15, "text": "machine learning"}, {"st": 25, "ed": 27, "text": "feature selection"}, {"st": 34, "ed": 36, "text": "submodular functions"}, {"st": 41, "ed": 44, "text": "strong theoretical guarantees"}, {"st": 50, "ed": 52, "text": "submodular function"}, {"st": 155, "ed": 157, "text": "jointly learning"}, {"st": 166, "ed": 168, "text": "real world"}]
[{"st": 5, "ed": 7, "text": "batch normalization"}, {"st": 12, "ed": 14, "text": "deep networks"}, {"st": 15, "ed": 17, "text": "faster training"}, {"st": 22, "ed": 24, "text": "wide variety"}, {"st": 61, "ed": 63, "text": "weight decay"}, {"st": 91, "ed": 93, "text": "weight decay"}, {"st": 94, "ed": 96, "text": "learning rate"}, {"st": 106, "ed": 108, "text": "l 2"}, {"st": 121, "ed": 123, "text": "substantially improve"}, {"st": 123, "ed": 125, "text": "numerical stability"}, {"st": 126, "ed": 128, "text": "low precision"}]
[{"st": 12, "ed": 15, "text": "source and target"}, {"st": 15, "ed": 17, "text": "labeled data"}, {"st": 95, "ed": 97, "text": "recently proposed"}, {"st": 97, "ed": 99, "text": "semi supervised"}]
[{"st": 3, "ed": 5, "text": "back propagation"}, {"st": 10, "ed": 12, "text": "deep networks"}, {"st": 22, "ed": 24, "text": "exploding gradients"}, {"st": 24, "ed": 26, "text": "vanishing gradients"}, {"st": 30, "ed": 32, "text": "weight initialization"}, {"st": 49, "ed": 51, "text": "training procedure"}, {"st": 65, "ed": 67, "text": "network architecture"}, {"st": 74, "ed": 76, "text": "highly nonlinear"}, {"st": 96, "ed": 98, "text": "network weights"}, {"st": 105, "ed": 107, "text": "wide variety"}, {"st": 147, "ed": 149, "text": "back propagation"}, {"st": 154, "ed": 156, "text": "learning algorithms"}]
[{"st": 2, "ed": 5, "text": "learning from demonstration"}, {"st": 12, "ed": 14, "text": "real world"}, {"st": 31, "ed": 33, "text": "training data"}, {"st": 48, "ed": 50, "text": "existing approaches"}, {"st": 65, "ed": 67, "text": "domain knowledge"}, {"st": 75, "ed": 77, "text": "weakly supervised"}, {"st": 126, "ed": 128, "text": "multiple domains"}, {"st": 150, "ed": 152, "text": "fully supervised"}]
[{"st": 13, "ed": 15, "text": "linear classification"}, {"st": 17, "ed": 19, "text": "logistic regression"}, {"st": 48, "ed": 50, "text": "loss functions"}, {"st": 57, "ed": 59, "text": "linear classifier"}, {"st": 68, "ed": 70, "text": "l 2"}, {"st": 70, "ed": 72, "text": "max margin"}, {"st": 75, "ed": 77, "text": "convergence rate"}, {"st": 79, "ed": 81, "text": "maximum margin"}, {"st": 84, "ed": 86, "text": "step size"}, {"st": 107, "ed": 109, "text": "loss functions"}, {"st": 119, "ed": 121, "text": "convergence rate"}, {"st": 122, "ed": 124, "text": "loss functions"}, {"st": 141, "ed": 143, "text": "convergence rate"}, {"st": 173, "ed": 175, "text": "loss functions"}, {"st": 189, "ed": 191, "text": "convergence rate"}, {"st": 197, "ed": 199, "text": "sqrt t"}, {"st": 201, "ed": 203, "text": "exponential loss"}, {"st": 206, "ed": 208, "text": "step sizes"}]
[{"st": 4, "ed": 6, "text": "learning rate"}, {"st": 15, "ed": 17, "text": "neural net"}, {"st": 43, "ed": 45, "text": "expected loss"}, {"st": 47, "ed": 49, "text": "training procedure"}, {"st": 54, "ed": 56, "text": "training procedure"}, {"st": 70, "ed": 73, "text": "orders of magnitude"}, {"st": 80, "ed": 82, "text": "neural net"}, {"st": 97, "ed": 99, "text": "step sizes"}, {"st": 114, "ed": 116, "text": "cost function"}, {"st": 147, "ed": 150, "text": "standard benchmark datasets"}, {"st": 158, "ed": 160, "text": "learning rate"}, {"st": 162, "ed": 165, "text": "orders of magnitude"}, {"st": 204, "ed": 206, "text": "neural net"}]
[{"st": 16, "ed": 18, "text": "linear combinations"}, {"st": 19, "ed": 21, "text": "decision trees"}, {"st": 24, "ed": 26, "text": "infinite dimensional"}, {"st": 58, "ed": 61, "text": "synthetic and real"}, {"st": 67, "ed": 69, "text": "excellent performance"}]
[{"st": 3, "ed": 6, "text": "convolutional neural networks"}, {"st": 97, "ed": 99, "text": "square tiling"}, {"st": 100, "ed": 102, "text": "a 4"}, {"st": 103, "ed": 105, "text": "rotational symmetry"}, {"st": 200, "ed": 202, "text": "significantly outperforms"}, {"st": 208, "ed": 210, "text": "scene classification"}, {"st": 214, "ed": 216, "text": "pre trained"}]
[{"st": 0, "ed": 2, "text": "deep learning"}, {"st": 10, "ed": 12, "text": "predictive modeling"}, {"st": 13, "ed": 15, "text": "pattern recognition"}, {"st": 19, "ed": 21, "text": "deep learning"}, {"st": 24, "ed": 26, "text": "machine learning"}, {"st": 29, "ed": 31, "text": "lower level"}, {"st": 42, "ed": 44, "text": "higher level"}, {"st": 49, "ed": 52, "text": "deep neural networks"}, {"st": 57, "ed": 59, "text": "hyper parameters"}, {"st": 74, "ed": 76, "text": "machine learning"}, {"st": 96, "ed": 98, "text": "improve performance"}, {"st": 98, "ed": 100, "text": "ensemble methods"}, {"st": 132, "ed": 134, "text": "log loss"}, {"st": 139, "ed": 142, "text": "deep neural networks"}, {"st": 145, "ed": 147, "text": "machine learning"}, {"st": 171, "ed": 173, "text": "hyper parameter"}, {"st": 176, "ed": 178, "text": "machine learning"}, {"st": 180, "ed": 182, "text": "hyper parameters"}, {"st": 189, "ed": 191, "text": "fast convergence"}, {"st": 209, "ed": 211, "text": "base learners"}, {"st": 232, "ed": 234, "text": "task specific"}]
[{"st": 28, "ed": 30, "text": "recent advances"}, {"st": 31, "ed": 33, "text": "machine learning"}, {"st": 48, "ed": 50, "text": "machine learning"}, {"st": 66, "ed": 68, "text": "deep learning"}, {"st": 71, "ed": 73, "text": "von neumann"}, {"st": 104, "ed": 106, "text": "n gram"}, {"st": 108, "ed": 110, "text": "natural language"}, {"st": 114, "ed": 117, "text": "recurrent neural networks"}, {"st": 129, "ed": 131, "text": "benchmark datasets"}, {"st": 134, "ed": 136, "text": "neural networks"}, {"st": 154, "ed": 156, "text": "neural network"}, {"st": 167, "ed": 169, "text": "machine learning"}]
[{"st": 2, "ed": 4, "text": "real world"}, {"st": 44, "ed": 46, "text": "maximum margin"}, {"st": 48, "ed": 51, "text": "taking advantage of"}, {"st": 52, "ed": 54, "text": "maximum entropy"}, {"st": 57, "ed": 59, "text": "maximum margin"}, {"st": 103, "ed": 106, "text": "simulated and real"}]
[{"st": 6, "ed": 8, "text": "automatically learn"}, {"st": 36, "ed": 38, "text": "proposed method"}, {"st": 55, "ed": 57, "text": "probabilistic model"}, {"st": 86, "ed": 88, "text": "motion capture"}]
[{"st": 2, "ed": 4, "text": "neural networks"}, {"st": 11, "ed": 13, "text": "network topology"}, {"st": 13, "ed": 15, "text": "optimization procedure"}, {"st": 21, "ed": 23, "text": "pre trained"}, {"st": 27, "ed": 29, "text": "time consuming"}, {"st": 34, "ed": 36, "text": "machine learning"}, {"st": 58, "ed": 60, "text": "prior knowledge"}, {"st": 69, "ed": 71, "text": "machine learning"}, {"st": 75, "ed": 77, "text": "network design"}, {"st": 87, "ed": 89, "text": "reinforcement learning"}, {"st": 97, "ed": 99, "text": "multiple tasks"}, {"st": 111, "ed": 113, "text": "image classification"}, {"st": 116, "ed": 118, "text": "machine learning"}, {"st": 152, "ed": 154, "text": "improves performance"}, {"st": 155, "ed": 157, "text": "cifar 10"}, {"st": 157, "ed": 159, "text": "image recognition"}]
[{"st": 1, "ed": 3, "text": "real world"}, {"st": 25, "ed": 27, "text": "labeled data"}, {"st": 47, "ed": 49, "text": "true label"}, {"st": 71, "ed": 74, "text": "expectation maximization em"}, {"st": 79, "ed": 82, "text": "simple yet effective"}, {"st": 98, "ed": 100, "text": "faster convergence"}, {"st": 102, "ed": 104, "text": "similar accuracy"}, {"st": 111, "ed": 113, "text": "proposed method"}, {"st": 121, "ed": 123, "text": "multiple labels"}, {"st": 139, "ed": 141, "text": "significant speedup"}, {"st": 161, "ed": 163, "text": "competitive accuracy"}]
[{"st": 1, "ed": 3, "text": "multi instance"}, {"st": 11, "ed": 13, "text": "feature vectors"}, {"st": 38, "ed": 40, "text": "random vectors"}, {"st": 61, "ed": 63, "text": "training set"}, {"st": 65, "ed": 67, "text": "class label"}, {"st": 81, "ed": 83, "text": "classification problems"}, {"st": 85, "ed": 87, "text": "probability distributions"}, {"st": 95, "ed": 97, "text": "kullback leibler"}, {"st": 109, "ed": 111, "text": "training sets"}, {"st": 129, "ed": 131, "text": "random vectors"}]
[{"st": 2, "ed": 4, "text": "deep learning"}, {"st": 72, "ed": 74, "text": "parameter sharing"}, {"st": 107, "ed": 109, "text": "matrix completion"}, {"st": 158, "ed": 160, "text": "generalization performance"}]
[{"st": 1, "ed": 3, "text": "supervised learning"}, {"st": 15, "ed": 17, "text": "labelled data"}, {"st": 48, "ed": 50, "text": "loss functions"}, {"st": 73, "ed": 76, "text": "stochastic gradient descent"}, {"st": 143, "ed": 145, "text": "loss function"}, {"st": 249, "ed": 251, "text": "penalty functions"}, {"st": 289, "ed": 292, "text": "modern machine learning"}]
[{"st": 4, "ed": 6, "text": "social networks"}, {"st": 71, "ed": 73, "text": "dynamic programming"}, {"st": 78, "ed": 80, "text": "existing approaches"}, {"st": 87, "ed": 89, "text": "submodular functions"}, {"st": 103, "ed": 105, "text": "existing approaches"}, {"st": 113, "ed": 116, "text": "the past decade"}, {"st": 152, "ed": 158, "text": "experiments on synthetic and real world"}, {"st": 164, "ed": 166, "text": "machine learning"}, {"st": 195, "ed": 198, "text": "naive bayes classifier"}, {"st": 199, "ed": 201, "text": "decision trees"}, {"st": 204, "ed": 206, "text": "random forest"}, {"st": 210, "ed": 212, "text": "real world"}, {"st": 215, "ed": 217, "text": "previous approaches"}]
[{"st": 1, "ed": 3, "text": "loss functions"}, {"st": 4, "ed": 6, "text": "representation learning"}, {"st": 17, "ed": 19, "text": "word embeddings"}, {"st": 41, "ed": 43, "text": "representation learning"}, {"st": 52, "ed": 54, "text": "time series"}, {"st": 62, "ed": 64, "text": "low cost"}, {"st": 79, "ed": 81, "text": "gauge theory"}, {"st": 86, "ed": 88, "text": "optimization algorithm"}, {"st": 105, "ed": 109, "text": "orders of magnitude faster"}, {"st": 122, "ed": 124, "text": "matrix factorization"}, {"st": 125, "ed": 127, "text": "word embedding"}, {"st": 141, "ed": 143, "text": "shared representation"}]
[{"st": 2, "ed": 4, "text": "classification problems"}, {"st": 4, "ed": 6, "text": "learning algorithms"}, {"st": 15, "ed": 17, "text": "extremely large"}, {"st": 38, "ed": 40, "text": "error correcting"}]
[{"st": 14, "ed": 16, "text": "real world"}, {"st": 37, "ed": 39, "text": "generative models"}, {"st": 53, "ed": 55, "text": "neural networks"}, {"st": 129, "ed": 131, "text": "key challenges"}, {"st": 133, "ed": 135, "text": "generative models"}, {"st": 150, "ed": 152, "text": "generation process"}, {"st": 167, "ed": 169, "text": "generative models"}]
[{"st": 0, "ed": 2, "text": "low precision"}, {"st": 14, "ed": 16, "text": "machine learning"}, {"st": 37, "ed": 39, "text": "low precision"}, {"st": 74, "ed": 76, "text": "low precision"}, {"st": 76, "ed": 79, "text": "stochastic gradient descent"}, {"st": 99, "ed": 101, "text": "low precision"}, {"st": 104, "ed": 106, "text": "key idea"}, {"st": 121, "ed": 123, "text": "technique called"}, {"st": 141, "ed": 144, "text": "times faster than"}, {"st": 168, "ed": 170, "text": "low precision"}, {"st": 173, "ed": 175, "text": "deep learning"}]
[{"st": 5, "ed": 7, "text": "deep learning"}, {"st": 37, "ed": 39, "text": "generating adversarial"}, {"st": 45, "ed": 47, "text": "classification tasks"}, {"st": 56, "ed": 58, "text": "proposed method"}, {"st": 65, "ed": 67, "text": "adversarial noise"}, {"st": 85, "ed": 87, "text": "neural networks"}, {"st": 98, "ed": 100, "text": "competitive performance"}]
[{"st": 24, "ed": 26, "text": "based method"}, {"st": 62, "ed": 64, "text": "based method"}, {"st": 123, "ed": 125, "text": "based method"}, {"st": 138, "ed": 142, "text": "national renewable energy laboratory"}, {"st": 149, "ed": 151, "text": "based method"}, {"st": 169, "ed": 171, "text": "based method"}, {"st": 192, "ed": 195, "text": "mean square error"}]
[{"st": 0, "ed": 2, "text": "machine learning"}, {"st": 39, "ed": 41, "text": "predictive model"}, {"st": 58, "ed": 60, "text": "predictive model"}, {"st": 110, "ed": 112, "text": "prediction models"}, {"st": 158, "ed": 160, "text": "predictive models"}, {"st": 188, "ed": 190, "text": "reward function"}, {"st": 206, "ed": 208, "text": "significantly improve"}, {"st": 238, "ed": 240, "text": "machine learning"}]
[]
[{"st": 74, "ed": 76, "text": "logistic regression"}, {"st": 76, "ed": 78, "text": "decision tree"}, {"st": 78, "ed": 80, "text": "random forest"}, {"st": 82, "ed": 85, "text": "support vector machine"}, {"st": 111, "ed": 113, "text": "type ii"}, {"st": 142, "ed": 145, "text": "support vector machine"}]
[{"st": 30, "ed": 32, "text": "neural network"}, {"st": 35, "ed": 37, "text": "speech recognition"}, {"st": 80, "ed": 82, "text": "speech recognition"}, {"st": 84, "ed": 86, "text": "low latency"}, {"st": 112, "ed": 114, "text": "image classification"}, {"st": 119, "ed": 122, "text": "convolutional neural networks"}, {"st": 135, "ed": 137, "text": "adversarial training"}]
[{"st": 4, "ed": 6, "text": "machine learning"}, {"st": 38, "ed": 40, "text": "supervised learning"}, {"st": 42, "ed": 44, "text": "surrogate loss"}, {"st": 72, "ed": 74, "text": "loss function"}, {"st": 78, "ed": 80, "text": "hinge loss"}, {"st": 83, "ed": 86, "text": "support vector machine"}, {"st": 89, "ed": 91, "text": "optimization problem"}, {"st": 98, "ed": 100, "text": "convex optimization"}, {"st": 123, "ed": 125, "text": "statistical learning"}]
[{"st": 0, "ed": 2, "text": "class imbalance"}, {"st": 4, "ed": 6, "text": "classification problems"}, {"st": 20, "ed": 22, "text": "imbalanced datasets"}, {"st": 31, "ed": 33, "text": "majority class"}, {"st": 54, "ed": 56, "text": "ensemble methods"}, {"st": 76, "ed": 78, "text": "empirical analysis"}, {"st": 89, "ed": 91, "text": "multi class"}, {"st": 106, "ed": 108, "text": "experiments conducted"}, {"st": 110, "ed": 112, "text": "multi class"}, {"st": 121, "ed": 123, "text": "f measure"}, {"st": 138, "ed": 140, "text": "important role"}]
[{"st": 7, "ed": 10, "text": "end to end"}, {"st": 19, "ed": 21, "text": "time series"}, {"st": 40, "ed": 42, "text": "time series"}, {"st": 52, "ed": 54, "text": "time series"}, {"st": 58, "ed": 60, "text": "time series"}, {"st": 97, "ed": 101, "text": "gated recurrent unit gru"}, {"st": 101, "ed": 106, "text": "long short term memory lstm"}, {"st": 107, "ed": 111, "text": "convolutional neural network cnn"}, {"st": 112, "ed": 116, "text": "multi layer perceptron mlp"}]
[{"st": 6, "ed": 8, "text": "multi objective"}, {"st": 8, "ed": 10, "text": "contextual bandit"}, {"st": 19, "ed": 21, "text": "contextual bandit"}, {"st": 78, "ed": 80, "text": "performance metric"}, {"st": 105, "ed": 107, "text": "context dependent"}, {"st": 116, "ed": 119, "text": "online learning algorithm"}, {"st": 204, "ed": 206, "text": "near optimal"}, {"st": 214, "ed": 216, "text": "regret bound"}]
[{"st": 6, "ed": 8, "text": "deep networks"}, {"st": 31, "ed": 33, "text": "deep network"}, {"st": 92, "ed": 94, "text": "low dimensional"}, {"st": 120, "ed": 122, "text": "t sne"}, {"st": 123, "ed": 126, "text": "qualitatively and quantitatively"}]
[{"st": 11, "ed": 13, "text": "low dimensional"}, {"st": 23, "ed": 25, "text": "representation learning"}, {"st": 35, "ed": 37, "text": "low dimensional"}, {"st": 65, "ed": 67, "text": "complex nonlinear"}, {"st": 90, "ed": 92, "text": "point process"}, {"st": 101, "ed": 103, "text": "unsupervised learning"}, {"st": 108, "ed": 111, "text": "approach significantly outperforms"}, {"st": 115, "ed": 117, "text": "real world"}, {"st": 123, "ed": 125, "text": "link prediction"}]
[{"st": 1, "ed": 3, "text": "multitask learning"}, {"st": 18, "ed": 20, "text": "multitask learning"}, {"st": 57, "ed": 59, "text": "closely related"}, {"st": 76, "ed": 78, "text": "improve performance"}, {"st": 79, "ed": 82, "text": "single task learning"}, {"st": 86, "ed": 88, "text": "multitask learning"}, {"st": 108, "ed": 110, "text": "multitask learning"}, {"st": 130, "ed": 132, "text": "deep learning"}]
[{"st": 5, "ed": 7, "text": "structure learning"}, {"st": 70, "ed": 72, "text": "l1 regularized"}, {"st": 72, "ed": 74, "text": "linear regression"}, {"st": 80, "ed": 82, "text": "sample complexity"}, {"st": 84, "ed": 86, "text": "proposed approach"}]
[{"st": 3, "ed": 5, "text": "neural network"}, {"st": 14, "ed": 17, "text": "rectified linear units"}, {"st": 35, "ed": 37, "text": "neural network"}]
[{"st": 3, "ed": 5, "text": "multi kernel"}, {"st": 10, "ed": 12, "text": "signal processing"}, {"st": 25, "ed": 27, "text": "multi kernel"}, {"st": 30, "ed": 32, "text": "kernel function"}, {"st": 36, "ed": 38, "text": "linear combination"}, {"st": 52, "ed": 54, "text": "kernel function"}, {"st": 66, "ed": 68, "text": "optimization problem"}, {"st": 86, "ed": 88, "text": "real world"}, {"st": 94, "ed": 96, "text": "multi kernel"}, {"st": 96, "ed": 98, "text": "based approach"}, {"st": 101, "ed": 103, "text": "kernel based"}]
[{"st": 4, "ed": 6, "text": "contextual bandits"}, {"st": 12, "ed": 14, "text": "bandit problem"}, {"st": 24, "ed": 26, "text": "linear function"}, {"st": 47, "ed": 49, "text": "sqrt t"}, {"st": 55, "ed": 57, "text": "linear function"}, {"st": 58, "ed": 60, "text": "d dimensional"}, {"st": 85, "ed": 87, "text": "empirical evaluation"}]
[{"st": 3, "ed": 5, "text": "hand designed"}, {"st": 9, "ed": 11, "text": "machine learning"}, {"st": 29, "ed": 31, "text": "typically requires"}, {"st": 37, "ed": 39, "text": "computationally expensive"}, {"st": 50, "ed": 52, "text": "convex optimization"}, {"st": 62, "ed": 64, "text": "frank wolfe"}, {"st": 65, "ed": 67, "text": "recurrent networks"}, {"st": 69, "ed": 71, "text": "frank wolfe"}, {"st": 95, "ed": 97, "text": "hand designed"}, {"st": 108, "ed": 111, "text": "support vector machines"}]
[{"st": 2, "ed": 4, "text": "machine learning"}, {"st": 23, "ed": 25, "text": "conventional wisdom"}]
[{"st": 1, "ed": 3, "text": "neural net"}, {"st": 14, "ed": 16, "text": "neural nets"}, {"st": 18, "ed": 20, "text": "reinforcement learning"}, {"st": 87, "ed": 89, "text": "fully connected"}, {"st": 90, "ed": 92, "text": "convolutional networks"}, {"st": 96, "ed": 98, "text": "significant speedups"}, {"st": 100, "ed": 102, "text": "neural networks"}, {"st": 116, "ed": 118, "text": "outperforms previous"}, {"st": 147, "ed": 149, "text": "existing methods"}]
[{"st": 18, "ed": 20, "text": "cost function"}, {"st": 54, "ed": 56, "text": "cost function"}, {"st": 68, "ed": 70, "text": "input dependent"}, {"st": 109, "ed": 111, "text": "synthetic data"}, {"st": 125, "ed": 127, "text": "multi dimensional"}, {"st": 139, "ed": 141, "text": "neural network"}]
[{"st": 4, "ed": 6, "text": "recently proposed"}, {"st": 7, "ed": 9, "text": "convolutional network"}, {"st": 64, "ed": 66, "text": "transition matrix"}]
[{"st": 101, "ed": 103, "text": "non negativity"}, {"st": 149, "ed": 151, "text": "non negativity"}, {"st": 165, "ed": 167, "text": "optimization framework"}, {"st": 168, "ed": 170, "text": "alternating optimization"}, {"st": 171, "ed": 175, "text": "alternating direction method of"}, {"st": 183, "ed": 186, "text": "electronic health record"}, {"st": 189, "ed": 192, "text": "hundreds of thousands"}, {"st": 196, "ed": 198, "text": "significant speedups"}, {"st": 221, "ed": 223, "text": "method outperforms"}]
[{"st": 1, "ed": 3, "text": "matrix completion"}, {"st": 23, "ed": 25, "text": "machine learning"}, {"st": 48, "ed": 50, "text": "practical applications"}, {"st": 52, "ed": 54, "text": "recommendation systems"}, {"st": 78, "ed": 80, "text": "matrix completion"}, {"st": 99, "ed": 101, "text": "matrix completion"}, {"st": 141, "ed": 143, "text": "classification problem"}, {"st": 175, "ed": 177, "text": "semi supervised"}, {"st": 179, "ed": 181, "text": "recently proposed"}]
[{"st": 5, "ed": 7, "text": "near optimal"}, {"st": 7, "ed": 9, "text": "arm identification"}, {"st": 17, "ed": 20, "text": "armed bandit problem"}, {"st": 46, "ed": 48, "text": "sample complexity"}, {"st": 51, "ed": 53, "text": "near optimal"}, {"st": 53, "ed": 55, "text": "arm identification"}, {"st": 75, "ed": 77, "text": "sample complexity"}]
[{"st": 0, "ed": 3, "text": "deep neural networks"}, {"st": 8, "ed": 10, "text": "machine learning"}, {"st": 11, "ed": 13, "text": "image recognition"}, {"st": 13, "ed": 15, "text": "machine translation"}, {"st": 19, "ed": 21, "text": "deep learning"}, {"st": 52, "ed": 54, "text": "deep learning"}, {"st": 65, "ed": 67, "text": "desirable properties"}, {"st": 84, "ed": 87, "text": "k nearest neighbors"}, {"st": 94, "ed": 97, "text": "k nearest neighbors"}, {"st": 118, "ed": 120, "text": "training points"}, {"st": 154, "ed": 156, "text": "adversarial examples"}, {"st": 172, "ed": 174, "text": "nearest neighbors"}, {"st": 228, "ed": 230, "text": "nearest neighbors"}]
[{"st": 1, "ed": 3, "text": "reinforcement learning"}, {"st": 35, "ed": 37, "text": "thompson sampling"}, {"st": 61, "ed": 63, "text": "monte carlo"}, {"st": 74, "ed": 76, "text": "near optimal"}, {"st": 78, "ed": 80, "text": "bandit problems"}]
[{"st": 0, "ed": 3, "text": "generative adversarial networks"}, {"st": 11, "ed": 13, "text": "convergence properties"}, {"st": 90, "ed": 92, "text": "theoretical analysis"}]
[{"st": 7, "ed": 9, "text": "large scale"}, {"st": 17, "ed": 19, "text": "building block"}, {"st": 21, "ed": 23, "text": "application domains"}, {"st": 50, "ed": 52, "text": "graph based"}, {"st": 90, "ed": 92, "text": "simultaneously learns"}, {"st": 94, "ed": 96, "text": "affinity matrix"}, {"st": 144, "ed": 147, "text": "synthetic and real"}, {"st": 151, "ed": 153, "text": "proposed algorithm"}]
[{"st": 5, "ed": 7, "text": "domain adaptation"}, {"st": 96, "ed": 99, "text": "source and target"}, {"st": 119, "ed": 122, "text": "source and target"}, {"st": 140, "ed": 142, "text": "proposed method"}, {"st": 147, "ed": 149, "text": "domain adaptation"}, {"st": 164, "ed": 166, "text": "domain adaptation"}]
[{"st": 7, "ed": 9, "text": "dosage form"}, {"st": 40, "ed": 43, "text": "trial and error"}, {"st": 61, "ed": 63, "text": "prediction model"}, {"st": 71, "ed": 74, "text": "artificial neural network"}, {"st": 76, "ed": 80, "text": "deep neural network dnn"}, {"st": 98, "ed": 100, "text": "training set"}, {"st": 102, "ed": 104, "text": "validation set"}, {"st": 136, "ed": 138, "text": "training set"}, {"st": 138, "ed": 140, "text": "validation set"}, {"st": 174, "ed": 177, "text": "deep neural network"}, {"st": 181, "ed": 183, "text": "selection algorithm"}, {"st": 201, "ed": 203, "text": "quality control"}, {"st": 215, "ed": 217, "text": "prediction model"}]
[{"st": 0, "ed": 2, "text": "large scale"}, {"st": 2, "ed": 5, "text": "deep neural networks"}, {"st": 23, "ed": 26, "text": "deep neural networks"}, {"st": 39, "ed": 41, "text": "neural networks"}, {"st": 46, "ed": 48, "text": "neural networks"}, {"st": 207, "ed": 209, "text": "detailed analysis"}]
[{"st": 0, "ed": 2, "text": "deep learning"}, {"st": 4, "ed": 8, "text": "convolutional neural networks cnns"}, {"st": 48, "ed": 50, "text": "time series"}, {"st": 59, "ed": 61, "text": "time series"}, {"st": 76, "ed": 78, "text": "non trivial"}, {"st": 119, "ed": 121, "text": "time series"}, {"st": 126, "ed": 128, "text": "convolutional layers"}, {"st": 131, "ed": 133, "text": "adjacency matrix"}, {"st": 154, "ed": 156, "text": "method produces"}, {"st": 168, "ed": 170, "text": "synthetic data"}, {"st": 177, "ed": 179, "text": "motion capture"}]
[{"st": 2, "ed": 4, "text": "optimal transport"}, {"st": 10, "ed": 13, "text": "generative adversarial nets"}, {"st": 38, "ed": 40, "text": "optimal transport"}, {"st": 52, "ed": 54, "text": "feature space"}, {"st": 59, "ed": 61, "text": "distance function"}, {"st": 92, "ed": 94, "text": "benchmark problems"}]
[{"st": 5, "ed": 7, "text": "deep learning"}, {"st": 12, "ed": 14, "text": "large margin"}, {"st": 23, "ed": 25, "text": "decision boundary"}, {"st": 40, "ed": 43, "text": "classification and regression"}, {"st": 46, "ed": 48, "text": "large margin"}, {"st": 58, "ed": 60, "text": "feature representation"}, {"st": 65, "ed": 67, "text": "neural networks"}, {"st": 91, "ed": 93, "text": "loss function"}, {"st": 105, "ed": 107, "text": "deep network"}, {"st": 110, "ed": 112, "text": "hidden layers"}, {"st": 129, "ed": 131, "text": "decision boundary"}, {"st": 140, "ed": 142, "text": "standard classification"}, {"st": 148, "ed": 150, "text": "empirical results"}, {"st": 152, "ed": 155, "text": "mnist cifar 10"}, {"st": 159, "ed": 161, "text": "multiple tasks"}, {"st": 164, "ed": 166, "text": "training sets"}, {"st": 191, "ed": 193, "text": "regularization techniques"}, {"st": 195, "ed": 197, "text": "weight decay"}]
[{"st": 10, "ed": 13, "text": "attracted much attention"}, {"st": 14, "ed": 16, "text": "machine learning"}, {"st": 18, "ed": 20, "text": "large scale"}, {"st": 45, "ed": 47, "text": "method called"}, {"st": 129, "ed": 132, "text": "linear convergence rate"}, {"st": 147, "ed": 149, "text": "faster convergence"}, {"st": 159, "ed": 161, "text": "real data"}]
[{"st": 16, "ed": 18, "text": "time series"}, {"st": 21, "ed": 23, "text": "statistical models"}, {"st": 32, "ed": 34, "text": "limited data"}, {"st": 44, "ed": 46, "text": "time series"}, {"st": 54, "ed": 56, "text": "well calibrated"}, {"st": 69, "ed": 71, "text": "time series"}, {"st": 128, "ed": 130, "text": "functional form"}, {"st": 137, "ed": 139, "text": "nonparametric bayesian"}, {"st": 148, "ed": 150, "text": "time series"}, {"st": 157, "ed": 159, "text": "gaussian process"}, {"st": 171, "ed": 173, "text": "functional form"}, {"st": 183, "ed": 185, "text": "well calibrated"}]
[{"st": 38, "ed": 41, "text": "taking into account"}, {"st": 152, "ed": 154, "text": "input sequence"}, {"st": 157, "ed": 159, "text": "deep neural"}, {"st": 161, "ed": 164, "text": "recurrent neural networks"}, {"st": 166, "ed": 168, "text": "attention mechanism"}, {"st": 171, "ed": 173, "text": "conditional probability"}, {"st": 214, "ed": 216, "text": "proposed method"}]
[{"st": 7, "ed": 10, "text": "gaussian process gp"}, {"st": 16, "ed": 18, "text": "well calibrated"}, {"st": 20, "ed": 22, "text": "recent advances"}, {"st": 30, "ed": 32, "text": "marginal likelihood"}, {"st": 83, "ed": 85, "text": "space complexity"}, {"st": 100, "ed": 102, "text": "times faster"}, {"st": 110, "ed": 112, "text": "existing methods"}, {"st": 113, "ed": 115, "text": "without sacrificing"}]
[{"st": 7, "ed": 9, "text": "training set"}, {"st": 18, "ed": 21, "text": "modern machine learning"}, {"st": 30, "ed": 32, "text": "theoretical framework"}, {"st": 109, "ed": 112, "text": "k nearest neighbors"}, {"st": 129, "ed": 131, "text": "theoretical framework"}, {"st": 136, "ed": 138, "text": "machine learning"}]
[{"st": 4, "ed": 6, "text": "neural networks"}, {"st": 11, "ed": 13, "text": "neural networks"}, {"st": 17, "ed": 19, "text": "extremely large"}, {"st": 58, "ed": 60, "text": "large scale"}, {"st": 70, "ed": 72, "text": "semi supervised"}, {"st": 72, "ed": 74, "text": "node classification"}, {"st": 93, "ed": 95, "text": "wide variety"}, {"st": 100, "ed": 102, "text": "semi supervised"}, {"st": 110, "ed": 112, "text": "similar performance"}]
[{"st": 10, "ed": 12, "text": "adversarial examples"}, {"st": 24, "ed": 26, "text": "adversarial training"}, {"st": 88, "ed": 90, "text": "adversarial examples"}, {"st": 92, "ed": 94, "text": "adversarial training"}, {"st": 107, "ed": 109, "text": "adversarial training"}, {"st": 134, "ed": 136, "text": "white box"}, {"st": 160, "ed": 162, "text": "black box"}, {"st": 198, "ed": 200, "text": "black box"}]
[{"st": 1, "ed": 3, "text": "time series"}, {"st": 25, "ed": 27, "text": "time series"}, {"st": 34, "ed": 36, "text": "moving average"}, {"st": 46, "ed": 48, "text": "moving average"}, {"st": 78, "ed": 80, "text": "computational power"}, {"st": 88, "ed": 90, "text": "machine learning"}, {"st": 95, "ed": 97, "text": "deep learning"}, {"st": 103, "ed": 105, "text": "time series"}, {"st": 121, "ed": 123, "text": "deep learning"}, {"st": 127, "ed": 129, "text": "time series"}, {"st": 133, "ed": 136, "text": "short term memory"}, {"st": 144, "ed": 146, "text": "empirical studies"}, {"st": 154, "ed": 156, "text": "deep learning"}, {"st": 161, "ed": 163, "text": "outperform traditional"}, {"st": 175, "ed": 177, "text": "error rates"}, {"st": 210, "ed": 212, "text": "deep learning"}]
[{"st": 7, "ed": 9, "text": "back propagation"}, {"st": 42, "ed": 44, "text": "conjugate gradient"}, {"st": 65, "ed": 67, "text": "back propagation"}, {"st": 96, "ed": 98, "text": "scales linearly"}, {"st": 117, "ed": 119, "text": "application domains"}, {"st": 119, "ed": 121, "text": "associative memory"}, {"st": 125, "ed": 127, "text": "document classification"}, {"st": 132, "ed": 134, "text": "neural networks"}, {"st": 135, "ed": 137, "text": "hyperparameter optimization"}, {"st": 138, "ed": 140, "text": "fully connected"}, {"st": 142, "ed": 144, "text": "experiments demonstrate"}, {"st": 152, "ed": 155, "text": "efficient and effective"}]
[{"st": 8, "ed": 10, "text": "desirable properties"}, {"st": 11, "ed": 13, "text": "neural networks"}, {"st": 15, "ed": 17, "text": "provable guarantees"}, {"st": 22, "ed": 24, "text": "neural network"}, {"st": 54, "ed": 56, "text": "network architecture"}, {"st": 75, "ed": 77, "text": "activation functions"}, {"st": 80, "ed": 82, "text": "neural network"}, {"st": 90, "ed": 92, "text": "optimization problem"}, {"st": 99, "ed": 101, "text": "optimization problem"}]
[{"st": 7, "ed": 10, "text": "recurrent neural networks"}, {"st": 28, "ed": 30, "text": "exploding gradient"}, {"st": 47, "ed": 49, "text": "recurrent architecture"}, {"st": 51, "ed": 53, "text": "recurrent unit"}, {"st": 73, "ed": 75, "text": "hidden states"}, {"st": 97, "ed": 99, "text": "residual learning"}, {"st": 146, "ed": 148, "text": "fewer parameters"}]
[{"st": 11, "ed": 13, "text": "interactive learning"}, {"st": 24, "ed": 26, "text": "active learning"}, {"st": 36, "ed": 39, "text": "rate of convergence"}, {"st": 40, "ed": 43, "text": "theoretically and empirically"}]
[{"st": 21, "ed": 23, "text": "intensive care"}, {"st": 48, "ed": 50, "text": "time consuming"}, {"st": 67, "ed": 69, "text": "features extracted"}, {"st": 116, "ed": 118, "text": "extracted features"}, {"st": 123, "ed": 125, "text": "decision tree"}, {"st": 125, "ed": 127, "text": "linear discriminant"}, {"st": 127, "ed": 129, "text": "logistic regression"}, {"st": 129, "ed": 132, "text": "support vector machine"}, {"st": 133, "ed": 135, "text": "random forest"}, {"st": 135, "ed": 137, "text": "boosted trees"}, {"st": 143, "ed": 145, "text": "k nn"}, {"st": 154, "ed": 156, "text": "proposed method"}, {"st": 172, "ed": 174, "text": "intensive care"}, {"st": 186, "ed": 188, "text": "proposed method"}, {"st": 191, "ed": 193, "text": "precision recall"}, {"st": 206, "ed": 208, "text": "decision tree"}, {"st": 250, "ed": 252, "text": "comparable performance"}]
[{"st": 1, "ed": 3, "text": "positive unlabeled"}, {"st": 10, "ed": 12, "text": "real world"}, {"st": 16, "ed": 18, "text": "text classification"}, {"st": 37, "ed": 39, "text": "unlabeled samples"}, {"st": 43, "ed": 46, "text": "positive and negative"}, {"st": 59, "ed": 61, "text": "complex data"}, {"st": 63, "ed": 65, "text": "negative samples"}, {"st": 82, "ed": 84, "text": "learning framework"}, {"st": 101, "ed": 103, "text": "feature selection"}, {"st": 109, "ed": 111, "text": "generalization error"}, {"st": 145, "ed": 147, "text": "unlabeled samples"}, {"st": 153, "ed": 155, "text": "sample size"}, {"st": 171, "ed": 173, "text": "real world"}]
[{"st": 7, "ed": 9, "text": "detection problem"}, {"st": 11, "ed": 13, "text": "linear model"}, {"st": 27, "ed": 29, "text": "x 1"}, {"st": 41, "ed": 43, "text": "generating process"}, {"st": 209, "ed": 213, "text": "synthetic and real data"}]
[{"st": 4, "ed": 7, "text": "deep neural networks"}, {"st": 40, "ed": 42, "text": "trained models"}, {"st": 44, "ed": 46, "text": "image classification"}, {"st": 72, "ed": 74, "text": "dnn training"}, {"st": 98, "ed": 100, "text": "machine learning"}, {"st": 101, "ed": 103, "text": "image classification"}, {"st": 103, "ed": 105, "text": "machine translation"}, {"st": 105, "ed": 107, "text": "speech recognition"}, {"st": 107, "ed": 109, "text": "object detection"}, {"st": 109, "ed": 111, "text": "adversarial networks"}, {"st": 111, "ed": 113, "text": "reinforcement learning"}, {"st": 129, "ed": 131, "text": "deep learning"}, {"st": 152, "ed": 154, "text": "application domains"}, {"st": 181, "ed": 183, "text": "analysis tools"}, {"st": 199, "ed": 201, "text": "domain specific"}, {"st": 264, "ed": 266, "text": "future research"}, {"st": 269, "ed": 271, "text": "dnn training"}]
[{"st": 14, "ed": 16, "text": "data science"}, {"st": 38, "ed": 40, "text": "intrinsic dimension"}, {"st": 99, "ed": 101, "text": "nearest neighbor"}]
[{"st": 0, "ed": 3, "text": "gaussian processes gps"}, {"st": 6, "ed": 8, "text": "generative models"}, {"st": 14, "ed": 16, "text": "closed form"}, {"st": 18, "ed": 21, "text": "training and inference"}, {"st": 121, "ed": 123, "text": "unlike existing"}, {"st": 129, "ed": 131, "text": "proposed approach"}, {"st": 150, "ed": 152, "text": "real world"}]
[{"st": 0, "ed": 2, "text": "exponential families"}, {"st": 26, "ed": 29, "text": "kullback leibler kl"}, {"st": 60, "ed": 62, "text": "bregman divergence"}, {"st": 74, "ed": 76, "text": "exponential families"}, {"st": 79, "ed": 81, "text": "definite integral"}, {"st": 87, "ed": 89, "text": "time consuming"}, {"st": 90, "ed": 92, "text": "exponentially large"}, {"st": 100, "ed": 102, "text": "closed form"}, {"st": 133, "ed": 135, "text": "monte carlo"}, {"st": 142, "ed": 144, "text": "exponential family"}, {"st": 170, "ed": 172, "text": "monte carlo"}]
[{"st": 4, "ed": 6, "text": "deep learning"}, {"st": 10, "ed": 13, "text": "fast and accurate"}, {"st": 14, "ed": 16, "text": "decision making"}, {"st": 18, "ed": 20, "text": "structured data"}, {"st": 21, "ed": 23, "text": "ct images"}, {"st": 53, "ed": 55, "text": "prediction results"}, {"st": 68, "ed": 70, "text": "deep learning"}, {"st": 78, "ed": 80, "text": "prediction results"}, {"st": 82, "ed": 84, "text": "minority group"}, {"st": 134, "ed": 137, "text": "deep neural networks"}, {"st": 158, "ed": 160, "text": "neural networks"}, {"st": 171, "ed": 173, "text": "ct scan"}]
[{"st": 10, "ed": 12, "text": "learning rate"}, {"st": 15, "ed": 17, "text": "mini batch"}, {"st": 29, "ed": 31, "text": "large scale"}, {"st": 31, "ed": 33, "text": "machine learning"}, {"st": 44, "ed": 46, "text": "error prone"}, {"st": 63, "ed": 66, "text": "trial and error"}, {"st": 97, "ed": 99, "text": "applications including"}, {"st": 99, "ed": 101, "text": "image classification"}]
[{"st": 49, "ed": 51, "text": "error prone"}, {"st": 61, "ed": 63, "text": "analysis tools"}, {"st": 72, "ed": 75, "text": "end to end"}, {"st": 76, "ed": 78, "text": "machine learning"}, {"st": 165, "ed": 168, "text": "end to end"}, {"st": 168, "ed": 170, "text": "fully automated"}]
[{"st": 12, "ed": 14, "text": "practical applications"}, {"st": 18, "ed": 20, "text": "time consuming"}, {"st": 29, "ed": 31, "text": "meta learning"}, {"st": 40, "ed": 42, "text": "learning algorithms"}, {"st": 80, "ed": 82, "text": "automatically learn"}, {"st": 88, "ed": 90, "text": "latent variable"}, {"st": 120, "ed": 122, "text": "reinforcement learning"}, {"st": 124, "ed": 126, "text": "learning dynamics"}, {"st": 141, "ed": 143, "text": "reinforcement learning"}]
[{"st": 8, "ed": 10, "text": "multi agent"}, {"st": 20, "ed": 22, "text": "multi agent"}, {"st": 32, "ed": 35, "text": "deep generative models"}, {"st": 53, "ed": 55, "text": "multi agent"}, {"st": 66, "ed": 68, "text": "domain knowledge"}, {"st": 83, "ed": 85, "text": "low dimensional"}, {"st": 150, "ed": 152, "text": "generate realistic"}, {"st": 152, "ed": 154, "text": "multi agent"}, {"st": 168, "ed": 171, "text": "quantitative and qualitative"}, {"st": 174, "ed": 176, "text": "user study"}]
[{"st": 0, "ed": 2, "text": "domain adaptation"}, {"st": 5, "ed": 8, "text": "task of classifying"}, {"st": 14, "ed": 16, "text": "labeled dataset"}, {"st": 35, "ed": 38, "text": "source and target"}, {"st": 73, "ed": 75, "text": "domain adaptation"}, {"st": 85, "ed": 88, "text": "expectation maximization em"}, {"st": 98, "ed": 100, "text": "logistic regression"}, {"st": 101, "ed": 104, "text": "support vector machine"}, {"st": 110, "ed": 112, "text": "proposed method"}, {"st": 119, "ed": 121, "text": "linear classifier"}, {"st": 123, "ed": 126, "text": "source and target"}, {"st": 145, "ed": 147, "text": "deep features"}, {"st": 150, "ed": 152, "text": "pre trained"}, {"st": 160, "ed": 163, "text": "easy to implement"}, {"st": 171, "ed": 173, "text": "real life"}, {"st": 176, "ed": 179, "text": "text and image"}, {"st": 184, "ed": 186, "text": "method achieves"}, {"st": 196, "ed": 199, "text": "end to end"}, {"st": 200, "ed": 202, "text": "transfer learning"}]
[{"st": 0, "ed": 2, "text": "sparse models"}, {"st": 5, "ed": 7, "text": "linear regression"}, {"st": 8, "ed": 10, "text": "machine learning"}, {"st": 68, "ed": 70, "text": "highly correlated"}, {"st": 113, "ed": 115, "text": "side information"}, {"st": 127, "ed": 129, "text": "edge weights"}, {"st": 141, "ed": 143, "text": "total variation"}, {"st": 149, "ed": 151, "text": "highly correlated"}, {"st": 174, "ed": 176, "text": "theoretical guarantees"}, {"st": 177, "ed": 179, "text": "highly correlated"}, {"st": 191, "ed": 193, "text": "graph based"}, {"st": 195, "ed": 198, "text": "mean squared error"}, {"st": 232, "ed": 234, "text": "proposed approach"}, {"st": 242, "ed": 244, "text": "highly correlated"}, {"st": 251, "ed": 254, "text": "simulated and real"}]
[{"st": 0, "ed": 4, "text": "recurrent neural networks rnns"}, {"st": 5, "ed": 7, "text": "becoming increasingly"}, {"st": 9, "ed": 11, "text": "time series"}, {"st": 37, "ed": 39, "text": "network structure"}, {"st": 47, "ed": 49, "text": "weight matrix"}, {"st": 89, "ed": 91, "text": "proposed framework"}]
[{"st": 0, "ed": 4, "text": "generative adversarial networks gans"}, {"st": 37, "ed": 39, "text": "unknown distribution"}, {"st": 47, "ed": 49, "text": "objective function"}, {"st": 70, "ed": 72, "text": "theoretical understanding"}, {"st": 127, "ed": 130, "text": "point of view"}]
[{"st": 5, "ed": 7, "text": "metric learning"}, {"st": 8, "ed": 11, "text": "multi view data"}, {"st": 36, "ed": 38, "text": "multi modal"}, {"st": 45, "ed": 47, "text": "convex optimization"}, {"st": 49, "ed": 51, "text": "jointly learn"}, {"st": 66, "ed": 68, "text": "multi view"}, {"st": 68, "ed": 70, "text": "metric learning"}, {"st": 84, "ed": 87, "text": "large training sets"}, {"st": 96, "ed": 98, "text": "multi view"}, {"st": 98, "ed": 100, "text": "kernel matrix"}, {"st": 114, "ed": 116, "text": "real world"}]
[{"st": 1, "ed": 3, "text": "topic models"}, {"st": 24, "ed": 27, "text": "word co occurrence"}, {"st": 51, "ed": 53, "text": "topic models"}, {"st": 83, "ed": 86, "text": "gaussian processes gps"}, {"st": 101, "ed": 104, "text": "long term memory"}, {"st": 109, "ed": 111, "text": "event detection"}, {"st": 119, "ed": 121, "text": "approximate inference"}, {"st": 128, "ed": 131, "text": "stochastic variational inference"}, {"st": 152, "ed": 154, "text": "large scale"}]
[{"st": 5, "ed": 8, "text": "electronic health records"}, {"st": 21, "ed": 23, "text": "important information"}, {"st": 39, "ed": 41, "text": "time series"}, {"st": 49, "ed": 51, "text": "missing data"}, {"st": 68, "ed": 70, "text": "colorectal cancer"}, {"st": 94, "ed": 97, "text": "multivariate time series"}, {"st": 100, "ed": 102, "text": "missing data"}, {"st": 125, "ed": 127, "text": "supervised classification"}]
[{"st": 35, "ed": 37, "text": "feature extraction"}, {"st": 43, "ed": 45, "text": "analytical hierarchy"}, {"st": 53, "ed": 55, "text": "machine learning"}, {"st": 84, "ed": 86, "text": "feature extraction"}]
[{"st": 10, "ed": 12, "text": "learning algorithm"}, {"st": 93, "ed": 95, "text": "ridge regression"}, {"st": 98, "ed": 100, "text": "positive semidefinite"}, {"st": 119, "ed": 121, "text": "ridge regression"}, {"st": 132, "ed": 134, "text": "statistical properties"}, {"st": 136, "ed": 138, "text": "proposed algorithm"}, {"st": 151, "ed": 153, "text": "generalization performance"}, {"st": 164, "ed": 166, "text": "online learning"}]
[{"st": 60, "ed": 62, "text": "machine learning"}, {"st": 64, "ed": 66, "text": "density based"}]
[{"st": 3, "ed": 5, "text": "open source"}, {"st": 8, "ed": 10, "text": "machine learning"}, {"st": 10, "ed": 12, "text": "time series"}, {"st": 16, "ed": 18, "text": "sliding window"}, {"st": 28, "ed": 30, "text": "classification regression"}, {"st": 44, "ed": 46, "text": "scikit learn"}, {"st": 50, "ed": 52, "text": "scikit learn"}]
[{"st": 32, "ed": 34, "text": "feature extraction"}, {"st": 50, "ed": 53, "text": "gaussian mixture models"}, {"st": 55, "ed": 58, "text": "hidden markov models"}, {"st": 65, "ed": 67, "text": "neural networks"}, {"st": 80, "ed": 82, "text": "similar accuracy"}]
[{"st": 0, "ed": 3, "text": "deep neural networks"}, {"st": 17, "ed": 19, "text": "training examples"}, {"st": 41, "ed": 43, "text": "training procedure"}, {"st": 46, "ed": 49, "text": "stochastic gradient descent"}, {"st": 93, "ed": 95, "text": "feedforward networks"}, {"st": 98, "ed": 100, "text": "activation function"}, {"st": 107, "ed": 109, "text": "learning rate"}, {"st": 115, "ed": 117, "text": "weight vectors"}, {"st": 139, "ed": 141, "text": "input data"}, {"st": 168, "ed": 170, "text": "input data"}, {"st": 181, "ed": 183, "text": "linear interpolation"}, {"st": 192, "ed": 194, "text": "generalization properties"}]
[{"st": 0, "ed": 2, "text": "supervised learning"}, {"st": 19, "ed": 21, "text": "input output"}, {"st": 55, "ed": 57, "text": "learning paradigm"}, {"st": 81, "ed": 83, "text": "theoretical analysis"}, {"st": 150, "ed": 152, "text": "neural network"}, {"st": 157, "ed": 159, "text": "generalization performance"}]
[{"st": 6, "ed": 8, "text": "deep learning"}, {"st": 24, "ed": 26, "text": "increasing complexity"}, {"st": 61, "ed": 63, "text": "deep learning"}, {"st": 66, "ed": 68, "text": "residual network"}, {"st": 71, "ed": 73, "text": "closely related"}, {"st": 92, "ed": 94, "text": "preliminary results"}]
[{"st": 6, "ed": 8, "text": "combinatorial optimization"}, {"st": 68, "ed": 70, "text": "significantly improve"}, {"st": 77, "ed": 79, "text": "learning algorithms"}]
[{"st": 4, "ed": 6, "text": "cervical cancer"}, {"st": 11, "ed": 13, "text": "early detection"}, {"st": 36, "ed": 38, "text": "fluorescence spectroscopy"}, {"st": 91, "ed": 93, "text": "statistical analysis"}, {"st": 124, "ed": 126, "text": "multi layered"}, {"st": 127, "ed": 130, "text": "radial basis function"}]
[{"st": 86, "ed": 88, "text": "italian alps"}]
[{"st": 31, "ed": 33, "text": "neural networks"}, {"st": 76, "ed": 78, "text": "neural network"}]
[{"st": 37, "ed": 39, "text": "input variables"}, {"st": 66, "ed": 68, "text": "mutual information"}, {"st": 77, "ed": 79, "text": "mutual information"}, {"st": 84, "ed": 86, "text": "input variables"}]
[{"st": 29, "ed": 31, "text": "evolutionary algorithm"}, {"st": 33, "ed": 35, "text": "hyper parameter"}, {"st": 38, "ed": 40, "text": "kernel based"}, {"st": 99, "ed": 101, "text": "statistical analysis"}, {"st": 124, "ed": 126, "text": "method yields"}, {"st": 177, "ed": 179, "text": "time series"}]
[{"st": 13, "ed": 15, "text": "recent years"}, {"st": 36, "ed": 38, "text": "network topology"}, {"st": 59, "ed": 61, "text": "mobile computing"}, {"st": 107, "ed": 109, "text": "anomaly detection"}, {"st": 178, "ed": 180, "text": "important features"}, {"st": 191, "ed": 193, "text": "feature selection"}, {"st": 202, "ed": 204, "text": "genetic algorithm"}, {"st": 213, "ed": 215, "text": "fitness function"}, {"st": 260, "ed": 262, "text": "false alarm"}]
[{"st": 0, "ed": 2, "text": "back propagation"}, {"st": 9, "ed": 11, "text": "learning algorithm"}, {"st": 25, "ed": 27, "text": "learning rate"}, {"st": 40, "ed": 42, "text": "online learning"}, {"st": 47, "ed": 49, "text": "optimization methods"}, {"st": 59, "ed": 61, "text": "learning rate"}, {"st": 72, "ed": 74, "text": "proposed algorithm"}, {"st": 85, "ed": 89, "text": "feed forward neural networks"}]
[{"st": 0, "ed": 2, "text": "text classification"}, {"st": 16, "ed": 18, "text": "supervised learning"}, {"st": 36, "ed": 38, "text": "text classification"}, {"st": 60, "ed": 62, "text": "feature set"}, {"st": 69, "ed": 72, "text": "naive bayes classifier"}, {"st": 85, "ed": 87, "text": "genetic algorithm"}, {"st": 108, "ed": 110, "text": "text classification"}]
[{"st": 41, "ed": 43, "text": "learning theory"}, {"st": 64, "ed": 66, "text": "open problems"}, {"st": 147, "ed": 149, "text": "loss function"}, {"st": 173, "ed": 175, "text": "loss function"}, {"st": 177, "ed": 179, "text": "mild conditions"}, {"st": 186, "ed": 188, "text": "quadratic loss"}, {"st": 230, "ed": 232, "text": "quadratic loss"}]
[{"st": 12, "ed": 14, "text": "wide variety"}, {"st": 30, "ed": 32, "text": "neural network"}, {"st": 67, "ed": 69, "text": "problem involving"}, {"st": 70, "ed": 72, "text": "real valued"}, {"st": 107, "ed": 109, "text": "traditional methods"}]
[{"st": 35, "ed": 37, "text": "genetic algorithm"}, {"st": 40, "ed": 42, "text": "pid controller"}, {"st": 60, "ed": 62, "text": "iterative method"}]
[{"st": 62, "ed": 64, "text": "recently proposed"}, {"st": 64, "ed": 66, "text": "neural network"}, {"st": 113, "ed": 115, "text": "proposed method"}, {"st": 130, "ed": 132, "text": "proposed method"}, {"st": 137, "ed": 139, "text": "neural network"}]
[{"st": 8, "ed": 10, "text": "machine learning"}, {"st": 92, "ed": 94, "text": "standard benchmark"}]
[{"st": 9, "ed": 11, "text": "associative memory"}, {"st": 49, "ed": 51, "text": "visual cortex"}, {"st": 85, "ed": 87, "text": "previous approaches"}, {"st": 94, "ed": 96, "text": "exponentially large"}]
[{"st": 7, "ed": 9, "text": "riemannian manifold"}, {"st": 23, "ed": 26, "text": "radial basis function"}, {"st": 178, "ed": 180, "text": "smooth function"}]
[{"st": 8, "ed": 10, "text": "linear optimization"}, {"st": 20, "ed": 22, "text": "constrained optimization"}, {"st": 75, "ed": 77, "text": "previous studies"}, {"st": 84, "ed": 86, "text": "sufficient conditions"}, {"st": 99, "ed": 101, "text": "step size"}, {"st": 110, "ed": 112, "text": "linear function"}]
[{"st": 0, "ed": 2, "text": "recent advances"}, {"st": 68, "ed": 70, "text": "phase change"}, {"st": 90, "ed": 92, "text": "phase change"}]
[{"st": 41, "ed": 43, "text": "statistical physics"}, {"st": 47, "ed": 49, "text": "saddle point"}]
[{"st": 57, "ed": 59, "text": "feature extraction"}, {"st": 101, "ed": 103, "text": "weighted sum"}, {"st": 108, "ed": 110, "text": "linear classifier"}, {"st": 120, "ed": 122, "text": "linear classifier"}, {"st": 143, "ed": 145, "text": "genetic programming"}, {"st": 176, "ed": 178, "text": "proposed method"}, {"st": 182, "ed": 184, "text": "benchmark datasets"}]
[]
[{"st": 18, "ed": 20, "text": "widely applied"}, {"st": 42, "ed": 44, "text": "k means"}, {"st": 50, "ed": 52, "text": "based clustering"}, {"st": 62, "ed": 64, "text": "k means"}, {"st": 79, "ed": 81, "text": "global optimization"}, {"st": 86, "ed": 88, "text": "local optimum"}, {"st": 95, "ed": 97, "text": "data clustering"}, {"st": 103, "ed": 105, "text": "k means"}, {"st": 109, "ed": 111, "text": "proposed algorithm"}, {"st": 115, "ed": 117, "text": "k means"}]
[{"st": 10, "ed": 12, "text": "neural networks"}, {"st": 14, "ed": 16, "text": "activation functions"}, {"st": 19, "ed": 23, "text": "rectified linear unit relu"}, {"st": 27, "ed": 29, "text": "de facto"}, {"st": 71, "ed": 73, "text": "neural network"}, {"st": 75, "ed": 77, "text": "activation functions"}, {"st": 125, "ed": 127, "text": "numerical examples"}, {"st": 146, "ed": 148, "text": "low pass"}]
[{"st": 3, "ed": 5, "text": "efficient learning"}, {"st": 11, "ed": 13, "text": "neural networks"}, {"st": 20, "ed": 22, "text": "np complete"}, {"st": 22, "ed": 24, "text": "discrete optimization"}, {"st": 91, "ed": 93, "text": "belief propagation"}, {"st": 94, "ed": 96, "text": "without resorting"}, {"st": 99, "ed": 101, "text": "special cases"}, {"st": 120, "ed": 122, "text": "l 0"}, {"st": 135, "ed": 137, "text": "learning problems"}, {"st": 147, "ed": 149, "text": "fully connected"}]
[{"st": 15, "ed": 17, "text": "open ended"}, {"st": 34, "ed": 36, "text": "neural network"}, {"st": 81, "ed": 83, "text": "stanford university"}, {"st": 95, "ed": 98, "text": "orders of magnitude"}]
[{"st": 2, "ed": 4, "text": "machine learning"}, {"st": 9, "ed": 12, "text": "recurrent neural networks"}, {"st": 36, "ed": 38, "text": "numerical simulations"}, {"st": 45, "ed": 47, "text": "prior knowledge"}, {"st": 80, "ed": 83, "text": "easy to implement"}]
[{"st": 10, "ed": 12, "text": "event detection"}, {"st": 13, "ed": 15, "text": "real life"}, {"st": 18, "ed": 20, "text": "bi directional"}, {"st": 21, "ed": 24, "text": "short term memory"}, {"st": 25, "ed": 29, "text": "recurrent neural networks rnns"}, {"st": 49, "ed": 51, "text": "multiple classes"}, {"st": 68, "ed": 70, "text": "real life"}, {"st": 84, "ed": 86, "text": "proposed method"}, {"st": 86, "ed": 88, "text": "outperforms previous"}, {"st": 91, "ed": 93, "text": "large margin"}, {"st": 123, "ed": 125, "text": "relative improvement"}]
[{"st": 0, "ed": 2, "text": "distributed training"}, {"st": 3, "ed": 5, "text": "deep learning"}, {"st": 7, "ed": 9, "text": "large scale"}, {"st": 9, "ed": 11, "text": "training data"}]
[{"st": 9, "ed": 14, "text": "convolutional neural network cnn architecture"}, {"st": 21, "ed": 23, "text": "deep cnn"}, {"st": 28, "ed": 30, "text": "pooling layers"}, {"st": 34, "ed": 37, "text": "fully connected layers"}, {"st": 57, "ed": 59, "text": "deep architectures"}, {"st": 68, "ed": 70, "text": "convolutional filters"}, {"st": 72, "ed": 74, "text": "convolutional layer"}, {"st": 76, "ed": 78, "text": "max pooling"}, {"st": 92, "ed": 94, "text": "discriminative features"}, {"st": 119, "ed": 121, "text": "event recognition"}, {"st": 125, "ed": 127, "text": "deep architectures"}, {"st": 133, "ed": 135, "text": "recognition accuracy"}, {"st": 140, "ed": 142, "text": "relative error"}]
[{"st": 0, "ed": 2, "text": "machine learning"}, {"st": 26, "ed": 28, "text": "financial market"}, {"st": 35, "ed": 37, "text": "machine learning"}, {"st": 39, "ed": 42, "text": "vulnerable to adversarial"}, {"st": 69, "ed": 71, "text": "machine learning"}, {"st": 83, "ed": 85, "text": "machine learning"}, {"st": 102, "ed": 104, "text": "adversarial samples"}, {"st": 106, "ed": 108, "text": "neural network"}, {"st": 113, "ed": 115, "text": "classification tasks"}, {"st": 118, "ed": 120, "text": "computer vision"}, {"st": 131, "ed": 133, "text": "machine learning"}, {"st": 139, "ed": 142, "text": "recurrent neural networks"}, {"st": 156, "ed": 158, "text": "adversarial samples"}, {"st": 160, "ed": 164, "text": "feed forward neural networks"}]
[{"st": 67, "ed": 69, "text": "large scale"}, {"st": 113, "ed": 116, "text": "supervised and unsupervised"}, {"st": 116, "ed": 118, "text": "learning algorithms"}, {"st": 132, "ed": 134, "text": "character recognition"}, {"st": 134, "ed": 136, "text": "case study"}]
[{"st": 9, "ed": 11, "text": "auto encoder"}, {"st": 20, "ed": 22, "text": "auto encoder"}, {"st": 38, "ed": 40, "text": "auto encoder"}, {"st": 80, "ed": 82, "text": "auto encoders"}, {"st": 100, "ed": 102, "text": "noise level"}]
[{"st": 22, "ed": 24, "text": "time series"}, {"st": 72, "ed": 75, "text": "artificial neural network"}, {"st": 78, "ed": 81, "text": "multi layer perceptron"}, {"st": 96, "ed": 98, "text": "training data"}, {"st": 122, "ed": 124, "text": "new york"}, {"st": 127, "ed": 129, "text": "prediction results"}]
[{"st": 57, "ed": 61, "text": "number of training samples"}, {"st": 116, "ed": 118, "text": "output space"}, {"st": 125, "ed": 127, "text": "input space"}, {"st": 150, "ed": 152, "text": "neural networks"}, {"st": 155, "ed": 157, "text": "prediction error"}, {"st": 194, "ed": 196, "text": "active learning"}, {"st": 204, "ed": 206, "text": "training samples"}]
[{"st": 31, "ed": 34, "text": "deep neural network"}, {"st": 40, "ed": 42, "text": "speech signals"}]
[{"st": 1, "ed": 3, "text": "cocktail party"}, {"st": 6, "ed": 8, "text": "human brain"}, {"st": 17, "ed": 19, "text": "signal processing"}, {"st": 25, "ed": 27, "text": "cocktail party"}, {"st": 39, "ed": 42, "text": "deep neural networks"}, {"st": 84, "ed": 86, "text": "neural networks"}, {"st": 97, "ed": 99, "text": "cocktail party"}]
[{"st": 6, "ed": 8, "text": "key challenge"}, {"st": 9, "ed": 11, "text": "signal processing"}, {"st": 21, "ed": 24, "text": "a long standing"}, {"st": 30, "ed": 32, "text": "source separation"}, {"st": 44, "ed": 47, "text": "deep neural network"}, {"st": 51, "ed": 53, "text": "cocktail party"}, {"st": 72, "ed": 75, "text": "deep neural networks"}]
[{"st": 1, "ed": 3, "text": "machine learning"}, {"st": 4, "ed": 6, "text": "back propagation"}, {"st": 7, "ed": 11, "text": "multi layer neural networks"}, {"st": 11, "ed": 13, "text": "deep learning"}, {"st": 20, "ed": 22, "text": "reinforcement learning"}, {"st": 32, "ed": 34, "text": "deep learning"}, {"st": 71, "ed": 73, "text": "classification task"}, {"st": 97, "ed": 99, "text": "preliminary results"}, {"st": 108, "ed": 110, "text": "linear regression"}, {"st": 112, "ed": 114, "text": "hidden layers"}]
[{"st": 148, "ed": 150, "text": "theoretical bounds"}]
[{"st": 65, "ed": 67, "text": "data security"}, {"st": 95, "ed": 97, "text": "multi core"}, {"st": 158, "ed": 160, "text": "real life"}, {"st": 185, "ed": 187, "text": "deep cnn"}, {"st": 196, "ed": 198, "text": "face detection"}]
[{"st": 30, "ed": 32, "text": "time series"}, {"st": 35, "ed": 38, "text": "artificial neural network"}, {"st": 43, "ed": 45, "text": "moving average"}]
[{"st": 34, "ed": 36, "text": "hidden variables"}, {"st": 122, "ed": 125, "text": "artificial neural networks"}]
[{"st": 11, "ed": 13, "text": "sensory input"}, {"st": 53, "ed": 55, "text": "random sampling"}, {"st": 68, "ed": 70, "text": "natural image"}, {"st": 85, "ed": 87, "text": "activity patterns"}, {"st": 121, "ed": 123, "text": "receptive field"}, {"st": 129, "ed": 131, "text": "activity patterns"}, {"st": 161, "ed": 163, "text": "layer wise"}]
[{"st": 17, "ed": 20, "text": "generalized linear model"}, {"st": 23, "ed": 25, "text": "increasingly popular"}, {"st": 40, "ed": 42, "text": "computationally tractable"}, {"st": 94, "ed": 96, "text": "spatio temporal"}, {"st": 96, "ed": 98, "text": "receptive fields"}]
[{"st": 6, "ed": 8, "text": "fundamental problems"}, {"st": 18, "ed": 20, "text": "machine learning"}, {"st": 37, "ed": 39, "text": "real world"}, {"st": 98, "ed": 101, "text": "recurrent neural networks"}, {"st": 105, "ed": 107, "text": "traditional methods"}, {"st": 129, "ed": 131, "text": "large scale"}, {"st": 140, "ed": 142, "text": "search engine"}, {"st": 147, "ed": 149, "text": "significantly improve"}, {"st": 151, "ed": 153, "text": "prediction accuracy"}]
[{"st": 10, "ed": 13, "text": "convolutional neural networks"}]
[{"st": 0, "ed": 2, "text": "neural network"}, {"st": 26, "ed": 28, "text": "probabilistic models"}, {"st": 29, "ed": 31, "text": "network design"}, {"st": 53, "ed": 55, "text": "random variables"}, {"st": 60, "ed": 62, "text": "statistical mechanics"}, {"st": 83, "ed": 85, "text": "bayes rule"}, {"st": 103, "ed": 106, "text": "markov random field"}]
[{"st": 0, "ed": 2, "text": "consumer debt"}, {"st": 44, "ed": 46, "text": "computational intelligence"}, {"st": 65, "ed": 67, "text": "linear regression"}, {"st": 75, "ed": 77, "text": "neural networks"}, {"st": 99, "ed": 101, "text": "data mining"}, {"st": 108, "ed": 110, "text": "neural networks"}, {"st": 133, "ed": 135, "text": "real world"}]
[{"st": 0, "ed": 2, "text": "deep learning"}, {"st": 14, "ed": 16, "text": "deep learning"}, {"st": 18, "ed": 21, "text": "ability to capture"}, {"st": 38, "ed": 40, "text": "deep learning"}, {"st": 44, "ed": 46, "text": "deep architectures"}, {"st": 73, "ed": 75, "text": "deep learning"}, {"st": 79, "ed": 81, "text": "representation learning"}, {"st": 84, "ed": 86, "text": "deep learning"}, {"st": 120, "ed": 122, "text": "deep learning"}, {"st": 132, "ed": 135, "text": "deep neural networks"}, {"st": 137, "ed": 139, "text": "higher accuracy"}, {"st": 142, "ed": 144, "text": "classification task"}, {"st": 149, "ed": 151, "text": "logistic regression"}, {"st": 162, "ed": 164, "text": "deep learning"}, {"st": 181, "ed": 183, "text": "deep learning"}]
[{"st": 0, "ed": 2, "text": "programming language"}, {"st": 5, "ed": 7, "text": "natural language"}, {"st": 17, "ed": 19, "text": "software engineering"}, {"st": 34, "ed": 36, "text": "natural language"}, {"st": 64, "ed": 67, "text": "convolutional neural network"}, {"st": 69, "ed": 71, "text": "programming language"}, {"st": 94, "ed": 96, "text": "programming language"}, {"st": 122, "ed": 124, "text": "baseline methods"}]
[{"st": 8, "ed": 10, "text": "deep learning"}, {"st": 11, "ed": 13, "text": "deep learning"}, {"st": 15, "ed": 17, "text": "computationally intensive"}, {"st": 57, "ed": 59, "text": "linear algebra"}, {"st": 77, "ed": 79, "text": "deep learning"}, {"st": 124, "ed": 126, "text": "deep learning"}, {"st": 173, "ed": 175, "text": "convolutional networks"}, {"st": 175, "ed": 177, "text": "improves performance"}, {"st": 181, "ed": 183, "text": "standard model"}]
[{"st": 15, "ed": 17, "text": "weight updates"}, {"st": 61, "ed": 63, "text": "learning algorithms"}, {"st": 64, "ed": 66, "text": "regret bounds"}, {"st": 90, "ed": 92, "text": "nonparametric regression"}, {"st": 102, "ed": 105, "text": "a sufficient condition"}, {"st": 120, "ed": 122, "text": "real world"}]
[{"st": 1, "ed": 3, "text": "recent years"}, {"st": 3, "ed": 5, "text": "deep learning"}, {"st": 11, "ed": 13, "text": "machine learning"}, {"st": 22, "ed": 24, "text": "quantum computers"}, {"st": 28, "ed": 30, "text": "efficiently solve"}, {"st": 42, "ed": 44, "text": "quantum computing"}, {"st": 54, "ed": 57, "text": "restricted boltzmann machine"}, {"st": 67, "ed": 69, "text": "deep learning"}, {"st": 75, "ed": 77, "text": "significant improvements"}, {"st": 97, "ed": 99, "text": "multi layer"}, {"st": 99, "ed": 101, "text": "fully connected"}]
[{"st": 7, "ed": 10, "text": "artificial neural networks"}, {"st": 28, "ed": 30, "text": "linear classifiers"}, {"st": 33, "ed": 35, "text": "important role"}, {"st": 50, "ed": 52, "text": "machine learning"}, {"st": 63, "ed": 65, "text": "quantum information"}, {"st": 83, "ed": 85, "text": "activation function"}, {"st": 94, "ed": 96, "text": "mathcal o"}]
[{"st": 13, "ed": 15, "text": "predictive model"}, {"st": 22, "ed": 24, "text": "without compromising"}, {"st": 81, "ed": 83, "text": "achieve high"}, {"st": 86, "ed": 88, "text": "neural networks"}, {"st": 174, "ed": 176, "text": "activation functions"}, {"st": 180, "ed": 182, "text": "neural networks"}, {"st": 205, "ed": 207, "text": "neural network"}]
[{"st": 5, "ed": 7, "text": "neural networks"}, {"st": 24, "ed": 26, "text": "neural network"}, {"st": 29, "ed": 31, "text": "feature vector"}, {"st": 74, "ed": 76, "text": "cross validation"}]
[{"st": 9, "ed": 11, "text": "source separation"}, {"st": 53, "ed": 55, "text": "weight vectors"}, {"st": 116, "ed": 118, "text": "promising results"}]
[{"st": 6, "ed": 9, "text": "convolutional neural network"}, {"st": 23, "ed": 26, "text": "fast fourier transform"}, {"st": 47, "ed": 49, "text": "significant speedups"}, {"st": 64, "ed": 66, "text": "open source"}, {"st": 77, "ed": 79, "text": "convolutional layers"}]
[{"st": 7, "ed": 9, "text": "amino acid"}, {"st": 18, "ed": 22, "text": "feed forward neural networks"}, {"st": 27, "ed": 29, "text": "sliding window"}, {"st": 38, "ed": 41, "text": "recurrent neural networks"}, {"st": 46, "ed": 50, "text": "feed forward neural network"}, {"st": 59, "ed": 62, "text": "recurrent neural network"}, {"st": 64, "ed": 67, "text": "short term memory"}, {"st": 101, "ed": 103, "text": "feed forward"}, {"st": 107, "ed": 110, "text": "short term memory"}]
[{"st": 7, "ed": 10, "text": "online learning algorithm"}, {"st": 11, "ed": 14, "text": "extreme learning machines"}, {"st": 35, "ed": 37, "text": "estimation error"}, {"st": 85, "ed": 87, "text": "real world"}, {"st": 90, "ed": 92, "text": "combustion engine"}, {"st": 102, "ed": 104, "text": "case studies"}, {"st": 125, "ed": 127, "text": "class imbalance"}]
[{"st": 5, "ed": 7, "text": "computationally efficient"}, {"st": 10, "ed": 12, "text": "deep learning"}, {"st": 20, "ed": 22, "text": "computational efficiency"}, {"st": 24, "ed": 26, "text": "deep learning"}]
[{"st": 1, "ed": 4, "text": "deep neural networks"}, {"st": 50, "ed": 52, "text": "cocktail party"}, {"st": 52, "ed": 54, "text": "source separation"}, {"st": 69, "ed": 71, "text": "speech signals"}]
[{"st": 12, "ed": 14, "text": "key challenge"}, {"st": 15, "ed": 17, "text": "source separation"}, {"st": 21, "ed": 24, "text": "deep neural networks"}, {"st": 36, "ed": 38, "text": "cocktail party"}, {"st": 38, "ed": 40, "text": "speech separation"}, {"st": 94, "ed": 96, "text": "real world"}]
[{"st": 33, "ed": 35, "text": "deep learning"}, {"st": 39, "ed": 41, "text": "object recognition"}, {"st": 55, "ed": 57, "text": "input image"}, {"st": 77, "ed": 79, "text": "deep learning"}, {"st": 122, "ed": 124, "text": "benchmark datasets"}, {"st": 127, "ed": 129, "text": "deep architectures"}, {"st": 145, "ed": 147, "text": "convolutional networks"}, {"st": 157, "ed": 159, "text": "majority vote"}]
[{"st": 0, "ed": 2, "text": "compressive sensing"}, {"st": 14, "ed": 16, "text": "raw data"}, {"st": 39, "ed": 41, "text": "source data"}, {"st": 47, "ed": 49, "text": "sparse representation"}, {"st": 83, "ed": 85, "text": "learning algorithm"}, {"st": 93, "ed": 95, "text": "neural network"}, {"st": 99, "ed": 101, "text": "sparse coding"}, {"st": 105, "ed": 107, "text": "sparse codes"}, {"st": 114, "ed": 116, "text": "hidden layer"}, {"st": 128, "ed": 130, "text": "real data"}, {"st": 134, "ed": 136, "text": "proposed method"}, {"st": 138, "ed": 140, "text": "sparsity inducing"}]
[{"st": 2, "ed": 4, "text": "convex optimization"}, {"st": 13, "ed": 15, "text": "building blocks"}, {"st": 16, "ed": 18, "text": "deep learning"}, {"st": 24, "ed": 26, "text": "empirical success"}, {"st": 29, "ed": 31, "text": "neural networks"}, {"st": 48, "ed": 51, "text": "online convex optimization"}, {"st": 76, "ed": 78, "text": "main result"}, {"st": 84, "ed": 86, "text": "convolutional network"}, {"st": 110, "ed": 112, "text": "neural network"}, {"st": 115, "ed": 118, "text": "rate of convergence"}, {"st": 137, "ed": 139, "text": "neural network"}, {"st": 157, "ed": 159, "text": "deep architectures"}, {"st": 171, "ed": 173, "text": "bandit algorithms"}]
[{"st": 3, "ed": 5, "text": "weight update"}, {"st": 28, "ed": 30, "text": "spike timing"}, {"st": 89, "ed": 91, "text": "machine learning"}, {"st": 98, "ed": 100, "text": "neural activity"}, {"st": 103, "ed": 105, "text": "objective function"}, {"st": 112, "ed": 115, "text": "stochastic gradient descent"}]
[{"st": 8, "ed": 10, "text": "back propagation"}, {"st": 10, "ed": 12, "text": "neural network"}, {"st": 13, "ed": 15, "text": "machine learning"}, {"st": 18, "ed": 20, "text": "non trivial"}, {"st": 53, "ed": 55, "text": "deep learning"}, {"st": 57, "ed": 59, "text": "cloud computing"}, {"st": 63, "ed": 65, "text": "large scale"}, {"st": 91, "ed": 93, "text": "large scale"}, {"st": 104, "ed": 106, "text": "deep learning"}, {"st": 110, "ed": 112, "text": "handwritten digits"}, {"st": 118, "ed": 120, "text": "deep learning"}, {"st": 120, "ed": 122, "text": "neural network"}, {"st": 153, "ed": 155, "text": "handwritten digit"}]
[{"st": 61, "ed": 63, "text": "quantitative analysis"}, {"st": 82, "ed": 84, "text": "convex functions"}, {"st": 87, "ed": 89, "text": "recognition task"}, {"st": 91, "ed": 93, "text": "mnist dataset"}]
[{"st": 23, "ed": 25, "text": "real valued"}, {"st": 25, "ed": 27, "text": "time series"}, {"st": 58, "ed": 60, "text": "recurrent networks"}, {"st": 61, "ed": 63, "text": "echo state"}, {"st": 118, "ed": 120, "text": "time series"}, {"st": 155, "ed": 157, "text": "time series"}, {"st": 174, "ed": 176, "text": "real world"}, {"st": 179, "ed": 181, "text": "time series"}, {"st": 191, "ed": 193, "text": "recently gained"}, {"st": 196, "ed": 198, "text": "complex systems"}]
[{"st": 11, "ed": 13, "text": "ill posed"}, {"st": 15, "ed": 17, "text": "missing data"}, {"st": 37, "ed": 39, "text": "matrix factorization"}, {"st": 41, "ed": 45, "text": "deep recurrent neural networks"}, {"st": 70, "ed": 72, "text": "learning strategy"}, {"st": 84, "ed": 86, "text": "parallel processing"}, {"st": 88, "ed": 90, "text": "processing units"}, {"st": 93, "ed": 95, "text": "processing units"}, {"st": 119, "ed": 121, "text": "least square"}, {"st": 135, "ed": 138, "text": "single nucleotide polymorphisms"}]
[{"st": 9, "ed": 11, "text": "mobile phones"}, {"st": 14, "ed": 16, "text": "activity recognition"}, {"st": 25, "ed": 27, "text": "recognition accuracy"}, {"st": 43, "ed": 46, "text": "human activity recognition"}, {"st": 50, "ed": 52, "text": "deep learning"}, {"st": 58, "ed": 60, "text": "activity recognition"}, {"st": 64, "ed": 66, "text": "recognition accuracy"}, {"st": 75, "ed": 77, "text": "handcrafted features"}, {"st": 97, "ed": 99, "text": "deep learning"}, {"st": 100, "ed": 103, "text": "hidden markov models"}, {"st": 116, "ed": 118, "text": "hierarchical representations"}, {"st": 120, "ed": 122, "text": "activity recognition"}, {"st": 141, "ed": 143, "text": "real world"}, {"st": 151, "ed": 154, "text": "human activity recognition"}]
[{"st": 0, "ed": 2, "text": "traditional methods"}, {"st": 6, "ed": 8, "text": "information retrieval"}, {"st": 15, "ed": 17, "text": "feature engineering"}, {"st": 27, "ed": 29, "text": "feature engineering"}, {"st": 37, "ed": 39, "text": "feature engineering"}, {"st": 56, "ed": 59, "text": "convolutional neural networks"}, {"st": 70, "ed": 72, "text": "feature extraction"}, {"st": 73, "ed": 75, "text": "learning algorithms"}, {"st": 80, "ed": 83, "text": "end to end"}, {"st": 88, "ed": 91, "text": "convolutional neural network"}, {"st": 99, "ed": 101, "text": "traditional methods"}]
[{"st": 0, "ed": 3, "text": "deep neural networks"}, {"st": 8, "ed": 11, "text": "non convex optimization"}, {"st": 31, "ed": 33, "text": "batch size"}, {"st": 65, "ed": 67, "text": "gradient based"}, {"st": 73, "ed": 75, "text": "loss function"}, {"st": 86, "ed": 88, "text": "loss functions"}, {"st": 118, "ed": 120, "text": "selection pressure"}, {"st": 159, "ed": 161, "text": "mnist dataset"}]
[{"st": 59, "ed": 61, "text": "natural gradient"}, {"st": 66, "ed": 69, "text": "exploration and exploitation"}]
[{"st": 2, "ed": 5, "text": "recurrent neural networks"}, {"st": 13, "ed": 15, "text": "real life"}, {"st": 15, "ed": 17, "text": "recommender systems"}, {"st": 55, "ed": 57, "text": "matrix factorization"}, {"st": 97, "ed": 99, "text": "rnn based"}, {"st": 123, "ed": 125, "text": "ranking loss"}]
[{"st": 3, "ed": 7, "text": "recurrent neural network architecture"}]
[{"st": 5, "ed": 7, "text": "machine learning"}, {"st": 93, "ed": 95, "text": "preliminary experiments"}, {"st": 96, "ed": 98, "text": "promising results"}, {"st": 99, "ed": 101, "text": "large scale"}, {"st": 101, "ed": 104, "text": "deep neural network"}]
[{"st": 1, "ed": 3, "text": "incremental learning"}, {"st": 101, "ed": 103, "text": "low level"}, {"st": 115, "ed": 117, "text": "low level"}, {"st": 135, "ed": 137, "text": "approach called"}]
[{"st": 19, "ed": 21, "text": "big data"}, {"st": 45, "ed": 47, "text": "article presents"}, {"st": 53, "ed": 55, "text": "deep learning"}, {"st": 62, "ed": 64, "text": "learning framework"}, {"st": 70, "ed": 72, "text": "deep learning"}, {"st": 89, "ed": 91, "text": "deep model"}, {"st": 101, "ed": 103, "text": "deep model"}, {"st": 123, "ed": 125, "text": "deep models"}, {"st": 128, "ed": 130, "text": "hidden layers"}, {"st": 137, "ed": 139, "text": "context aware"}, {"st": 139, "ed": 141, "text": "activity recognition"}, {"st": 144, "ed": 146, "text": "real world"}]
[{"st": 4, "ed": 6, "text": "machine learning"}, {"st": 18, "ed": 21, "text": "deep neural network"}, {"st": 40, "ed": 42, "text": "machine learning"}, {"st": 51, "ed": 53, "text": "network architecture"}, {"st": 112, "ed": 114, "text": "significant reduction"}, {"st": 122, "ed": 124, "text": "vgg 16"}, {"st": 143, "ed": 145, "text": "vgg 16"}, {"st": 146, "ed": 148, "text": "batch size"}, {"st": 172, "ed": 174, "text": "performance loss"}]
[{"st": 11, "ed": 14, "text": "recurrent neural networks"}, {"st": 44, "ed": 46, "text": "prediction error"}, {"st": 49, "ed": 52, "text": "short term memory"}, {"st": 57, "ed": 59, "text": "recurrent networks"}, {"st": 65, "ed": 67, "text": "input signal"}, {"st": 102, "ed": 104, "text": "fisher information"}, {"st": 118, "ed": 120, "text": "fisher information"}, {"st": 132, "ed": 134, "text": "fisher information"}, {"st": 135, "ed": 137, "text": "notoriously difficult"}, {"st": 143, "ed": 146, "text": "probability density function"}, {"st": 162, "ed": 165, "text": "takes advantage of"}, {"st": 166, "ed": 168, "text": "recently developed"}, {"st": 173, "ed": 175, "text": "fisher information"}, {"st": 186, "ed": 188, "text": "echo state"}, {"st": 203, "ed": 205, "text": "echo state"}, {"st": 223, "ed": 225, "text": "fisher information"}, {"st": 233, "ed": 236, "text": "real world data"}]
[{"st": 16, "ed": 19, "text": "deep neural networks"}, {"st": 39, "ed": 41, "text": "feature learning"}, {"st": 44, "ed": 46, "text": "explicit feature"}, {"st": 58, "ed": 60, "text": "discriminative features"}, {"st": 83, "ed": 85, "text": "deep networks"}, {"st": 100, "ed": 102, "text": "feature learning"}, {"st": 117, "ed": 119, "text": "pooling layers"}, {"st": 125, "ed": 127, "text": "audio signal"}, {"st": 127, "ed": 129, "text": "significantly improves"}, {"st": 130, "ed": 132, "text": "recognition performance"}]
[{"st": 7, "ed": 10, "text": "mean square error"}, {"st": 14, "ed": 16, "text": "echo state"}, {"st": 16, "ed": 18, "text": "neural networks"}, {"st": 22, "ed": 25, "text": "training and testing"}, {"st": 52, "ed": 54, "text": "input data"}, {"st": 56, "ed": 58, "text": "network size"}]
[{"st": 7, "ed": 9, "text": "open source"}, {"st": 14, "ed": 16, "text": "deep learning"}, {"st": 17, "ed": 20, "text": "convolutional neural networks"}, {"st": 70, "ed": 72, "text": "deep learning"}, {"st": 97, "ed": 99, "text": "deep learning"}, {"st": 103, "ed": 105, "text": "app store"}, {"st": 114, "ed": 116, "text": "deep learning"}]
[{"st": 5, "ed": 7, "text": "machine learning"}, {"st": 7, "ed": 9, "text": "deep learning"}, {"st": 25, "ed": 27, "text": "large scale"}, {"st": 33, "ed": 35, "text": "practical applications"}, {"st": 46, "ed": 48, "text": "deep learning"}, {"st": 61, "ed": 63, "text": "low power"}, {"st": 77, "ed": 79, "text": "large scale"}, {"st": 79, "ed": 81, "text": "deep learning"}, {"st": 94, "ed": 96, "text": "processing units"}, {"st": 108, "ed": 110, "text": "deep learning"}, {"st": 143, "ed": 145, "text": "power consumption"}]
[{"st": 12, "ed": 15, "text": "deep neural networks"}, {"st": 46, "ed": 48, "text": "local models"}, {"st": 101, "ed": 103, "text": "framework called"}, {"st": 117, "ed": 119, "text": "local models"}, {"st": 126, "ed": 128, "text": "local models"}, {"st": 136, "ed": 138, "text": "loss functions"}, {"st": 148, "ed": 150, "text": "ensemble based"}, {"st": 187, "ed": 189, "text": "multiple times"}, {"st": 204, "ed": 206, "text": "based method"}]
[{"st": 3, "ed": 5, "text": "multi layered"}, {"st": 5, "ed": 7, "text": "generative models"}, {"st": 72, "ed": 74, "text": "recurrent network"}, {"st": 79, "ed": 81, "text": "fixed point"}, {"st": 101, "ed": 103, "text": "auto encoder"}, {"st": 141, "ed": 143, "text": "weighted sum"}, {"st": 200, "ed": 202, "text": "prediction error"}, {"st": 253, "ed": 255, "text": "recurrent neural"}]
[{"st": 0, "ed": 3, "text": "deep neural networks"}, {"st": 6, "ed": 8, "text": "machine learning"}, {"st": 35, "ed": 37, "text": "machine learning"}, {"st": 54, "ed": 56, "text": "image classification"}, {"st": 75, "ed": 77, "text": "remains unclear"}, {"st": 112, "ed": 114, "text": "highly effective"}, {"st": 119, "ed": 121, "text": "neural networks"}, {"st": 144, "ed": 146, "text": "computer vision"}]
[{"st": 0, "ed": 3, "text": "deep neural networks"}, {"st": 28, "ed": 30, "text": "speech signal"}, {"st": 68, "ed": 70, "text": "deep learning"}, {"st": 76, "ed": 78, "text": "audio visual"}, {"st": 89, "ed": 91, "text": "neural network"}, {"st": 95, "ed": 97, "text": "visual cues"}, {"st": 101, "ed": 103, "text": "fully connected"}, {"st": 107, "ed": 109, "text": "audio signal"}, {"st": 115, "ed": 118, "text": "short term memory"}]
[{"st": 5, "ed": 8, "text": "convolutional neural network"}, {"st": 19, "ed": 21, "text": "scene classification"}, {"st": 22, "ed": 25, "text": "multi label classification"}, {"st": 45, "ed": 48, "text": "fold cross validation"}, {"st": 50, "ed": 52, "text": "scene recognition"}, {"st": 63, "ed": 65, "text": "error rate"}, {"st": 97, "ed": 99, "text": "convolutional layers"}, {"st": 104, "ed": 108, "text": "short time fourier transform"}, {"st": 111, "ed": 113, "text": "pooling layer"}, {"st": 121, "ed": 124, "text": "fully connected layers"}, {"st": 126, "ed": 128, "text": "fully connected"}, {"st": 128, "ed": 130, "text": "output layer"}]
[{"st": 10, "ed": 12, "text": "sensing applications"}, {"st": 50, "ed": 52, "text": "computational cost"}, {"st": 73, "ed": 75, "text": "classification performance"}, {"st": 86, "ed": 89, "text": "deep neural networks"}, {"st": 95, "ed": 97, "text": "classification accuracy"}, {"st": 101, "ed": 103, "text": "computational costs"}, {"st": 104, "ed": 107, "text": "gaussian mixture models"}, {"st": 117, "ed": 120, "text": "support vector machines"}, {"st": 128, "ed": 131, "text": "accuracy and computational"}]
[{"st": 2, "ed": 4, "text": "deep learning"}, {"st": 8, "ed": 10, "text": "belief propagation"}, {"st": 18, "ed": 20, "text": "belief propagation"}, {"st": 37, "ed": 39, "text": "deep learning"}, {"st": 46, "ed": 48, "text": "belief propagation"}, {"st": 94, "ed": 96, "text": "belief propagation"}]
[{"st": 12, "ed": 14, "text": "large scale"}, {"st": 14, "ed": 16, "text": "neural networks"}, {"st": 17, "ed": 19, "text": "distributed computing"}, {"st": 21, "ed": 23, "text": "experiments demonstrate"}, {"st": 47, "ed": 49, "text": "neural networks"}, {"st": 54, "ed": 56, "text": "mnist handwritten"}]
[{"st": 21, "ed": 23, "text": "gradient based"}, {"st": 27, "ed": 30, "text": "rate of convergence"}, {"st": 51, "ed": 54, "text": "stochastic gradient descent"}, {"st": 71, "ed": 76, "text": "cifar 10 and cifar 100"}, {"st": 112, "ed": 114, "text": "source code"}, {"st": 115, "ed": 119, "text": "available at https github.com"}]
[{"st": 4, "ed": 7, "text": "recurrent neural network"}, {"st": 15, "ed": 19, "text": "convolutional neural networks cnns"}, {"st": 20, "ed": 22, "text": "local feature"}, {"st": 24, "ed": 27, "text": "recurrent neural networks"}, {"st": 71, "ed": 73, "text": "strong performance"}, {"st": 92, "ed": 94, "text": "feature extraction"}]
[{"st": 1, "ed": 3, "text": "learning process"}, {"st": 4, "ed": 6, "text": "multi layered"}, {"st": 6, "ed": 8, "text": "neural networks"}, {"st": 19, "ed": 21, "text": "neural network"}, {"st": 38, "ed": 40, "text": "weight matrix"}, {"st": 55, "ed": 57, "text": "principal components"}, {"st": 59, "ed": 61, "text": "weight matrix"}, {"st": 71, "ed": 73, "text": "quantum computing"}, {"st": 81, "ed": 83, "text": "speed ups"}, {"st": 107, "ed": 110, "text": "artificial neural networks"}, {"st": 114, "ed": 116, "text": "learning rule"}]
[{"st": 24, "ed": 26, "text": "convolutional layers"}, {"st": 42, "ed": 47, "text": "deep convolutional neural networks cnns"}, {"st": 85, "ed": 87, "text": "batch normalization"}, {"st": 87, "ed": 89, "text": "residual learning"}, {"st": 103, "ed": 105, "text": "fully convolutional"}, {"st": 109, "ed": 112, "text": "fully connected layers"}, {"st": 122, "ed": 124, "text": "receptive field"}, {"st": 127, "ed": 129, "text": "convolutional layer"}, {"st": 136, "ed": 138, "text": "receptive fields"}, {"st": 147, "ed": 149, "text": "performance gains"}, {"st": 154, "ed": 156, "text": "evaluation shows"}, {"st": 180, "ed": 182, "text": "recognition task"}]
[{"st": 1, "ed": 3, "text": "large data"}, {"st": 4, "ed": 9, "text": "deep convolutional neural networks cnns"}, {"st": 32, "ed": 34, "text": "parallel computing"}, {"st": 60, "ed": 62, "text": "existing works"}, {"st": 66, "ed": 68, "text": "computational efficiency"}]
[{"st": 31, "ed": 33, "text": "achieve high"}, {"st": 37, "ed": 39, "text": "time consuming"}, {"st": 50, "ed": 53, "text": "deep neural network"}, {"st": 76, "ed": 78, "text": "dnn based"}, {"st": 170, "ed": 172, "text": "proposed approach"}, {"st": 195, "ed": 197, "text": "real world"}, {"st": 208, "ed": 211, "text": "training and testing"}]
[{"st": 1, "ed": 4, "text": "deep neural networks"}, {"st": 9, "ed": 11, "text": "challenging problems"}, {"st": 51, "ed": 53, "text": "variational inference"}, {"st": 102, "ed": 106, "text": "convolutional neural networks cnns"}, {"st": 176, "ed": 179, "text": "spike and slab"}, {"st": 179, "ed": 181, "text": "variational distribution"}, {"st": 191, "ed": 194, "text": "spike and slab"}]
[{"st": 0, "ed": 4, "text": "generative adversarial networks gans"}, {"st": 10, "ed": 12, "text": "generative model"}, {"st": 88, "ed": 90, "text": "image generation"}, {"st": 93, "ed": 95, "text": "proposed framework"}, {"st": 101, "ed": 103, "text": "higher quality"}]
[{"st": 1, "ed": 3, "text": "sensing applications"}, {"st": 5, "ed": 7, "text": "time series"}, {"st": 35, "ed": 37, "text": "activity recognition"}, {"st": 137, "ed": 139, "text": "deep learning"}, {"st": 156, "ed": 159, "text": "convolutional and recurrent"}, {"st": 159, "ed": 161, "text": "neural networks"}, {"st": 214, "ed": 216, "text": "challenging tasks"}, {"st": 222, "ed": 225, "text": "human activity recognition"}, {"st": 233, "ed": 235, "text": "significantly outperforms"}, {"st": 261, "ed": 263, "text": "low latency"}]
[{"st": 14, "ed": 16, "text": "open question"}, {"st": 45, "ed": 47, "text": "supervised learning"}, {"st": 52, "ed": 54, "text": "recurrent connections"}, {"st": 100, "ed": 102, "text": "local learning"}]
[{"st": 3, "ed": 5, "text": "low complexity"}, {"st": 22, "ed": 24, "text": "open research"}, {"st": 32, "ed": 36, "text": "feed forward neural network"}, {"st": 41, "ed": 43, "text": "belief propagation"}, {"st": 55, "ed": 59, "text": "recurrent neural network architecture"}, {"st": 68, "ed": 71, "text": "bit error rate"}, {"st": 75, "ed": 79, "text": "feed forward neural network"}, {"st": 86, "ed": 88, "text": "improved performance"}, {"st": 89, "ed": 91, "text": "belief propagation"}, {"st": 117, "ed": 119, "text": "computational complexity"}, {"st": 124, "ed": 126, "text": "low complexity"}]
[{"st": 22, "ed": 26, "text": "deep neural network dnn"}, {"st": 26, "ed": 28, "text": "based methods"}, {"st": 50, "ed": 54, "text": "convolutional neural network cnn"}, {"st": 71, "ed": 75, "text": "gated recurrent unit gru"}, {"st": 76, "ed": 80, "text": "recurrent neural networks rnns"}, {"st": 108, "ed": 110, "text": "spatial features"}, {"st": 125, "ed": 128, "text": "detection and classification"}, {"st": 141, "ed": 143, "text": "dnn based"}, {"st": 151, "ed": 153, "text": "error rate"}, {"st": 163, "ed": 165, "text": "spatial features"}, {"st": 176, "ed": 179, "text": "end to end"}]
[{"st": 2, "ed": 5, "text": "end to end"}, {"st": 8, "ed": 10, "text": "hierarchical representations"}, {"st": 11, "ed": 13, "text": "raw data"}, {"st": 14, "ed": 18, "text": "deep convolutional neural networks"}, {"st": 24, "ed": 26, "text": "image text"}, {"st": 52, "ed": 56, "text": "deep convolutional neural networks"}, {"st": 72, "ed": 74, "text": "frame level"}, {"st": 80, "ed": 82, "text": "deep architectures"}, {"st": 130, "ed": 132, "text": "learned features"}]
[{"st": 11, "ed": 13, "text": "image classification"}, {"st": 29, "ed": 31, "text": "image classification"}, {"st": 50, "ed": 54, "text": "convolutional neural networks cnn"}, {"st": 58, "ed": 60, "text": "multi level"}, {"st": 75, "ed": 77, "text": "feature learning"}, {"st": 101, "ed": 103, "text": "pre trained"}, {"st": 103, "ed": 105, "text": "convolutional networks"}, {"st": 120, "ed": 122, "text": "fully connected"}, {"st": 138, "ed": 140, "text": "multi level"}, {"st": 141, "ed": 143, "text": "multi scale"}, {"st": 145, "ed": 147, "text": "highly effective"}, {"st": 153, "ed": 155, "text": "proposed method"}, {"st": 155, "ed": 157, "text": "outperforms previous"}]
[{"st": 0, "ed": 3, "text": "deep reinforcement learning"}, {"st": 4, "ed": 6, "text": "becoming increasingly"}, {"st": 21, "ed": 23, "text": "feature representations"}, {"st": 25, "ed": 27, "text": "sensory input"}, {"st": 66, "ed": 69, "text": "deep neural network"}]
[{"st": 1, "ed": 3, "text": "speech enhancement"}, {"st": 13, "ed": 15, "text": "higher level"}, {"st": 25, "ed": 27, "text": "noise conditions"}, {"st": 37, "ed": 39, "text": "deep networks"}, {"st": 46, "ed": 49, "text": "ability to learn"}, {"st": 63, "ed": 66, "text": "generative adversarial networks"}, {"st": 83, "ed": 86, "text": "end to end"}, {"st": 93, "ed": 95, "text": "noise conditions"}, {"st": 157, "ed": 159, "text": "speech enhancement"}]
[{"st": 0, "ed": 3, "text": "deep convolutional networks"}, {"st": 9, "ed": 11, "text": "machine learning"}, {"st": 37, "ed": 39, "text": "inductive bias"}, {"st": 41, "ed": 43, "text": "prior knowledge"}, {"st": 46, "ed": 48, "text": "network architecture"}, {"st": 64, "ed": 66, "text": "quantum physics"}, {"st": 91, "ed": 93, "text": "convolutional network"}, {"st": 110, "ed": 112, "text": "deep convolutional"}, {"st": 120, "ed": 122, "text": "wave function"}, {"st": 135, "ed": 137, "text": "quantum entanglement"}, {"st": 144, "ed": 146, "text": "deep network"}, {"st": 186, "ed": 188, "text": "convolutional network"}, {"st": 197, "ed": 199, "text": "inductive bias"}, {"st": 201, "ed": 203, "text": "deep network"}, {"st": 233, "ed": 235, "text": "theoretically analyze"}, {"st": 237, "ed": 239, "text": "empirically validate"}, {"st": 261, "ed": 264, "text": "deep convolutional network"}, {"st": 275, "ed": 277, "text": "quantum entanglement"}]
[{"st": 0, "ed": 2, "text": "unsupervised learning"}, {"st": 6, "ed": 8, "text": "associative memory"}, {"st": 27, "ed": 30, "text": "restricted boltzmann machine"}, {"st": 49, "ed": 51, "text": "building block"}, {"st": 54, "ed": 56, "text": "deep learning"}, {"st": 70, "ed": 72, "text": "internal representation"}]
[{"st": 24, "ed": 26, "text": "recently shown"}, {"st": 98, "ed": 100, "text": "theoretical framework"}, {"st": 172, "ed": 174, "text": "computational performance"}, {"st": 185, "ed": 187, "text": "computational performance"}, {"st": 194, "ed": 196, "text": "network architecture"}, {"st": 205, "ed": 207, "text": "neural activity"}, {"st": 208, "ed": 210, "text": "motor cortex"}, {"st": 219, "ed": 221, "text": "reinforcement learning"}]
[{"st": 14, "ed": 16, "text": "domain specific"}, {"st": 40, "ed": 42, "text": "neural networks"}, {"st": 111, "ed": 115, "text": "out of order execution"}]
[{"st": 57, "ed": 59, "text": "neural network"}, {"st": 109, "ed": 111, "text": "differential evolution"}, {"st": 126, "ed": 128, "text": "differential evolution"}]
[{"st": 3, "ed": 5, "text": "maximum likelihood"}, {"st": 14, "ed": 16, "text": "extrasolar planet"}, {"st": 32, "ed": 34, "text": "highly nonlinear"}, {"st": 46, "ed": 48, "text": "parameter space"}, {"st": 66, "ed": 68, "text": "challenging problem"}, {"st": 87, "ed": 89, "text": "method called"}, {"st": 100, "ed": 102, "text": "parameter space"}, {"st": 141, "ed": 143, "text": "large scale"}, {"st": 149, "ed": 151, "text": "random sampling"}, {"st": 188, "ed": 191, "text": "curse of dimensionality"}, {"st": 203, "ed": 205, "text": "lower dimensional"}, {"st": 210, "ed": 212, "text": "proposed algorithm"}, {"st": 220, "ed": 222, "text": "real data"}]
[{"st": 1, "ed": 3, "text": "neural networks"}, {"st": 44, "ed": 46, "text": "neural architectures"}, {"st": 69, "ed": 71, "text": "bubble sort"}, {"st": 101, "ed": 103, "text": "neural network"}, {"st": 123, "ed": 125, "text": "neural architectures"}]
[{"st": 3, "ed": 5, "text": "membrane protein"}, {"st": 65, "ed": 67, "text": "transfer learning"}, {"st": 70, "ed": 72, "text": "significantly improve"}, {"st": 88, "ed": 90, "text": "membrane proteins"}, {"st": 100, "ed": 102, "text": "deep model"}, {"st": 114, "ed": 116, "text": "prediction accuracy"}, {"st": 120, "ed": 122, "text": "deep model"}, {"st": 148, "ed": 150, "text": "deep model"}, {"st": 227, "ed": 229, "text": "deep model"}, {"st": 256, "ed": 258, "text": "deep learning"}]
[{"st": 3, "ed": 5, "text": "neural network"}, {"st": 39, "ed": 41, "text": "von neumann"}, {"st": 61, "ed": 63, "text": "mathematical model"}, {"st": 72, "ed": 74, "text": "sparse coding"}]
[{"st": 18, "ed": 20, "text": "stochastic optimization"}, {"st": 35, "ed": 37, "text": "recently proposed"}, {"st": 70, "ed": 72, "text": "transformation matrix"}, {"st": 87, "ed": 89, "text": "mathcal o"}, {"st": 98, "ed": 100, "text": "mathcal o"}, {"st": 108, "ed": 110, "text": "limited memory"}, {"st": 121, "ed": 123, "text": "large scale"}, {"st": 137, "ed": 139, "text": "large scale"}, {"st": 148, "ed": 150, "text": "generating adversarial"}, {"st": 155, "ed": 157, "text": "random forest"}]
[{"st": 30, "ed": 32, "text": "deep learning"}, {"st": 73, "ed": 75, "text": "layer wise"}, {"st": 95, "ed": 97, "text": "accuracy loss"}, {"st": 103, "ed": 105, "text": "accuracy loss"}, {"st": 139, "ed": 141, "text": "source code"}]
[{"st": 25, "ed": 27, "text": "training phase"}, {"st": 36, "ed": 38, "text": "multiple levels"}, {"st": 49, "ed": 51, "text": "space exploration"}, {"st": 58, "ed": 60, "text": "convergence rates"}]
[{"st": 11, "ed": 14, "text": "recurrent neural networks"}, {"st": 35, "ed": 37, "text": "expressive power"}]
[{"st": 8, "ed": 12, "text": "recurrent neural network rnn"}, {"st": 17, "ed": 19, "text": "rnn models"}, {"st": 23, "ed": 25, "text": "natural language"}, {"st": 26, "ed": 28, "text": "machine translation"}, {"st": 37, "ed": 39, "text": "rnn models"}, {"st": 53, "ed": 55, "text": "rnn models"}, {"st": 66, "ed": 68, "text": "deep learning"}, {"st": 75, "ed": 77, "text": "neural networks"}, {"st": 94, "ed": 96, "text": "optimization framework"}, {"st": 107, "ed": 109, "text": "rnn model"}, {"st": 110, "ed": 112, "text": "activity recognition"}, {"st": 122, "ed": 124, "text": "rnn models"}]
[{"st": 0, "ed": 2, "text": "recent advances"}, {"st": 3, "ed": 5, "text": "deep learning"}, {"st": 13, "ed": 15, "text": "sensing applications"}, {"st": 22, "ed": 24, "text": "embedded devices"}, {"st": 39, "ed": 42, "text": "deep neural networks"}, {"st": 76, "ed": 78, "text": "neural networks"}, {"st": 88, "ed": 90, "text": "deep learning"}, {"st": 92, "ed": 94, "text": "sensing applications"}, {"st": 95, "ed": 97, "text": "fully connected"}, {"st": 97, "ed": 100, "text": "convolutional and recurrent"}, {"st": 100, "ed": 102, "text": "neural networks"}, {"st": 113, "ed": 115, "text": "weight matrices"}, {"st": 120, "ed": 122, "text": "weight matrices"}, {"st": 124, "ed": 126, "text": "neural network"}, {"st": 155, "ed": 157, "text": "sensing applications"}, {"st": 182, "ed": 184, "text": "conduct experiments"}, {"st": 188, "ed": 190, "text": "related tasks"}, {"st": 217, "ed": 220, "text": "deep neural networks"}, {"st": 266, "ed": 269, "text": "deep neural networks"}, {"st": 270, "ed": 272, "text": "resource constrained"}]
[{"st": 7, "ed": 10, "text": "deep neural networks"}, {"st": 15, "ed": 17, "text": "feed forward"}, {"st": 52, "ed": 55, "text": "artificial neural network"}, {"st": 65, "ed": 67, "text": "standard benchmark"}, {"st": 73, "ed": 75, "text": "feed forward"}, {"st": 81, "ed": 83, "text": "feed forward"}]
[{"st": 7, "ed": 9, "text": "deep learning"}]
[{"st": 8, "ed": 11, "text": "jet propulsion laboratory"}, {"st": 68, "ed": 70, "text": "neural networks"}, {"st": 89, "ed": 91, "text": "accurate prediction"}, {"st": 103, "ed": 105, "text": "accurate predictions"}]
[{"st": 0, "ed": 4, "text": "convolutional neural networks cnns"}, {"st": 6, "ed": 8, "text": "successfully applied"}, {"st": 12, "ed": 14, "text": "generative modeling"}, {"st": 23, "ed": 25, "text": "trained cnn"}, {"st": 29, "ed": 31, "text": "decision making"}, {"st": 88, "ed": 90, "text": "convolutional layers"}, {"st": 105, "ed": 107, "text": "randomly selected"}]
[{"st": 4, "ed": 6, "text": "neural network"}, {"st": 9, "ed": 11, "text": "signal processing"}, {"st": 36, "ed": 38, "text": "dimensional vector"}, {"st": 39, "ed": 41, "text": "neural network"}]
[{"st": 9, "ed": 11, "text": "neural network"}, {"st": 59, "ed": 61, "text": "applications including"}, {"st": 66, "ed": 68, "text": "hand crafted"}, {"st": 103, "ed": 105, "text": "neural networks"}]
[{"st": 6, "ed": 8, "text": "increasingly popular"}, {"st": 57, "ed": 60, "text": "deep neural networks"}, {"st": 62, "ed": 64, "text": "accurate estimates"}, {"st": 71, "ed": 73, "text": "competing methods"}, {"st": 74, "ed": 76, "text": "logistic regression"}, {"st": 76, "ed": 78, "text": "random forest"}, {"st": 83, "ed": 85, "text": "classification performance"}, {"st": 92, "ed": 95, "text": "cross entropy loss"}, {"st": 118, "ed": 121, "text": "trained neural network"}]
[{"st": 13, "ed": 15, "text": "machine learning"}, {"st": 19, "ed": 21, "text": "generating adversarial"}, {"st": 61, "ed": 63, "text": "feed forward"}, {"st": 66, "ed": 68, "text": "machine learning"}]
[{"st": 23, "ed": 26, "text": "neural network based"}, {"st": 70, "ed": 72, "text": "rgb d"}, {"st": 101, "ed": 103, "text": "success rate"}, {"st": 115, "ed": 117, "text": "real world"}]
[{"st": 0, "ed": 2, "text": "deep learning"}, {"st": 41, "ed": 43, "text": "computation graph"}, {"st": 80, "ed": 82, "text": "computer vision"}]
[{"st": 0, "ed": 4, "text": "convolutional neural networks cnns"}, {"st": 53, "ed": 55, "text": "improves performance"}, {"st": 67, "ed": 69, "text": "network pruning"}, {"st": 96, "ed": 99, "text": "weights and activations"}, {"st": 121, "ed": 124, "text": "weights and activations"}, {"st": 153, "ed": 155, "text": "neural networks"}]
[{"st": 19, "ed": 22, "text": "principal component analysis"}, {"st": 32, "ed": 34, "text": "network structures"}]
[{"st": 33, "ed": 36, "text": "end to end"}, {"st": 211, "ed": 213, "text": "decision process"}]
[{"st": 1, "ed": 4, "text": "convolutional neural networks"}, {"st": 21, "ed": 23, "text": "input space"}, {"st": 38, "ed": 40, "text": "deep learning"}, {"st": 50, "ed": 53, "text": "convolutional neural networks"}, {"st": 60, "ed": 62, "text": "phylogenetic tree"}, {"st": 81, "ed": 83, "text": "multidimensional scaling"}, {"st": 86, "ed": 88, "text": "phylogenetic tree"}, {"st": 99, "ed": 101, "text": "domain adaptation"}, {"st": 103, "ed": 105, "text": "synthetic data"}, {"st": 119, "ed": 122, "text": "inflammatory bowel disease"}, {"st": 127, "ed": 129, "text": "classification performance"}, {"st": 137, "ed": 140, "text": "support vector machines"}, {"st": 141, "ed": 143, "text": "random forest"}, {"st": 146, "ed": 148, "text": "fully connected"}, {"st": 148, "ed": 150, "text": "neural network"}, {"st": 161, "ed": 163, "text": "deep learning"}, {"st": 188, "ed": 190, "text": "convolutional layer"}, {"st": 209, "ed": 211, "text": "image data"}, {"st": 217, "ed": 219, "text": "deep learning"}, {"st": 219, "ed": 222, "text": "convolutional neural networks"}]
[{"st": 0, "ed": 2, "text": "recent studies"}, {"st": 21, "ed": 23, "text": "supervised learning"}]
[{"st": 12, "ed": 14, "text": "machine learning"}, {"st": 29, "ed": 31, "text": "neural networks"}, {"st": 70, "ed": 72, "text": "genetic algorithms"}, {"st": 88, "ed": 90, "text": "directly optimize"}]
[{"st": 0, "ed": 4, "text": "recurrent neural networks rnns"}, {"st": 5, "ed": 8, "text": "shown promising results"}, {"st": 11, "ed": 13, "text": "speech processing"}, {"st": 50, "ed": 52, "text": "rnn based"}, {"st": 74, "ed": 76, "text": "rnn based"}, {"st": 97, "ed": 100, "text": "end to end"}, {"st": 111, "ed": 113, "text": "rnn models"}, {"st": 129, "ed": 131, "text": "rnn models"}, {"st": 136, "ed": 138, "text": "resource constrained"}]
[{"st": 5, "ed": 7, "text": "deep learning"}, {"st": 101, "ed": 103, "text": "deep learning"}]
[{"st": 5, "ed": 7, "text": "deep learning"}, {"st": 41, "ed": 43, "text": "deep learning"}, {"st": 63, "ed": 65, "text": "deep learning"}, {"st": 76, "ed": 78, "text": "learning process"}, {"st": 80, "ed": 82, "text": "resnet 50"}, {"st": 85, "ed": 87, "text": "imagenet dataset"}]
[{"st": 10, "ed": 12, "text": "neural networks"}, {"st": 15, "ed": 17, "text": "low precision"}, {"st": 23, "ed": 26, "text": "trained from scratch"}, {"st": 29, "ed": 31, "text": "fixed point"}, {"st": 43, "ed": 45, "text": "network architectures"}, {"st": 111, "ed": 114, "text": "orders of magnitude"}, {"st": 161, "ed": 163, "text": "https github.com"}]
[{"st": 8, "ed": 10, "text": "low latency"}, {"st": 14, "ed": 17, "text": "deep neural network"}, {"st": 43, "ed": 47, "text": "multi layer perceptron mlp"}, {"st": 48, "ed": 50, "text": "neural network"}, {"st": 52, "ed": 56, "text": "recurrent neural network rnn"}, {"st": 67, "ed": 70, "text": "deep neural networks"}, {"st": 75, "ed": 77, "text": "numerical results"}, {"st": 106, "ed": 108, "text": "neural network"}]
[{"st": 0, "ed": 2, "text": "l 2"}, {"st": 4, "ed": 6, "text": "weight decay"}, {"st": 11, "ed": 14, "text": "stochastic gradient descent"}, {"st": 18, "ed": 20, "text": "learning rate"}, {"st": 39, "ed": 41, "text": "deep learning"}, {"st": 46, "ed": 48, "text": "l 2"}, {"st": 52, "ed": 54, "text": "weight decay"}, {"st": 76, "ed": 78, "text": "weight decay"}, {"st": 82, "ed": 84, "text": "weight decay"}, {"st": 95, "ed": 97, "text": "empirical evidence"}, {"st": 107, "ed": 109, "text": "weight decay"}, {"st": 115, "ed": 117, "text": "learning rate"}, {"st": 125, "ed": 127, "text": "substantially improves"}, {"st": 129, "ed": 131, "text": "generalization performance"}, {"st": 140, "ed": 142, "text": "image classification"}, {"st": 178, "ed": 180, "text": "cifar 10"}, {"st": 183, "ed": 185, "text": "source code"}, {"st": 186, "ed": 190, "text": "available at https github.com"}]
[{"st": 0, "ed": 4, "text": "recurrent neural networks rnns"}, {"st": 22, "ed": 24, "text": "low power"}, {"st": 55, "ed": 58, "text": "short term memory"}, {"st": 86, "ed": 88, "text": "rnn models"}]
[{"st": 5, "ed": 9, "text": "generative adversarial networks gans"}, {"st": 10, "ed": 12, "text": "speech enhancement"}, {"st": 20, "ed": 24, "text": "automatic speech recognition asr"}, {"st": 33, "ed": 35, "text": "additive noise"}, {"st": 38, "ed": 40, "text": "speech signals"}, {"st": 81, "ed": 83, "text": "recent advances"}, {"st": 84, "ed": 86, "text": "image processing"}, {"st": 152, "ed": 154, "text": "a 7"}]
[{"st": 7, "ed": 9, "text": "time series"}, {"st": 15, "ed": 18, "text": "electronic health records"}, {"st": 25, "ed": 28, "text": "recurrent neural networks"}, {"st": 33, "ed": 35, "text": "neural networks"}, {"st": 41, "ed": 43, "text": "time series"}, {"st": 64, "ed": 66, "text": "missing values"}, {"st": 70, "ed": 74, "text": "recurrent neural network rnn"}, {"st": 77, "ed": 79, "text": "recently proposed"}, {"st": 79, "ed": 81, "text": "recurrent architecture"}, {"st": 82, "ed": 85, "text": "gated recurrent unit"}, {"st": 87, "ed": 89, "text": "specifically designed"}, {"st": 107, "ed": 109, "text": "time series"}, {"st": 122, "ed": 124, "text": "rnn based"}]
[{"st": 11, "ed": 13, "text": "random variable"}]
[{"st": 2, "ed": 4, "text": "weight update"}, {"st": 15, "ed": 17, "text": "neural networks"}, {"st": 46, "ed": 48, "text": "deep learning"}, {"st": 53, "ed": 55, "text": "activation functions"}, {"st": 58, "ed": 60, "text": "weight update"}, {"st": 77, "ed": 79, "text": "layer perceptron"}, {"st": 84, "ed": 86, "text": "classification accuracy"}, {"st": 87, "ed": 89, "text": "mnist handwritten"}, {"st": 104, "ed": 106, "text": "extremely high"}]
[{"st": 65, "ed": 67, "text": "prediction error"}]
[{"st": 3, "ed": 5, "text": "neural networks"}, {"st": 114, "ed": 117, "text": "spiking neural networks"}, {"st": 120, "ed": 123, "text": "artificial neural networks"}]
[{"st": 10, "ed": 12, "text": "neural network"}, {"st": 40, "ed": 42, "text": "learning algorithm"}, {"st": 51, "ed": 54, "text": "text to speech"}, {"st": 67, "ed": 69, "text": "back propagation"}, {"st": 69, "ed": 71, "text": "learning algorithm"}, {"st": 127, "ed": 129, "text": "neural network"}]
[{"st": 0, "ed": 2, "text": "machine learning"}, {"st": 11, "ed": 13, "text": "application domains"}, {"st": 27, "ed": 29, "text": "machine learning"}, {"st": 31, "ed": 33, "text": "neural networks"}, {"st": 38, "ed": 40, "text": "machine learning"}, {"st": 48, "ed": 51, "text": "end to end"}, {"st": 97, "ed": 99, "text": "case study"}, {"st": 104, "ed": 106, "text": "computer vision"}]
[{"st": 0, "ed": 3, "text": "deep neural networks"}, {"st": 4, "ed": 6, "text": "becoming increasingly"}, {"st": 48, "ed": 50, "text": "neural network"}, {"st": 63, "ed": 65, "text": "neural network"}]
[{"st": 15, "ed": 17, "text": "low precision"}, {"st": 17, "ed": 19, "text": "floating point"}, {"st": 75, "ed": 77, "text": "cifar 10"}, {"st": 89, "ed": 91, "text": "neural networks"}, {"st": 141, "ed": 143, "text": "fixed point"}, {"st": 148, "ed": 150, "text": "neural network"}, {"st": 176, "ed": 178, "text": "resnet 50"}, {"st": 180, "ed": 182, "text": "vgg 16"}, {"st": 195, "ed": 198, "text": "number of iterations"}, {"st": 206, "ed": 208, "text": "hyper parameters"}, {"st": 214, "ed": 217, "text": "end to end"}]
[{"st": 32, "ed": 34, "text": "biological neural"}, {"st": 120, "ed": 123, "text": "end to end"}, {"st": 225, "ed": 227, "text": "machine learning"}, {"st": 240, "ed": 242, "text": "neural nets"}, {"st": 262, "ed": 264, "text": "biologically plausible"}, {"st": 264, "ed": 266, "text": "optimization method"}]
[{"st": 14, "ed": 17, "text": "deep neural network"}, {"st": 38, "ed": 40, "text": "without compromising"}, {"st": 40, "ed": 42, "text": "classification accuracy"}, {"st": 82, "ed": 84, "text": "image recognition"}, {"st": 91, "ed": 93, "text": "classification accuracy"}]
[]
[{"st": 7, "ed": 10, "text": "deep neural networks"}, {"st": 52, "ed": 54, "text": "competing methods"}, {"st": 58, "ed": 60, "text": "exponentially large"}, {"st": 123, "ed": 127, "text": "recurrent neural network rnn"}]
[{"st": 11, "ed": 13, "text": "computational requirements"}, {"st": 19, "ed": 21, "text": "current approaches"}, {"st": 23, "ed": 25, "text": "training process"}, {"st": 58, "ed": 60, "text": "large scale"}, {"st": 60, "ed": 62, "text": "distributed training"}, {"st": 78, "ed": 80, "text": "layer wise"}, {"st": 112, "ed": 114, "text": "layer wise"}, {"st": 121, "ed": 123, "text": "training speed"}]
[{"st": 25, "ed": 27, "text": "predictive models"}, {"st": 27, "ed": 29, "text": "previous studies"}, {"st": 43, "ed": 45, "text": "machine learning"}, {"st": 87, "ed": 90, "text": "deep neural networks"}, {"st": 96, "ed": 98, "text": "control flow"}, {"st": 108, "ed": 110, "text": "source code"}, {"st": 113, "ed": 115, "text": "multi view"}, {"st": 115, "ed": 117, "text": "multi layer"}, {"st": 120, "ed": 123, "text": "convolutional neural networks"}, {"st": 132, "ed": 134, "text": "real world"}, {"st": 139, "ed": 141, "text": "significantly outperforms"}, {"st": 146, "ed": 148, "text": "deep learning"}]
[{"st": 10, "ed": 12, "text": "biological neural"}, {"st": 27, "ed": 29, "text": "machine learning"}, {"st": 38, "ed": 40, "text": "biological neural"}, {"st": 53, "ed": 55, "text": "biological neural"}, {"st": 60, "ed": 62, "text": "competitive inhibition"}, {"st": 141, "ed": 143, "text": "training samples"}, {"st": 159, "ed": 161, "text": "machine learning"}, {"st": 164, "ed": 166, "text": "nearest neighbors"}, {"st": 166, "ed": 169, "text": "support vector machines"}, {"st": 170, "ed": 174, "text": "convolutional neural networks cnns"}, {"st": 194, "ed": 197, "text": "deep neural nets"}, {"st": 206, "ed": 208, "text": "learning rate"}, {"st": 213, "ed": 215, "text": "neural network"}]
[{"st": 3, "ed": 5, "text": "deep learning"}, {"st": 26, "ed": 29, "text": "generative adversarial nets"}, {"st": 38, "ed": 40, "text": "gradient based"}, {"st": 49, "ed": 51, "text": "becoming increasingly"}, {"st": 55, "ed": 57, "text": "multi objective"}, {"st": 118, "ed": 120, "text": "conservation law"}, {"st": 141, "ed": 143, "text": "fixed points"}, {"st": 153, "ed": 155, "text": "recently proposed"}, {"st": 159, "ed": 161, "text": "nash equilibria"}]
[{"st": 0, "ed": 2, "text": "air traffic"}, {"st": 101, "ed": 103, "text": "decision support"}, {"st": 139, "ed": 141, "text": "genetic algorithm"}, {"st": 150, "ed": 152, "text": "finite state"}]
[{"st": 16, "ed": 18, "text": "higgs boson"}, {"st": 37, "ed": 39, "text": "prior knowledge"}, {"st": 52, "ed": 54, "text": "multi instance"}]
[{"st": 3, "ed": 5, "text": "multi layer"}, {"st": 5, "ed": 9, "text": "feed forward neural networks"}, {"st": 38, "ed": 41, "text": "municipal solid waste"}, {"st": 46, "ed": 48, "text": "fluidized bed"}, {"st": 50, "ed": 53, "text": "artificial neural networks"}, {"st": 64, "ed": 66, "text": "back propagation"}, {"st": 69, "ed": 71, "text": "cross validation"}, {"st": 96, "ed": 98, "text": "hidden layers"}, {"st": 103, "ed": 105, "text": "hidden layer"}, {"st": 106, "ed": 108, "text": "activation function"}, {"st": 113, "ed": 115, "text": "monte carlo"}, {"st": 129, "ed": 131, "text": "neural network"}, {"st": 134, "ed": 136, "text": "multiple output"}, {"st": 157, "ed": 159, "text": "network architecture"}, {"st": 187, "ed": 189, "text": "fluidized bed"}]
[{"st": 4, "ed": 6, "text": "machine learning"}, {"st": 76, "ed": 78, "text": "learning algorithm"}, {"st": 113, "ed": 115, "text": "predictive accuracy"}]
[{"st": 8, "ed": 10, "text": "cross validation"}, {"st": 15, "ed": 17, "text": "real valued"}, {"st": 25, "ed": 27, "text": "real valued"}, {"st": 53, "ed": 55, "text": "cross validation"}, {"st": 68, "ed": 70, "text": "linear regression"}, {"st": 71, "ed": 73, "text": "instance based"}]
[{"st": 10, "ed": 12, "text": "cross validation"}, {"st": 16, "ed": 18, "text": "supervised learning"}, {"st": 37, "ed": 39, "text": "cross validation"}, {"st": 43, "ed": 45, "text": "training examples"}, {"st": 68, "ed": 70, "text": "training set"}, {"st": 80, "ed": 82, "text": "cross validation"}, {"st": 89, "ed": 91, "text": "training set"}, {"st": 107, "ed": 109, "text": "instance based"}, {"st": 119, "ed": 121, "text": "instance based"}, {"st": 121, "ed": 123, "text": "learning algorithm"}, {"st": 131, "ed": 134, "text": "k nearest neighbors"}, {"st": 158, "ed": 160, "text": "instance based"}, {"st": 163, "ed": 165, "text": "theoretical analysis"}, {"st": 182, "ed": 184, "text": "cross validation"}]
[{"st": 18, "ed": 20, "text": "real world"}, {"st": 36, "ed": 38, "text": "machine learning"}, {"st": 107, "ed": 109, "text": "cost sensitive"}]
[{"st": 4, "ed": 7, "text": "problem of classifying"}, {"st": 11, "ed": 13, "text": "context sensitive"}, {"st": 48, "ed": 50, "text": "classification algorithms"}, {"st": 69, "ed": 71, "text": "gas turbine"}]
[{"st": 10, "ed": 12, "text": "context sensitive"}, {"st": 14, "ed": 17, "text": "supervised machine learning"}, {"st": 49, "ed": 51, "text": "machine learning"}]
[{"st": 6, "ed": 8, "text": "machine learning"}, {"st": 11, "ed": 13, "text": "supervised learning"}, {"st": 24, "ed": 26, "text": "multi dimensional"}, {"st": 26, "ed": 28, "text": "feature space"}, {"st": 41, "ed": 43, "text": "training examples"}, {"st": 53, "ed": 55, "text": "learning algorithm"}, {"st": 91, "ed": 93, "text": "context sensitive"}, {"st": 96, "ed": 98, "text": "context sensitive"}, {"st": 122, "ed": 124, "text": "automatically identify"}, {"st": 124, "ed": 126, "text": "context sensitive"}, {"st": 128, "ed": 130, "text": "context sensitive"}, {"st": 170, "ed": 172, "text": "previously proposed"}]
[{"st": 4, "ed": 7, "text": "problem of classifying"}, {"st": 11, "ed": 13, "text": "context sensitive"}, {"st": 48, "ed": 50, "text": "classification algorithms"}, {"st": 69, "ed": 71, "text": "gas turbine"}]
[{"st": 11, "ed": 13, "text": "low dimensional"}, {"st": 20, "ed": 22, "text": "higher dimensional"}, {"st": 42, "ed": 44, "text": "closely related"}, {"st": 50, "ed": 52, "text": "intrinsic dimension"}, {"st": 85, "ed": 87, "text": "approach called"}, {"st": 90, "ed": 92, "text": "spanning tree"}, {"st": 146, "ed": 148, "text": "spanning tree"}]
[{"st": 2, "ed": 4, "text": "ensemble methods"}, {"st": 67, "ed": 69, "text": "point set"}]
[{"st": 6, "ed": 8, "text": "neural network"}, {"st": 14, "ed": 16, "text": "input data"}, {"st": 48, "ed": 50, "text": "objective function"}]
[{"st": 0, "ed": 3, "text": "canonical correlation analysis"}, {"st": 34, "ed": 36, "text": "kernel method"}, {"st": 38, "ed": 41, "text": "support vector machine"}, {"st": 60, "ed": 62, "text": "kernel method"}]
[{"st": 31, "ed": 33, "text": "latent space"}, {"st": 39, "ed": 42, "text": "markov random field"}, {"st": 51, "ed": 53, "text": "recent advances"}]
[{"st": 5, "ed": 7, "text": "pattern recognition"}, {"st": 17, "ed": 19, "text": "computer vision"}, {"st": 31, "ed": 33, "text": "pattern recognition"}, {"st": 57, "ed": 59, "text": "assignment problem"}, {"st": 65, "ed": 67, "text": "objective function"}, {"st": 87, "ed": 89, "text": "efficient algorithms"}, {"st": 94, "ed": 96, "text": "assignment problem"}, {"st": 124, "ed": 126, "text": "graph matching"}, {"st": 147, "ed": 149, "text": "training examples"}, {"st": 167, "ed": 169, "text": "substantially improve"}, {"st": 187, "ed": 189, "text": "learning scheme"}]
[{"st": 29, "ed": 31, "text": "image datasets"}, {"st": 66, "ed": 68, "text": "graphical model"}, {"st": 122, "ed": 124, "text": "substantial improvements"}]
[{"st": 3, "ed": 5, "text": "low cost"}, {"st": 62, "ed": 64, "text": "low dimensional"}, {"st": 86, "ed": 88, "text": "low dimensional"}, {"st": 136, "ed": 138, "text": "signal processing"}, {"st": 140, "ed": 142, "text": "recent results"}, {"st": 143, "ed": 145, "text": "dimensionality reduction"}, {"st": 154, "ed": 156, "text": "data compression"}, {"st": 159, "ed": 161, "text": "random projections"}]
[{"st": 2, "ed": 4, "text": "boosting algorithms"}, {"st": 19, "ed": 21, "text": "soft margin"}, {"st": 24, "ed": 26, "text": "hinge loss"}, {"st": 39, "ed": 41, "text": "boosting algorithms"}, {"st": 47, "ed": 49, "text": "boosting algorithms"}, {"st": 97, "ed": 99, "text": "based optimization"}, {"st": 111, "ed": 113, "text": "classification results"}, {"st": 120, "ed": 122, "text": "boosting algorithms"}, {"st": 125, "ed": 127, "text": "faster convergence"}, {"st": 130, "ed": 132, "text": "weak classifiers"}]
[{"st": 13, "ed": 15, "text": "boosting algorithms"}, {"st": 30, "ed": 32, "text": "generalization error"}, {"st": 45, "ed": 47, "text": "training data"}, {"st": 53, "ed": 55, "text": "boosting algorithms"}, {"st": 60, "ed": 63, "text": "convex loss function"}, {"st": 79, "ed": 81, "text": "boosting algorithm"}, {"st": 108, "ed": 110, "text": "optimization algorithm"}]
[{"st": 4, "ed": 6, "text": "parameter free"}, {"st": 6, "ed": 8, "text": "similarity measure"}, {"st": 13, "ed": 15, "text": "pattern recognition"}, {"st": 77, "ed": 79, "text": "real world"}, {"st": 87, "ed": 89, "text": "pattern recognition"}, {"st": 89, "ed": 91, "text": "data mining"}]
[{"st": 1, "ed": 3, "text": "feature mapping"}, {"st": 49, "ed": 51, "text": "learning algorithm"}, {"st": 74, "ed": 76, "text": "previous methods"}, {"st": 92, "ed": 94, "text": "learning algorithm"}, {"st": 129, "ed": 131, "text": "learning algorithms"}]
[{"st": 24, "ed": 26, "text": "based optimization"}, {"st": 35, "ed": 37, "text": "optimal solution"}, {"st": 41, "ed": 43, "text": "image classification"}, {"st": 58, "ed": 60, "text": "land cover"}, {"st": 85, "ed": 87, "text": "image classification"}, {"st": 101, "ed": 103, "text": "feature extraction"}, {"st": 119, "ed": 121, "text": "land cover"}, {"st": 174, "ed": 176, "text": "highly accurate"}, {"st": 176, "ed": 178, "text": "land cover"}, {"st": 185, "ed": 187, "text": "proposed algorithm"}]
[{"st": 6, "ed": 8, "text": "gesture recognition"}, {"st": 19, "ed": 21, "text": "method generates"}, {"st": 27, "ed": 29, "text": "optical flow"}, {"st": 39, "ed": 41, "text": "feature vector"}, {"st": 70, "ed": 72, "text": "sign language"}]
[{"st": 9, "ed": 12, "text": "static and dynamic"}, {"st": 174, "ed": 176, "text": "computer vision"}]
[{"st": 3, "ed": 6, "text": "support vector machines"}, {"st": 9, "ed": 11, "text": "multiple classifiers"}, {"st": 20, "ed": 23, "text": "global and local"}, {"st": 40, "ed": 42, "text": "mahalanobis distance"}]
[{"st": 1, "ed": 3, "text": "intrinsic dimensionality"}, {"st": 10, "ed": 12, "text": "pattern recognition"}, {"st": 14, "ed": 17, "text": "principal component analysis"}, {"st": 20, "ed": 22, "text": "powerful tool"}, {"st": 49, "ed": 51, "text": "pca based"}, {"st": 54, "ed": 56, "text": "intrinsic dimension"}, {"st": 62, "ed": 64, "text": "method works"}, {"st": 102, "ed": 104, "text": "proposed method"}, {"st": 112, "ed": 114, "text": "intrinsic dimension"}, {"st": 144, "ed": 150, "text": "experiments on synthetic and real world"}]
[{"st": 34, "ed": 37, "text": "optical character recognition"}, {"st": 86, "ed": 88, "text": "feature set"}, {"st": 197, "ed": 199, "text": "recognition rate"}, {"st": 203, "ed": 206, "text": "fold cross validation"}]
[{"st": 6, "ed": 8, "text": "remote sensing"}, {"st": 35, "ed": 37, "text": "remote sensing"}, {"st": 37, "ed": 39, "text": "image classification"}, {"st": 54, "ed": 56, "text": "image classification"}, {"st": 63, "ed": 65, "text": "remote sensing"}, {"st": 77, "ed": 79, "text": "remote sensing"}, {"st": 81, "ed": 83, "text": "classification algorithms"}, {"st": 93, "ed": 95, "text": "remote sensing"}, {"st": 106, "ed": 108, "text": "remote sensing"}, {"st": 117, "ed": 120, "text": "supervised and unsupervised"}, {"st": 126, "ed": 128, "text": "classification methods"}]
[{"st": 12, "ed": 14, "text": "facial expressions"}, {"st": 17, "ed": 19, "text": "image sequences"}, {"st": 29, "ed": 32, "text": "linear discriminant analysis"}, {"st": 37, "ed": 39, "text": "dimensionality reduction"}, {"st": 55, "ed": 58, "text": "linear discriminant analysis"}, {"st": 102, "ed": 104, "text": "small sample"}, {"st": 121, "ed": 123, "text": "ensemble learning"}, {"st": 126, "ed": 128, "text": "data fusion"}, {"st": 140, "ed": 142, "text": "proposed method"}, {"st": 152, "ed": 154, "text": "temporal information"}, {"st": 181, "ed": 184, "text": "linear discriminant analysis"}, {"st": 185, "ed": 187, "text": "subspace learning"}, {"st": 187, "ed": 189, "text": "facial expression"}]
[{"st": 0, "ed": 4, "text": "facial action coding system"}, {"st": 15, "ed": 18, "text": "hidden markov models"}, {"st": 26, "ed": 28, "text": "facial action"}, {"st": 101, "ed": 103, "text": "neural network"}, {"st": 140, "ed": 142, "text": "neural network"}, {"st": 162, "ed": 164, "text": "dimension reduction"}, {"st": 177, "ed": 179, "text": "temporal information"}, {"st": 186, "ed": 188, "text": "extensive experiments"}, {"st": 197, "ed": 199, "text": "proposed method"}, {"st": 209, "ed": 211, "text": "data fusion"}, {"st": 211, "ed": 213, "text": "facial action"}, {"st": 215, "ed": 218, "text": "hidden markov models"}, {"st": 219, "ed": 221, "text": "neural network"}]
[{"st": 10, "ed": 12, "text": "facial action"}, {"st": 59, "ed": 61, "text": "dimensionality reduction"}, {"st": 71, "ed": 73, "text": "input image"}, {"st": 84, "ed": 87, "text": "positive and negative"}, {"st": 91, "ed": 94, "text": "curse of dimensionality"}, {"st": 95, "ed": 97, "text": "extensive experiments"}, {"st": 106, "ed": 108, "text": "proposed method"}, {"st": 116, "ed": 118, "text": "temporal information"}, {"st": 144, "ed": 146, "text": "facial expressions"}, {"st": 146, "ed": 148, "text": "multi modal"}, {"st": 153, "ed": 155, "text": "lie detection"}]
[{"st": 5, "ed": 7, "text": "method called"}, {"st": 37, "ed": 39, "text": "main diagonal"}, {"st": 65, "ed": 67, "text": "main diagonal"}, {"st": 121, "ed": 123, "text": "recognition accuracy"}, {"st": 137, "ed": 139, "text": "face database"}, {"st": 143, "ed": 145, "text": "recognition accuracy"}]
[{"st": 42, "ed": 44, "text": "real world"}, {"st": 51, "ed": 53, "text": "quality assessment"}, {"st": 62, "ed": 64, "text": "quality assessment"}, {"st": 105, "ed": 107, "text": "local neighborhood"}, {"st": 128, "ed": 131, "text": "local and global"}, {"st": 157, "ed": 160, "text": "benchmark data sets"}]
[{"st": 67, "ed": 69, "text": "weakly supervised"}, {"st": 108, "ed": 110, "text": "prior knowledge"}, {"st": 131, "ed": 133, "text": "natural images"}]
[{"st": 14, "ed": 16, "text": "local models"}, {"st": 23, "ed": 25, "text": "image patches"}, {"st": 28, "ed": 30, "text": "linear combinations"}, {"st": 48, "ed": 50, "text": "image restoration"}, {"st": 71, "ed": 73, "text": "recent works"}, {"st": 153, "ed": 155, "text": "convex optimization"}, {"st": 160, "ed": 164, "text": "synthetic and real data"}, {"st": 169, "ed": 171, "text": "proposed approach"}]
[{"st": 5, "ed": 7, "text": "l1 norm"}, {"st": 7, "ed": 10, "text": "semi supervised learning"}, {"st": 13, "ed": 15, "text": "image analysis"}, {"st": 18, "ed": 20, "text": "l1 norm"}, {"st": 37, "ed": 39, "text": "l1 norm"}, {"st": 55, "ed": 58, "text": "semi supervised learning"}, {"st": 60, "ed": 62, "text": "l1 norm"}, {"st": 87, "ed": 89, "text": "sparse coding"}, {"st": 92, "ed": 94, "text": "l1 norm"}, {"st": 103, "ed": 105, "text": "sparse coding"}, {"st": 106, "ed": 108, "text": "proposed algorithm"}, {"st": 122, "ed": 124, "text": "important applications"}, {"st": 126, "ed": 128, "text": "image analysis"}, {"st": 132, "ed": 134, "text": "image classification"}, {"st": 135, "ed": 137, "text": "noise reduction"}, {"st": 138, "ed": 141, "text": "visual and textual"}, {"st": 141, "ed": 144, "text": "bag of words"}, {"st": 157, "ed": 159, "text": "image representation"}, {"st": 164, "ed": 167, "text": "visual and textual"}, {"st": 175, "ed": 177, "text": "promising performance"}]
[{"st": 7, "ed": 9, "text": "text documents"}, {"st": 59, "ed": 61, "text": "learned representations"}, {"st": 72, "ed": 74, "text": "generative model"}, {"st": 87, "ed": 89, "text": "latent variables"}, {"st": 174, "ed": 176, "text": "latin alphabet"}]
[{"st": 3, "ed": 6, "text": "random fourier features"}, {"st": 18, "ed": 20, "text": "large scale"}, {"st": 36, "ed": 38, "text": "finite set"}, {"st": 55, "ed": 57, "text": "monte carlo"}, {"st": 82, "ed": 84, "text": "optimization process"}, {"st": 109, "ed": 111, "text": "group lasso"}, {"st": 113, "ed": 115, "text": "feature vectors"}, {"st": 118, "ed": 120, "text": "linear combination"}, {"st": 125, "ed": 128, "text": "efficient and scalable"}, {"st": 132, "ed": 135, "text": "multiple kernel learning"}, {"st": 156, "ed": 158, "text": "kernel learning"}, {"st": 163, "ed": 166, "text": "fast and accurate"}, {"st": 174, "ed": 176, "text": "visual object"}]
[{"st": 7, "ed": 9, "text": "technique called"}, {"st": 25, "ed": 27, "text": "classification error"}, {"st": 29, "ed": 31, "text": "nearest neighbor"}, {"st": 39, "ed": 41, "text": "distance learning"}, {"st": 59, "ed": 62, "text": "k nearest neighbor"}, {"st": 62, "ed": 64, "text": "k nn"}, {"st": 71, "ed": 73, "text": "feature set"}, {"st": 91, "ed": 93, "text": "k nn"}, {"st": 99, "ed": 101, "text": "feature vectors"}, {"st": 107, "ed": 109, "text": "feature vectors"}, {"st": 152, "ed": 154, "text": "important issue"}, {"st": 216, "ed": 218, "text": "improved performance"}, {"st": 226, "ed": 228, "text": "real world"}, {"st": 242, "ed": 244, "text": "ensemble learning"}]
[{"st": 4, "ed": 6, "text": "structural information"}, {"st": 12, "ed": 14, "text": "image reconstruction"}, {"st": 34, "ed": 36, "text": "linear combination"}, {"st": 53, "ed": 55, "text": "image reconstruction"}, {"st": 85, "ed": 87, "text": "recently gained"}, {"st": 132, "ed": 134, "text": "norm minimization"}, {"st": 149, "ed": 152, "text": "conjugate gradient method"}, {"st": 174, "ed": 176, "text": "image denoising"}, {"st": 183, "ed": 185, "text": "numerical results"}, {"st": 186, "ed": 188, "text": "competitive performance"}]
[{"st": 0, "ed": 2, "text": "feature extraction"}, {"st": 7, "ed": 9, "text": "visual content"}, {"st": 13, "ed": 15, "text": "feature extraction"}, {"st": 20, "ed": 22, "text": "raw image"}, {"st": 28, "ed": 30, "text": "decision making"}, {"st": 76, "ed": 78, "text": "feature selection"}, {"st": 94, "ed": 96, "text": "white matter"}, {"st": 96, "ed": 98, "text": "gray matter"}, {"st": 112, "ed": 114, "text": "mr images"}, {"st": 145, "ed": 148, "text": "linear discriminant analysis"}, {"st": 156, "ed": 159, "text": "support vector machine"}, {"st": 185, "ed": 187, "text": "feature selection"}, {"st": 207, "ed": 209, "text": "feature set"}]
[{"st": 0, "ed": 2, "text": "k means"}, {"st": 32, "ed": 34, "text": "initialization methods"}, {"st": 68, "ed": 70, "text": "initialization methods"}, {"st": 103, "ed": 105, "text": "initialization methods"}, {"st": 106, "ed": 108, "text": "perform poorly"}]
[{"st": 5, "ed": 7, "text": "computer aided"}, {"st": 73, "ed": 75, "text": "ground truth"}, {"st": 104, "ed": 106, "text": "false positives"}, {"st": 122, "ed": 124, "text": "false positives"}]
[{"st": 0, "ed": 2, "text": "likelihood based"}, {"st": 9, "ed": 11, "text": "computational complexity"}, {"st": 35, "ed": 38, "text": "taking into account"}, {"st": 57, "ed": 59, "text": "likelihood based"}]
[{"st": 1, "ed": 3, "text": "invariant representations"}, {"st": 57, "ed": 59, "text": "temporal coherence"}, {"st": 73, "ed": 75, "text": "spatial information"}, {"st": 114, "ed": 116, "text": "image classification"}]
[{"st": 0, "ed": 2, "text": "biologically inspired"}, {"st": 15, "ed": 17, "text": "important role"}, {"st": 18, "ed": 20, "text": "visual recognition"}, {"st": 54, "ed": 56, "text": "recognition systems"}, {"st": 78, "ed": 80, "text": "learning task"}, {"st": 84, "ed": 86, "text": "previously proposed"}, {"st": 86, "ed": 88, "text": "hand crafted"}, {"st": 103, "ed": 105, "text": "regularization terms"}, {"st": 109, "ed": 111, "text": "regularization term"}, {"st": 115, "ed": 117, "text": "strong performance"}, {"st": 136, "ed": 138, "text": "improved performance"}, {"st": 139, "ed": 141, "text": "hand crafted"}, {"st": 145, "ed": 150, "text": "cifar 10 and cifar 100"}]
[{"st": 23, "ed": 25, "text": "positive definite"}, {"st": 79, "ed": 81, "text": "metric learning"}]
[{"st": 8, "ed": 10, "text": "neural networks"}, {"st": 20, "ed": 22, "text": "diminishing returns"}, {"st": 47, "ed": 49, "text": "diminishing returns"}, {"st": 54, "ed": 56, "text": "training error"}, {"st": 63, "ed": 65, "text": "optimization method"}, {"st": 80, "ed": 82, "text": "optimization method"}, {"st": 92, "ed": 94, "text": "generalization error"}, {"st": 95, "ed": 97, "text": "large datasets"}]
[{"st": 42, "ed": 44, "text": "image patches"}, {"st": 83, "ed": 85, "text": "feature space"}, {"st": 87, "ed": 89, "text": "feature space"}, {"st": 95, "ed": 97, "text": "metric learning"}, {"st": 110, "ed": 112, "text": "look alike"}]
[{"st": 14, "ed": 16, "text": "training data"}, {"st": 37, "ed": 39, "text": "zero shot"}, {"st": 60, "ed": 63, "text": "zero shot learning"}]
[{"st": 6, "ed": 8, "text": "auto encoder"}, {"st": 14, "ed": 17, "text": "rectified linear units"}, {"st": 21, "ed": 24, "text": "number of iterations"}, {"st": 52, "ed": 54, "text": "loss function"}, {"st": 82, "ed": 84, "text": "deep networks"}, {"st": 98, "ed": 100, "text": "hidden units"}, {"st": 161, "ed": 165, "text": "number of hidden units"}, {"st": 170, "ed": 172, "text": "auto encoders"}, {"st": 173, "ed": 175, "text": "excellent performance"}]
[{"st": 2, "ed": 7, "text": "computer vision and machine learning"}, {"st": 13, "ed": 15, "text": "feature extraction"}, {"st": 25, "ed": 27, "text": "linear classifier"}, {"st": 36, "ed": 38, "text": "linear classifiers"}, {"st": 55, "ed": 57, "text": "kernel methods"}, {"st": 101, "ed": 103, "text": "pair wise"}, {"st": 167, "ed": 169, "text": "kernel matrix"}, {"st": 224, "ed": 226, "text": "deep learning"}]
[{"st": 1, "ed": 3, "text": "computer vision"}, {"st": 24, "ed": 26, "text": "spectral methods"}, {"st": 27, "ed": 29, "text": "semidefinite programming"}, {"st": 42, "ed": 45, "text": "easy to implement"}, {"st": 58, "ed": 60, "text": "computational complexity"}, {"st": 63, "ed": 65, "text": "large scale"}, {"st": 104, "ed": 107, "text": "efficient and scalable"}, {"st": 120, "ed": 122, "text": "extensive experiments"}, {"st": 124, "ed": 126, "text": "applications including"}, {"st": 127, "ed": 129, "text": "image segmentation"}, {"st": 142, "ed": 144, "text": "large scale"}]
[{"st": 0, "ed": 3, "text": "independent component analysis"}, {"st": 42, "ed": 44, "text": "sparse representations"}, {"st": 178, "ed": 180, "text": "sparse representations"}]
[{"st": 12, "ed": 14, "text": "recent developments"}, {"st": 19, "ed": 21, "text": "image analysis"}, {"st": 28, "ed": 30, "text": "non invasive"}, {"st": 46, "ed": 48, "text": "semi supervised"}, {"st": 49, "ed": 51, "text": "detection method"}, {"st": 77, "ed": 79, "text": "ghent altarpiece"}, {"st": 82, "ed": 84, "text": "northern europe"}, {"st": 129, "ed": 131, "text": "recently developed"}, {"st": 193, "ed": 195, "text": "classification results"}, {"st": 197, "ed": 199, "text": "random forest"}]
[{"st": 0, "ed": 2, "text": "k means"}, {"st": 32, "ed": 34, "text": "initialization methods"}, {"st": 86, "ed": 88, "text": "highly successful"}, {"st": 89, "ed": 91, "text": "initialization methods"}, {"st": 118, "ed": 120, "text": "based approach"}, {"st": 142, "ed": 144, "text": "machine learning"}, {"st": 153, "ed": 155, "text": "highly competitive"}, {"st": 160, "ed": 162, "text": "random initialization"}, {"st": 166, "ed": 168, "text": "k means"}, {"st": 171, "ed": 173, "text": "proposed approach"}, {"st": 173, "ed": 175, "text": "significantly improves"}]
[{"st": 1, "ed": 3, "text": "random walks"}, {"st": 35, "ed": 37, "text": "fully automated"}, {"st": 62, "ed": 64, "text": "discriminative learning"}, {"st": 81, "ed": 83, "text": "training samples"}, {"st": 132, "ed": 135, "text": "support vector machine"}, {"st": 148, "ed": 150, "text": "baseline methods"}]
[{"st": 18, "ed": 20, "text": "real world"}, {"st": 41, "ed": 43, "text": "real world"}, {"st": 46, "ed": 48, "text": "et al"}, {"st": 87, "ed": 89, "text": "machine learning"}, {"st": 121, "ed": 123, "text": "frame rate"}, {"st": 182, "ed": 184, "text": "significantly outperforms"}, {"st": 193, "ed": 195, "text": "machine learning"}, {"st": 196, "ed": 198, "text": "significant improvements"}]
[{"st": 58, "ed": 60, "text": "d dimensional"}, {"st": 80, "ed": 82, "text": "d dimensional"}, {"st": 86, "ed": 88, "text": "d dimensional"}, {"st": 88, "ed": 90, "text": "linear subspaces"}, {"st": 106, "ed": 109, "text": "number of iterations"}, {"st": 124, "ed": 126, "text": "online algorithm"}]
[{"st": 9, "ed": 11, "text": "digital video"}, {"st": 25, "ed": 27, "text": "computer vision"}, {"st": 38, "ed": 40, "text": "mid level"}, {"st": 40, "ed": 42, "text": "computer vision"}, {"st": 46, "ed": 48, "text": "object detection"}, {"st": 52, "ed": 54, "text": "inference problem"}, {"st": 67, "ed": 70, "text": "spatial and temporal"}, {"st": 87, "ed": 90, "text": "markov random fields"}, {"st": 94, "ed": 96, "text": "powerful tool"}, {"st": 122, "ed": 124, "text": "existing techniques"}, {"st": 142, "ed": 144, "text": "statistical mechanics"}, {"st": 151, "ed": 153, "text": "approximate inference"}, {"st": 163, "ed": 165, "text": "problems involving"}, {"st": 183, "ed": 185, "text": "existing techniques"}, {"st": 194, "ed": 196, "text": "main contribution"}, {"st": 226, "ed": 228, "text": "computer vision"}]
[{"st": 6, "ed": 8, "text": "random fields"}, {"st": 16, "ed": 18, "text": "expressive power"}, {"st": 30, "ed": 32, "text": "spatial relations"}]
[{"st": 6, "ed": 10, "text": "restricted boltzmann machine rbm"}, {"st": 33, "ed": 35, "text": "learning scheme"}]
[{"st": 26, "ed": 28, "text": "hyperspectral image"}, {"st": 56, "ed": 58, "text": "linear combination"}, {"st": 82, "ed": 84, "text": "linear combinations"}, {"st": 149, "ed": 151, "text": "hyperspectral image"}]
[{"st": 1, "ed": 3, "text": "random walks"}, {"st": 35, "ed": 37, "text": "fully automated"}, {"st": 62, "ed": 64, "text": "discriminative learning"}, {"st": 81, "ed": 83, "text": "training samples"}, {"st": 133, "ed": 136, "text": "support vector machine"}, {"st": 149, "ed": 151, "text": "baseline methods"}]
[{"st": 15, "ed": 17, "text": "speech data"}, {"st": 21, "ed": 23, "text": "kernel methods"}, {"st": 40, "ed": 43, "text": "dynamic time warping"}, {"st": 79, "ed": 81, "text": "dynamic programming"}, {"st": 127, "ed": 129, "text": "positive definite"}, {"st": 141, "ed": 143, "text": "practical applications"}, {"st": 146, "ed": 148, "text": "encouraging results"}, {"st": 150, "ed": 152, "text": "speech recognition"}]
[{"st": 3, "ed": 5, "text": "face recognition"}, {"st": 19, "ed": 21, "text": "low dimensional"}, {"st": 39, "ed": 42, "text": "linear discriminant analysis"}, {"st": 65, "ed": 67, "text": "classification accuracy"}, {"st": 71, "ed": 73, "text": "small sample"}, {"st": 89, "ed": 91, "text": "kernel based"}, {"st": 97, "ed": 100, "text": "support vector machine"}, {"st": 112, "ed": 114, "text": "cost effective"}, {"st": 124, "ed": 126, "text": "classification accuracy"}, {"st": 143, "ed": 145, "text": "proposed method"}]
[{"st": 30, "ed": 33, "text": "support vector machine"}, {"st": 41, "ed": 43, "text": "face detection"}, {"st": 90, "ed": 92, "text": "classification accuracy"}, {"st": 100, "ed": 102, "text": "decision trees"}, {"st": 103, "ed": 105, "text": "neural networks"}, {"st": 117, "ed": 119, "text": "proposed method"}]
[{"st": 2, "ed": 4, "text": "promising results"}, {"st": 21, "ed": 23, "text": "weak classifiers"}, {"st": 68, "ed": 70, "text": "image dataset"}, {"st": 102, "ed": 104, "text": "object category"}]
[{"st": 9, "ed": 11, "text": "object detection"}, {"st": 18, "ed": 20, "text": "visual features"}]
[{"st": 2, "ed": 4, "text": "promising results"}, {"st": 5, "ed": 7, "text": "visual object"}, {"st": 19, "ed": 21, "text": "weak classifiers"}, {"st": 68, "ed": 70, "text": "image dataset"}, {"st": 130, "ed": 132, "text": "object category"}]
[{"st": 3, "ed": 5, "text": "clustering algorithm"}, {"st": 6, "ed": 8, "text": "affinity propagation"}, {"st": 29, "ed": 31, "text": "large scale"}, {"st": 50, "ed": 52, "text": "large scale"}, {"st": 63, "ed": 65, "text": "affinity propagation"}, {"st": 72, "ed": 74, "text": "affinity propagation"}, {"st": 102, "ed": 105, "text": "number of iterations"}, {"st": 151, "ed": 153, "text": "chinese calligraphy"}]
[{"st": 4, "ed": 6, "text": "distance metrics"}, {"st": 11, "ed": 13, "text": "image classification"}, {"st": 29, "ed": 31, "text": "mahalanobis distance"}, {"st": 52, "ed": 54, "text": "semidefinite programming"}, {"st": 76, "ed": 78, "text": "positive semidefinite"}, {"st": 98, "ed": 100, "text": "positive semidefinite"}, {"st": 102, "ed": 104, "text": "weak learners"}, {"st": 106, "ed": 109, "text": "efficient and scalable"}, {"st": 117, "ed": 120, "text": "easy to implement"}, {"st": 138, "ed": 140, "text": "proposed algorithm"}, {"st": 140, "ed": 143, "text": "compares favorably to"}, {"st": 152, "ed": 154, "text": "classification accuracy"}]
[{"st": 7, "ed": 9, "text": "lie groups"}, {"st": 26, "ed": 28, "text": "computational complexity"}, {"st": 29, "ed": 31, "text": "parameter estimation"}, {"st": 36, "ed": 38, "text": "linear transformation"}, {"st": 51, "ed": 53, "text": "local minima"}, {"st": 80, "ed": 83, "text": "learning and inference"}, {"st": 96, "ed": 98, "text": "natural image"}]
[{"st": 11, "ed": 13, "text": "computer science"}, {"st": 38, "ed": 40, "text": "input data"}, {"st": 55, "ed": 57, "text": "practical problems"}, {"st": 103, "ed": 105, "text": "low dimensional"}, {"st": 144, "ed": 146, "text": "low dimensional"}, {"st": 186, "ed": 188, "text": "learning algorithm"}, {"st": 199, "ed": 203, "text": "synthetic and real world"}, {"st": 216, "ed": 218, "text": "local neighborhood"}]
[{"st": 11, "ed": 13, "text": "real valued"}, {"st": 25, "ed": 27, "text": "linear representation"}, {"st": 48, "ed": 50, "text": "face recognition"}, {"st": 58, "ed": 60, "text": "gabor filter"}, {"st": 70, "ed": 72, "text": "spatial frequency"}, {"st": 77, "ed": 79, "text": "face representation"}, {"st": 96, "ed": 98, "text": "facial expression"}, {"st": 101, "ed": 103, "text": "gabor filter"}, {"st": 105, "ed": 107, "text": "face images"}, {"st": 132, "ed": 134, "text": "feature spaces"}, {"st": 135, "ed": 137, "text": "low dimensional"}, {"st": 139, "ed": 142, "text": "support vector machines"}, {"st": 168, "ed": 170, "text": "experiment results"}]
[{"st": 1, "ed": 3, "text": "sparse coding"}, {"st": 10, "ed": 12, "text": "basis functions"}, {"st": 14, "ed": 16, "text": "natural image"}, {"st": 35, "ed": 38, "text": "visual object recognition"}, {"st": 49, "ed": 51, "text": "optimization algorithms"}, {"st": 63, "ed": 66, "text": "simple and efficient"}, {"st": 92, "ed": 94, "text": "sparse coding"}, {"st": 96, "ed": 99, "text": "visual object recognition"}]
[{"st": 1, "ed": 3, "text": "decision boundaries"}, {"st": 4, "ed": 6, "text": "bayes classifier"}, {"st": 27, "ed": 29, "text": "class conditional"}, {"st": 73, "ed": 76, "text": "curse of dimensionality"}, {"st": 78, "ed": 80, "text": "feature space"}, {"st": 81, "ed": 83, "text": "small sample"}, {"st": 119, "ed": 121, "text": "cross validation"}, {"st": 140, "ed": 142, "text": "proposed method"}, {"st": 159, "ed": 161, "text": "real data"}, {"st": 166, "ed": 168, "text": "proposed algorithm"}, {"st": 171, "ed": 173, "text": "classification accuracies"}, {"st": 174, "ed": 176, "text": "pattern classification"}, {"st": 179, "ed": 181, "text": "facial expression"}, {"st": 185, "ed": 187, "text": "bayes classifier"}, {"st": 187, "ed": 190, "text": "curse of dimensionality"}, {"st": 193, "ed": 195, "text": "pattern classification"}]
[{"st": 0, "ed": 2, "text": "kernel based"}, {"st": 2, "ed": 4, "text": "machine learning"}, {"st": 12, "ed": 14, "text": "original input"}, {"st": 14, "ed": 16, "text": "feature space"}, {"st": 19, "ed": 21, "text": "feature space"}, {"st": 36, "ed": 38, "text": "kernel based"}, {"st": 38, "ed": 41, "text": "classification and regression"}, {"st": 44, "ed": 47, "text": "support vector machines"}, {"st": 52, "ed": 54, "text": "remote sensing"}, {"st": 59, "ed": 61, "text": "civil engineering"}, {"st": 71, "ed": 74, "text": "support vector machines"}, {"st": 102, "ed": 104, "text": "kernel based"}, {"st": 108, "ed": 111, "text": "classification and regression"}, {"st": 125, "ed": 128, "text": "support vector machines"}, {"st": 136, "ed": 138, "text": "kernel functions"}, {"st": 160, "ed": 162, "text": "remote sensing"}, {"st": 171, "ed": 173, "text": "civil engineering"}]
[{"st": 16, "ed": 18, "text": "image analysis"}, {"st": 19, "ed": 21, "text": "computer vision"}, {"st": 46, "ed": 48, "text": "real number"}]
[{"st": 4, "ed": 6, "text": "compressed sensing"}, {"st": 8, "ed": 10, "text": "compressed sensing"}, {"st": 75, "ed": 77, "text": "sparse models"}, {"st": 92, "ed": 94, "text": "significantly faster"}, {"st": 119, "ed": 121, "text": "approximation error"}, {"st": 129, "ed": 131, "text": "significantly smaller"}, {"st": 178, "ed": 181, "text": "gaussian mixture models"}, {"st": 185, "ed": 187, "text": "gaussian distributions"}, {"st": 230, "ed": 232, "text": "gaussian distributions"}, {"st": 242, "ed": 245, "text": "expectation maximization algorithm"}, {"st": 269, "ed": 271, "text": "sensing applications"}, {"st": 279, "ed": 281, "text": "improved results"}]
[{"st": 2, "ed": 4, "text": "labeled examples"}, {"st": 51, "ed": 53, "text": "confidence bounds"}, {"st": 58, "ed": 60, "text": "ground truth"}, {"st": 74, "ed": 76, "text": "generative model"}, {"st": 105, "ed": 107, "text": "class conditional"}]
[{"st": 62, "ed": 64, "text": "highly correlated"}, {"st": 75, "ed": 77, "text": "ensemble learning"}, {"st": 160, "ed": 162, "text": "loss function"}, {"st": 182, "ed": 184, "text": "convex optimization"}, {"st": 186, "ed": 188, "text": "extensive experiments"}, {"st": 195, "ed": 197, "text": "benchmark datasets"}, {"st": 200, "ed": 202, "text": "proposed approach"}, {"st": 202, "ed": 204, "text": "consistently outperforms"}, {"st": 213, "ed": 215, "text": "random forests"}, {"st": 223, "ed": 225, "text": "semi definite"}]
[{"st": 48, "ed": 50, "text": "haar wavelet"}, {"st": 62, "ed": 64, "text": "total variation"}, {"st": 72, "ed": 74, "text": "supervised classification"}]
[{"st": 1, "ed": 4, "text": "support vector machines"}, {"st": 11, "ed": 13, "text": "de facto"}, {"st": 16, "ed": 18, "text": "visual perception"}, {"st": 34, "ed": 36, "text": "pedestrian detection"}, {"st": 61, "ed": 63, "text": "feature extraction"}, {"st": 121, "ed": 123, "text": "expression recognition"}, {"st": 124, "ed": 126, "text": "pedestrian detection"}]
[{"st": 0, "ed": 4, "text": "deep convolutional neural networks"}, {"st": 11, "ed": 13, "text": "image recognition"}, {"st": 23, "ed": 25, "text": "building block"}, {"st": 40, "ed": 42, "text": "max pooling"}, {"st": 76, "ed": 78, "text": "max pooling"}, {"st": 103, "ed": 105, "text": "back propagation"}, {"st": 119, "ed": 121, "text": "proposed method"}, {"st": 124, "ed": 126, "text": "image classification"}, {"st": 136, "ed": 138, "text": "recognition performance"}, {"st": 141, "ed": 144, "text": "convolutional neural networks"}, {"st": 149, "ed": 151, "text": "pre trained"}, {"st": 162, "ed": 164, "text": "image classification"}, {"st": 169, "ed": 173, "text": "mnist and cifar 10"}]
[{"st": 27, "ed": 29, "text": "invariance properties"}, {"st": 45, "ed": 47, "text": "temporal coherence"}, {"st": 67, "ed": 69, "text": "latent variables"}, {"st": 104, "ed": 106, "text": "relational model"}, {"st": 119, "ed": 121, "text": "feature space"}, {"st": 132, "ed": 134, "text": "nearest neighbour"}, {"st": 138, "ed": 140, "text": "face database"}]
[{"st": 0, "ed": 3, "text": "zero shot learning"}, {"st": 10, "ed": 12, "text": "training data"}, {"st": 28, "ed": 30, "text": "embedding space"}, {"st": 34, "ed": 36, "text": "unseen classes"}, {"st": 39, "ed": 41, "text": "previous works"}, {"st": 44, "ed": 46, "text": "embedding space"}, {"st": 105, "ed": 107, "text": "unseen classes"}, {"st": 185, "ed": 187, "text": "zero shot"}, {"st": 194, "ed": 196, "text": "class label"}, {"st": 206, "ed": 208, "text": "closed form"}, {"st": 225, "ed": 227, "text": "computational efficiency"}, {"st": 229, "ed": 231, "text": "proposed method"}]
[{"st": 1, "ed": 3, "text": "object detectors"}, {"st": 4, "ed": 7, "text": "massive amounts of"}, {"st": 7, "ed": 9, "text": "labeled training"}, {"st": 71, "ed": 73, "text": "streaming data"}, {"st": 81, "ed": 83, "text": "labeled data"}, {"st": 88, "ed": 90, "text": "data stream"}, {"st": 92, "ed": 95, "text": "self paced learning"}, {"st": 118, "ed": 120, "text": "learning algorithm"}, {"st": 123, "ed": 126, "text": "multi task learning"}, {"st": 153, "ed": 155, "text": "jointly learn"}, {"st": 160, "ed": 162, "text": "instance level"}, {"st": 178, "ed": 180, "text": "real world"}]
[{"st": 0, "ed": 2, "text": "structured output"}, {"st": 5, "ed": 7, "text": "challenging problem"}, {"st": 15, "ed": 17, "text": "large datasets"}, {"st": 37, "ed": 39, "text": "learning framework"}, {"st": 40, "ed": 42, "text": "structured prediction"}, {"st": 46, "ed": 48, "text": "training instances"}, {"st": 61, "ed": 63, "text": "loss functions"}, {"st": 87, "ed": 89, "text": "image segmentation"}, {"st": 93, "ed": 95, "text": "wide variety"}, {"st": 132, "ed": 134, "text": "bounding boxes"}, {"st": 137, "ed": 139, "text": "image level"}, {"st": 143, "ed": 145, "text": "object categories"}, {"st": 150, "ed": 152, "text": "experimental evaluation"}, {"st": 159, "ed": 161, "text": "loss functions"}]
[{"st": 4, "ed": 6, "text": "weakly labeled"}, {"st": 12, "ed": 14, "text": "object detection"}, {"st": 48, "ed": 50, "text": "submodular optimization"}, {"st": 65, "ed": 68, "text": "positive and negative"}, {"st": 78, "ed": 80, "text": "weakly supervised"}, {"st": 85, "ed": 87, "text": "pascal voc"}]
[{"st": 4, "ed": 6, "text": "based approach"}, {"st": 25, "ed": 27, "text": "neural network"}, {"st": 43, "ed": 47, "text": "trained end to end"}, {"st": 53, "ed": 55, "text": "training examples"}, {"st": 56, "ed": 58, "text": "competitive performance"}]
[{"st": 7, "ed": 11, "text": "multiple instance learning mil"}, {"st": 11, "ed": 13, "text": "method called"}, {"st": 28, "ed": 30, "text": "unlike previous"}, {"st": 46, "ed": 48, "text": "training instances"}, {"st": 66, "ed": 68, "text": "total number"}, {"st": 108, "ed": 110, "text": "large scale"}, {"st": 110, "ed": 112, "text": "image classification"}]
[{"st": 13, "ed": 15, "text": "sparse linear"}, {"st": 39, "ed": 41, "text": "sparse representation"}, {"st": 43, "ed": 45, "text": "classification models"}, {"st": 53, "ed": 55, "text": "optimization framework"}, {"st": 85, "ed": 87, "text": "cost functions"}, {"st": 123, "ed": 125, "text": "sparse coding"}, {"st": 126, "ed": 129, "text": "dictionary learning algorithms"}, {"st": 141, "ed": 143, "text": "digit classification"}, {"st": 144, "ed": 146, "text": "face recognition"}]
[{"st": 55, "ed": 57, "text": "based method"}]
[{"st": 43, "ed": 45, "text": "cost function"}, {"st": 89, "ed": 91, "text": "hierarchical models"}]
[{"st": 11, "ed": 13, "text": "human brain"}, {"st": 13, "ed": 15, "text": "white matter"}, {"st": 20, "ed": 22, "text": "magnetic resonance"}, {"st": 33, "ed": 35, "text": "neural architecture"}, {"st": 37, "ed": 39, "text": "human brain"}, {"st": 44, "ed": 46, "text": "white matter"}, {"st": 75, "ed": 77, "text": "angular resolution"}, {"st": 115, "ed": 117, "text": "fiber bundle"}]
[{"st": 13, "ed": 15, "text": "class labels"}, {"st": 50, "ed": 52, "text": "recently introduced"}, {"st": 101, "ed": 103, "text": "proposed approach"}, {"st": 111, "ed": 114, "text": "unsupervised and supervised"}]
[{"st": 3, "ed": 5, "text": "texture classification"}, {"st": 59, "ed": 61, "text": "carefully designed"}, {"st": 105, "ed": 107, "text": "proposed approach"}, {"st": 107, "ed": 109, "text": "significantly improves"}, {"st": 115, "ed": 117, "text": "texture classification"}]
[{"st": 0, "ed": 2, "text": "visual tracking"}, {"st": 5, "ed": 7, "text": "object appearance"}, {"st": 32, "ed": 35, "text": "discrete cosine transform"}, {"st": 48, "ed": 50, "text": "basis functions"}, {"st": 112, "ed": 114, "text": "similarity measure"}, {"st": 146, "ed": 149, "text": "discrete cosine transform"}, {"st": 153, "ed": 156, "text": "discrete cosine transform"}]
[{"st": 19, "ed": 21, "text": "feature extraction"}, {"st": 30, "ed": 32, "text": "feature extraction"}, {"st": 35, "ed": 37, "text": "mutual information"}, {"st": 60, "ed": 62, "text": "feature extraction"}, {"st": 65, "ed": 67, "text": "feature set"}, {"st": 96, "ed": 98, "text": "feature extraction"}, {"st": 113, "ed": 115, "text": "mutual information"}, {"st": 115, "ed": 117, "text": "feature extraction"}, {"st": 123, "ed": 125, "text": "proposed method"}]
[{"st": 61, "ed": 63, "text": "linear regression"}, {"st": 94, "ed": 96, "text": "supervised learning"}, {"st": 109, "ed": 111, "text": "linear model"}, {"st": 145, "ed": 147, "text": "prediction accuracy"}]
[{"st": 4, "ed": 6, "text": "classification accuracy"}, {"st": 7, "ed": 9, "text": "image representations"}, {"st": 23, "ed": 25, "text": "feature level"}, {"st": 27, "ed": 29, "text": "image representations"}, {"st": 83, "ed": 85, "text": "image classification"}]
[{"st": 15, "ed": 17, "text": "scale space"}, {"st": 44, "ed": 46, "text": "texture classification"}, {"st": 84, "ed": 86, "text": "base classifier"}, {"st": 130, "ed": 132, "text": "small sample"}, {"st": 140, "ed": 142, "text": "feature spaces"}, {"st": 155, "ed": 158, "text": "support vector machine"}]
[{"st": 9, "ed": 11, "text": "machine learning"}, {"st": 18, "ed": 20, "text": "active learning"}, {"st": 70, "ed": 72, "text": "electron microscopy"}]
[{"st": 1, "ed": 3, "text": "existing approaches"}, {"st": 10, "ed": 12, "text": "hash function"}, {"st": 14, "ed": 16, "text": "optimization process"}, {"st": 44, "ed": 46, "text": "optimization problems"}, {"st": 67, "ed": 69, "text": "loss functions"}, {"st": 78, "ed": 80, "text": "existing approaches"}, {"st": 93, "ed": 95, "text": "problem specific"}, {"st": 110, "ed": 112, "text": "hash function"}, {"st": 145, "ed": 147, "text": "extensively studied"}, {"st": 151, "ed": 153, "text": "extensive experiments"}, {"st": 156, "ed": 158, "text": "proposed framework"}]
[{"st": 4, "ed": 6, "text": "maximum margin"}, {"st": 19, "ed": 21, "text": "computer vision"}, {"st": 36, "ed": 39, "text": "conditional random fields"}, {"st": 57, "ed": 59, "text": "exact inference"}, {"st": 63, "ed": 65, "text": "np hard"}, {"st": 106, "ed": 108, "text": "approximate inference"}, {"st": 111, "ed": 113, "text": "cutting plane"}, {"st": 121, "ed": 123, "text": "proposed method"}, {"st": 132, "ed": 134, "text": "computer vision"}]
[{"st": 0, "ed": 2, "text": "deep learning"}, {"st": 7, "ed": 9, "text": "classification performance"}, {"st": 24, "ed": 26, "text": "object classification"}, {"st": 33, "ed": 35, "text": "unified framework"}, {"st": 41, "ed": 43, "text": "machine learning"}, {"st": 45, "ed": 47, "text": "deep learning"}, {"st": 47, "ed": 49, "text": "probabilistic models"}, {"st": 50, "ed": 52, "text": "kernel methods"}, {"st": 68, "ed": 70, "text": "contextual information"}, {"st": 71, "ed": 73, "text": "natural images"}, {"st": 76, "ed": 78, "text": "latent tree"}, {"st": 78, "ed": 80, "text": "probabilistic model"}, {"st": 84, "ed": 86, "text": "co occurrences"}, {"st": 94, "ed": 96, "text": "pre trained"}, {"st": 112, "ed": 114, "text": "co occurrences"}, {"st": 116, "ed": 118, "text": "kernel methods"}, {"st": 132, "ed": 134, "text": "neural network"}, {"st": 140, "ed": 142, "text": "object classification"}, {"st": 159, "ed": 161, "text": "precision recall"}, {"st": 165, "ed": 167, "text": "ms coco"}, {"st": 174, "ed": 176, "text": "latent variables"}, {"st": 213, "ed": 216, "text": "k means clustering"}, {"st": 222, "ed": 224, "text": "significantly improves"}, {"st": 224, "ed": 226, "text": "scene classification"}, {"st": 249, "ed": 251, "text": "unified framework"}, {"st": 253, "ed": 255, "text": "object classification"}]
[{"st": 4, "ed": 6, "text": "computer vision"}, {"st": 13, "ed": 15, "text": "machine learning"}, {"st": 17, "ed": 19, "text": "large scale"}, {"st": 28, "ed": 30, "text": "deep learning"}, {"st": 55, "ed": 57, "text": "natural language"}, {"st": 90, "ed": 92, "text": "unsupervised learning"}, {"st": 93, "ed": 95, "text": "supervised learning"}, {"st": 98, "ed": 100, "text": "sentence level"}, {"st": 160, "ed": 162, "text": "large scale"}]
[{"st": 26, "ed": 28, "text": "feature selection"}, {"st": 39, "ed": 41, "text": "quadratic programming"}, {"st": 41, "ed": 43, "text": "feature selection"}, {"st": 50, "ed": 52, "text": "feature selection"}, {"st": 67, "ed": 69, "text": "feature selection"}, {"st": 79, "ed": 81, "text": "existing approaches"}, {"st": 98, "ed": 100, "text": "computationally expensive"}, {"st": 114, "ed": 116, "text": "feature selection"}, {"st": 118, "ed": 121, "text": "k means clustering"}, {"st": 130, "ed": 132, "text": "k means"}, {"st": 157, "ed": 159, "text": "feature space"}, {"st": 166, "ed": 168, "text": "fine grained"}, {"st": 188, "ed": 190, "text": "irrelevant features"}, {"st": 200, "ed": 202, "text": "k means"}, {"st": 234, "ed": 236, "text": "k means"}, {"st": 245, "ed": 248, "text": "number of clusters"}, {"st": 261, "ed": 264, "text": "publicly available datasets"}, {"st": 269, "ed": 271, "text": "significant computational"}, {"st": 288, "ed": 290, "text": "feature selection"}]
[{"st": 25, "ed": 27, "text": "visual recognition"}, {"st": 29, "ed": 32, "text": "received much attention"}, {"st": 44, "ed": 46, "text": "l 2"}, {"st": 51, "ed": 53, "text": "learning framework"}, {"st": 55, "ed": 57, "text": "simultaneously learn"}, {"st": 106, "ed": 109, "text": "semi supervised learning"}, {"st": 118, "ed": 120, "text": "proposed approach"}, {"st": 130, "ed": 132, "text": "proposed method"}, {"st": 176, "ed": 178, "text": "optimization method"}, {"st": 220, "ed": 222, "text": "large scale"}, {"st": 228, "ed": 230, "text": "proposed approach"}, {"st": 230, "ed": 233, "text": "compares favorably with"}, {"st": 244, "ed": 246, "text": "deep convolutional"}]
[{"st": 7, "ed": 9, "text": "classification performance"}, {"st": 21, "ed": 23, "text": "training examples"}, {"st": 39, "ed": 41, "text": "feature selection"}, {"st": 43, "ed": 45, "text": "f test"}, {"st": 57, "ed": 59, "text": "learning algorithms"}, {"st": 83, "ed": 85, "text": "performance measures"}]
[{"st": 6, "ed": 8, "text": "multi class"}, {"st": 9, "ed": 11, "text": "image segmentation"}, {"st": 17, "ed": 19, "text": "exact inference"}, {"st": 31, "ed": 33, "text": "map inference"}, {"st": 41, "ed": 44, "text": "markov random field"}, {"st": 76, "ed": 78, "text": "image pixels"}, {"st": 105, "ed": 108, "text": "convolutional neural networks"}, {"st": 146, "ed": 149, "text": "compares favorably to"}, {"st": 149, "ed": 151, "text": "joint training"}, {"st": 153, "ed": 155, "text": "fully connected"}]
[{"st": 22, "ed": 28, "text": "alternating direction method of multipliers admm"}, {"st": 33, "ed": 35, "text": "minimization problem"}, {"st": 37, "ed": 41, "text": "feed forward neural network"}, {"st": 52, "ed": 54, "text": "linear unit"}, {"st": 121, "ed": 123, "text": "extensive experiments"}]
[{"st": 3, "ed": 5, "text": "distance metric"}, {"st": 6, "ed": 8, "text": "feature space"}, {"st": 21, "ed": 23, "text": "real world"}, {"st": 25, "ed": 27, "text": "metric learning"}, {"st": 38, "ed": 40, "text": "optimization problem"}, {"st": 42, "ed": 44, "text": "time consuming"}, {"st": 64, "ed": 66, "text": "training set"}, {"st": 81, "ed": 83, "text": "metric learning"}, {"st": 83, "ed": 85, "text": "method called"}, {"st": 93, "ed": 95, "text": "component analysis"}, {"st": 114, "ed": 116, "text": "similarity graph"}, {"st": 128, "ed": 131, "text": "variational lower bound"}, {"st": 133, "ed": 135, "text": "log likelihood"}, {"st": 143, "ed": 146, "text": "publicly available datasets"}, {"st": 149, "ed": 151, "text": "proposed method"}, {"st": 166, "ed": 168, "text": "training set"}, {"st": 174, "ed": 176, "text": "proposed method"}, {"st": 186, "ed": 188, "text": "metric learning"}]
[{"st": 8, "ed": 10, "text": "semi supervised"}, {"st": 10, "ed": 12, "text": "structured output"}, {"st": 19, "ed": 21, "text": "structured outputs"}, {"st": 36, "ed": 38, "text": "input output"}, {"st": 45, "ed": 47, "text": "traditional methods"}, {"st": 92, "ed": 94, "text": "existing methods"}, {"st": 110, "ed": 112, "text": "structured outputs"}, {"st": 117, "ed": 120, "text": "each data point"}, {"st": 141, "ed": 143, "text": "structured prediction"}, {"st": 155, "ed": 158, "text": "benchmark data sets"}, {"st": 164, "ed": 166, "text": "natural image"}]
[{"st": 0, "ed": 2, "text": "object categorization"}, {"st": 4, "ed": 6, "text": "challenging problem"}, {"st": 28, "ed": 30, "text": "object categorization"}, {"st": 87, "ed": 89, "text": "classification task"}, {"st": 91, "ed": 94, "text": "multiple kernel learning"}, {"st": 115, "ed": 118, "text": "l 1 norm"}, {"st": 156, "ed": 158, "text": "norm regularization"}, {"st": 177, "ed": 180, "text": "multiple kernel learning"}, {"st": 188, "ed": 191, "text": "l 1 norm"}, {"st": 216, "ed": 218, "text": "empirical results"}, {"st": 219, "ed": 221, "text": "benchmark datasets"}, {"st": 222, "ed": 224, "text": "significant improvement"}]
[{"st": 25, "ed": 27, "text": "fully connected"}]
[{"st": 7, "ed": 12, "text": "deep convolutional neural network cnn"}, {"st": 20, "ed": 22, "text": "deep networks"}, {"st": 23, "ed": 25, "text": "hyperspectral image"}, {"st": 36, "ed": 38, "text": "hyperspectral image"}, {"st": 44, "ed": 46, "text": "deep cnn"}, {"st": 76, "ed": 78, "text": "multi scale"}, {"st": 96, "ed": 98, "text": "feature maps"}, {"st": 101, "ed": 103, "text": "multi scale"}, {"st": 119, "ed": 121, "text": "feature map"}, {"st": 136, "ed": 139, "text": "fully convolutional network"}, {"st": 150, "ed": 152, "text": "proposed approach"}, {"st": 156, "ed": 158, "text": "benchmark datasets"}, {"st": 175, "ed": 177, "text": "classification performance"}, {"st": 179, "ed": 181, "text": "proposed approach"}]
[{"st": 0, "ed": 3, "text": "multi task learning"}, {"st": 4, "ed": 6, "text": "convolutional networks"}, {"st": 8, "ed": 10, "text": "remarkable success"}, {"st": 31, "ed": 33, "text": "multi task"}, {"st": 38, "ed": 40, "text": "network architectures"}, {"st": 56, "ed": 58, "text": "principled approach"}, {"st": 75, "ed": 77, "text": "cross stitch"}, {"st": 96, "ed": 98, "text": "cross stitch"}, {"st": 107, "ed": 109, "text": "task specific"}, {"st": 111, "ed": 113, "text": "proposed method"}, {"st": 115, "ed": 117, "text": "multiple tasks"}, {"st": 120, "ed": 122, "text": "improved performance"}, {"st": 123, "ed": 125, "text": "baseline methods"}]
[{"st": 3, "ed": 5, "text": "object detection"}, {"st": 19, "ed": 21, "text": "training procedure"}, {"st": 116, "ed": 118, "text": "detection performance"}, {"st": 121, "ed": 124, "text": "pascal voc 2007"}, {"st": 143, "ed": 145, "text": "ms coco"}, {"st": 168, "ed": 171, "text": "pascal voc 2007"}]
[{"st": 10, "ed": 12, "text": "unsupervised learning"}, {"st": 14, "ed": 16, "text": "deep representations"}, {"st": 26, "ed": 28, "text": "clustering algorithm"}, {"st": 44, "ed": 48, "text": "convolutional neural network cnn"}, {"st": 64, "ed": 66, "text": "forward pass"}, {"st": 67, "ed": 69, "text": "representation learning"}, {"st": 74, "ed": 76, "text": "key idea"}, {"st": 89, "ed": 91, "text": "clustering results"}, {"st": 114, "ed": 117, "text": "end to end"}, {"st": 131, "ed": 133, "text": "extensive experiments"}, {"st": 136, "ed": 138, "text": "method outperforms"}, {"st": 154, "ed": 156, "text": "learned representations"}]
[{"st": 11, "ed": 13, "text": "image sequences"}, {"st": 54, "ed": 56, "text": "matrix completion"}, {"st": 58, "ed": 60, "text": "objective function"}, {"st": 65, "ed": 67, "text": "low rank"}, {"st": 90, "ed": 92, "text": "low rank"}, {"st": 116, "ed": 118, "text": "optimization algorithms"}, {"st": 129, "ed": 131, "text": "empirical results"}, {"st": 134, "ed": 136, "text": "image sequences"}, {"st": 139, "ed": 141, "text": "simulated data"}]
[{"st": 11, "ed": 14, "text": "deep neural networks"}, {"st": 14, "ed": 16, "text": "small perturbations"}, {"st": 18, "ed": 20, "text": "visual input"}, {"st": 36, "ed": 38, "text": "deep architectures"}, {"st": 49, "ed": 51, "text": "computer vision"}, {"st": 61, "ed": 63, "text": "deep networks"}, {"st": 74, "ed": 76, "text": "image processing"}, {"st": 116, "ed": 118, "text": "large scale"}]
[{"st": 0, "ed": 2, "text": "object recognition"}, {"st": 19, "ed": 21, "text": "object recognition"}, {"st": 22, "ed": 24, "text": "machine learning"}, {"st": 29, "ed": 31, "text": "kernel learning"}, {"st": 56, "ed": 59, "text": "k nearest neighbor"}, {"st": 59, "ed": 62, "text": "support vector machines"}, {"st": 62, "ed": 64, "text": "local learning"}, {"st": 70, "ed": 72, "text": "scale invariant"}, {"st": 93, "ed": 95, "text": "distance function"}, {"st": 107, "ed": 109, "text": "object categorization"}]
[{"st": 23, "ed": 25, "text": "learning scheme"}, {"st": 33, "ed": 37, "text": "convolutional neural network cnn"}, {"st": 44, "ed": 46, "text": "spatio temporal"}, {"st": 50, "ed": 52, "text": "input image"}, {"st": 57, "ed": 59, "text": "optical flow"}, {"st": 104, "ed": 106, "text": "linear programming"}, {"st": 114, "ed": 117, "text": "simple and efficient"}, {"st": 122, "ed": 124, "text": "complex models"}]
[{"st": 12, "ed": 14, "text": "difficult task"}, {"st": 15, "ed": 17, "text": "computer vision"}, {"st": 47, "ed": 49, "text": "synthetic data"}, {"st": 50, "ed": 52, "text": "ground truth"}, {"st": 65, "ed": 68, "text": "end to end"}, {"st": 142, "ed": 146, "text": "real and synthetic data"}, {"st": 207, "ed": 209, "text": "vision applications"}]
[{"st": 25, "ed": 27, "text": "challenging task"}, {"st": 66, "ed": 68, "text": "feature vectors"}, {"st": 81, "ed": 83, "text": "feature extractor"}, {"st": 86, "ed": 88, "text": "convolutional network"}, {"st": 93, "ed": 95, "text": "feature vectors"}, {"st": 121, "ed": 123, "text": "object categories"}, {"st": 148, "ed": 150, "text": "class distributions"}, {"st": 164, "ed": 166, "text": "convolutional network"}, {"st": 166, "ed": 168, "text": "feature extractor"}, {"st": 169, "ed": 173, "text": "trained end to end"}, {"st": 174, "ed": 176, "text": "raw pixels"}, {"st": 221, "ed": 223, "text": "competing approaches"}]
[{"st": 79, "ed": 81, "text": "subspace segmentation"}, {"st": 101, "ed": 103, "text": "distance function"}, {"st": 144, "ed": 146, "text": "high dimensional"}, {"st": 160, "ed": 162, "text": "low dimensional"}, {"st": 166, "ed": 168, "text": "outperforms existing"}, {"st": 196, "ed": 198, "text": "computer vision"}, {"st": 208, "ed": 210, "text": "motion segmentation"}]
[{"st": 0, "ed": 2, "text": "pedestrian detection"}, {"st": 14, "ed": 16, "text": "successful applications"}, {"st": 17, "ed": 19, "text": "deep learning"}, {"st": 29, "ed": 31, "text": "competitive results"}, {"st": 38, "ed": 40, "text": "convolutional network"}, {"st": 50, "ed": 52, "text": "multi stage"}, {"st": 74, "ed": 76, "text": "sparse coding"}, {"st": 77, "ed": 79, "text": "pre train"}]
[{"st": 0, "ed": 2, "text": "face recognition"}, {"st": 14, "ed": 16, "text": "facial expression"}, {"st": 42, "ed": 44, "text": "face recognition"}, {"st": 51, "ed": 53, "text": "face detection"}, {"st": 91, "ed": 93, "text": "experiment results"}]
[{"st": 100, "ed": 102, "text": "efficient algorithms"}, {"st": 116, "ed": 118, "text": "feature extraction"}, {"st": 134, "ed": 136, "text": "dimensionality reduction"}, {"st": 148, "ed": 150, "text": "significantly improve"}, {"st": 153, "ed": 156, "text": "classification and clustering"}]
[{"st": 3, "ed": 5, "text": "clustering algorithm"}, {"st": 13, "ed": 15, "text": "image classification"}, {"st": 30, "ed": 32, "text": "computational complexity"}, {"st": 40, "ed": 42, "text": "large scale"}, {"st": 48, "ed": 50, "text": "clustering algorithm"}, {"st": 55, "ed": 57, "text": "hierarchical structure"}, {"st": 58, "ed": 60, "text": "parallel computing"}, {"st": 89, "ed": 91, "text": "hierarchical structure"}, {"st": 98, "ed": 100, "text": "real world"}, {"st": 100, "ed": 102, "text": "large scale"}, {"st": 102, "ed": 104, "text": "image data"}, {"st": 115, "ed": 117, "text": "wide applicability"}, {"st": 126, "ed": 128, "text": "clustering accuracy"}]
[{"st": 13, "ed": 15, "text": "computer vision"}, {"st": 18, "ed": 20, "text": "highly effective"}, {"st": 50, "ed": 52, "text": "dictionary learning"}, {"st": 61, "ed": 63, "text": "learned features"}, {"st": 91, "ed": 93, "text": "clustering algorithms"}, {"st": 104, "ed": 106, "text": "based methods"}]
[{"st": 7, "ed": 9, "text": "feature extraction"}, {"st": 18, "ed": 20, "text": "finite dimensional"}, {"st": 39, "ed": 41, "text": "recently introduced"}, {"st": 69, "ed": 71, "text": "domain knowledge"}, {"st": 73, "ed": 75, "text": "image analysis"}, {"st": 85, "ed": 87, "text": "finite dimensional"}, {"st": 87, "ed": 89, "text": "feature vector"}, {"st": 100, "ed": 102, "text": "finite set"}, {"st": 124, "ed": 126, "text": "feature vectors"}, {"st": 162, "ed": 164, "text": "unsupervised learning"}, {"st": 186, "ed": 188, "text": "feature extraction"}, {"st": 194, "ed": 197, "text": "supervised and unsupervised"}]
[{"st": 0, "ed": 2, "text": "spectral clustering"}, {"st": 4, "ed": 6, "text": "powerful tool"}, {"st": 16, "ed": 18, "text": "context aware"}, {"st": 19, "ed": 21, "text": "similarity measure"}, {"st": 26, "ed": 28, "text": "spectral clustering"}, {"st": 44, "ed": 47, "text": "k nearest neighbor"}, {"st": 61, "ed": 63, "text": "pairwise similarity"}, {"st": 148, "ed": 150, "text": "spectral clustering"}, {"st": 153, "ed": 155, "text": "theoretical analysis"}]
[{"st": 0, "ed": 2, "text": "ensemble methods"}, {"st": 29, "ed": 31, "text": "ensemble learning"}, {"st": 35, "ed": 37, "text": "unlike previous"}, {"st": 44, "ed": 47, "text": "support vector machines"}, {"st": 78, "ed": 80, "text": "ensemble methods"}, {"st": 96, "ed": 98, "text": "ensemble learning"}, {"st": 101, "ed": 103, "text": "kernel methods"}, {"st": 129, "ed": 131, "text": "ensemble learning"}, {"st": 136, "ed": 138, "text": "multi class"}, {"st": 138, "ed": 140, "text": "ensemble learning"}]
[{"st": 7, "ed": 9, "text": "spectral clustering"}, {"st": 17, "ed": 19, "text": "positive semidefinite"}, {"st": 34, "ed": 36, "text": "proposed algorithm"}, {"st": 47, "ed": 49, "text": "affinity matrix"}, {"st": 62, "ed": 64, "text": "semidefinite programming"}, {"st": 72, "ed": 74, "text": "computational complexity"}, {"st": 91, "ed": 93, "text": "proposed algorithm"}, {"st": 104, "ed": 106, "text": "faster convergence"}, {"st": 108, "ed": 110, "text": "proposed algorithm"}, {"st": 121, "ed": 123, "text": "point based"}, {"st": 133, "ed": 135, "text": "real world"}, {"st": 135, "ed": 137, "text": "image data"}, {"st": 148, "ed": 150, "text": "spectral clustering"}, {"st": 152, "ed": 154, "text": "proposed algorithm"}, {"st": 156, "ed": 158, "text": "clustering performance"}, {"st": 169, "ed": 171, "text": "larger scale"}]
[{"st": 4, "ed": 6, "text": "parameter free"}, {"st": 12, "ed": 15, "text": "unsupervised feature learning"}, {"st": 41, "ed": 43, "text": "discriminative features"}]
[{"st": 12, "ed": 14, "text": "computer vision"}, {"st": 38, "ed": 40, "text": "image level"}, {"st": 79, "ed": 81, "text": "quasi newton"}, {"st": 84, "ed": 86, "text": "experiments demonstrate"}, {"st": 88, "ed": 90, "text": "proposed approach"}, {"st": 93, "ed": 95, "text": "relative improvement"}, {"st": 107, "ed": 110, "text": "pascal voc 2007"}]
[{"st": 3, "ed": 5, "text": "linear models"}, {"st": 6, "ed": 8, "text": "feature vectors"}, {"st": 30, "ed": 32, "text": "learning rule"}, {"st": 37, "ed": 40, "text": "empirical risk minimization"}, {"st": 61, "ed": 63, "text": "empirical results"}, {"st": 76, "ed": 78, "text": "linear models"}]
[{"st": 0, "ed": 2, "text": "face representation"}, {"st": 4, "ed": 6, "text": "crucial step"}, {"st": 7, "ed": 9, "text": "face recognition"}, {"st": 12, "ed": 14, "text": "face representation"}, {"st": 26, "ed": 28, "text": "hand crafted"}, {"st": 49, "ed": 52, "text": "easy to implement"}, {"st": 52, "ed": 54, "text": "deep learning"}, {"st": 66, "ed": 68, "text": "deep network"}, {"st": 87, "ed": 89, "text": "training procedure"}, {"st": 109, "ed": 111, "text": "multi scale"}, {"st": 128, "ed": 130, "text": "recognition accuracy"}, {"st": 167, "ed": 169, "text": "face images"}, {"st": 170, "ed": 172, "text": "social network"}]
[{"st": 10, "ed": 12, "text": "large scale"}, {"st": 14, "ed": 17, "text": "conditional random fields"}, {"st": 24, "ed": 26, "text": "closed form"}, {"st": 29, "ed": 31, "text": "maximum likelihood"}, {"st": 35, "ed": 37, "text": "graphical model"}, {"st": 54, "ed": 56, "text": "closed form"}, {"st": 68, "ed": 71, "text": "orders of magnitude"}, {"st": 74, "ed": 76, "text": "maximum likelihood"}, {"st": 99, "ed": 101, "text": "joint inference"}, {"st": 113, "ed": 115, "text": "image segmentation"}, {"st": 119, "ed": 121, "text": "highly efficient"}, {"st": 126, "ed": 128, "text": "probabilistic inference"}, {"st": 135, "ed": 137, "text": "image segmentation"}, {"st": 141, "ed": 143, "text": "training sets"}, {"st": 176, "ed": 178, "text": "large scale"}, {"st": 186, "ed": 188, "text": "image segmentation"}, {"st": 196, "ed": 199, "text": "semi supervised learning"}]
[{"st": 55, "ed": 57, "text": "cosine similarity"}, {"st": 98, "ed": 100, "text": "cosine similarity"}]
[{"st": 30, "ed": 32, "text": "statistical properties"}, {"st": 49, "ed": 51, "text": "x ray"}, {"st": 66, "ed": 68, "text": "gabor filter"}, {"st": 75, "ed": 77, "text": "standard deviation"}, {"st": 97, "ed": 99, "text": "co occurrence"}, {"st": 107, "ed": 109, "text": "feature extraction"}, {"st": 110, "ed": 112, "text": "features extracted"}, {"st": 113, "ed": 115, "text": "statistical methods"}, {"st": 118, "ed": 121, "text": "support vector machine"}, {"st": 138, "ed": 140, "text": "co occurrence"}, {"st": 141, "ed": 143, "text": "feature extraction"}, {"st": 146, "ed": 148, "text": "higher accuracy"}, {"st": 156, "ed": 158, "text": "feature extraction"}, {"st": 165, "ed": 167, "text": "gabor filter"}, {"st": 167, "ed": 169, "text": "feature extraction"}]
[{"st": 31, "ed": 34, "text": "image and text"}, {"st": 62, "ed": 64, "text": "topic modeling"}, {"st": 65, "ed": 69, "text": "non negative matrix factorization"}]
[{"st": 61, "ed": 63, "text": "semi supervised"}, {"st": 63, "ed": 65, "text": "incremental learning"}, {"st": 82, "ed": 84, "text": "proposed method"}, {"st": 125, "ed": 127, "text": "practical problems"}, {"st": 135, "ed": 137, "text": "concept drift"}, {"st": 159, "ed": 161, "text": "real world"}, {"st": 161, "ed": 163, "text": "visual data"}, {"st": 164, "ed": 166, "text": "non stationary"}]
[{"st": 1, "ed": 3, "text": "recent advances"}, {"st": 4, "ed": 6, "text": "computer vision"}, {"st": 30, "ed": 32, "text": "existing techniques"}, {"st": 78, "ed": 80, "text": "comparative study"}, {"st": 98, "ed": 100, "text": "existing methods"}, {"st": 122, "ed": 124, "text": "proposed approach"}]
[{"st": 2, "ed": 4, "text": "vision systems"}, {"st": 24, "ed": 26, "text": "scales linearly"}, {"st": 39, "ed": 41, "text": "input images"}, {"st": 81, "ed": 83, "text": "based method"}, {"st": 114, "ed": 116, "text": "proposed method"}, {"st": 134, "ed": 136, "text": "preliminary experiments"}]
[{"st": 7, "ed": 10, "text": "convolutional neural networks"}, {"st": 14, "ed": 16, "text": "classification tasks"}, {"st": 37, "ed": 39, "text": "recognition tasks"}, {"st": 77, "ed": 79, "text": "convolutional neural"}, {"st": 102, "ed": 105, "text": "convolutional neural networks"}]
[{"st": 0, "ed": 2, "text": "quality control"}, {"st": 8, "ed": 10, "text": "textile industry"}, {"st": 21, "ed": 23, "text": "highly competitive"}, {"st": 53, "ed": 55, "text": "computer vision"}]
[{"st": 7, "ed": 9, "text": "transfer learning"}, {"st": 35, "ed": 37, "text": "real world"}, {"st": 46, "ed": 48, "text": "source data"}, {"st": 92, "ed": 94, "text": "computer vision"}, {"st": 98, "ed": 100, "text": "transfer learning"}, {"st": 102, "ed": 104, "text": "feature selection"}, {"st": 107, "ed": 109, "text": "small sample"}, {"st": 123, "ed": 125, "text": "computational cost"}]
[{"st": 11, "ed": 13, "text": "valuable information"}, {"st": 90, "ed": 92, "text": "comparative study"}, {"st": 100, "ed": 102, "text": "fine art"}, {"st": 107, "ed": 109, "text": "comparative study"}, {"st": 124, "ed": 126, "text": "generative models"}, {"st": 140, "ed": 142, "text": "level features"}, {"st": 143, "ed": 145, "text": "low level"}, {"st": 147, "ed": 149, "text": "level features"}]
[{"st": 6, "ed": 8, "text": "important information"}, {"st": 36, "ed": 38, "text": "detection problem"}, {"st": 43, "ed": 45, "text": "object detection"}, {"st": 53, "ed": 56, "text": "curse of dimensionality"}, {"st": 65, "ed": 67, "text": "linear classification"}, {"st": 82, "ed": 85, "text": "field of view"}, {"st": 96, "ed": 98, "text": "sparse linear"}, {"st": 127, "ed": 129, "text": "instance level"}, {"st": 163, "ed": 165, "text": "cross validation"}, {"st": 170, "ed": 172, "text": "false positives"}, {"st": 201, "ed": 203, "text": "compare favorably"}]
[{"st": 9, "ed": 11, "text": "face alignment"}, {"st": 55, "ed": 57, "text": "non trivial"}, {"st": 60, "ed": 62, "text": "inference tasks"}, {"st": 79, "ed": 81, "text": "deep model"}, {"st": 115, "ed": 117, "text": "outperforms existing"}, {"st": 117, "ed": 119, "text": "face alignment"}]
[]
[{"st": 7, "ed": 9, "text": "accuracy loss"}, {"st": 17, "ed": 19, "text": "source domain"}, {"st": 38, "ed": 40, "text": "computer vision"}, {"st": 43, "ed": 45, "text": "image classification"}, {"st": 45, "ed": 47, "text": "object detection"}, {"st": 48, "ed": 50, "text": "object category"}, {"st": 58, "ed": 60, "text": "domain adaptation"}, {"st": 119, "ed": 121, "text": "source domain"}, {"st": 140, "ed": 143, "text": "proof of concept"}, {"st": 148, "ed": 150, "text": "pedestrian detection"}, {"st": 151, "ed": 153, "text": "object category"}, {"st": 194, "ed": 196, "text": "recognition accuracy"}, {"st": 235, "ed": 237, "text": "object category"}]
[{"st": 1, "ed": 3, "text": "image features"}, {"st": 12, "ed": 15, "text": "speed and accuracy"}, {"st": 16, "ed": 18, "text": "large scale"}, {"st": 21, "ed": 23, "text": "image retrieval"}, {"st": 24, "ed": 26, "text": "supervised hashing"}, {"st": 50, "ed": 52, "text": "existing approaches"}, {"st": 57, "ed": 59, "text": "hash function"}, {"st": 61, "ed": 63, "text": "optimization process"}, {"st": 86, "ed": 88, "text": "optimization problems"}, {"st": 111, "ed": 113, "text": "loss functions"}, {"st": 117, "ed": 119, "text": "proposed framework"}, {"st": 123, "ed": 125, "text": "existing approaches"}, {"st": 138, "ed": 140, "text": "problem specific"}, {"st": 155, "ed": 157, "text": "hash function"}, {"st": 184, "ed": 186, "text": "large scale"}, {"st": 219, "ed": 221, "text": "large scale"}, {"st": 228, "ed": 231, "text": "boosted decision trees"}, {"st": 233, "ed": 235, "text": "hash functions"}, {"st": 247, "ed": 249, "text": "experiments demonstrate"}, {"st": 251, "ed": 253, "text": "proposed method"}, {"st": 253, "ed": 255, "text": "significantly outperforms"}]
[{"st": 0, "ed": 3, "text": "distance metric learning"}, {"st": 9, "ed": 11, "text": "training data"}, {"st": 13, "ed": 15, "text": "distance metric"}, {"st": 29, "ed": 31, "text": "metric learning"}, {"st": 38, "ed": 40, "text": "nonconvex optimization"}, {"st": 44, "ed": 46, "text": "metric learning"}, {"st": 50, "ed": 52, "text": "large scale"}, {"st": 58, "ed": 60, "text": "metric learning"}, {"st": 63, "ed": 65, "text": "classification problem"}, {"st": 72, "ed": 75, "text": "support vector machines"}, {"st": 81, "ed": 84, "text": "easy to implement"}, {"st": 90, "ed": 92, "text": "large scale"}, {"st": 95, "ed": 97, "text": "metric learning"}, {"st": 99, "ed": 101, "text": "positive semidefinite"}, {"st": 102, "ed": 104, "text": "metric learning"}, {"st": 109, "ed": 111, "text": "metric learning"}, {"st": 121, "ed": 123, "text": "global optimality"}, {"st": 132, "ed": 134, "text": "handwritten digit"}, {"st": 135, "ed": 137, "text": "face verification"}, {"st": 138, "ed": 141, "text": "person re identification"}, {"st": 145, "ed": 147, "text": "metric learning"}, {"st": 150, "ed": 152, "text": "classification accuracy"}]
[{"st": 17, "ed": 19, "text": "multiple classifiers"}, {"st": 91, "ed": 93, "text": "instance specific"}, {"st": 143, "ed": 145, "text": "based methods"}, {"st": 151, "ed": 153, "text": "labeled training"}]
[{"st": 4, "ed": 6, "text": "maximum margin"}, {"st": 6, "ed": 8, "text": "clustering method"}, {"st": 17, "ed": 19, "text": "maximum margin"}, {"st": 60, "ed": 62, "text": "standard datasets"}, {"st": 65, "ed": 67, "text": "method outperforms"}, {"st": 69, "ed": 71, "text": "hierarchical clustering"}, {"st": 76, "ed": 78, "text": "semantically meaningful"}]
[{"st": 55, "ed": 57, "text": "training points"}, {"st": 89, "ed": 91, "text": "semi supervised"}, {"st": 108, "ed": 110, "text": "learning algorithms"}, {"st": 117, "ed": 119, "text": "proposed algorithm"}, {"st": 121, "ed": 124, "text": "radial basis function"}, {"st": 129, "ed": 131, "text": "objective function"}, {"st": 159, "ed": 161, "text": "regularization term"}, {"st": 175, "ed": 177, "text": "class labels"}]
[{"st": 5, "ed": 7, "text": "machine translation"}, {"st": 8, "ed": 10, "text": "object detection"}, {"st": 13, "ed": 15, "text": "attention based"}, {"st": 17, "ed": 19, "text": "automatically learns"}, {"st": 38, "ed": 40, "text": "standard backpropagation"}, {"st": 60, "ed": 62, "text": "automatically learn"}, {"st": 92, "ed": 94, "text": "benchmark datasets"}]
[{"st": 14, "ed": 16, "text": "deep learning"}, {"st": 26, "ed": 28, "text": "natural image"}, {"st": 38, "ed": 41, "text": "deep neural networks"}, {"st": 52, "ed": 54, "text": "dataset size"}, {"st": 56, "ed": 58, "text": "significant improvements"}, {"st": 72, "ed": 75, "text": "deep neural network"}, {"st": 79, "ed": 81, "text": "creative commons"}, {"st": 100, "ed": 103, "text": "image and video"}, {"st": 118, "ed": 121, "text": "high performance computing"}, {"st": 128, "ed": 130, "text": "preliminary results"}, {"st": 131, "ed": 134, "text": "future research directions"}]
[{"st": 8, "ed": 11, "text": "labeled and unlabeled"}, {"st": 19, "ed": 22, "text": "semi supervised learning"}, {"st": 35, "ed": 39, "text": "non negative matrix factorization"}, {"st": 50, "ed": 52, "text": "learning machines"}, {"st": 59, "ed": 61, "text": "labeled data"}, {"st": 73, "ed": 75, "text": "proposed framework"}, {"st": 98, "ed": 100, "text": "low dimensional"}, {"st": 114, "ed": 116, "text": "benchmark datasets"}, {"st": 119, "ed": 122, "text": "semi supervised learning"}, {"st": 124, "ed": 126, "text": "encouraging results"}]
[{"st": 3, "ed": 5, "text": "multi layer"}, {"st": 27, "ed": 29, "text": "latent variable"}, {"st": 34, "ed": 36, "text": "building block"}, {"st": 48, "ed": 50, "text": "random variables"}, {"st": 57, "ed": 59, "text": "feature map"}, {"st": 70, "ed": 72, "text": "multi layer"}, {"st": 80, "ed": 82, "text": "belief propagation"}, {"st": 113, "ed": 115, "text": "deep networks"}, {"st": 132, "ed": 134, "text": "prior knowledge"}, {"st": 178, "ed": 180, "text": "circuit diagram"}, {"st": 191, "ed": 193, "text": "learning algorithms"}]
[{"st": 11, "ed": 13, "text": "examples include"}, {"st": 15, "ed": 18, "text": "optical character recognition"}, {"st": 26, "ed": 28, "text": "nearest neighbor"}, {"st": 36, "ed": 38, "text": "worst case"}, {"st": 38, "ed": 40, "text": "optimal solution"}, {"st": 62, "ed": 64, "text": "wide applicability"}, {"st": 114, "ed": 116, "text": "linear programming"}, {"st": 177, "ed": 179, "text": "classification tasks"}]
[{"st": 0, "ed": 2, "text": "error correcting"}, {"st": 10, "ed": 12, "text": "multi class"}, {"st": 19, "ed": 21, "text": "pattern recognition"}, {"st": 35, "ed": 37, "text": "multi class"}, {"st": 55, "ed": 57, "text": "error correcting"}, {"st": 72, "ed": 74, "text": "pair wise"}, {"st": 84, "ed": 86, "text": "error correcting"}, {"st": 102, "ed": 104, "text": "error correction"}, {"st": 149, "ed": 151, "text": "discrete optimization"}, {"st": 162, "ed": 165, "text": "block coordinate descent"}, {"st": 181, "ed": 183, "text": "error correction"}, {"st": 200, "ed": 202, "text": "error correction"}]
[{"st": 61, "ed": 63, "text": "multiple modalities"}, {"st": 79, "ed": 81, "text": "deep learning"}, {"st": 91, "ed": 94, "text": "convolutional neural network"}, {"st": 97, "ed": 99, "text": "visual information"}, {"st": 103, "ed": 105, "text": "deep belief"}, {"st": 115, "ed": 117, "text": "k means"}, {"st": 124, "ed": 126, "text": "visual features"}, {"st": 136, "ed": 138, "text": "spatio temporal"}]
[{"st": 0, "ed": 3, "text": "convolutional neural networks"}, {"st": 12, "ed": 14, "text": "excellent results"}, {"st": 21, "ed": 23, "text": "image classification"}, {"st": 23, "ed": 25, "text": "object detection"}, {"st": 43, "ed": 45, "text": "convolutional networks"}, {"st": 77, "ed": 79, "text": "joint training"}, {"st": 87, "ed": 89, "text": "image segmentation"}, {"st": 92, "ed": 94, "text": "encouraging results"}, {"st": 97, "ed": 100, "text": "pascal voc 2012"}]
[{"st": 6, "ed": 8, "text": "synthetic data"}, {"st": 12, "ed": 14, "text": "synthetic data"}, {"st": 19, "ed": 21, "text": "real data"}, {"st": 30, "ed": 32, "text": "real data"}, {"st": 41, "ed": 45, "text": "real and synthetic data"}, {"st": 46, "ed": 48, "text": "jointly learn"}, {"st": 49, "ed": 53, "text": "synthetic and real data"}, {"st": 74, "ed": 76, "text": "feature representation"}, {"st": 81, "ed": 83, "text": "proposed approach"}, {"st": 84, "ed": 86, "text": "conduct experiments"}]
[{"st": 5, "ed": 7, "text": "learning framework"}, {"st": 44, "ed": 46, "text": "jointly learns"}, {"st": 53, "ed": 55, "text": "feature space"}, {"st": 58, "ed": 60, "text": "embedding space"}, {"st": 114, "ed": 116, "text": "zero shot"}]
[{"st": 7, "ed": 9, "text": "temporal information"}, {"st": 14, "ed": 17, "text": "convolutional neural networks"}, {"st": 24, "ed": 26, "text": "spatio temporal"}, {"st": 37, "ed": 39, "text": "convolutional layers"}, {"st": 40, "ed": 42, "text": "spatio temporal"}]
[{"st": 0, "ed": 3, "text": "zero shot learning"}, {"st": 4, "ed": 6, "text": "received increasing"}, {"st": 18, "ed": 20, "text": "training data"}, {"st": 21, "ed": 23, "text": "large scale"}, {"st": 28, "ed": 31, "text": "achieved great success"}, {"st": 70, "ed": 72, "text": "multi label"}, {"st": 88, "ed": 90, "text": "multi label"}, {"st": 90, "ed": 93, "text": "zero shot learning"}, {"st": 101, "ed": 103, "text": "multi label"}, {"st": 109, "ed": 111, "text": "training data"}, {"st": 120, "ed": 122, "text": "multi output"}, {"st": 145, "ed": 147, "text": "word vectors"}, {"st": 150, "ed": 153, "text": "zero shot learning"}, {"st": 155, "ed": 157, "text": "multi label"}, {"st": 173, "ed": 175, "text": "learning strategy"}, {"st": 191, "ed": 194, "text": "zero shot learning"}, {"st": 200, "ed": 202, "text": "multi label"}, {"st": 206, "ed": 208, "text": "method outperforms"}]
[{"st": 1, "ed": 4, "text": "zero shot learning"}, {"st": 6, "ed": 8, "text": "received increasing"}, {"st": 10, "ed": 12, "text": "key idea"}, {"st": 55, "ed": 57, "text": "existing approaches"}, {"st": 63, "ed": 65, "text": "word vectors"}, {"st": 77, "ed": 79, "text": "low level"}, {"st": 106, "ed": 108, "text": "semantic representation"}, {"st": 111, "ed": 113, "text": "zero shot"}, {"st": 155, "ed": 157, "text": "multi label"}]
[{"st": 10, "ed": 12, "text": "sparse representation"}, {"st": 15, "ed": 17, "text": "proposed approach"}, {"st": 18, "ed": 20, "text": "probability distributions"}, {"st": 40, "ed": 42, "text": "class labels"}, {"st": 61, "ed": 63, "text": "class specific"}, {"st": 71, "ed": 73, "text": "proposed approach"}, {"st": 100, "ed": 103, "text": "hierarchical bayesian model"}, {"st": 166, "ed": 168, "text": "sparse representation"}, {"st": 176, "ed": 178, "text": "consistently outperforms"}]
[{"st": 0, "ed": 2, "text": "fully convolutional"}, {"st": 2, "ed": 4, "text": "neural networks"}, {"st": 10, "ed": 12, "text": "input images"}, {"st": 26, "ed": 28, "text": "floating point"}, {"st": 51, "ed": 53, "text": "fully convolutional"}, {"st": 53, "ed": 55, "text": "neural network"}, {"st": 91, "ed": 93, "text": "dot product"}, {"st": 125, "ed": 127, "text": "conduct experiments"}, {"st": 129, "ed": 132, "text": "pascal voc 2012"}, {"st": 162, "ed": 164, "text": "achieve comparable"}]
[{"st": 23, "ed": 27, "text": "signal to noise ratio"}, {"st": 43, "ed": 46, "text": "convolutional neural networks"}, {"st": 66, "ed": 68, "text": "transfer learning"}, {"st": 69, "ed": 71, "text": "pre processing"}]
[{"st": 23, "ed": 25, "text": "weakly supervised"}, {"st": 68, "ed": 70, "text": "large scale"}, {"st": 70, "ed": 72, "text": "medical imaging"}, {"st": 93, "ed": 95, "text": "deep convolutional"}, {"st": 108, "ed": 110, "text": "feature representation"}, {"st": 152, "ed": 154, "text": "learned features"}]
[{"st": 8, "ed": 10, "text": "generate realistic"}, {"st": 20, "ed": 22, "text": "computer graphics"}, {"st": 72, "ed": 74, "text": "image synthesis"}, {"st": 86, "ed": 88, "text": "generate realistic"}, {"st": 97, "ed": 99, "text": "image synthesis"}, {"st": 135, "ed": 137, "text": "feed forward"}, {"st": 158, "ed": 160, "text": "image synthesis"}]
[{"st": 5, "ed": 7, "text": "similarity metric"}, {"st": 9, "ed": 12, "text": "end to end"}, {"st": 14, "ed": 16, "text": "deep networks"}, {"st": 26, "ed": 28, "text": "recent approaches"}, {"st": 32, "ed": 34, "text": "performance degradation"}, {"st": 39, "ed": 41, "text": "training procedure"}, {"st": 56, "ed": 58, "text": "metric learning"}, {"st": 78, "ed": 80, "text": "structured prediction"}, {"st": 103, "ed": 105, "text": "retrieval tasks"}]
[{"st": 6, "ed": 8, "text": "natural images"}, {"st": 43, "ed": 45, "text": "prior knowledge"}, {"st": 56, "ed": 59, "text": "end to end"}, {"st": 88, "ed": 90, "text": "pair wise"}, {"st": 119, "ed": 121, "text": "mnist dataset"}, {"st": 123, "ed": 126, "text": "proof of concept"}, {"st": 138, "ed": 140, "text": "significantly outperforms"}, {"st": 150, "ed": 152, "text": "cifar 10"}]
[{"st": 6, "ed": 10, "text": "convolutional neural network cnn"}, {"st": 49, "ed": 51, "text": "neural network"}, {"st": 52, "ed": 54, "text": "transfer learning"}, {"st": 120, "ed": 122, "text": "experimentally demonstrate"}, {"st": 130, "ed": 132, "text": "vgg 16"}, {"st": 141, "ed": 144, "text": "synthetic aperture radar"}]
[{"st": 4, "ed": 6, "text": "deep learning"}, {"st": 9, "ed": 11, "text": "residual network"}, {"st": 28, "ed": 31, "text": "end to end"}, {"st": 48, "ed": 50, "text": "computer vision"}, {"st": 53, "ed": 55, "text": "image classification"}, {"st": 55, "ed": 57, "text": "object detection"}, {"st": 70, "ed": 72, "text": "computational efficiency"}, {"st": 73, "ed": 75, "text": "residual networks"}, {"st": 78, "ed": 80, "text": "imagenet classification"}, {"st": 82, "ed": 84, "text": "object detection"}]
[{"st": 20, "ed": 22, "text": "spatial transformer"}, {"st": 65, "ed": 67, "text": "spatial transformer"}, {"st": 103, "ed": 105, "text": "classification problems"}, {"st": 106, "ed": 108, "text": "real world"}]
[{"st": 8, "ed": 10, "text": "challenging problem"}, {"st": 15, "ed": 17, "text": "computer vision"}, {"st": 65, "ed": 68, "text": "deep neural network"}, {"st": 81, "ed": 83, "text": "input image"}, {"st": 137, "ed": 139, "text": "feed forward"}, {"st": 139, "ed": 141, "text": "deep network"}, {"st": 210, "ed": 212, "text": "extensively evaluate"}, {"st": 217, "ed": 219, "text": "challenging task"}, {"st": 229, "ed": 232, "text": "quantitative and qualitative"}]
[{"st": 25, "ed": 27, "text": "simultaneously learns"}, {"st": 50, "ed": 53, "text": "end to end"}, {"st": 53, "ed": 55, "text": "multi modal"}, {"st": 55, "ed": 58, "text": "convolutional neural network"}, {"st": 61, "ed": 63, "text": "input images"}, {"st": 64, "ed": 66, "text": "sheet music"}, {"st": 105, "ed": 108, "text": "deep neural networks"}, {"st": 114, "ed": 116, "text": "image processing"}, {"st": 119, "ed": 121, "text": "sheet music"}, {"st": 126, "ed": 128, "text": "future research"}]
[{"st": 11, "ed": 13, "text": "frame rate"}, {"st": 14, "ed": 16, "text": "compressive sensing"}, {"st": 19, "ed": 21, "text": "prior works"}, {"st": 25, "ed": 27, "text": "iterative optimization"}, {"st": 27, "ed": 29, "text": "based approaches"}, {"st": 49, "ed": 51, "text": "original input"}, {"st": 80, "ed": 82, "text": "compression ratio"}, {"st": 84, "ed": 86, "text": "spatial temporal"}, {"st": 92, "ed": 94, "text": "experiment results"}, {"st": 97, "ed": 99, "text": "significantly outperforms"}, {"st": 108, "ed": 110, "text": "post processing"}, {"st": 121, "ed": 123, "text": "frame rate"}, {"st": 152, "ed": 155, "text": "orders of magnitude"}, {"st": 165, "ed": 167, "text": "source code"}, {"st": 168, "ed": 170, "text": "https github.com"}]
[{"st": 26, "ed": 28, "text": "video games"}, {"st": 34, "ed": 36, "text": "reinforcement learning"}, {"st": 48, "ed": 50, "text": "soft attention"}, {"st": 54, "ed": 58, "text": "deep q network dqn"}, {"st": 94, "ed": 96, "text": "soft attention"}]
[{"st": 6, "ed": 9, "text": "computer aided diagnosis"}, {"st": 12, "ed": 14, "text": "traditional methods"}, {"st": 20, "ed": 22, "text": "training data"}, {"st": 28, "ed": 30, "text": "computational models"}, {"st": 42, "ed": 44, "text": "deep convolutional"}, {"st": 46, "ed": 48, "text": "natural image"}, {"st": 50, "ed": 52, "text": "multi instance"}, {"st": 62, "ed": 65, "text": "end to end"}, {"st": 67, "ed": 69, "text": "multi instance"}, {"st": 95, "ed": 97, "text": "multi instance"}, {"st": 113, "ed": 115, "text": "deep networks"}]
[{"st": 29, "ed": 31, "text": "deep convolutional"}, {"st": 33, "ed": 35, "text": "natural image"}, {"st": 37, "ed": 40, "text": "conditional random fields"}, {"st": 47, "ed": 50, "text": "end to end"}, {"st": 59, "ed": 62, "text": "fully convolutional network"}, {"st": 104, "ed": 106, "text": "adversarial training"}, {"st": 137, "ed": 140, "text": "end to end"}, {"st": 143, "ed": 145, "text": "adversarial training"}]
[{"st": 18, "ed": 22, "text": "convolutional neural network cnn"}, {"st": 77, "ed": 79, "text": "convolutional architectures"}, {"st": 81, "ed": 83, "text": "non linearity"}, {"st": 152, "ed": 154, "text": "non linearity"}, {"st": 197, "ed": 199, "text": "non linearity"}]
[{"st": 1, "ed": 3, "text": "recent years"}, {"st": 30, "ed": 32, "text": "object categories"}, {"st": 78, "ed": 80, "text": "skip connections"}, {"st": 88, "ed": 90, "text": "low level"}, {"st": 100, "ed": 102, "text": "low level"}, {"st": 174, "ed": 176, "text": "contextual information"}, {"st": 177, "ed": 179, "text": "low level"}, {"st": 213, "ed": 215, "text": "multi scale"}]
[{"st": 4, "ed": 7, "text": "recurrent neural networks"}, {"st": 10, "ed": 12, "text": "image caption"}, {"st": 33, "ed": 35, "text": "competitive performance"}, {"st": 53, "ed": 55, "text": "hidden state"}, {"st": 91, "ed": 93, "text": "ms coco"}, {"st": 96, "ed": 99, "text": "extensive experimental results"}, {"st": 102, "ed": 104, "text": "method outperforms"}, {"st": 106, "ed": 109, "text": "recurrent neural network"}]
[{"st": 3, "ed": 6, "text": "support vector machines"}, {"st": 9, "ed": 11, "text": "multiple classifiers"}, {"st": 20, "ed": 23, "text": "global and local"}, {"st": 40, "ed": 42, "text": "mahalanobis distance"}]
[{"st": 18, "ed": 20, "text": "scene recognition"}, {"st": 20, "ed": 22, "text": "object detection"}, {"st": 23, "ed": 25, "text": "motion planning"}, {"st": 37, "ed": 39, "text": "machine learning"}, {"st": 39, "ed": 41, "text": "approach called"}, {"st": 55, "ed": 57, "text": "computer vision"}, {"st": 97, "ed": 100, "text": "markov random fields"}, {"st": 109, "ed": 111, "text": "ground truth"}, {"st": 170, "ed": 172, "text": "learning algorithm"}, {"st": 193, "ed": 195, "text": "unsupervised training"}, {"st": 218, "ed": 220, "text": "energy minimization"}, {"st": 260, "ed": 262, "text": "evaluation metrics"}]
[{"st": 0, "ed": 2, "text": "transfer learning"}, {"st": 8, "ed": 10, "text": "training sets"}, {"st": 43, "ed": 45, "text": "transfer learning"}, {"st": 46, "ed": 48, "text": "visual recognition"}]
[{"st": 0, "ed": 2, "text": "image denoising"}, {"st": 16, "ed": 18, "text": "noise free"}, {"st": 44, "ed": 46, "text": "multi layer"}, {"st": 73, "ed": 75, "text": "image denoising"}, {"st": 79, "ed": 81, "text": "method achieves"}, {"st": 114, "ed": 116, "text": "extensively studied"}, {"st": 123, "ed": 125, "text": "gaussian noise"}, {"st": 139, "ed": 141, "text": "excellent results"}]
[{"st": 0, "ed": 2, "text": "image denoising"}, {"st": 16, "ed": 18, "text": "noise free"}, {"st": 25, "ed": 27, "text": "multi layer"}, {"st": 31, "ed": 33, "text": "image denoising"}, {"st": 41, "ed": 43, "text": "gaussian noise"}, {"st": 45, "ed": 47, "text": "gaussian noise"}, {"st": 96, "ed": 98, "text": "hidden units"}, {"st": 109, "ed": 111, "text": "multi layer"}]
[{"st": 21, "ed": 23, "text": "low dimensional"}, {"st": 57, "ed": 59, "text": "low dimensional"}, {"st": 98, "ed": 100, "text": "low dimensional"}]
[{"st": 5, "ed": 7, "text": "supervised learning"}, {"st": 30, "ed": 32, "text": "error rates"}, {"st": 72, "ed": 74, "text": "existing methods"}, {"st": 85, "ed": 88, "text": "field of view"}, {"st": 153, "ed": 155, "text": "electron microscopy"}, {"st": 182, "ed": 185, "text": "supervised and unsupervised"}, {"st": 190, "ed": 193, "text": "field of view"}, {"st": 211, "ed": 214, "text": "open source software"}]
[{"st": 4, "ed": 6, "text": "object detection"}, {"st": 10, "ed": 12, "text": "false positive"}, {"st": 85, "ed": 88, "text": "false positive rate"}, {"st": 98, "ed": 100, "text": "ensemble learning"}, {"st": 109, "ed": 111, "text": "user defined"}, {"st": 113, "ed": 115, "text": "false positive"}, {"st": 117, "ed": 119, "text": "directly optimizing"}, {"st": 131, "ed": 133, "text": "false positive"}, {"st": 135, "ed": 137, "text": "proposed method"}, {"st": 161, "ed": 165, "text": "synthetic and real world"}, {"st": 192, "ed": 194, "text": "ensemble learning"}]
[{"st": 4, "ed": 6, "text": "machine learning"}, {"st": 9, "ed": 11, "text": "supervised learning"}, {"st": 17, "ed": 19, "text": "labeled examples"}, {"st": 38, "ed": 40, "text": "labeled examples"}, {"st": 72, "ed": 74, "text": "supervised learning"}, {"st": 76, "ed": 78, "text": "small sample"}, {"st": 86, "ed": 89, "text": "visual object recognition"}, {"st": 97, "ed": 99, "text": "starting point"}, {"st": 107, "ed": 109, "text": "image representations"}, {"st": 122, "ed": 124, "text": "sample complexity"}, {"st": 220, "ed": 222, "text": "deep learning"}, {"st": 222, "ed": 224, "text": "convolutional architectures"}, {"st": 242, "ed": 244, "text": "visual cortex"}]
[{"st": 27, "ed": 29, "text": "large scale"}, {"st": 48, "ed": 50, "text": "online algorithms"}, {"st": 59, "ed": 61, "text": "typically requires"}, {"st": 75, "ed": 77, "text": "stochastic optimization"}, {"st": 82, "ed": 84, "text": "theoretical properties"}, {"st": 99, "ed": 101, "text": "output spaces"}, {"st": 128, "ed": 130, "text": "support vectors"}, {"st": 144, "ed": 146, "text": "efficient learning"}, {"st": 178, "ed": 180, "text": "recognition systems"}, {"st": 182, "ed": 184, "text": "pose estimation"}, {"st": 187, "ed": 189, "text": "object recognition"}]
[{"st": 3, "ed": 5, "text": "time series"}, {"st": 12, "ed": 14, "text": "challenging problem"}, {"st": 15, "ed": 17, "text": "great importance"}, {"st": 33, "ed": 35, "text": "decision making"}, {"st": 43, "ed": 45, "text": "time series"}, {"st": 50, "ed": 52, "text": "main idea"}, {"st": 70, "ed": 72, "text": "posterior probability"}, {"st": 76, "ed": 78, "text": "proposed method"}, {"st": 119, "ed": 121, "text": "wind tunnel"}]
[{"st": 0, "ed": 2, "text": "error correcting"}, {"st": 32, "ed": 36, "text": "feed forward neural networks"}, {"st": 62, "ed": 64, "text": "neural networks"}, {"st": 75, "ed": 77, "text": "back propagation"}, {"st": 88, "ed": 90, "text": "face recognition"}]
[{"st": 0, "ed": 2, "text": "object recognition"}, {"st": 19, "ed": 21, "text": "contextual information"}, {"st": 25, "ed": 27, "text": "object recognition"}, {"st": 57, "ed": 59, "text": "common sense"}, {"st": 61, "ed": 63, "text": "co occurrence"}, {"st": 93, "ed": 95, "text": "co occurrences"}, {"st": 106, "ed": 108, "text": "co occurrence"}, {"st": 122, "ed": 124, "text": "optimization problem"}, {"st": 138, "ed": 140, "text": "optimization problem"}, {"st": 153, "ed": 155, "text": "significant improvements"}]
[{"st": 9, "ed": 11, "text": "image classification"}, {"st": 84, "ed": 86, "text": "classification tasks"}, {"st": 109, "ed": 111, "text": "classification tasks"}]
[{"st": 9, "ed": 11, "text": "object detection"}, {"st": 38, "ed": 40, "text": "training data"}, {"st": 53, "ed": 55, "text": "dynamic programming"}, {"st": 118, "ed": 121, "text": "loss in accuracy"}, {"st": 125, "ed": 128, "text": "pascal voc 2007"}]
[{"st": 12, "ed": 14, "text": "convolutional networks"}, {"st": 16, "ed": 18, "text": "object recognition"}, {"st": 31, "ed": 33, "text": "floating point"}, {"st": 67, "ed": 69, "text": "convolutional filters"}, {"st": 73, "ed": 75, "text": "significantly reduce"}, {"st": 91, "ed": 93, "text": "convolutional layers"}]
[{"st": 0, "ed": 2, "text": "supervised hashing"}, {"st": 26, "ed": 28, "text": "hash functions"}, {"st": 44, "ed": 46, "text": "kernel functions"}, {"st": 51, "ed": 53, "text": "non linearity"}, {"st": 58, "ed": 60, "text": "retrieval performance"}, {"st": 74, "ed": 77, "text": "boosted decision trees"}, {"st": 79, "ed": 81, "text": "non linearity"}, {"st": 113, "ed": 115, "text": "inference problem"}, {"st": 125, "ed": 127, "text": "large scale"}, {"st": 131, "ed": 133, "text": "hash functions"}, {"st": 135, "ed": 138, "text": "boosted decision trees"}, {"st": 143, "ed": 145, "text": "experiments demonstrate"}, {"st": 147, "ed": 149, "text": "proposed method"}, {"st": 149, "ed": 151, "text": "significantly outperforms"}, {"st": 171, "ed": 174, "text": "orders of magnitude"}]
[{"st": 18, "ed": 23, "text": "computer vision and machine learning"}, {"st": 122, "ed": 124, "text": "cost effective"}, {"st": 124, "ed": 126, "text": "human intelligence"}, {"st": 137, "ed": 139, "text": "sampling algorithm"}, {"st": 149, "ed": 151, "text": "higher quality"}]
[{"st": 1, "ed": 3, "text": "large scale"}, {"st": 3, "ed": 5, "text": "image data"}, {"st": 6, "ed": 8, "text": "object categories"}, {"st": 14, "ed": 16, "text": "received increasing"}, {"st": 29, "ed": 31, "text": "nearest neighbor"}, {"st": 33, "ed": 36, "text": "shown promising results"}, {"st": 49, "ed": 51, "text": "metric learning"}, {"st": 60, "ed": 62, "text": "existing algorithms"}, {"st": 66, "ed": 68, "text": "large scale"}, {"st": 77, "ed": 79, "text": "similarity learning"}, {"st": 112, "ed": 114, "text": "similarity measure"}, {"st": 117, "ed": 119, "text": "large margin"}, {"st": 140, "ed": 142, "text": "lower dimensional"}, {"st": 150, "ed": 152, "text": "similarity function"}, {"st": 157, "ed": 160, "text": "stochastic gradient descent"}, {"st": 165, "ed": 167, "text": "sampling strategy"}, {"st": 191, "ed": 194, "text": "tens of thousands"}]
[{"st": 6, "ed": 8, "text": "van gogh"}, {"st": 19, "ed": 21, "text": "feature extraction"}, {"st": 70, "ed": 72, "text": "van gogh"}, {"st": 87, "ed": 89, "text": "numerical results"}, {"st": 96, "ed": 98, "text": "classification accuracy"}, {"st": 103, "ed": 105, "text": "cross validation"}, {"st": 132, "ed": 134, "text": "classification accuracy"}, {"st": 191, "ed": 193, "text": "discriminative features"}, {"st": 194, "ed": 196, "text": "van gogh"}, {"st": 224, "ed": 226, "text": "van gogh"}]
[{"st": 7, "ed": 9, "text": "large scale"}, {"st": 30, "ed": 32, "text": "loss functions"}, {"st": 36, "ed": 38, "text": "evaluation criteria"}, {"st": 41, "ed": 43, "text": "performance measures"}, {"st": 61, "ed": 63, "text": "directly optimize"}, {"st": 68, "ed": 70, "text": "optimization problem"}, {"st": 85, "ed": 87, "text": "structured output"}, {"st": 92, "ed": 94, "text": "optimization problem"}, {"st": 102, "ed": 104, "text": "cutting plane"}, {"st": 118, "ed": 120, "text": "image retrieval"}]
[{"st": 62, "ed": 64, "text": "weakly supervised"}]
[{"st": 19, "ed": 23, "text": "magnetic resonance imaging mri"}, {"st": 59, "ed": 62, "text": "support vector machine"}, {"st": 67, "ed": 69, "text": "feature selection"}, {"st": 72, "ed": 74, "text": "multi objective"}, {"st": 105, "ed": 108, "text": "shown promising results"}, {"st": 129, "ed": 132, "text": "fold cross validation"}]
[{"st": 3, "ed": 5, "text": "remote sensing"}, {"st": 13, "ed": 15, "text": "remote sensing"}, {"st": 26, "ed": 28, "text": "remote sensing"}, {"st": 28, "ed": 30, "text": "image classification"}, {"st": 45, "ed": 47, "text": "maximum likelihood"}, {"st": 55, "ed": 57, "text": "fuzzy logic"}]
[{"st": 1, "ed": 3, "text": "learning problems"}, {"st": 5, "ed": 7, "text": "object classification"}, {"st": 18, "ed": 21, "text": "bag of words"}, {"st": 25, "ed": 27, "text": "great success"}, {"st": 31, "ed": 33, "text": "visual features"}, {"st": 56, "ed": 58, "text": "visual words"}, {"st": 85, "ed": 87, "text": "feature selection"}, {"st": 113, "ed": 115, "text": "main idea"}, {"st": 129, "ed": 131, "text": "latent variables"}, {"st": 138, "ed": 141, "text": "support vector machine"}, {"st": 179, "ed": 181, "text": "spatio temporal"}, {"st": 190, "ed": 192, "text": "feature selection"}, {"st": 214, "ed": 217, "text": "multiple kernel learning"}, {"st": 218, "ed": 221, "text": "multiple instance learning"}, {"st": 226, "ed": 229, "text": "pascal voc 2007"}]
[{"st": 3, "ed": 5, "text": "optimization problem"}, {"st": 14, "ed": 16, "text": "cost function"}, {"st": 32, "ed": 34, "text": "feature selection"}, {"st": 47, "ed": 49, "text": "optimal solutions"}, {"st": 100, "ed": 103, "text": "branch and bound"}]
[{"st": 43, "ed": 46, "text": "large training sets"}, {"st": 61, "ed": 63, "text": "image pixels"}, {"st": 138, "ed": 140, "text": "decision tree"}, {"st": 155, "ed": 157, "text": "building block"}, {"st": 160, "ed": 163, "text": "ability to learn"}, {"st": 257, "ed": 259, "text": "medical image"}, {"st": 262, "ed": 264, "text": "multi label"}]
[{"st": 15, "ed": 18, "text": "semi supervised learning"}, {"st": 27, "ed": 29, "text": "low rank"}, {"st": 55, "ed": 57, "text": "low rank"}, {"st": 58, "ed": 60, "text": "sparse matrix"}, {"st": 67, "ed": 69, "text": "linear combination"}, {"st": 148, "ed": 150, "text": "extensive experiments"}, {"st": 152, "ed": 155, "text": "publicly available datasets"}, {"st": 158, "ed": 160, "text": "proposed method"}, {"st": 171, "ed": 173, "text": "large margin"}, {"st": 175, "ed": 177, "text": "semi supervised"}]
[{"st": 5, "ed": 7, "text": "k means"}, {"st": 10, "ed": 12, "text": "clustering algorithm"}, {"st": 16, "ed": 18, "text": "application domains"}, {"st": 46, "ed": 48, "text": "cluster centers"}, {"st": 56, "ed": 58, "text": "initialization methods"}, {"st": 124, "ed": 126, "text": "common practice"}, {"st": 152, "ed": 154, "text": "computational requirements"}, {"st": 157, "ed": 159, "text": "highly efficient"}, {"st": 159, "ed": 161, "text": "k means"}, {"st": 168, "ed": 170, "text": "empirical performance"}, {"st": 179, "ed": 181, "text": "k means"}, {"st": 181, "ed": 183, "text": "initialization methods"}, {"st": 195, "ed": 197, "text": "machine learning"}, {"st": 206, "ed": 208, "text": "initialization methods"}]
[{"st": 19, "ed": 21, "text": "object appearance"}, {"st": 32, "ed": 34, "text": "learning rules"}, {"st": 37, "ed": 39, "text": "recent results"}, {"st": 50, "ed": 52, "text": "based method"}, {"st": 107, "ed": 109, "text": "previous studies"}, {"st": 142, "ed": 144, "text": "class specific"}, {"st": 173, "ed": 175, "text": "face recognition"}, {"st": 177, "ed": 179, "text": "natural images"}]
[{"st": 13, "ed": 15, "text": "computer vision"}, {"st": 39, "ed": 41, "text": "ground truth"}, {"st": 69, "ed": 71, "text": "recognition rate"}, {"st": 79, "ed": 81, "text": "training sample"}, {"st": 86, "ed": 88, "text": "deep models"}, {"st": 93, "ed": 95, "text": "computer vision"}, {"st": 98, "ed": 100, "text": "training data"}, {"st": 108, "ed": 110, "text": "transfer learning"}, {"st": 120, "ed": 122, "text": "weakly labeled"}, {"st": 137, "ed": 139, "text": "natural images"}, {"st": 169, "ed": 171, "text": "transfer learning"}, {"st": 191, "ed": 193, "text": "weakly labeled"}, {"st": 195, "ed": 197, "text": "learning process"}, {"st": 210, "ed": 212, "text": "image data"}, {"st": 225, "ed": 227, "text": "visual recognition"}, {"st": 231, "ed": 233, "text": "training data"}, {"st": 244, "ed": 246, "text": "computer vision"}, {"st": 289, "ed": 291, "text": "time consuming"}]
[{"st": 0, "ed": 2, "text": "sparse filtering"}, {"st": 5, "ed": 7, "text": "feature learning"}, {"st": 9, "ed": 11, "text": "image classification"}, {"st": 20, "ed": 22, "text": "sparse filtering"}, {"st": 36, "ed": 38, "text": "sparse filtering"}, {"st": 42, "ed": 44, "text": "early stopping"}, {"st": 71, "ed": 73, "text": "pre processing"}, {"st": 84, "ed": 86, "text": "image classification"}, {"st": 87, "ed": 89, "text": "sparse filtering"}]
[{"st": 4, "ed": 6, "text": "object detection"}, {"st": 10, "ed": 12, "text": "false positive"}, {"st": 69, "ed": 71, "text": "ensemble learning"}, {"st": 80, "ed": 82, "text": "user defined"}, {"st": 84, "ed": 86, "text": "false positive"}, {"st": 88, "ed": 90, "text": "directly optimizing"}, {"st": 102, "ed": 104, "text": "object detection"}, {"st": 112, "ed": 114, "text": "low level"}, {"st": 114, "ed": 116, "text": "visual features"}, {"st": 139, "ed": 143, "text": "synthetic and real world"}, {"st": 170, "ed": 172, "text": "ensemble learning"}, {"st": 189, "ed": 191, "text": "pedestrian detection"}]
[{"st": 9, "ed": 11, "text": "classification algorithm"}, {"st": 29, "ed": 31, "text": "computationally expensive"}, {"st": 60, "ed": 62, "text": "classification techniques"}, {"st": 70, "ed": 73, "text": "support vector machines"}, {"st": 83, "ed": 85, "text": "classification accuracy"}, {"st": 107, "ed": 109, "text": "linearly separable"}, {"st": 116, "ed": 118, "text": "neural network"}]
[{"st": 10, "ed": 12, "text": "face recognition"}, {"st": 16, "ed": 18, "text": "sparse representation"}, {"st": 20, "ed": 22, "text": "feature selection"}, {"st": 27, "ed": 29, "text": "feature selection"}, {"st": 46, "ed": 48, "text": "computational cost"}, {"st": 49, "ed": 51, "text": "sparse representation"}, {"st": 64, "ed": 67, "text": "extreme learning machine"}, {"st": 67, "ed": 69, "text": "auto encoder"}, {"st": 76, "ed": 78, "text": "local features"}, {"st": 82, "ed": 84, "text": "multiple scales"}, {"st": 89, "ed": 91, "text": "feature based"}, {"st": 105, "ed": 107, "text": "face image"}, {"st": 111, "ed": 113, "text": "feature based"}, {"st": 124, "ed": 126, "text": "feature based"}, {"st": 136, "ed": 138, "text": "feature based"}, {"st": 146, "ed": 148, "text": "face images"}, {"st": 153, "ed": 155, "text": "low dimensional"}, {"st": 173, "ed": 175, "text": "sparse representation"}, {"st": 186, "ed": 188, "text": "computational cost"}, {"st": 201, "ed": 203, "text": "feature based"}]
[{"st": 0, "ed": 2, "text": "feature selection"}, {"st": 3, "ed": 5, "text": "large scale"}, {"st": 14, "ed": 16, "text": "machine learning"}, {"st": 19, "ed": 22, "text": "online feature selection"}, {"st": 30, "ed": 33, "text": "efficient and scalable"}, {"st": 64, "ed": 67, "text": "online feature selection"}, {"st": 70, "ed": 73, "text": "simple yet effective"}, {"st": 81, "ed": 83, "text": "large scale"}, {"st": 99, "ed": 102, "text": "online feature selection"}, {"st": 113, "ed": 115, "text": "important features"}, {"st": 131, "ed": 134, "text": "high computational cost"}, {"st": 143, "ed": 146, "text": "online feature selection"}, {"st": 149, "ed": 151, "text": "based approach"}, {"st": 167, "ed": 170, "text": "efficient and scalable"}, {"st": 171, "ed": 173, "text": "large scale"}, {"st": 173, "ed": 175, "text": "feature selection"}]
[{"st": 3, "ed": 5, "text": "dictionary learning"}, {"st": 9, "ed": 11, "text": "ell 1"}, {"st": 25, "ed": 27, "text": "gaussian noise"}, {"st": 29, "ed": 31, "text": "proposed algorithm"}, {"st": 39, "ed": 41, "text": "ell 2"}, {"st": 48, "ed": 50, "text": "ell 1"}, {"st": 50, "ed": 52, "text": "k svd"}, {"st": 67, "ed": 69, "text": "ell 1"}, {"st": 80, "ed": 82, "text": "ell 1"}, {"st": 82, "ed": 84, "text": "k svd"}, {"st": 94, "ed": 96, "text": "k svd"}, {"st": 113, "ed": 115, "text": "gaussian noise"}, {"st": 133, "ed": 135, "text": "ell 1"}, {"st": 135, "ed": 137, "text": "k svd"}, {"st": 140, "ed": 142, "text": "k svd"}, {"st": 147, "ed": 149, "text": "training set"}, {"st": 155, "ed": 157, "text": "proposed algorithm"}, {"st": 159, "ed": 161, "text": "natural images"}, {"st": 172, "ed": 174, "text": "ell 1"}, {"st": 174, "ed": 176, "text": "k svd"}, {"st": 183, "ed": 187, "text": "signal to noise ratio"}, {"st": 189, "ed": 191, "text": "k svd"}, {"st": 198, "ed": 200, "text": "structural similarity"}, {"st": 217, "ed": 219, "text": "ell 1"}]
[{"st": 3, "ed": 5, "text": "generative model"}, {"st": 14, "ed": 16, "text": "missing data"}, {"st": 28, "ed": 30, "text": "rank tensor"}, {"st": 40, "ed": 42, "text": "local information"}, {"st": 58, "ed": 60, "text": "rank tensor"}, {"st": 67, "ed": 69, "text": "latent factors"}, {"st": 110, "ed": 112, "text": "closed form"}, {"st": 112, "ed": 114, "text": "variational inference"}, {"st": 116, "ed": 118, "text": "fully bayesian"}, {"st": 127, "ed": 129, "text": "scales linearly"}, {"st": 166, "ed": 168, "text": "sparsity inducing"}, {"st": 180, "ed": 182, "text": "low rank"}, {"st": 185, "ed": 187, "text": "sparse representation"}, {"st": 198, "ed": 200, "text": "extensive experiments"}, {"st": 211, "ed": 216, "text": "synthetic and real world datasets"}]
[{"st": 11, "ed": 14, "text": "hidden markov models"}, {"st": 141, "ed": 143, "text": "character level"}, {"st": 176, "ed": 178, "text": "training data"}]
[{"st": 14, "ed": 17, "text": "multiple kernel learning"}, {"st": 32, "ed": 34, "text": "extensively studied"}, {"st": 38, "ed": 41, "text": "support vector machines"}, {"st": 63, "ed": 65, "text": "convex optimization"}, {"st": 82, "ed": 84, "text": "computer vision"}, {"st": 89, "ed": 91, "text": "optimization procedure"}, {"st": 93, "ed": 96, "text": "guaranteed to converge"}, {"st": 98, "ed": 100, "text": "global optimum"}, {"st": 106, "ed": 108, "text": "experimentally demonstrate"}, {"st": 129, "ed": 131, "text": "dimensionality reduction"}, {"st": 132, "ed": 134, "text": "cross modal"}, {"st": 149, "ed": 151, "text": "recently proposed"}]
[{"st": 3, "ed": 5, "text": "image classification"}, {"st": 30, "ed": 32, "text": "kernel method"}, {"st": 75, "ed": 77, "text": "similarity measure"}, {"st": 84, "ed": 87, "text": "simple yet effective"}, {"st": 88, "ed": 90, "text": "similarity measure"}, {"st": 117, "ed": 119, "text": "similarity measure"}, {"st": 143, "ed": 145, "text": "training set"}, {"st": 186, "ed": 188, "text": "excellent performance"}]
[{"st": 2, "ed": 4, "text": "kernel methods"}, {"st": 7, "ed": 9, "text": "metric spaces"}, {"st": 22, "ed": 24, "text": "gaussian kernel"}, {"st": 30, "ed": 32, "text": "positive definite"}, {"st": 36, "ed": 38, "text": "metric space"}, {"st": 50, "ed": 52, "text": "riemannian manifold"}, {"st": 54, "ed": 56, "text": "gaussian kernel"}, {"st": 58, "ed": 60, "text": "positive definite"}, {"st": 62, "ed": 64, "text": "riemannian manifold"}, {"st": 74, "ed": 76, "text": "gaussian kernels"}, {"st": 123, "ed": 125, "text": "theoretical results"}]
[{"st": 13, "ed": 15, "text": "challenging task"}]
[{"st": 7, "ed": 9, "text": "deep architectures"}, {"st": 12, "ed": 15, "text": "deep belief networks"}, {"st": 17, "ed": 19, "text": "deep architectures"}, {"st": 23, "ed": 26, "text": "restricted boltzmann machines"}, {"st": 31, "ed": 33, "text": "generative model"}, {"st": 59, "ed": 61, "text": "free energy"}, {"st": 92, "ed": 94, "text": "error rate"}, {"st": 103, "ed": 105, "text": "result shows"}, {"st": 107, "ed": 109, "text": "proposed method"}, {"st": 120, "ed": 122, "text": "error rate"}, {"st": 123, "ed": 125, "text": "general classification"}, {"st": 130, "ed": 132, "text": "error rate"}, {"st": 136, "ed": 138, "text": "error rate"}, {"st": 146, "ed": 148, "text": "classification error"}, {"st": 154, "ed": 156, "text": "error rate"}]
[{"st": 0, "ed": 2, "text": "mean shift"}, {"st": 29, "ed": 31, "text": "mean shift"}, {"st": 66, "ed": 68, "text": "low dimensional"}, {"st": 68, "ed": 70, "text": "feature spaces"}, {"st": 78, "ed": 80, "text": "mean shift"}]
[{"st": 0, "ed": 2, "text": "domain adaptation"}, {"st": 10, "ed": 12, "text": "source domain"}, {"st": 25, "ed": 27, "text": "classification tasks"}, {"st": 49, "ed": 51, "text": "domain invariant"}, {"st": 95, "ed": 97, "text": "alternating optimization"}, {"st": 116, "ed": 118, "text": "domain adaptation"}]
[{"st": 4, "ed": 6, "text": "visual recognition"}, {"st": 13, "ed": 17, "text": "amounts of training data"}, {"st": 56, "ed": 58, "text": "weakly supervised"}, {"st": 59, "ed": 61, "text": "based approach"}, {"st": 93, "ed": 95, "text": "pascal voc"}, {"st": 99, "ed": 101, "text": "visual search"}, {"st": 122, "ed": 124, "text": "reinforcement learning"}, {"st": 136, "ed": 138, "text": "weakly supervised"}, {"st": 162, "ed": 164, "text": "sliding window"}]
[{"st": 4, "ed": 6, "text": "challenging problem"}, {"st": 7, "ed": 9, "text": "computer vision"}, {"st": 13, "ed": 15, "text": "object tracking"}, {"st": 20, "ed": 22, "text": "spatio temporal"}, {"st": 22, "ed": 24, "text": "statistical learning"}, {"st": 71, "ed": 73, "text": "spatio temporal"}, {"st": 73, "ed": 75, "text": "multi task"}, {"st": 75, "ed": 77, "text": "structured output"}, {"st": 90, "ed": 92, "text": "multi task"}, {"st": 112, "ed": 114, "text": "structured learning"}, {"st": 116, "ed": 118, "text": "feature construction"}, {"st": 121, "ed": 123, "text": "metric learning"}, {"st": 126, "ed": 128, "text": "intra class"}, {"st": 130, "ed": 132, "text": "inter class"}, {"st": 143, "ed": 145, "text": "joint learning"}]
[{"st": 66, "ed": 68, "text": "structured prediction"}, {"st": 70, "ed": 72, "text": "loss function"}, {"st": 105, "ed": 107, "text": "dynamic programming"}, {"st": 114, "ed": 116, "text": "greedy algorithm"}, {"st": 138, "ed": 140, "text": "outperforms existing"}]
[{"st": 10, "ed": 12, "text": "clustering method"}, {"st": 98, "ed": 100, "text": "clustering problem"}, {"st": 111, "ed": 113, "text": "undesired edges"}, {"st": 118, "ed": 120, "text": "undesired edges"}, {"st": 151, "ed": 153, "text": "spanning tree"}, {"st": 154, "ed": 157, "text": "k nearest neighbor"}, {"st": 163, "ed": 165, "text": "proposed method"}, {"st": 200, "ed": 202, "text": "proposed method"}, {"st": 215, "ed": 217, "text": "clustering method"}, {"st": 223, "ed": 225, "text": "shows significant"}, {"st": 235, "ed": 237, "text": "proposed method"}]
[{"st": 0, "ed": 3, "text": "the paper presents"}, {"st": 5, "ed": 7, "text": "technique called"}, {"st": 16, "ed": 18, "text": "machine learning"}, {"st": 32, "ed": 34, "text": "training data"}, {"st": 36, "ed": 38, "text": "existing approaches"}, {"st": 85, "ed": 87, "text": "affine transformation"}, {"st": 89, "ed": 91, "text": "proposed approach"}, {"st": 146, "ed": 148, "text": "handwriting recognition"}, {"st": 158, "ed": 161, "text": "support vector machine"}, {"st": 172, "ed": 174, "text": "significantly outperforms"}, {"st": 177, "ed": 179, "text": "affine transformation"}, {"st": 183, "ed": 185, "text": "training data"}]
[{"st": 14, "ed": 17, "text": "convolutional neural networks"}, {"st": 32, "ed": 34, "text": "least squares"}, {"st": 37, "ed": 39, "text": "low rank"}, {"st": 70, "ed": 72, "text": "convolutional layer"}, {"st": 77, "ed": 79, "text": "convolutional layers"}, {"st": 89, "ed": 91, "text": "fine tuned"}, {"st": 93, "ed": 95, "text": "training data"}, {"st": 96, "ed": 98, "text": "standard backpropagation"}, {"st": 113, "ed": 115, "text": "previous approaches"}, {"st": 193, "ed": 195, "text": "top 5"}]
[{"st": 104, "ed": 106, "text": "classification loss"}]
[{"st": 3, "ed": 5, "text": "strong baseline"}, {"st": 7, "ed": 10, "text": "unsupervised feature learning"}, {"st": 21, "ed": 23, "text": "future frames"}, {"st": 32, "ed": 35, "text": "spatial and temporal"}, {"st": 69, "ed": 71, "text": "image patches"}, {"st": 104, "ed": 106, "text": "non trivial"}]
[{"st": 10, "ed": 12, "text": "feature selection"}, {"st": 55, "ed": 57, "text": "handwritten digit"}]
[{"st": 6, "ed": 8, "text": "positive definite"}, {"st": 16, "ed": 18, "text": "computer vision"}, {"st": 21, "ed": 23, "text": "distance measures"}, {"st": 70, "ed": 72, "text": "riemannian geometry"}, {"st": 96, "ed": 98, "text": "proposed approach"}, {"st": 103, "ed": 105, "text": "distance measures"}]
[{"st": 9, "ed": 11, "text": "pairwise constraint"}, {"st": 17, "ed": 19, "text": "pairwise constraint"}, {"st": 24, "ed": 26, "text": "pairwise constraints"}, {"st": 42, "ed": 44, "text": "constraint propagation"}, {"st": 47, "ed": 49, "text": "multi view"}, {"st": 61, "ed": 63, "text": "constraint propagation"}, {"st": 68, "ed": 70, "text": "pairwise constraints"}, {"st": 92, "ed": 94, "text": "constraint propagation"}, {"st": 96, "ed": 99, "text": "semi supervised learning"}, {"st": 105, "ed": 107, "text": "efficiently solved"}, {"st": 132, "ed": 134, "text": "constraint propagation"}, {"st": 136, "ed": 139, "text": "semi supervised learning"}, {"st": 161, "ed": 163, "text": "constraint propagation"}, {"st": 171, "ed": 173, "text": "pairwise constraints"}, {"st": 179, "ed": 181, "text": "cross view"}, {"st": 185, "ed": 187, "text": "promising performance"}]
[{"st": 5, "ed": 7, "text": "face recognition"}, {"st": 9, "ed": 12, "text": "a long standing"}, {"st": 14, "ed": 16, "text": "computer vision"}, {"st": 17, "ed": 19, "text": "existing literature"}, {"st": 23, "ed": 25, "text": "face alignment"}, {"st": 29, "ed": 31, "text": "achieve high"}, {"st": 65, "ed": 67, "text": "face recognition"}, {"st": 73, "ed": 75, "text": "proposed algorithm"}, {"st": 132, "ed": 134, "text": "norm minimization"}, {"st": 154, "ed": 156, "text": "extensive experiments"}, {"st": 169, "ed": 171, "text": "existing methods"}, {"st": 173, "ed": 175, "text": "face recognition"}]
[{"st": 9, "ed": 11, "text": "received increasing"}, {"st": 13, "ed": 15, "text": "large scale"}, {"st": 29, "ed": 31, "text": "semantic similarity"}, {"st": 56, "ed": 58, "text": "multiple labels"}, {"st": 70, "ed": 72, "text": "ranking based"}, {"st": 75, "ed": 77, "text": "hash functions"}, {"st": 83, "ed": 85, "text": "multi label"}, {"st": 89, "ed": 93, "text": "deep convolutional neural network"}, {"st": 96, "ed": 98, "text": "hash functions"}, {"st": 99, "ed": 101, "text": "jointly learn"}, {"st": 101, "ed": 103, "text": "feature representations"}, {"st": 108, "ed": 110, "text": "hash codes"}, {"st": 148, "ed": 150, "text": "surrogate loss"}, {"st": 156, "ed": 158, "text": "optimization problem"}, {"st": 176, "ed": 178, "text": "proposed approach"}, {"st": 190, "ed": 192, "text": "evaluation metrics"}, {"st": 195, "ed": 197, "text": "multi label"}]
[{"st": 2, "ed": 4, "text": "model based"}, {"st": 11, "ed": 13, "text": "hyper parameter"}, {"st": 13, "ed": 15, "text": "optimization strategy"}, {"st": 27, "ed": 29, "text": "hyper parameters"}, {"st": 93, "ed": 95, "text": "object recognition"}]
[{"st": 6, "ed": 8, "text": "classification problem"}, {"st": 36, "ed": 38, "text": "real world"}, {"st": 51, "ed": 53, "text": "real data"}, {"st": 54, "ed": 56, "text": "previous methods"}, {"st": 67, "ed": 70, "text": "convolutional neural networks"}, {"st": 79, "ed": 82, "text": "convolutional auto encoder"}, {"st": 85, "ed": 87, "text": "real world"}, {"st": 93, "ed": 95, "text": "proposed method"}, {"st": 102, "ed": 104, "text": "top 5"}, {"st": 106, "ed": 108, "text": "real world"}]
[{"st": 10, "ed": 12, "text": "feature selection"}, {"st": 17, "ed": 19, "text": "large margin"}, {"st": 19, "ed": 21, "text": "linear classification"}, {"st": 24, "ed": 26, "text": "l 2"}, {"st": 28, "ed": 31, "text": "0 p 1"}, {"st": 50, "ed": 52, "text": "optimization problem"}, {"st": 60, "ed": 63, "text": "0 p 1"}, {"st": 67, "ed": 69, "text": "iterative algorithm"}, {"st": 80, "ed": 82, "text": "optimization problem"}, {"st": 87, "ed": 89, "text": "proposed algorithm"}, {"st": 94, "ed": 97, "text": "publicly available datasets"}, {"st": 108, "ed": 110, "text": "feature selection"}]
[{"st": 7, "ed": 9, "text": "detection algorithms"}, {"st": 18, "ed": 21, "text": "unsupervised feature learning"}, {"st": 32, "ed": 34, "text": "feature learning"}, {"st": 58, "ed": 60, "text": "auto encoder"}, {"st": 71, "ed": 73, "text": "feature learning"}, {"st": 74, "ed": 76, "text": "metric learning"}]
[{"st": 6, "ed": 8, "text": "real world"}, {"st": 19, "ed": 21, "text": "problem specific"}, {"st": 21, "ed": 23, "text": "cost function"}, {"st": 42, "ed": 44, "text": "desired properties"}, {"st": 45, "ed": 47, "text": "performance measures"}, {"st": 60, "ed": 62, "text": "performance measures"}, {"st": 97, "ed": 99, "text": "decision making"}, {"st": 143, "ed": 145, "text": "cost functions"}, {"st": 163, "ed": 165, "text": "performance measures"}, {"st": 171, "ed": 175, "text": "synthetic and real world"}]
[{"st": 0, "ed": 3, "text": "multi task learning"}, {"st": 8, "ed": 10, "text": "computer vision"}, {"st": 20, "ed": 22, "text": "related problems"}, {"st": 23, "ed": 25, "text": "object detection"}, {"st": 37, "ed": 39, "text": "key idea"}, {"st": 67, "ed": 71, "text": "reproducing kernel hilbert spaces"}, {"st": 87, "ed": 89, "text": "convex optimization"}, {"st": 96, "ed": 98, "text": "alternating minimization"}, {"st": 104, "ed": 106, "text": "proposed method"}, {"st": 106, "ed": 109, "text": "compares favorably to"}]
[{"st": 1, "ed": 3, "text": "generalization error"}, {"st": 5, "ed": 8, "text": "support vector machine"}, {"st": 45, "ed": 47, "text": "joint learning"}, {"st": 63, "ed": 65, "text": "transformation matrix"}, {"st": 92, "ed": 94, "text": "feature space"}, {"st": 100, "ed": 102, "text": "margin based"}, {"st": 105, "ed": 107, "text": "joint learning"}, {"st": 117, "ed": 119, "text": "alternating minimization"}, {"st": 151, "ed": 154, "text": "principal component analysis"}, {"st": 160, "ed": 162, "text": "joint learning"}, {"st": 172, "ed": 174, "text": "machine learning"}, {"st": 192, "ed": 194, "text": "margin based"}]
[{"st": 0, "ed": 2, "text": "deep learning"}, {"st": 4, "ed": 6, "text": "successfully applied"}, {"st": 7, "ed": 10, "text": "image super resolution"}, {"st": 20, "ed": 22, "text": "super resolution"}, {"st": 36, "ed": 39, "text": "convolutional auto encoder"}, {"st": 42, "ed": 44, "text": "pre trained"}, {"st": 54, "ed": 56, "text": "fine tuned"}, {"st": 57, "ed": 59, "text": "multi scale"}, {"st": 103, "ed": 105, "text": "performance improvements"}]
[{"st": 0, "ed": 4, "text": "convolutional neural networks cnns"}, {"st": 10, "ed": 12, "text": "feature representations"}, {"st": 13, "ed": 15, "text": "object recognition"}, {"st": 18, "ed": 20, "text": "multi label"}, {"st": 23, "ed": 25, "text": "multiple objects"}, {"st": 42, "ed": 44, "text": "local information"}, {"st": 79, "ed": 81, "text": "multi label"}, {"st": 85, "ed": 87, "text": "multi class"}, {"st": 87, "ed": 89, "text": "multi instance"}, {"st": 99, "ed": 101, "text": "feature representation"}, {"st": 110, "ed": 112, "text": "ground truth"}, {"st": 112, "ed": 114, "text": "bounding box"}, {"st": 122, "ed": 124, "text": "local information"}, {"st": 126, "ed": 128, "text": "nearest neighbor"}, {"st": 135, "ed": 137, "text": "multi view"}, {"st": 140, "ed": 142, "text": "multi view"}, {"st": 142, "ed": 144, "text": "multi instance"}, {"st": 158, "ed": 160, "text": "generalization ability"}, {"st": 185, "ed": 187, "text": "hand crafted"}, {"st": 187, "ed": 189, "text": "feature based"}, {"st": 192, "ed": 194, "text": "based methods"}, {"st": 196, "ed": 198, "text": "multi label"}, {"st": 205, "ed": 207, "text": "discriminative power"}, {"st": 209, "ed": 211, "text": "generalization ability"}]
[{"st": 0, "ed": 3, "text": "deep generative models"}, {"st": 11, "ed": 13, "text": "complex data"}, {"st": 17, "ed": 19, "text": "input data"}, {"st": 46, "ed": 48, "text": "max margin"}, {"st": 48, "ed": 51, "text": "deep generative models"}, {"st": 59, "ed": 61, "text": "max margin"}, {"st": 65, "ed": 67, "text": "discriminative power"}, {"st": 87, "ed": 89, "text": "empirical results"}, {"st": 97, "ed": 99, "text": "max margin"}, {"st": 101, "ed": 103, "text": "significantly improve"}, {"st": 104, "ed": 106, "text": "prediction performance"}, {"st": 130, "ed": 135, "text": "deep convolutional neural networks cnns"}]
[{"st": 0, "ed": 4, "text": "convolutional neural networks cnns"}, {"st": 13, "ed": 15, "text": "computer vision"}, {"st": 22, "ed": 24, "text": "optical flow"}, {"st": 48, "ed": 50, "text": "optical flow"}, {"st": 50, "ed": 52, "text": "estimation problem"}, {"st": 54, "ed": 56, "text": "supervised learning"}, {"st": 74, "ed": 76, "text": "feature vectors"}, {"st": 82, "ed": 84, "text": "ground truth"}, {"st": 88, "ed": 90, "text": "sufficiently large"}, {"st": 123, "ed": 125, "text": "competitive accuracy"}]
[{"st": 1, "ed": 3, "text": "sparse coding"}, {"st": 5, "ed": 7, "text": "highly successful"}, {"st": 8, "ed": 10, "text": "image classification"}, {"st": 29, "ed": 31, "text": "sparse coding"}, {"st": 40, "ed": 44, "text": "scale invariant feature transform"}, {"st": 60, "ed": 62, "text": "sparse coding"}, {"st": 64, "ed": 66, "text": "numerical experiments"}, {"st": 77, "ed": 79, "text": "sparse coding"}]
[{"st": 0, "ed": 2, "text": "deep learning"}, {"st": 4, "ed": 8, "text": "convolutional neural network cnn"}, {"st": 16, "ed": 18, "text": "feature extraction"}, {"st": 23, "ed": 25, "text": "image classification"}, {"st": 40, "ed": 42, "text": "deep features"}, {"st": 51, "ed": 53, "text": "nearest neighbor"}, {"st": 53, "ed": 56, "text": "support vector machines"}, {"st": 57, "ed": 60, "text": "extreme learning machines"}, {"st": 61, "ed": 63, "text": "image classification"}, {"st": 64, "ed": 66, "text": "deep convolutional"}, {"st": 74, "ed": 76, "text": "object recognition"}, {"st": 78, "ed": 80, "text": "multiple sources"}, {"st": 88, "ed": 90, "text": "deep features"}, {"st": 99, "ed": 101, "text": "trained cnn"}, {"st": 103, "ed": 105, "text": "convolutional layers"}, {"st": 107, "ed": 110, "text": "fully connected layers"}, {"st": 114, "ed": 116, "text": "experiments demonstrate"}, {"st": 122, "ed": 124, "text": "cross domain"}]
[{"st": 12, "ed": 14, "text": "image level"}, {"st": 16, "ed": 18, "text": "image level"}, {"st": 27, "ed": 31, "text": "convolutional neural network cnn"}, {"st": 43, "ed": 45, "text": "loss function"}, {"st": 55, "ed": 57, "text": "output space"}, {"st": 78, "ed": 81, "text": "stochastic gradient descent"}, {"st": 83, "ed": 85, "text": "key idea"}, {"st": 89, "ed": 91, "text": "training objective"}, {"st": 96, "ed": 98, "text": "linear models"}, {"st": 106, "ed": 108, "text": "extensive experiments"}, {"st": 126, "ed": 128, "text": "weakly supervised"}]
[{"st": 6, "ed": 8, "text": "time series"}, {"st": 11, "ed": 13, "text": "hidden unit"}, {"st": 19, "ed": 21, "text": "hidden units"}, {"st": 23, "ed": 25, "text": "latent structure"}, {"st": 29, "ed": 31, "text": "hidden units"}, {"st": 39, "ed": 41, "text": "temporal dependencies"}, {"st": 50, "ed": 52, "text": "time series"}, {"st": 57, "ed": 60, "text": "conditional random field"}, {"st": 66, "ed": 68, "text": "decision boundaries"}, {"st": 74, "ed": 76, "text": "grows exponentially"}, {"st": 85, "ed": 87, "text": "strong performance"}, {"st": 96, "ed": 98, "text": "computer vision"}, {"st": 98, "ed": 100, "text": "tasks including"}, {"st": 101, "ed": 103, "text": "character recognition"}, {"st": 103, "ed": 105, "text": "speech recognition"}, {"st": 105, "ed": 107, "text": "facial expression"}, {"st": 120, "ed": 122, "text": "facial action"}, {"st": 127, "ed": 129, "text": "hidden unit"}]
[{"st": 15, "ed": 17, "text": "decision trees"}, {"st": 20, "ed": 22, "text": "random forest"}, {"st": 31, "ed": 33, "text": "exhaustive search"}, {"st": 45, "ed": 47, "text": "linear combination"}, {"st": 59, "ed": 61, "text": "linear combination"}, {"st": 69, "ed": 71, "text": "latent variable"}, {"st": 82, "ed": 84, "text": "classification loss"}, {"st": 88, "ed": 90, "text": "decision tree"}, {"st": 95, "ed": 98, "text": "stochastic gradient descent"}, {"st": 114, "ed": 116, "text": "decision trees"}, {"st": 119, "ed": 121, "text": "significantly outperform"}, {"st": 121, "ed": 123, "text": "random forest"}, {"st": 138, "ed": 141, "text": "multi class classification"}]
[{"st": 39, "ed": 41, "text": "generative model"}, {"st": 72, "ed": 74, "text": "discriminative power"}, {"st": 78, "ed": 80, "text": "continuous variables"}, {"st": 88, "ed": 90, "text": "discriminative learning"}, {"st": 108, "ed": 110, "text": "generative models"}, {"st": 122, "ed": 124, "text": "parameter learning"}, {"st": 126, "ed": 128, "text": "kernel learning"}, {"st": 131, "ed": 133, "text": "generalization error"}, {"st": 143, "ed": 145, "text": "max margin"}, {"st": 157, "ed": 159, "text": "classification performance"}, {"st": 163, "ed": 166, "text": "advantages and disadvantages"}, {"st": 191, "ed": 193, "text": "network analysis"}, {"st": 209, "ed": 212, "text": "directed acyclic graph"}, {"st": 215, "ed": 217, "text": "theoretical guarantee"}]
[{"st": 0, "ed": 4, "text": "convolutional neural networks cnns"}, {"st": 41, "ed": 43, "text": "multi dimensional"}, {"st": 51, "ed": 53, "text": "spatio temporal"}, {"st": 71, "ed": 74, "text": "short term memory"}, {"st": 136, "ed": 138, "text": "image segmentation"}, {"st": 142, "ed": 144, "text": "competitive results"}]
[{"st": 4, "ed": 6, "text": "detection method"}, {"st": 8, "ed": 13, "text": "deep convolutional neural network cnn"}, {"st": 18, "ed": 20, "text": "object detection"}, {"st": 24, "ed": 26, "text": "classification problem"}, {"st": 66, "ed": 68, "text": "object detection"}, {"st": 82, "ed": 84, "text": "bounding box"}, {"st": 91, "ed": 93, "text": "detection task"}, {"st": 105, "ed": 108, "text": "pascal voc 2007"}]
[{"st": 3, "ed": 6, "text": "gaussian mixture model"}, {"st": 54, "ed": 56, "text": "sampling algorithm"}, {"st": 63, "ed": 65, "text": "pixel density"}, {"st": 72, "ed": 74, "text": "highly accurate"}, {"st": 99, "ed": 101, "text": "method outperforms"}, {"st": 104, "ed": 107, "text": "precision and recall"}, {"st": 114, "ed": 116, "text": "computational cost"}]
[{"st": 41, "ed": 43, "text": "unsupervised learning"}, {"st": 45, "ed": 48, "text": "takes advantage of"}, {"st": 63, "ed": 65, "text": "clustering problems"}, {"st": 103, "ed": 105, "text": "real world"}, {"st": 142, "ed": 144, "text": "experimentally demonstrate"}, {"st": 146, "ed": 148, "text": "proposed method"}, {"st": 153, "ed": 155, "text": "unsupervised manner"}]
[{"st": 0, "ed": 2, "text": "tree structured"}, {"st": 33, "ed": 35, "text": "tree structured"}, {"st": 65, "ed": 68, "text": "negative matrix factorization"}, {"st": 110, "ed": 112, "text": "distance metric"}, {"st": 115, "ed": 117, "text": "clustering algorithm"}, {"st": 130, "ed": 132, "text": "based clustering"}, {"st": 140, "ed": 142, "text": "simulated data"}]
[{"st": 16, "ed": 18, "text": "recent successes"}, {"st": 19, "ed": 21, "text": "visual recognition"}, {"st": 23, "ed": 25, "text": "vision systems"}, {"st": 65, "ed": 67, "text": "large scale"}, {"st": 78, "ed": 80, "text": "large scale"}, {"st": 89, "ed": 91, "text": "structured data"}, {"st": 127, "ed": 129, "text": "competitive results"}, {"st": 138, "ed": 140, "text": "retrieval tasks"}]
[{"st": 1, "ed": 3, "text": "visual recognition"}, {"st": 18, "ed": 20, "text": "domain shift"}, {"st": 23, "ed": 25, "text": "real world"}, {"st": 66, "ed": 68, "text": "domain adaptation"}, {"st": 72, "ed": 74, "text": "domain shift"}, {"st": 95, "ed": 98, "text": "unsupervised domain adaptation"}, {"st": 121, "ed": 123, "text": "domain adaptation"}]
[{"st": 1, "ed": 3, "text": "recent years"}, {"st": 8, "ed": 10, "text": "metric learning"}, {"st": 81, "ed": 83, "text": "input space"}, {"st": 106, "ed": 108, "text": "input data"}, {"st": 110, "ed": 112, "text": "linearly separable"}, {"st": 124, "ed": 130, "text": "experiments on synthetic and real world"}, {"st": 140, "ed": 142, "text": "metric learning"}, {"st": 149, "ed": 151, "text": "significant improvements"}]
[{"st": 0, "ed": 2, "text": "structured output"}, {"st": 11, "ed": 13, "text": "structured output"}, {"st": 15, "ed": 17, "text": "input data"}, {"st": 19, "ed": 21, "text": "structured outputs"}, {"st": 33, "ed": 35, "text": "training set"}, {"st": 36, "ed": 38, "text": "input output"}, {"st": 46, "ed": 48, "text": "real world"}, {"st": 64, "ed": 66, "text": "input data"}, {"st": 68, "ed": 70, "text": "structured outputs"}, {"st": 83, "ed": 85, "text": "training set"}, {"st": 88, "ed": 90, "text": "input output"}, {"st": 93, "ed": 95, "text": "input data"}, {"st": 102, "ed": 104, "text": "semi supervised"}, {"st": 104, "ed": 106, "text": "structured output"}, {"st": 118, "ed": 120, "text": "nearest neighbor"}, {"st": 123, "ed": 125, "text": "input space"}, {"st": 145, "ed": 147, "text": "structured output"}, {"st": 149, "ed": 151, "text": "training data"}, {"st": 160, "ed": 162, "text": "structured output"}, {"st": 168, "ed": 170, "text": "structured outputs"}, {"st": 194, "ed": 196, "text": "structured outputs"}, {"st": 202, "ed": 204, "text": "prediction error"}, {"st": 217, "ed": 219, "text": "experiment results"}, {"st": 221, "ed": 224, "text": "benchmark data sets"}]
[{"st": 84, "ed": 86, "text": "sparse linear"}]
[{"st": 8, "ed": 10, "text": "object detection"}, {"st": 24, "ed": 26, "text": "learning algorithms"}, {"st": 33, "ed": 36, "text": "false positive rate"}, {"st": 44, "ed": 46, "text": "objective functions"}, {"st": 61, "ed": 63, "text": "optimal solution"}, {"st": 87, "ed": 89, "text": "learning algorithm"}, {"st": 116, "ed": 118, "text": "weak classifiers"}, {"st": 126, "ed": 128, "text": "computation cost"}, {"st": 185, "ed": 187, "text": "computation cost"}, {"st": 210, "ed": 212, "text": "weak classifiers"}, {"st": 222, "ed": 224, "text": "face detection"}, {"st": 226, "ed": 229, "text": "effectiveness and efficiency"}]
[{"st": 5, "ed": 7, "text": "compressed sensing"}, {"st": 61, "ed": 63, "text": "conditional probability"}, {"st": 90, "ed": 93, "text": "short term memory"}, {"st": 115, "ed": 117, "text": "cross entropy"}, {"st": 144, "ed": 146, "text": "extensive experiments"}, {"st": 148, "ed": 150, "text": "real world"}, {"st": 155, "ed": 157, "text": "proposed method"}, {"st": 157, "ed": 159, "text": "significantly outperforms"}, {"st": 166, "ed": 168, "text": "matching pursuit"}, {"st": 179, "ed": 181, "text": "proposed method"}, {"st": 189, "ed": 191, "text": "compressive sensing"}, {"st": 193, "ed": 195, "text": "trained model"}, {"st": 203, "ed": 205, "text": "proposed method"}, {"st": 215, "ed": 217, "text": "training data"}, {"st": 223, "ed": 225, "text": "training data"}]
[{"st": 0, "ed": 2, "text": "object detection"}, {"st": 7, "ed": 9, "text": "computer vision"}, {"st": 25, "ed": 28, "text": "fast and accurate"}, {"st": 36, "ed": 38, "text": "proposal distribution"}, {"st": 116, "ed": 118, "text": "randomly generated"}, {"st": 144, "ed": 146, "text": "proposal distribution"}, {"st": 194, "ed": 196, "text": "method called"}, {"st": 205, "ed": 207, "text": "face detection"}, {"st": 217, "ed": 219, "text": "source code"}]
[{"st": 6, "ed": 8, "text": "multi kernel"}, {"st": 9, "ed": 11, "text": "learning algorithm"}, {"st": 19, "ed": 21, "text": "classifier performance"}, {"st": 28, "ed": 30, "text": "kernel function"}, {"st": 33, "ed": 35, "text": "parameter tuning"}, {"st": 44, "ed": 46, "text": "linear combination"}, {"st": 65, "ed": 67, "text": "objective function"}, {"st": 80, "ed": 82, "text": "objective function"}, {"st": 95, "ed": 97, "text": "iterative algorithm"}, {"st": 99, "ed": 101, "text": "cutting plane"}, {"st": 110, "ed": 112, "text": "pattern classification"}, {"st": 118, "ed": 120, "text": "performance measure"}, {"st": 123, "ed": 125, "text": "experiment results"}, {"st": 127, "ed": 129, "text": "proposed algorithm"}]
[{"st": 1, "ed": 3, "text": "early detection"}, {"st": 7, "ed": 9, "text": "liver cancer"}, {"st": 24, "ed": 26, "text": "remains challenging"}, {"st": 55, "ed": 57, "text": "machine learning"}, {"st": 87, "ed": 89, "text": "training set"}, {"st": 140, "ed": 142, "text": "proposed framework"}, {"st": 150, "ed": 152, "text": "ct images"}, {"st": 166, "ed": 168, "text": "method achieves"}]
[{"st": 19, "ed": 22, "text": "approximate nearest neighbor"}, {"st": 24, "ed": 26, "text": "supervised hashing"}, {"st": 67, "ed": 69, "text": "fully connected"}, {"st": 88, "ed": 90, "text": "cifar 10"}, {"st": 105, "ed": 107, "text": "cifar 10"}, {"st": 131, "ed": 133, "text": "feature extractor"}, {"st": 134, "ed": 136, "text": "hash functions"}]
[{"st": 3, "ed": 8, "text": "convolutional neural network cnn architecture"}, {"st": 9, "ed": 11, "text": "facial expression"}, {"st": 19, "ed": 21, "text": "hand crafted"}, {"st": 21, "ed": 23, "text": "feature extraction"}, {"st": 30, "ed": 33, "text": "convolutional neural network"}, {"st": 39, "ed": 41, "text": "extracted features"}, {"st": 56, "ed": 58, "text": "standard datasets"}, {"st": 65, "ed": 67, "text": "facial expression"}, {"st": 130, "ed": 132, "text": "facial expression"}, {"st": 187, "ed": 189, "text": "real world"}]
[{"st": 4, "ed": 6, "text": "method called"}, {"st": 61, "ed": 63, "text": "dynamic programming"}, {"st": 67, "ed": 69, "text": "bounding boxes"}, {"st": 73, "ed": 75, "text": "online learning"}, {"st": 118, "ed": 121, "text": "inference and learning"}, {"st": 127, "ed": 130, "text": "positive and negative"}, {"st": 138, "ed": 140, "text": "latent structure"}]
[{"st": 1, "ed": 4, "text": "bag of words"}, {"st": 99, "ed": 101, "text": "latent variables"}, {"st": 125, "ed": 127, "text": "log likelihood"}, {"st": 168, "ed": 170, "text": "free energy"}, {"st": 174, "ed": 176, "text": "hyper parameters"}, {"st": 192, "ed": 194, "text": "performance improvements"}]
[{"st": 0, "ed": 4, "text": "multiple instance learning mil"}, {"st": 15, "ed": 17, "text": "vision applications"}, {"st": 19, "ed": 21, "text": "image classification"}, {"st": 21, "ed": 23, "text": "object detection"}, {"st": 42, "ed": 44, "text": "multiple instance"}, {"st": 81, "ed": 83, "text": "optimization problem"}, {"st": 85, "ed": 87, "text": "efficiently solved"}, {"st": 92, "ed": 94, "text": "extensive experiments"}, {"st": 131, "ed": 133, "text": "pascal voc"}]
[{"st": 0, "ed": 2, "text": "multi instance"}, {"st": 2, "ed": 4, "text": "multi label"}, {"st": 8, "ed": 10, "text": "challenging problem"}, {"st": 23, "ed": 25, "text": "applications including"}, {"st": 25, "ed": 27, "text": "breast cancer"}, {"st": 40, "ed": 42, "text": "breast cancer"}]
[{"st": 47, "ed": 49, "text": "orthogonal complement"}, {"st": 87, "ed": 89, "text": "ell 1"}, {"st": 89, "ed": 91, "text": "minimization problem"}, {"st": 98, "ed": 100, "text": "principal component"}, {"st": 104, "ed": 107, "text": "provide theoretical guarantees"}, {"st": 119, "ed": 121, "text": "orthogonal complement"}, {"st": 137, "ed": 139, "text": "linear programming"}, {"st": 169, "ed": 171, "text": "linear programming"}, {"st": 181, "ed": 183, "text": "global minimum"}, {"st": 195, "ed": 197, "text": "alternating minimization"}, {"st": 200, "ed": 202, "text": "least squares"}, {"st": 208, "ed": 210, "text": "large scale"}, {"st": 211, "ed": 213, "text": "extensive experiments"}, {"st": 214, "ed": 216, "text": "synthetic data"}, {"st": 250, "ed": 252, "text": "based methods"}]
[{"st": 1, "ed": 3, "text": "subspace clustering"}, {"st": 35, "ed": 37, "text": "dimensional subspaces"}, {"st": 61, "ed": 63, "text": "noisy data"}, {"st": 109, "ed": 111, "text": "subspace clustering"}, {"st": 135, "ed": 137, "text": "proposed method"}, {"st": 142, "ed": 144, "text": "low rank"}, {"st": 144, "ed": 146, "text": "subspace clustering"}]
[{"st": 0, "ed": 3, "text": "sparse subspace clustering"}, {"st": 6, "ed": 9, "text": "sparse subspace clustering"}, {"st": 13, "ed": 15, "text": "ell 1"}, {"st": 37, "ed": 39, "text": "ell 1"}, {"st": 41, "ed": 43, "text": "ell 2"}, {"st": 54, "ed": 56, "text": "similarity graph"}, {"st": 72, "ed": 74, "text": "sparse representation"}, {"st": 96, "ed": 99, "text": "sparse subspace clustering"}, {"st": 111, "ed": 114, "text": "sparse subspace clustering"}, {"st": 114, "ed": 116, "text": "method named"}, {"st": 116, "ed": 118, "text": "ell 0"}, {"st": 130, "ed": 133, "text": "sparse subspace clustering"}, {"st": 139, "ed": 141, "text": "sparse representation"}, {"st": 145, "ed": 147, "text": "ell 0"}, {"st": 173, "ed": 175, "text": "optimal solution"}, {"st": 177, "ed": 179, "text": "optimization problem"}, {"st": 180, "ed": 182, "text": "ell 0"}, {"st": 193, "ed": 195, "text": "ell 0"}, {"st": 207, "ed": 209, "text": "similarity graph"}, {"st": 222, "ed": 225, "text": "extensive experimental results"}, {"st": 233, "ed": 235, "text": "ell 0"}, {"st": 240, "ed": 242, "text": "clustering methods"}, {"st": 249, "ed": 251, "text": "ell 0"}]
[{"st": 5, "ed": 7, "text": "deep architecture"}, {"st": 39, "ed": 41, "text": "recently developed"}, {"st": 43, "ed": 45, "text": "network structure"}, {"st": 50, "ed": 52, "text": "multilayer perceptron"}, {"st": 72, "ed": 74, "text": "linear activation"}, {"st": 81, "ed": 83, "text": "vanishing gradients"}, {"st": 91, "ed": 93, "text": "batch normalization"}, {"st": 121, "ed": 123, "text": "pooling layers"}, {"st": 135, "ed": 137, "text": "receptive field"}, {"st": 175, "ed": 177, "text": "classification performance"}, {"st": 184, "ed": 187, "text": "mnist cifar 10"}, {"st": 188, "ed": 190, "text": "cifar 100"}, {"st": 192, "ed": 194, "text": "comparable performance"}]
[{"st": 3, "ed": 5, "text": "recent progress"}, {"st": 6, "ed": 8, "text": "generative models"}, {"st": 16, "ed": 18, "text": "natural language"}, {"st": 49, "ed": 51, "text": "generative models"}, {"st": 52, "ed": 54, "text": "image generation"}, {"st": 63, "ed": 65, "text": "higher quality"}, {"st": 78, "ed": 80, "text": "previously unseen"}]
[{"st": 14, "ed": 16, "text": "real life"}, {"st": 49, "ed": 51, "text": "gauge group"}, {"st": 89, "ed": 92, "text": "low dimensional manifold"}, {"st": 138, "ed": 141, "text": "mnist and cifar10"}]
[{"st": 0, "ed": 2, "text": "recent years"}, {"st": 9, "ed": 11, "text": "large scale"}, {"st": 21, "ed": 24, "text": "hand crafted features"}, {"st": 44, "ed": 46, "text": "feature learning"}, {"st": 51, "ed": 54, "text": "deep neural networks"}, {"st": 98, "ed": 100, "text": "feature learning"}, {"st": 113, "ed": 115, "text": "method called"}, {"st": 117, "ed": 119, "text": "supervised hashing"}, {"st": 123, "ed": 125, "text": "feature learning"}, {"st": 136, "ed": 138, "text": "real datasets"}, {"st": 156, "ed": 158, "text": "image retrieval"}]
[{"st": 0, "ed": 2, "text": "decision trees"}, {"st": 17, "ed": 19, "text": "decision tree"}, {"st": 78, "ed": 80, "text": "linear combination"}, {"st": 83, "ed": 85, "text": "decision trees"}, {"st": 88, "ed": 90, "text": "structured prediction"}, {"st": 91, "ed": 93, "text": "latent variables"}, {"st": 142, "ed": 145, "text": "stochastic gradient descent"}, {"st": 156, "ed": 158, "text": "classification benchmarks"}, {"st": 164, "ed": 166, "text": "decision trees"}, {"st": 168, "ed": 170, "text": "decision tree"}]
[{"st": 3, "ed": 5, "text": "soft attention"}, {"st": 17, "ed": 19, "text": "multi layered"}, {"st": 19, "ed": 23, "text": "recurrent neural networks rnns"}, {"st": 24, "ed": 29, "text": "long short term memory lstm"}]
[{"st": 0, "ed": 2, "text": "spectral clustering"}, {"st": 8, "ed": 10, "text": "clustering approaches"}, {"st": 20, "ed": 22, "text": "spectral clustering"}, {"st": 42, "ed": 44, "text": "linear map"}, {"st": 60, "ed": 62, "text": "affinity matrix"}, {"st": 80, "ed": 82, "text": "recent years"}, {"st": 86, "ed": 88, "text": "noise reduction"}, {"st": 102, "ed": 104, "text": "affinity matrix"}, {"st": 109, "ed": 111, "text": "affinity matrix"}, {"st": 114, "ed": 116, "text": "distance metric"}, {"st": 122, "ed": 124, "text": "affinity matrix"}, {"st": 126, "ed": 128, "text": "positive semidefinite"}, {"st": 144, "ed": 146, "text": "objective function"}, {"st": 183, "ed": 185, "text": "extensive experiments"}, {"st": 189, "ed": 193, "text": "real world data sets"}, {"st": 195, "ed": 198, "text": "effectiveness and efficiency"}]
[{"st": 28, "ed": 30, "text": "convolutional network"}, {"st": 37, "ed": 39, "text": "labeled training"}, {"st": 60, "ed": 63, "text": "k nearest neighbor"}, {"st": 69, "ed": 71, "text": "text based"}, {"st": 91, "ed": 93, "text": "hand written"}]
[{"st": 3, "ed": 6, "text": "image super resolution"}, {"st": 12, "ed": 14, "text": "convolutional network"}, {"st": 33, "ed": 35, "text": "improve performance"}, {"st": 77, "ed": 79, "text": "method outperforms"}, {"st": 79, "ed": 81, "text": "previous methods"}]
[{"st": 3, "ed": 5, "text": "highly accurate"}, {"st": 6, "ed": 9, "text": "image super resolution"}, {"st": 16, "ed": 19, "text": "deep convolutional network"}, {"st": 25, "ed": 27, "text": "imagenet classification"}, {"st": 34, "ed": 36, "text": "network depth"}, {"st": 38, "ed": 40, "text": "significant improvement"}, {"st": 57, "ed": 59, "text": "deep network"}, {"st": 60, "ed": 62, "text": "contextual information"}, {"st": 64, "ed": 66, "text": "image regions"}, {"st": 74, "ed": 76, "text": "deep networks"}, {"st": 77, "ed": 79, "text": "convergence speed"}, {"st": 88, "ed": 91, "text": "simple yet effective"}, {"st": 99, "ed": 101, "text": "extremely high"}, {"st": 101, "ed": 103, "text": "learning rates"}, {"st": 117, "ed": 119, "text": "proposed method"}, {"st": 122, "ed": 124, "text": "existing methods"}]
[{"st": 4, "ed": 7, "text": "deep neural networks"}, {"st": 8, "ed": 10, "text": "achieved impressive"}, {"st": 13, "ed": 15, "text": "image classification"}, {"st": 62, "ed": 64, "text": "large scale"}, {"st": 78, "ed": 80, "text": "efficiently compute"}, {"st": 83, "ed": 85, "text": "deep networks"}, {"st": 94, "ed": 97, "text": "extensive experimental results"}, {"st": 100, "ed": 102, "text": "approach outperforms"}, {"st": 102, "ed": 104, "text": "recent methods"}, {"st": 109, "ed": 111, "text": "adversarial perturbations"}]
[{"st": 6, "ed": 8, "text": "deep learning"}, {"st": 12, "ed": 14, "text": "convolutional networks"}, {"st": 22, "ed": 24, "text": "predictive models"}, {"st": 38, "ed": 40, "text": "non trivial"}, {"st": 55, "ed": 57, "text": "convolutional network"}, {"st": 78, "ed": 80, "text": "machine learning"}, {"st": 106, "ed": 108, "text": "machine learning"}, {"st": 110, "ed": 113, "text": "hand crafted features"}]
[{"st": 20, "ed": 22, "text": "natural image"}, {"st": 26, "ed": 28, "text": "fine grained"}, {"st": 33, "ed": 35, "text": "coarse grained"}, {"st": 77, "ed": 79, "text": "deep learning"}, {"st": 93, "ed": 95, "text": "image classification"}, {"st": 104, "ed": 106, "text": "neural network"}, {"st": 121, "ed": 123, "text": "image datasets"}, {"st": 124, "ed": 126, "text": "empirical results"}]
[{"st": 1, "ed": 3, "text": "belief networks"}, {"st": 14, "ed": 16, "text": "neural network"}, {"st": 67, "ed": 69, "text": "inverse problems"}, {"st": 82, "ed": 84, "text": "belief networks"}, {"st": 114, "ed": 116, "text": "deep linear"}, {"st": 119, "ed": 121, "text": "linear unit"}, {"st": 139, "ed": 141, "text": "real valued"}, {"st": 141, "ed": 143, "text": "conditional distributions"}, {"st": 163, "ed": 165, "text": "skip connections"}, {"st": 187, "ed": 189, "text": "image denoising"}, {"st": 190, "ed": 192, "text": "facial expression"}]
[{"st": 0, "ed": 3, "text": "self paced learning"}, {"st": 29, "ed": 31, "text": "computer vision"}, {"st": 32, "ed": 34, "text": "pattern recognition"}, {"st": 65, "ed": 67, "text": "theoretical understanding"}, {"st": 97, "ed": 99, "text": "loss function"}, {"st": 190, "ed": 192, "text": "weakly labeled"}, {"st": 192, "ed": 194, "text": "large scale"}, {"st": 215, "ed": 217, "text": "manually annotated"}, {"st": 220, "ed": 222, "text": "method achieves"}, {"st": 228, "ed": 230, "text": "previous methods"}]
[{"st": 0, "ed": 2, "text": "mean field"}, {"st": 2, "ed": 4, "text": "variational inference"}, {"st": 18, "ed": 20, "text": "mean field"}, {"st": 24, "ed": 26, "text": "coordinate descent"}, {"st": 45, "ed": 47, "text": "ad hoc"}, {"st": 68, "ed": 70, "text": "proximal gradient"}, {"st": 70, "ed": 72, "text": "based approach"}, {"st": 97, "ed": 99, "text": "faster convergence"}, {"st": 107, "ed": 109, "text": "mean field"}]
[{"st": 8, "ed": 10, "text": "visual data"}, {"st": 34, "ed": 36, "text": "streaming data"}, {"st": 68, "ed": 70, "text": "object appearance"}, {"st": 85, "ed": 87, "text": "object tracking"}, {"st": 92, "ed": 94, "text": "excellent results"}, {"st": 95, "ed": 97, "text": "object tracking"}, {"st": 99, "ed": 101, "text": "standard datasets"}]
[{"st": 0, "ed": 2, "text": "deep networks"}, {"st": 4, "ed": 6, "text": "significant gains"}, {"st": 8, "ed": 10, "text": "visual recognition"}, {"st": 22, "ed": 24, "text": "deep networks"}, {"st": 40, "ed": 42, "text": "object class"}, {"st": 53, "ed": 55, "text": "deep network"}, {"st": 66, "ed": 68, "text": "closed set"}, {"st": 70, "ed": 72, "text": "deep networks"}, {"st": 89, "ed": 91, "text": "real world"}, {"st": 92, "ed": 94, "text": "open set"}, {"st": 101, "ed": 103, "text": "unseen classes"}, {"st": 112, "ed": 114, "text": "deep networks"}, {"st": 115, "ed": 117, "text": "open set"}, {"st": 168, "ed": 170, "text": "open set"}, {"st": 203, "ed": 205, "text": "open set"}, {"st": 211, "ed": 213, "text": "open set"}, {"st": 213, "ed": 215, "text": "deep networks"}, {"st": 216, "ed": 218, "text": "pre trained"}, {"st": 234, "ed": 236, "text": "open set"}, {"st": 241, "ed": 243, "text": "significantly outperforms"}, {"st": 243, "ed": 246, "text": "open set recognition"}, {"st": 249, "ed": 251, "text": "deep networks"}, {"st": 254, "ed": 256, "text": "deep networks"}]
[{"st": 7, "ed": 10, "text": "deep neural networks"}, {"st": 33, "ed": 35, "text": "labeled data"}, {"st": 43, "ed": 45, "text": "unsupervised learning"}, {"st": 54, "ed": 57, "text": "deep convolutional network"}, {"st": 64, "ed": 67, "text": "k means clustering"}, {"st": 107, "ed": 111, "text": "deep convolutional neural network"}, {"st": 129, "ed": 131, "text": "proposed algorithm"}]
[{"st": 1, "ed": 4, "text": "deep belief network"}, {"st": 8, "ed": 10, "text": "hidden layers"}, {"st": 12, "ed": 16, "text": "number of hidden units"}, {"st": 22, "ed": 24, "text": "raw pixels"}, {"st": 41, "ed": 44, "text": "discrete wavelet transform"}, {"st": 49, "ed": 51, "text": "computational complexity"}, {"st": 55, "ed": 57, "text": "low resolution"}]
[{"st": 36, "ed": 38, "text": "weighted sum"}, {"st": 63, "ed": 65, "text": "natural images"}, {"st": 81, "ed": 83, "text": "significantly smaller"}, {"st": 83, "ed": 85, "text": "mutual information"}, {"st": 126, "ed": 128, "text": "natural image"}, {"st": 138, "ed": 140, "text": "prior probability"}]
[{"st": 3, "ed": 5, "text": "adversarial examples"}, {"st": 13, "ed": 17, "text": "convolutional neural networks cnns"}, {"st": 52, "ed": 54, "text": "adversarial perturbations"}, {"st": 62, "ed": 64, "text": "linear classifier"}, {"st": 72, "ed": 74, "text": "image regions"}, {"st": 110, "ed": 112, "text": "significantly reduce"}, {"st": 171, "ed": 173, "text": "adversarial perturbation"}, {"st": 176, "ed": 178, "text": "adversarial perturbation"}, {"st": 180, "ed": 183, "text": "taking into account"}]
[{"st": 0, "ed": 2, "text": "recent studies"}, {"st": 5, "ed": 9, "text": "convolutional neural networks cnns"}, {"st": 18, "ed": 20, "text": "adversarial examples"}, {"st": 43, "ed": 45, "text": "additive noise"}, {"st": 48, "ed": 50, "text": "input image"}, {"st": 75, "ed": 77, "text": "max pooling"}, {"st": 114, "ed": 116, "text": "cifar 10"}, {"st": 133, "ed": 135, "text": "classification tasks"}]
[{"st": 4, "ed": 6, "text": "spatio temporal"}, {"st": 29, "ed": 31, "text": "visual memory"}, {"st": 35, "ed": 38, "text": "short term memory"}, {"st": 57, "ed": 59, "text": "optical flow"}, {"st": 75, "ed": 78, "text": "end to end"}, {"st": 93, "ed": 95, "text": "optical flow"}, {"st": 135, "ed": 137, "text": "ground truth"}, {"st": 162, "ed": 164, "text": "proposed framework"}, {"st": 165, "ed": 167, "text": "weakly supervised"}]
[{"st": 7, "ed": 9, "text": "application domains"}, {"st": 28, "ed": 30, "text": "learning representations"}, {"st": 44, "ed": 46, "text": "simultaneously learns"}, {"st": 46, "ed": 48, "text": "feature representations"}, {"st": 67, "ed": 69, "text": "feature space"}, {"st": 81, "ed": 84, "text": "image and text"}, {"st": 86, "ed": 88, "text": "significant improvement"}]
[{"st": 3, "ed": 5, "text": "sparse representation"}, {"st": 16, "ed": 18, "text": "signal processing"}, {"st": 27, "ed": 29, "text": "np hard"}, {"st": 29, "ed": 31, "text": "sparse coding"}, {"st": 73, "ed": 75, "text": "natural images"}, {"st": 86, "ed": 88, "text": "learning algorithm"}, {"st": 95, "ed": 97, "text": "convergence guarantee"}, {"st": 109, "ed": 111, "text": "preliminary experiments"}, {"st": 113, "ed": 115, "text": "promising performance"}, {"st": 120, "ed": 122, "text": "image representation"}, {"st": 127, "ed": 129, "text": "compressed sensing"}, {"st": 130, "ed": 132, "text": "magnetic resonance"}]
[{"st": 3, "ed": 5, "text": "generative model"}, {"st": 103, "ed": 105, "text": "variational bayes"}, {"st": 109, "ed": 111, "text": "fully differentiable"}, {"st": 115, "ed": 117, "text": "log likelihood"}]
[{"st": 4, "ed": 7, "text": "deep neural networks"}, {"st": 20, "ed": 23, "text": "stochastic gradient descent"}, {"st": 46, "ed": 48, "text": "batch normalization"}, {"st": 52, "ed": 54, "text": "recent years"}, {"st": 59, "ed": 61, "text": "performance degradation"}, {"st": 63, "ed": 65, "text": "small perturbations"}, {"st": 84, "ed": 86, "text": "training objective"}, {"st": 92, "ed": 94, "text": "multi layer"}, {"st": 111, "ed": 113, "text": "adversarial examples"}, {"st": 129, "ed": 131, "text": "achieve competitive"}, {"st": 138, "ed": 141, "text": "mnist cifar 10"}]
[{"st": 35, "ed": 37, "text": "low dimensional"}, {"st": 104, "ed": 106, "text": "invariance properties"}, {"st": 112, "ed": 114, "text": "image classification"}]
[{"st": 0, "ed": 2, "text": "deep networks"}, {"st": 7, "ed": 9, "text": "problems involving"}, {"st": 9, "ed": 11, "text": "image synthesis"}, {"st": 20, "ed": 22, "text": "input image"}, {"st": 26, "ed": 28, "text": "supervised training"}, {"st": 29, "ed": 31, "text": "image synthesis"}, {"st": 45, "ed": 47, "text": "generated image"}, {"st": 58, "ed": 60, "text": "loss function"}, {"st": 69, "ed": 71, "text": "image quality"}, {"st": 73, "ed": 75, "text": "structural similarity"}, {"st": 145, "ed": 147, "text": "ell 1"}, {"st": 148, "ed": 150, "text": "ell 2"}, {"st": 158, "ed": 160, "text": "training objective"}, {"st": 187, "ed": 189, "text": "super resolution"}, {"st": 192, "ed": 194, "text": "computer vision"}, {"st": 200, "ed": 202, "text": "convolutional architectures"}]
[{"st": 10, "ed": 12, "text": "neural networks"}, {"st": 39, "ed": 41, "text": "training set"}, {"st": 68, "ed": 70, "text": "active learning"}, {"st": 82, "ed": 86, "text": "convolutional neural network cnn"}, {"st": 106, "ed": 108, "text": "active learning"}, {"st": 131, "ed": 133, "text": "error rate"}, {"st": 137, "ed": 139, "text": "training set"}]
[{"st": 9, "ed": 11, "text": "visual object"}, {"st": 15, "ed": 17, "text": "convolutional networks"}, {"st": 21, "ed": 23, "text": "proposed approach"}, {"st": 27, "ed": 29, "text": "existing approaches"}, {"st": 30, "ed": 32, "text": "visual object"}, {"st": 53, "ed": 55, "text": "off line"}, {"st": 65, "ed": 67, "text": "visual tracking"}, {"st": 69, "ed": 72, "text": "end to end"}]
[{"st": 9, "ed": 11, "text": "visual object"}, {"st": 40, "ed": 42, "text": "recently proposed"}, {"st": 42, "ed": 45, "text": "end to end"}, {"st": 76, "ed": 78, "text": "visual input"}, {"st": 86, "ed": 88, "text": "recently proposed"}, {"st": 92, "ed": 94, "text": "attention model"}, {"st": 95, "ed": 97, "text": "spatial transformer"}, {"st": 114, "ed": 117, "text": "end to end"}]
[{"st": 1, "ed": 3, "text": "recent years"}, {"st": 3, "ed": 5, "text": "supervised learning"}, {"st": 6, "ed": 8, "text": "convolutional networks"}, {"st": 14, "ed": 16, "text": "computer vision"}, {"st": 18, "ed": 20, "text": "unsupervised learning"}, {"st": 42, "ed": 44, "text": "supervised learning"}, {"st": 54, "ed": 56, "text": "deep convolutional"}, {"st": 56, "ed": 59, "text": "generative adversarial networks"}, {"st": 79, "ed": 81, "text": "image datasets"}, {"st": 87, "ed": 89, "text": "deep convolutional"}, {"st": 111, "ed": 113, "text": "learned features"}]
[{"st": 13, "ed": 15, "text": "object detection"}, {"st": 18, "ed": 20, "text": "object detection"}, {"st": 42, "ed": 44, "text": "de facto"}, {"st": 90, "ed": 92, "text": "pedestrian detection"}]
[{"st": 45, "ed": 47, "text": "multi channel"}, {"st": 48, "ed": 50, "text": "time series"}, {"st": 94, "ed": 96, "text": "convolutional network"}, {"st": 114, "ed": 116, "text": "proposed approach"}, {"st": 144, "ed": 146, "text": "empirical evaluation"}, {"st": 150, "ed": 152, "text": "classification task"}, {"st": 153, "ed": 155, "text": "significant improvements"}, {"st": 156, "ed": 158, "text": "classification accuracy"}]
[{"st": 1, "ed": 6, "text": "deep convolutional neural networks cnns"}, {"st": 11, "ed": 13, "text": "object detection"}, {"st": 17, "ed": 19, "text": "post processing"}, {"st": 37, "ed": 39, "text": "computational complexity"}, {"st": 54, "ed": 57, "text": "end to end"}, {"st": 58, "ed": 62, "text": "deep neural network architecture"}, {"st": 65, "ed": 68, "text": "convolutional and recurrent"}, {"st": 75, "ed": 77, "text": "object instances"}, {"st": 79, "ed": 81, "text": "bounding boxes"}, {"st": 97, "ed": 99, "text": "post processing"}, {"st": 128, "ed": 130, "text": "proposed approach"}, {"st": 142, "ed": 144, "text": "promising results"}]
[{"st": 2, "ed": 4, "text": "distance metric"}, {"st": 10, "ed": 12, "text": "great importance"}, {"st": 19, "ed": 21, "text": "remarkable success"}, {"st": 27, "ed": 30, "text": "convolutional neural networks"}, {"st": 30, "ed": 32, "text": "recent works"}, {"st": 33, "ed": 36, "text": "shown promising results"}, {"st": 79, "ed": 81, "text": "neural network"}, {"st": 87, "ed": 89, "text": "pairwise distances"}, {"st": 110, "ed": 112, "text": "feature embedding"}, {"st": 116, "ed": 118, "text": "structured prediction"}, {"st": 153, "ed": 155, "text": "significant improvement"}, {"st": 158, "ed": 160, "text": "feature embedding"}]
[{"st": 14, "ed": 16, "text": "important information"}, {"st": 26, "ed": 29, "text": "deep network architecture"}, {"st": 98, "ed": 102, "text": "deep convolutional neural networks"}, {"st": 131, "ed": 133, "text": "large scale"}, {"st": 138, "ed": 140, "text": "pascal voc"}, {"st": 160, "ed": 163, "text": "orders of magnitude"}]
[{"st": 7, "ed": 9, "text": "features extracted"}, {"st": 10, "ed": 12, "text": "deep representations"}, {"st": 13, "ed": 17, "text": "convolutional neural networks cnns"}, {"st": 25, "ed": 27, "text": "image dataset"}, {"st": 41, "ed": 43, "text": "pre trained"}, {"st": 82, "ed": 84, "text": "class distributions"}, {"st": 101, "ed": 103, "text": "feature selection"}, {"st": 108, "ed": 110, "text": "proposed method"}, {"st": 112, "ed": 114, "text": "benchmark datasets"}, {"st": 122, "ed": 124, "text": "proposed method"}, {"st": 157, "ed": 159, "text": "transfer learning"}]
[{"st": 13, "ed": 17, "text": "convolutional neural networks cnns"}, {"st": 18, "ed": 20, "text": "complex tasks"}, {"st": 22, "ed": 24, "text": "imagenet classification"}, {"st": 31, "ed": 33, "text": "deep cnns"}, {"st": 39, "ed": 42, "text": "simple and effective"}, {"st": 67, "ed": 69, "text": "variational bayesian"}, {"st": 69, "ed": 71, "text": "matrix factorization"}, {"st": 79, "ed": 81, "text": "fine tuning"}, {"st": 118, "ed": 120, "text": "vgg 16"}]
[{"st": 6, "ed": 8, "text": "neural network"}, {"st": 10, "ed": 12, "text": "semi supervised"}, {"st": 30, "ed": 32, "text": "classification task"}, {"st": 38, "ed": 40, "text": "labeled data"}]
[{"st": 19, "ed": 22, "text": "learning to rank"}, {"st": 26, "ed": 28, "text": "machine learning"}, {"st": 51, "ed": 54, "text": "learning to rank"}, {"st": 61, "ed": 63, "text": "visual semantic"}, {"st": 73, "ed": 75, "text": "large scale"}, {"st": 80, "ed": 82, "text": "online marketplace"}, {"st": 91, "ed": 93, "text": "significantly improves"}, {"st": 98, "ed": 100, "text": "image features"}, {"st": 102, "ed": 104, "text": "fine grained"}]
[{"st": 17, "ed": 19, "text": "co occurrence"}, {"st": 35, "ed": 37, "text": "binary classification"}]
[{"st": 47, "ed": 49, "text": "internal representations"}, {"st": 75, "ed": 77, "text": "clustering technique"}, {"st": 89, "ed": 91, "text": "visual concepts"}, {"st": 106, "ed": 108, "text": "image patches"}, {"st": 112, "ed": 114, "text": "visual concepts"}, {"st": 167, "ed": 169, "text": "experiments demonstrate"}, {"st": 170, "ed": 172, "text": "visual concepts"}, {"st": 183, "ed": 185, "text": "visual concepts"}, {"st": 197, "ed": 199, "text": "visual concepts"}]
[{"st": 0, "ed": 3, "text": "convolutional neural networks"}, {"st": 5, "ed": 7, "text": "computer vision"}, {"st": 13, "ed": 15, "text": "visual tasks"}, {"st": 37, "ed": 39, "text": "pre trained"}, {"st": 76, "ed": 78, "text": "exploding gradients"}, {"st": 135, "ed": 137, "text": "pre training"}, {"st": 140, "ed": 142, "text": "computer vision"}, {"st": 145, "ed": 147, "text": "image classification"}, {"st": 148, "ed": 150, "text": "object detection"}, {"st": 154, "ed": 157, "text": "orders of magnitude"}, {"st": 161, "ed": 163, "text": "pre training"}, {"st": 166, "ed": 168, "text": "significantly outperforms"}, {"st": 174, "ed": 177, "text": "supervised and unsupervised"}]
[{"st": 17, "ed": 19, "text": "natural images"}, {"st": 102, "ed": 104, "text": "fully convolutional"}, {"st": 116, "ed": 118, "text": "object instances"}, {"st": 146, "ed": 148, "text": "image regions"}, {"st": 157, "ed": 160, "text": "conduct extensive experiments"}, {"st": 173, "ed": 176, "text": "approach significantly outperforms"}, {"st": 249, "ed": 251, "text": "computational resources"}]
[{"st": 7, "ed": 10, "text": "end to end"}, {"st": 65, "ed": 68, "text": "recurrent neural network"}]
[{"st": 3, "ed": 5, "text": "structured prediction"}, {"st": 11, "ed": 13, "text": "features extracted"}, {"st": 14, "ed": 17, "text": "convolutional neural networks"}, {"st": 21, "ed": 24, "text": "recurrent neural networks"}, {"st": 38, "ed": 40, "text": "recently introduced"}, {"st": 54, "ed": 56, "text": "challenging task"}, {"st": 94, "ed": 96, "text": "pre trained"}, {"st": 96, "ed": 98, "text": "convolutional layers"}, {"st": 112, "ed": 114, "text": "image resolution"}, {"st": 177, "ed": 179, "text": "structured prediction"}, {"st": 181, "ed": 183, "text": "source code"}, {"st": 189, "ed": 191, "text": "https github.com"}]
[{"st": 5, "ed": 7, "text": "large scale"}, {"st": 37, "ed": 42, "text": "convolutional neural network cnn architecture"}, {"st": 47, "ed": 50, "text": "end to end"}, {"st": 78, "ed": 80, "text": "image representation"}, {"st": 104, "ed": 106, "text": "training procedure"}, {"st": 110, "ed": 112, "text": "weakly supervised"}, {"st": 112, "ed": 114, "text": "ranking loss"}, {"st": 122, "ed": 125, "text": "end to end"}, {"st": 148, "ed": 150, "text": "significantly outperforms"}, {"st": 152, "ed": 154, "text": "image representations"}, {"st": 164, "ed": 166, "text": "recognition benchmarks"}, {"st": 175, "ed": 177, "text": "image representations"}, {"st": 179, "ed": 181, "text": "image retrieval"}]
[{"st": 7, "ed": 11, "text": "deep convolutional neural network"}, {"st": 21, "ed": 23, "text": "substantial improvements"}, {"st": 50, "ed": 52, "text": "multi resolution"}, {"st": 83, "ed": 85, "text": "f measure"}, {"st": 97, "ed": 99, "text": "improve performance"}, {"st": 103, "ed": 105, "text": "deep learning"}]
[{"st": 3, "ed": 7, "text": "convolutional neural networks cnn"}, {"st": 49, "ed": 51, "text": "ground truth"}, {"st": 62, "ed": 64, "text": "machine learning"}, {"st": 71, "ed": 73, "text": "automatically learns"}, {"st": 105, "ed": 107, "text": "automatically learns"}, {"st": 175, "ed": 177, "text": "fixed point"}, {"st": 177, "ed": 179, "text": "network structure"}, {"st": 182, "ed": 185, "text": "end to end"}]
[{"st": 0, "ed": 4, "text": "convolutional neural networks cnns"}, {"st": 18, "ed": 20, "text": "training data"}, {"st": 77, "ed": 79, "text": "output distribution"}, {"st": 83, "ed": 85, "text": "output distribution"}, {"st": 109, "ed": 112, "text": "input and output"}]
[{"st": 9, "ed": 11, "text": "computer vision"}, {"st": 29, "ed": 31, "text": "object detection"}, {"st": 40, "ed": 42, "text": "image captioning"}, {"st": 61, "ed": 63, "text": "fully convolutional"}, {"st": 75, "ed": 77, "text": "forward pass"}, {"st": 85, "ed": 89, "text": "trained end to end"}, {"st": 101, "ed": 103, "text": "convolutional network"}, {"st": 109, "ed": 113, "text": "recurrent neural network language"}, {"st": 135, "ed": 137, "text": "100 000"}, {"st": 143, "ed": 146, "text": "speed and accuracy"}]
[{"st": 8, "ed": 10, "text": "computer vision"}, {"st": 12, "ed": 14, "text": "face detection"}, {"st": 29, "ed": 31, "text": "lighting conditions"}, {"st": 95, "ed": 97, "text": "explicitly model"}, {"st": 109, "ed": 111, "text": "structured output"}]
[{"st": 11, "ed": 13, "text": "event detection"}, {"st": 19, "ed": 21, "text": "challenging problems"}, {"st": 31, "ed": 33, "text": "textual description"}, {"st": 49, "ed": 51, "text": "training samples"}, {"st": 66, "ed": 68, "text": "textual description"}, {"st": 73, "ed": 75, "text": "learning framework"}, {"st": 99, "ed": 101, "text": "training samples"}, {"st": 109, "ed": 112, "text": "support vector machine"}, {"st": 140, "ed": 142, "text": "large scale"}]
[{"st": 0, "ed": 2, "text": "existing methods"}, {"st": 10, "ed": 12, "text": "underlying structure"}, {"st": 41, "ed": 43, "text": "structural constraints"}, {"st": 53, "ed": 55, "text": "structural constraints"}, {"st": 63, "ed": 65, "text": "structured prediction"}, {"st": 70, "ed": 72, "text": "prediction problems"}, {"st": 103, "ed": 105, "text": "instance segmentation"}, {"st": 107, "ed": 109, "text": "method outperforms"}]
[{"st": 3, "ed": 5, "text": "supervised learning"}, {"st": 23, "ed": 25, "text": "supervised learning"}, {"st": 43, "ed": 45, "text": "surrogate loss"}]
[{"st": 1, "ed": 3, "text": "visual analytics"}, {"st": 30, "ed": 32, "text": "dimensionality reduction"}, {"st": 46, "ed": 48, "text": "distributed stochastic"}, {"st": 95, "ed": 98, "text": "speed and accuracy"}, {"st": 107, "ed": 109, "text": "visualization techniques"}, {"st": 151, "ed": 153, "text": "real world"}]
[{"st": 1, "ed": 3, "text": "deep architectures"}, {"st": 4, "ed": 6, "text": "image classification"}, {"st": 21, "ed": 23, "text": "image representations"}, {"st": 46, "ed": 48, "text": "deep networks"}, {"st": 85, "ed": 87, "text": "simultaneously learns"}, {"st": 109, "ed": 111, "text": "structure learning"}, {"st": 123, "ed": 126, "text": "cifar and imagenet"}, {"st": 129, "ed": 131, "text": "classification accuracy"}, {"st": 134, "ed": 136, "text": "faster training"}]
[{"st": 2, "ed": 5, "text": "convolutional neural networks"}, {"st": 17, "ed": 19, "text": "performance improvement"}, {"st": 72, "ed": 74, "text": "scene classification"}, {"st": 75, "ed": 77, "text": "extensive experiments"}, {"st": 80, "ed": 82, "text": "large scale"}, {"st": 105, "ed": 108, "text": "the research community"}]
[{"st": 8, "ed": 10, "text": "linear classifier"}, {"st": 13, "ed": 15, "text": "classification accuracy"}, {"st": 16, "ed": 18, "text": "deep learning"}, {"st": 21, "ed": 24, "text": "principal component analysis"}, {"st": 37, "ed": 40, "text": "principal component analysis"}, {"st": 55, "ed": 57, "text": "principal components"}, {"st": 62, "ed": 64, "text": "principal components"}, {"st": 77, "ed": 79, "text": "face recognition"}, {"st": 79, "ed": 81, "text": "object recognition"}, {"st": 89, "ed": 92, "text": "principal component analysis"}]
[{"st": 1, "ed": 3, "text": "machine learning"}, {"st": 8, "ed": 11, "text": "each data point"}, {"st": 25, "ed": 28, "text": "each data point"}, {"st": 47, "ed": 49, "text": "functional data"}, {"st": 51, "ed": 53, "text": "multivariate data"}, {"st": 56, "ed": 58, "text": "poor performance"}, {"st": 84, "ed": 86, "text": "functional data"}, {"st": 93, "ed": 95, "text": "low rank"}, {"st": 101, "ed": 105, "text": "synthetic and real data"}]
[{"st": 1, "ed": 3, "text": "machine learning"}, {"st": 31, "ed": 33, "text": "machine learning"}, {"st": 82, "ed": 84, "text": "machine learning"}]
[{"st": 28, "ed": 30, "text": "real applications"}, {"st": 37, "ed": 39, "text": "computational complexity"}, {"st": 110, "ed": 112, "text": "fewer parameters"}]
[{"st": 4, "ed": 6, "text": "data mining"}, {"st": 51, "ed": 53, "text": "noisy data"}, {"st": 63, "ed": 65, "text": "clustering methods"}, {"st": 66, "ed": 68, "text": "practical applications"}]
[{"st": 1, "ed": 4, "text": "recurrent neural networks"}, {"st": 41, "ed": 43, "text": "rnn model"}, {"st": 57, "ed": 59, "text": "attention based"}, {"st": 59, "ed": 61, "text": "rnn models"}, {"st": 94, "ed": 96, "text": "attention mechanisms"}, {"st": 106, "ed": 108, "text": "computer vision"}, {"st": 111, "ed": 113, "text": "attention based"}, {"st": 119, "ed": 121, "text": "attention based"}, {"st": 121, "ed": 123, "text": "rnn model"}, {"st": 132, "ed": 135, "text": "future research directions"}]
[{"st": 8, "ed": 10, "text": "large scale"}, {"st": 41, "ed": 43, "text": "deep learning"}, {"st": 76, "ed": 80, "text": "restricted boltzmann machine rbm"}, {"st": 93, "ed": 95, "text": "extensive experiments"}, {"st": 99, "ed": 101, "text": "visual search"}, {"st": 106, "ed": 108, "text": "proposed approach"}]
[{"st": 10, "ed": 12, "text": "cluster ensemble"}, {"st": 22, "ed": 24, "text": "statistical inference"}, {"st": 32, "ed": 34, "text": "sufficient conditions"}, {"st": 119, "ed": 121, "text": "empirical results"}, {"st": 123, "ed": 125, "text": "k means"}, {"st": 177, "ed": 179, "text": "statistical theory"}]
[{"st": 27, "ed": 29, "text": "existing approaches"}, {"st": 80, "ed": 82, "text": "significantly improves"}, {"st": 91, "ed": 93, "text": "semi supervised"}]
[{"st": 5, "ed": 7, "text": "quantitative analysis"}, {"st": 13, "ed": 16, "text": "detection and classification"}, {"st": 42, "ed": 44, "text": "features extracted"}, {"st": 69, "ed": 71, "text": "hidden markov"}, {"st": 84, "ed": 86, "text": "statistical models"}, {"st": 99, "ed": 101, "text": "structural information"}, {"st": 115, "ed": 117, "text": "outperform existing"}]
[{"st": 5, "ed": 8, "text": "convolutional neural networks"}, {"st": 25, "ed": 27, "text": "learned features"}, {"st": 56, "ed": 58, "text": "time series"}, {"st": 59, "ed": 63, "text": "deep convolutional neural networks"}]
[{"st": 25, "ed": 27, "text": "object category"}, {"st": 36, "ed": 38, "text": "feature extraction"}, {"st": 59, "ed": 61, "text": "learning algorithm"}, {"st": 83, "ed": 85, "text": "object recognition"}, {"st": 100, "ed": 104, "text": "convolutional neural networks cnn"}, {"st": 116, "ed": 118, "text": "error rates"}]
[{"st": 13, "ed": 15, "text": "convolutional network"}, {"st": 18, "ed": 20, "text": "image features"}, {"st": 35, "ed": 37, "text": "gradient based"}, {"st": 57, "ed": 59, "text": "generative model"}, {"st": 71, "ed": 73, "text": "adversarial training"}, {"st": 87, "ed": 89, "text": "adversarial networks"}]
[{"st": 21, "ed": 23, "text": "imaging data"}, {"st": 27, "ed": 29, "text": "image segmentation"}, {"st": 56, "ed": 58, "text": "class probabilities"}, {"st": 86, "ed": 88, "text": "manually annotated"}]
[{"st": 10, "ed": 12, "text": "supervised classification"}, {"st": 23, "ed": 25, "text": "weak classifiers"}, {"st": 46, "ed": 48, "text": "labeled data"}, {"st": 57, "ed": 59, "text": "labeled data"}, {"st": 65, "ed": 67, "text": "weak classifiers"}]
[{"st": 0, "ed": 2, "text": "sparse representation"}, {"st": 3, "ed": 6, "text": "attracted much attention"}, {"st": 11, "ed": 13, "text": "signal processing"}, {"st": 13, "ed": 15, "text": "image processing"}, {"st": 15, "ed": 17, "text": "computer vision"}, {"st": 20, "ed": 22, "text": "sparse representation"}, {"st": 60, "ed": 62, "text": "sparse representation"}, {"st": 72, "ed": 74, "text": "sparse representation"}, {"st": 102, "ed": 104, "text": "sparse representation"}, {"st": 105, "ed": 107, "text": "l 0"}, {"st": 107, "ed": 109, "text": "norm minimization"}, {"st": 109, "ed": 111, "text": "sparse representation"}, {"st": 115, "ed": 118, "text": "0 p 1"}, {"st": 119, "ed": 121, "text": "sparse representation"}, {"st": 122, "ed": 125, "text": "l 1 norm"}, {"st": 127, "ed": 129, "text": "sparse representation"}, {"st": 130, "ed": 132, "text": "l 2"}, {"st": 142, "ed": 144, "text": "sparse representation"}, {"st": 148, "ed": 150, "text": "sparse representation"}, {"st": 162, "ed": 164, "text": "constrained optimization"}, {"st": 166, "ed": 168, "text": "based optimization"}, {"st": 189, "ed": 191, "text": "sparse representation"}, {"st": 203, "ed": 205, "text": "sparse representation"}, {"st": 209, "ed": 211, "text": "comparative study"}, {"st": 213, "ed": 215, "text": "sparse representation"}]
[{"st": 11, "ed": 13, "text": "deep networks"}, {"st": 21, "ed": 23, "text": "prediction tasks"}, {"st": 28, "ed": 31, "text": "deep generative models"}, {"st": 39, "ed": 41, "text": "invariant representations"}, {"st": 48, "ed": 50, "text": "deep generative"}, {"st": 55, "ed": 57, "text": "external memory"}, {"st": 59, "ed": 61, "text": "attention mechanism"}, {"st": 84, "ed": 86, "text": "attention model"}, {"st": 90, "ed": 94, "text": "trained end to end"}, {"st": 105, "ed": 107, "text": "variational bayesian"}, {"st": 132, "ed": 134, "text": "invariant feature"}, {"st": 167, "ed": 169, "text": "tasks including"}, {"st": 171, "ed": 173, "text": "image generation"}]
[{"st": 8, "ed": 11, "text": "nearest neighbor search"}, {"st": 13, "ed": 15, "text": "large scale"}, {"st": 33, "ed": 35, "text": "auto encoder"}, {"st": 66, "ed": 68, "text": "training data"}, {"st": 105, "ed": 107, "text": "proposed method"}, {"st": 117, "ed": 119, "text": "large scale"}]
[{"st": 12, "ed": 15, "text": "hand crafted features"}, {"st": 34, "ed": 37, "text": "convolutional neural network"}, {"st": 40, "ed": 42, "text": "learning process"}, {"st": 49, "ed": 51, "text": "loss function"}, {"st": 59, "ed": 61, "text": "saliency map"}, {"st": 70, "ed": 72, "text": "large datasets"}, {"st": 73, "ed": 75, "text": "saliency prediction"}, {"st": 81, "ed": 84, "text": "end to end"}, {"st": 98, "ed": 101, "text": "trained from scratch"}, {"st": 126, "ed": 129, "text": "end to end"}]
[{"st": 13, "ed": 15, "text": "existing methods"}, {"st": 17, "ed": 19, "text": "low dimensional"}, {"st": 26, "ed": 28, "text": "geometric structure"}, {"st": 36, "ed": 38, "text": "computationally expensive"}, {"st": 50, "ed": 52, "text": "real life"}, {"st": 83, "ed": 85, "text": "gaussian process"}, {"st": 126, "ed": 128, "text": "proposed method"}, {"st": 139, "ed": 141, "text": "special case"}, {"st": 146, "ed": 149, "text": "extensive experimental results"}, {"st": 155, "ed": 157, "text": "proposed method"}]
[{"st": 0, "ed": 3, "text": "convolutional neural networks"}, {"st": 8, "ed": 10, "text": "internal representations"}, {"st": 14, "ed": 16, "text": "semantically meaningful"}, {"st": 41, "ed": 43, "text": "image regions"}, {"st": 64, "ed": 66, "text": "image regions"}, {"st": 73, "ed": 75, "text": "weakly supervised"}]
[{"st": 3, "ed": 5, "text": "deep learning"}, {"st": 7, "ed": 9, "text": "contour detection"}, {"st": 11, "ed": 13, "text": "fully convolutional"}, {"st": 13, "ed": 15, "text": "encoder decoder"}, {"st": 19, "ed": 21, "text": "low level"}, {"st": 21, "ed": 23, "text": "edge detection"}, {"st": 28, "ed": 30, "text": "higher level"}, {"st": 35, "ed": 39, "text": "trained end to end"}, {"st": 40, "ed": 42, "text": "pascal voc"}, {"st": 44, "ed": 46, "text": "ground truth"}, {"st": 56, "ed": 58, "text": "contour detection"}, {"st": 71, "ed": 73, "text": "object classes"}, {"st": 79, "ed": 81, "text": "ms coco"}, {"st": 88, "ed": 90, "text": "edge detection"}, {"st": 121, "ed": 123, "text": "pascal voc"}]
[{"st": 0, "ed": 3, "text": "deep neural networks"}, {"st": 10, "ed": 12, "text": "computer vision"}, {"st": 15, "ed": 17, "text": "image classification"}, {"st": 29, "ed": 31, "text": "training phase"}, {"st": 40, "ed": 42, "text": "labeled images"}, {"st": 43, "ed": 45, "text": "fine tune"}, {"st": 70, "ed": 72, "text": "learned features"}, {"st": 97, "ed": 99, "text": "batch normalization"}, {"st": 103, "ed": 105, "text": "generalization ability"}, {"st": 114, "ed": 116, "text": "batch normalization"}, {"st": 121, "ed": 123, "text": "approach achieves"}, {"st": 127, "ed": 129, "text": "domain adaptation"}, {"st": 134, "ed": 136, "text": "deep learning"}, {"st": 136, "ed": 138, "text": "domain adaptation"}, {"st": 177, "ed": 179, "text": "domain adaptation"}]
[{"st": 0, "ed": 3, "text": "deep residual networks"}, {"st": 10, "ed": 12, "text": "deep architectures"}, {"st": 30, "ed": 32, "text": "building blocks"}, {"st": 57, "ed": 59, "text": "skip connections"}, {"st": 93, "ed": 95, "text": "improved results"}, {"st": 101, "ed": 103, "text": "cifar 10"}, {"st": 106, "ed": 108, "text": "cifar 100"}, {"st": 117, "ed": 121, "text": "available at https github.com"}]
[{"st": 1, "ed": 5, "text": "convolutional neural networks cnns"}, {"st": 10, "ed": 12, "text": "powerful tool"}, {"st": 17, "ed": 19, "text": "machine learning"}, {"st": 34, "ed": 37, "text": "convolutional neural networks"}, {"st": 86, "ed": 89, "text": "simple yet effective"}, {"st": 96, "ed": 98, "text": "theoretically analyze"}, {"st": 119, "ed": 121, "text": "recognition performance"}, {"st": 122, "ed": 124, "text": "cifar 10"}, {"st": 147, "ed": 149, "text": "performance improvement"}]
[{"st": 0, "ed": 2, "text": "domain adaptation"}, {"st": 7, "ed": 9, "text": "machine learning"}, {"st": 15, "ed": 17, "text": "source domain"}, {"st": 62, "ed": 64, "text": "domain adaptation"}, {"st": 69, "ed": 72, "text": "deep neural networks"}, {"st": 90, "ed": 92, "text": "fine tuning"}, {"st": 96, "ed": 98, "text": "labeled samples"}, {"st": 122, "ed": 124, "text": "labeled samples"}, {"st": 126, "ed": 128, "text": "source domain"}, {"st": 136, "ed": 138, "text": "labeled samples"}, {"st": 153, "ed": 155, "text": "source domain"}]
[{"st": 8, "ed": 11, "text": "deep neural nets"}, {"st": 12, "ed": 14, "text": "gesture recognition"}, {"st": 38, "ed": 40, "text": "labeled dataset"}, {"st": 53, "ed": 55, "text": "lighting conditions"}]
[{"st": 6, "ed": 9, "text": "deep neural network"}, {"st": 22, "ed": 25, "text": "deep neural networks"}, {"st": 29, "ed": 31, "text": "image understanding"}, {"st": 36, "ed": 38, "text": "network topologies"}, {"st": 67, "ed": 69, "text": "gesture recognition"}, {"st": 83, "ed": 85, "text": "semi supervised"}]
[{"st": 11, "ed": 13, "text": "face image"}, {"st": 15, "ed": 17, "text": "low resolution"}, {"st": 17, "ed": 19, "text": "input image"}, {"st": 95, "ed": 98, "text": "deep network architecture"}, {"st": 99, "ed": 102, "text": "global and local"}, {"st": 113, "ed": 116, "text": "end to end"}, {"st": 121, "ed": 123, "text": "network design"}, {"st": 158, "ed": 160, "text": "deep network"}, {"st": 163, "ed": 165, "text": "loss function"}, {"st": 166, "ed": 168, "text": "super resolution"}, {"st": 186, "ed": 189, "text": "conduct extensive experiments"}]
[{"st": 24, "ed": 28, "text": "recurrent neural networks rnns"}, {"st": 30, "ed": 33, "text": "short term memory"}, {"st": 36, "ed": 38, "text": "feature representations"}, {"st": 42, "ed": 44, "text": "temporal dependencies"}, {"st": 48, "ed": 51, "text": "end to end"}, {"st": 51, "ed": 53, "text": "fully connected"}, {"st": 53, "ed": 55, "text": "deep lstm"}, {"st": 67, "ed": 69, "text": "co occurrences"}, {"st": 91, "ed": 93, "text": "regularization scheme"}, {"st": 96, "ed": 98, "text": "co occurrence"}, {"st": 105, "ed": 107, "text": "deep lstm"}]
[{"st": 7, "ed": 9, "text": "input image"}, {"st": 15, "ed": 17, "text": "recent methods"}, {"st": 22, "ed": 24, "text": "feed forward"}, {"st": 24, "ed": 27, "text": "convolutional neural networks"}, {"st": 37, "ed": 39, "text": "ground truth"}, {"st": 57, "ed": 59, "text": "loss functions"}, {"st": 61, "ed": 64, "text": "high level features"}, {"st": 81, "ed": 83, "text": "loss functions"}, {"st": 85, "ed": 87, "text": "feed forward"}, {"st": 97, "ed": 99, "text": "style transfer"}, {"st": 101, "ed": 103, "text": "feed forward"}, {"st": 109, "ed": 111, "text": "optimization problem"}, {"st": 114, "ed": 116, "text": "et al"}, {"st": 123, "ed": 125, "text": "based method"}, {"st": 129, "ed": 131, "text": "qualitative results"}, {"st": 134, "ed": 137, "text": "orders of magnitude"}, {"st": 143, "ed": 146, "text": "image super resolution"}]
[{"st": 8, "ed": 11, "text": "human pose estimation"}, {"st": 36, "ed": 38, "text": "convolutional neural"}, {"st": 92, "ed": 94, "text": "previous methods"}, {"st": 129, "ed": 131, "text": "competitive performance"}]
[{"st": 1, "ed": 3, "text": "hierarchical clustering"}, {"st": 6, "ed": 9, "text": "gaussian mixture model"}, {"st": 17, "ed": 19, "text": "mixture models"}, {"st": 66, "ed": 68, "text": "mixture model"}, {"st": 70, "ed": 72, "text": "method generates"}, {"st": 76, "ed": 78, "text": "higher quality"}]
[{"st": 6, "ed": 8, "text": "structured prediction"}, {"st": 15, "ed": 18, "text": "conditional random fields"}, {"st": 21, "ed": 23, "text": "deep learning"}, {"st": 25, "ed": 27, "text": "structured prediction"}, {"st": 31, "ed": 33, "text": "global optimum"}, {"st": 55, "ed": 57, "text": "closed form"}, {"st": 65, "ed": 67, "text": "deep structured"}, {"st": 72, "ed": 74, "text": "back propagation"}, {"st": 86, "ed": 88, "text": "hand crafted"}, {"st": 107, "ed": 109, "text": "deep architectures"}, {"st": 117, "ed": 120, "text": "end to end"}, {"st": 126, "ed": 128, "text": "numerical analysis"}, {"st": 131, "ed": 133, "text": "efficient algorithms"}, {"st": 134, "ed": 137, "text": "inference and learning"}, {"st": 159, "ed": 161, "text": "structured prediction"}, {"st": 162, "ed": 164, "text": "deep learning"}, {"st": 166, "ed": 168, "text": "multi resolution"}, {"st": 177, "ed": 179, "text": "optimization framework"}, {"st": 195, "ed": 197, "text": "image segmentation"}, {"st": 199, "ed": 201, "text": "substantial improvements"}, {"st": 212, "ed": 216, "text": "available at https github.com"}]
[{"st": 5, "ed": 7, "text": "efficient inference"}, {"st": 21, "ed": 23, "text": "probabilistic inference"}, {"st": 25, "ed": 28, "text": "recurrent neural network"}, {"st": 69, "ed": 72, "text": "variational auto encoders"}, {"st": 88, "ed": 90, "text": "multiple objects"}, {"st": 114, "ed": 116, "text": "forward pass"}]
[{"st": 4, "ed": 6, "text": "image quality"}, {"st": 12, "ed": 14, "text": "visual content"}, {"st": 32, "ed": 34, "text": "image quality"}, {"st": 114, "ed": 117, "text": "extensive experimental results"}, {"st": 133, "ed": 135, "text": "image quality"}, {"st": 146, "ed": 148, "text": "objective function"}, {"st": 155, "ed": 157, "text": "content delivery"}]
[{"st": 4, "ed": 6, "text": "important role"}, {"st": 26, "ed": 28, "text": "multiple labels"}, {"st": 32, "ed": 34, "text": "error prone"}, {"st": 45, "ed": 48, "text": "multi label classification"}, {"st": 50, "ed": 53, "text": "hidden markov models"}, {"st": 65, "ed": 67, "text": "raw data"}, {"st": 86, "ed": 89, "text": "multi label classification"}, {"st": 99, "ed": 101, "text": "multi label"}, {"st": 104, "ed": 106, "text": "multi class"}, {"st": 134, "ed": 136, "text": "scales linearly"}]
[{"st": 0, "ed": 3, "text": "unsupervised feature learning"}, {"st": 8, "ed": 10, "text": "classification tasks"}, {"st": 18, "ed": 20, "text": "sparse coding"}, {"st": 22, "ed": 24, "text": "feature representations"}, {"st": 35, "ed": 37, "text": "event detection"}, {"st": 49, "ed": 51, "text": "feature learning"}, {"st": 64, "ed": 66, "text": "classification accuracy"}]
[{"st": 8, "ed": 10, "text": "topic model"}, {"st": 16, "ed": 18, "text": "topic models"}, {"st": 22, "ed": 24, "text": "latent variable"}, {"st": 43, "ed": 45, "text": "feature selection"}, {"st": 49, "ed": 51, "text": "generative model"}, {"st": 164, "ed": 166, "text": "classification tasks"}, {"st": 170, "ed": 174, "text": "convolutional neural network cnn"}]
[{"st": 2, "ed": 4, "text": "natural images"}, {"st": 17, "ed": 19, "text": "training examples"}, {"st": 58, "ed": 60, "text": "visual concepts"}, {"st": 84, "ed": 86, "text": "visual features"}, {"st": 92, "ed": 94, "text": "natural language"}, {"st": 101, "ed": 103, "text": "recently introduced"}, {"st": 119, "ed": 121, "text": "proposed method"}, {"st": 126, "ed": 128, "text": "visual features"}, {"st": 141, "ed": 143, "text": "semantically meaningful"}, {"st": 148, "ed": 150, "text": "image regions"}]
[{"st": 4, "ed": 6, "text": "supervised learning"}, {"st": 22, "ed": 24, "text": "structured prediction"}, {"st": 26, "ed": 28, "text": "sequential data"}, {"st": 29, "ed": 31, "text": "main idea"}, {"st": 35, "ed": 38, "text": "short term memory"}, {"st": 43, "ed": 46, "text": "recurrent neural networks"}, {"st": 74, "ed": 76, "text": "detailed analysis"}, {"st": 119, "ed": 121, "text": "annotated data"}, {"st": 128, "ed": 130, "text": "main idea"}, {"st": 150, "ed": 152, "text": "domain adaptation"}, {"st": 161, "ed": 163, "text": "statistical properties"}]
[{"st": 7, "ed": 9, "text": "clustering method"}, {"st": 29, "ed": 31, "text": "k means"}, {"st": 86, "ed": 88, "text": "worst case"}, {"st": 90, "ed": 92, "text": "per iteration"}, {"st": 117, "ed": 120, "text": "number of clusters"}, {"st": 128, "ed": 130, "text": "k means"}, {"st": 139, "ed": 141, "text": "significantly lower"}, {"st": 159, "ed": 161, "text": "extensive experiments"}, {"st": 179, "ed": 181, "text": "standard datasets"}, {"st": 208, "ed": 210, "text": "k means"}]
[{"st": 1, "ed": 4, "text": "convolutional neural networks"}, {"st": 11, "ed": 13, "text": "structured prediction"}, {"st": 25, "ed": 27, "text": "object instances"}, {"st": 30, "ed": 32, "text": "instance segmentation"}, {"st": 42, "ed": 44, "text": "autonomous driving"}, {"st": 44, "ed": 46, "text": "image captioning"}, {"st": 57, "ed": 59, "text": "low level"}, {"st": 71, "ed": 74, "text": "end to end"}, {"st": 74, "ed": 78, "text": "recurrent neural network rnn"}, {"st": 81, "ed": 83, "text": "attention mechanism"}, {"st": 98, "ed": 100, "text": "jointly trained"}, {"st": 120, "ed": 122, "text": "competitive results"}]
[{"st": 3, "ed": 5, "text": "convolutional layer"}, {"st": 61, "ed": 63, "text": "wide variety"}, {"st": 72, "ed": 74, "text": "local spatial"}, {"st": 125, "ed": 127, "text": "mnist dataset"}, {"st": 167, "ed": 169, "text": "supervised tasks"}, {"st": 174, "ed": 176, "text": "optical flow"}]
[{"st": 6, "ed": 8, "text": "multi view"}, {"st": 11, "ed": 13, "text": "visual cues"}, {"st": 23, "ed": 25, "text": "subspace learning"}, {"st": 34, "ed": 36, "text": "multiple views"}, {"st": 36, "ed": 38, "text": "supervised learning"}, {"st": 45, "ed": 48, "text": "canonical correlation analysis"}, {"st": 53, "ed": 56, "text": "linear discriminant analysis"}, {"st": 75, "ed": 78, "text": "deep neural networks"}, {"st": 90, "ed": 92, "text": "multi view"}, {"st": 112, "ed": 114, "text": "multi view"}, {"st": 114, "ed": 116, "text": "embedding methods"}, {"st": 117, "ed": 120, "text": "visual object recognition"}, {"st": 121, "ed": 123, "text": "cross modal"}, {"st": 123, "ed": 125, "text": "image retrieval"}]
[{"st": 0, "ed": 3, "text": "self paced learning"}, {"st": 37, "ed": 39, "text": "existing methods"}, {"st": 68, "ed": 70, "text": "self paced"}, {"st": 89, "ed": 91, "text": "self paced"}, {"st": 100, "ed": 102, "text": "loss function"}, {"st": 114, "ed": 116, "text": "general framework"}, {"st": 138, "ed": 140, "text": "loss functions"}, {"st": 169, "ed": 172, "text": "supervised and unsupervised"}]
[{"st": 5, "ed": 7, "text": "classification algorithms"}, {"st": 7, "ed": 9, "text": "random forest"}, {"st": 18, "ed": 20, "text": "hidden layer"}, {"st": 20, "ed": 22, "text": "neural nets"}, {"st": 22, "ed": 25, "text": "extreme learning machines"}, {"st": 25, "ed": 28, "text": "k nearest neighbors"}, {"st": 33, "ed": 35, "text": "naive bayes"}, {"st": 38, "ed": 40, "text": "elastic net"}, {"st": 40, "ed": 42, "text": "logistic regression"}, {"st": 42, "ed": 44, "text": "sparse linear"}, {"st": 50, "ed": 52, "text": "linear classifiers"}, {"st": 54, "ed": 56, "text": "real life"}, {"st": 70, "ed": 72, "text": "random forest"}, {"st": 95, "ed": 97, "text": "error rate"}]
[{"st": 20, "ed": 22, "text": "semi supervised"}, {"st": 25, "ed": 27, "text": "multi view"}, {"st": 39, "ed": 41, "text": "computationally expensive"}, {"st": 62, "ed": 64, "text": "labelled data"}, {"st": 100, "ed": 102, "text": "strong performance"}]
[{"st": 9, "ed": 12, "text": "kernel density estimation"}, {"st": 15, "ed": 18, "text": "gaussian mixture models"}, {"st": 18, "ed": 20, "text": "input data"}, {"st": 37, "ed": 39, "text": "computational efficiency"}, {"st": 46, "ed": 48, "text": "approach produces"}, {"st": 61, "ed": 63, "text": "significant computational"}, {"st": 63, "ed": 65, "text": "performance gains"}, {"st": 74, "ed": 76, "text": "gaussian kernels"}, {"st": 78, "ed": 80, "text": "improve performance"}, {"st": 98, "ed": 100, "text": "times faster"}]
[{"st": 67, "ed": 69, "text": "feature extractor"}, {"st": 71, "ed": 74, "text": "convolutional neural networks"}, {"st": 78, "ed": 81, "text": "support vector machine"}, {"st": 94, "ed": 96, "text": "multi stage"}, {"st": 106, "ed": 108, "text": "deep learning"}, {"st": 111, "ed": 113, "text": "feature extractors"}, {"st": 119, "ed": 121, "text": "classification results"}, {"st": 127, "ed": 129, "text": "multi stage"}, {"st": 129, "ed": 131, "text": "decision making"}, {"st": 151, "ed": 153, "text": "false negatives"}]
[{"st": 0, "ed": 2, "text": "large scale"}, {"st": 2, "ed": 4, "text": "supervised classification"}, {"st": 9, "ed": 13, "text": "deep convolutional neural networks"}, {"st": 16, "ed": 20, "text": "amounts of training data"}, {"st": 36, "ed": 38, "text": "training process"}, {"st": 66, "ed": 68, "text": "convex optimization"}, {"st": 98, "ed": 100, "text": "optimization problem"}, {"st": 116, "ed": 118, "text": "standard classification"}, {"st": 126, "ed": 128, "text": "training set"}, {"st": 129, "ed": 131, "text": "without compromising"}]
[{"st": 1, "ed": 3, "text": "machine learning"}, {"st": 10, "ed": 12, "text": "social networking"}, {"st": 18, "ed": 20, "text": "feature selection"}, {"st": 39, "ed": 41, "text": "computational cost"}, {"st": 51, "ed": 53, "text": "feature selection"}, {"st": 76, "ed": 78, "text": "feature selection"}, {"st": 112, "ed": 114, "text": "support vectors"}, {"st": 143, "ed": 145, "text": "max margin"}, {"st": 153, "ed": 155, "text": "existing techniques"}, {"st": 163, "ed": 165, "text": "computationally efficient"}, {"st": 177, "ed": 180, "text": "coordinate descent algorithm"}, {"st": 194, "ed": 196, "text": "sparse representation"}, {"st": 209, "ed": 211, "text": "benchmark datasets"}, {"st": 222, "ed": 225, "text": "orders of magnitude"}, {"st": 242, "ed": 244, "text": "feature selection"}]
[{"st": 8, "ed": 10, "text": "generative models"}, {"st": 17, "ed": 19, "text": "deep learning"}, {"st": 55, "ed": 57, "text": "deep model"}, {"st": 67, "ed": 69, "text": "deep network"}, {"st": 91, "ed": 93, "text": "feed forward"}, {"st": 96, "ed": 100, "text": "trained end to end"}, {"st": 135, "ed": 137, "text": "encouraging results"}, {"st": 157, "ed": 159, "text": "visual object"}]
[{"st": 4, "ed": 6, "text": "image generation"}, {"st": 40, "ed": 42, "text": "class labels"}, {"st": 70, "ed": 72, "text": "convolutional network"}, {"st": 93, "ed": 95, "text": "facial expressions"}, {"st": 118, "ed": 120, "text": "convolutional layers"}, {"st": 126, "ed": 128, "text": "log likelihood"}]
[{"st": 0, "ed": 2, "text": "unsupervised learning"}, {"st": 3, "ed": 5, "text": "supervised learning"}, {"st": 17, "ed": 19, "text": "neural networks"}, {"st": 28, "ed": 30, "text": "remarkable success"}, {"st": 32, "ed": 34, "text": "computer vision"}, {"st": 38, "ed": 40, "text": "large scale"}, {"st": 40, "ed": 42, "text": "labeled images"}, {"st": 58, "ed": 60, "text": "unsupervised learning"}, {"st": 63, "ed": 66, "text": "supervised and unsupervised"}, {"st": 69, "ed": 71, "text": "large scale"}, {"st": 75, "ed": 77, "text": "neural networks"}, {"st": 91, "ed": 93, "text": "large scale"}, {"st": 101, "ed": 103, "text": "input images"}, {"st": 107, "ed": 109, "text": "local spatial"}, {"st": 112, "ed": 115, "text": "end to end"}, {"st": 143, "ed": 145, "text": "recently proposed"}, {"st": 176, "ed": 178, "text": "strong baseline"}, {"st": 179, "ed": 181, "text": "image classification"}, {"st": 185, "ed": 187, "text": "validation set"}]
[{"st": 8, "ed": 11, "text": "markov random field"}, {"st": 30, "ed": 32, "text": "unlike previous"}, {"st": 37, "ed": 39, "text": "iterative algorithm"}, {"st": 45, "ed": 49, "text": "convolutional neural network cnn"}, {"st": 57, "ed": 60, "text": "end to end"}, {"st": 84, "ed": 86, "text": "mean field"}, {"st": 100, "ed": 102, "text": "recent works"}, {"st": 109, "ed": 111, "text": "back propagation"}, {"st": 115, "ed": 117, "text": "achieve high"}, {"st": 134, "ed": 136, "text": "existing models"}, {"st": 147, "ed": 149, "text": "unified framework"}, {"st": 152, "ed": 154, "text": "contextual information"}, {"st": 202, "ed": 205, "text": "pascal voc 2012"}]
[{"st": 1, "ed": 3, "text": "machine learning"}, {"st": 6, "ed": 8, "text": "speech recognition"}, {"st": 8, "ed": 10, "text": "gesture recognition"}, {"st": 11, "ed": 13, "text": "handwriting recognition"}, {"st": 25, "ed": 28, "text": "conditional random field"}, {"st": 68, "ed": 70, "text": "neural networks"}, {"st": 82, "ed": 85, "text": "recurrent neural networks"}, {"st": 109, "ed": 111, "text": "gesture recognition"}, {"st": 115, "ed": 117, "text": "proposed method"}, {"st": 119, "ed": 122, "text": "hidden markov models"}]
[{"st": 5, "ed": 8, "text": "deep neural networks"}, {"st": 10, "ed": 13, "text": "deep convolutional networks"}, {"st": 21, "ed": 24, "text": "stochastic gradient descent"}, {"st": 51, "ed": 53, "text": "low precision"}, {"st": 53, "ed": 55, "text": "fixed point"}, {"st": 73, "ed": 75, "text": "neural networks"}, {"st": 123, "ed": 126, "text": "deep convolutional networks"}]
[{"st": 10, "ed": 12, "text": "future frames"}, {"st": 20, "ed": 22, "text": "traditional methods"}, {"st": 41, "ed": 43, "text": "future frames"}, {"st": 48, "ed": 50, "text": "probabilistic model"}, {"st": 61, "ed": 63, "text": "future frames"}, {"st": 88, "ed": 90, "text": "network structure"}, {"st": 93, "ed": 95, "text": "convolutional network"}, {"st": 99, "ed": 101, "text": "future frames"}, {"st": 102, "ed": 104, "text": "network structure"}, {"st": 110, "ed": 112, "text": "feature maps"}, {"st": 123, "ed": 125, "text": "synthetic data"}]
[{"st": 7, "ed": 9, "text": "encoder decoder"}, {"st": 9, "ed": 11, "text": "neural network"}, {"st": 22, "ed": 24, "text": "compressive sensing"}, {"st": 53, "ed": 55, "text": "proposed framework"}, {"st": 57, "ed": 60, "text": "end to end"}, {"st": 120, "ed": 122, "text": "wide variety"}, {"st": 123, "ed": 125, "text": "compressive sensing"}]
[{"st": 4, "ed": 6, "text": "unified framework"}, {"st": 7, "ed": 9, "text": "multi class"}, {"st": 9, "ed": 11, "text": "cost sensitive"}, {"st": 33, "ed": 35, "text": "weak learners"}, {"st": 52, "ed": 54, "text": "cost sensitive"}, {"st": 55, "ed": 57, "text": "weak learners"}, {"st": 80, "ed": 82, "text": "loss functions"}, {"st": 102, "ed": 104, "text": "decision trees"}, {"st": 147, "ed": 149, "text": "strong baselines"}, {"st": 152, "ed": 154, "text": "multi class"}]
[{"st": 28, "ed": 30, "text": "machine learning"}, {"st": 45, "ed": 47, "text": "contextual information"}, {"st": 73, "ed": 75, "text": "method called"}, {"st": 88, "ed": 90, "text": "machine learning"}, {"st": 94, "ed": 96, "text": "learning process"}, {"st": 104, "ed": 106, "text": "multi modal"}, {"st": 110, "ed": 112, "text": "prior knowledge"}, {"st": 131, "ed": 133, "text": "multi modal"}, {"st": 227, "ed": 229, "text": "comparable accuracy"}, {"st": 230, "ed": 232, "text": "supervised learning"}]
[{"st": 4, "ed": 6, "text": "neural network"}, {"st": 17, "ed": 19, "text": "computer vision"}, {"st": 28, "ed": 30, "text": "spatial transformer"}, {"st": 57, "ed": 59, "text": "neural network"}, {"st": 73, "ed": 76, "text": "end to end"}, {"st": 82, "ed": 84, "text": "domain knowledge"}, {"st": 102, "ed": 105, "text": "end to end"}, {"st": 107, "ed": 109, "text": "depth estimation"}, {"st": 110, "ed": 112, "text": "unsupervised learning"}, {"st": 119, "ed": 121, "text": "image reconstruction"}]
[{"st": 2, "ed": 4, "text": "great success"}, {"st": 5, "ed": 9, "text": "convolutional neural networks cnn"}, {"st": 11, "ed": 13, "text": "image classification"}, {"st": 17, "ed": 20, "text": "cifar and imagenet"}, {"st": 65, "ed": 68, "text": "gaussian mixture model"}, {"st": 101, "ed": 103, "text": "neural network"}, {"st": 111, "ed": 114, "text": "end to end"}, {"st": 133, "ed": 136, "text": "convolutional neural network"}, {"st": 144, "ed": 147, "text": "end to end"}, {"st": 165, "ed": 167, "text": "classification accuracy"}, {"st": 168, "ed": 170, "text": "computational efficiency"}, {"st": 173, "ed": 175, "text": "pascal voc"}, {"st": 175, "ed": 177, "text": "object classification"}]
[{"st": 8, "ed": 11, "text": "deep neural network"}, {"st": 24, "ed": 26, "text": "challenging problem"}, {"st": 45, "ed": 47, "text": "spatial information"}, {"st": 49, "ed": 52, "text": "convolutional neural network"}, {"st": 56, "ed": 58, "text": "invariant representation"}, {"st": 62, "ed": 64, "text": "proposed approach"}, {"st": 66, "ed": 68, "text": "previous results"}]
[{"st": 0, "ed": 2, "text": "neural network"}, {"st": 8, "ed": 11, "text": "vulnerable to adversarial"}, {"st": 13, "ed": 15, "text": "natural images"}, {"st": 21, "ed": 23, "text": "adversarial perturbation"}, {"st": 23, "ed": 25, "text": "specifically designed"}, {"st": 37, "ed": 39, "text": "adversarial images"}, {"st": 61, "ed": 63, "text": "adversarial images"}, {"st": 73, "ed": 75, "text": "machine learning"}, {"st": 81, "ed": 83, "text": "neural networks"}, {"st": 93, "ed": 95, "text": "image classification"}, {"st": 133, "ed": 135, "text": "classification accuracy"}]
[{"st": 1, "ed": 3, "text": "clustering problems"}, {"st": 4, "ed": 6, "text": "computer vision"}, {"st": 11, "ed": 13, "text": "classification problems"}, {"st": 20, "ed": 22, "text": "subspace clustering"}, {"st": 37, "ed": 39, "text": "face images"}, {"st": 69, "ed": 71, "text": "clustering algorithm"}, {"st": 130, "ed": 132, "text": "subspace clustering"}]
[{"st": 21, "ed": 23, "text": "deep learning"}]
[{"st": 0, "ed": 2, "text": "gesture recognition"}, {"st": 18, "ed": 20, "text": "statistical methods"}, {"st": 22, "ed": 25, "text": "hidden markov model"}, {"st": 30, "ed": 32, "text": "gesture recognition"}, {"st": 34, "ed": 36, "text": "low complexity"}, {"st": 36, "ed": 40, "text": "recurrent neural network rnn"}, {"st": 54, "ed": 58, "text": "convolutional neural network cnn"}, {"st": 71, "ed": 73, "text": "fixed point"}, {"st": 98, "ed": 100, "text": "power consumption"}]
[{"st": 2, "ed": 4, "text": "local binary"}, {"st": 10, "ed": 12, "text": "convolutional layers"}, {"st": 14, "ed": 18, "text": "convolutional neural networks cnn"}, {"st": 27, "ed": 29, "text": "local binary"}, {"st": 42, "ed": 44, "text": "pre defined"}, {"st": 45, "ed": 47, "text": "convolutional filters"}, {"st": 53, "ed": 55, "text": "training process"}, {"st": 57, "ed": 59, "text": "linear activation"}, {"st": 144, "ed": 146, "text": "local binary"}, {"st": 163, "ed": 165, "text": "local binary"}, {"st": 165, "ed": 168, "text": "convolutional neural networks"}, {"st": 181, "ed": 183, "text": "mnist svhn"}, {"st": 183, "ed": 185, "text": "cifar 10"}, {"st": 189, "ed": 191, "text": "significant computational"}]
[{"st": 77, "ed": 79, "text": "extensive simulations"}, {"st": 83, "ed": 85, "text": "carefully designed"}, {"st": 108, "ed": 110, "text": "preliminary results"}]
[{"st": 5, "ed": 7, "text": "convolutional networks"}, {"st": 45, "ed": 47, "text": "convolutional network"}, {"st": 58, "ed": 60, "text": "feed forward"}, {"st": 63, "ed": 65, "text": "convolutional networks"}, {"st": 92, "ed": 94, "text": "feature maps"}, {"st": 105, "ed": 107, "text": "feature maps"}, {"st": 123, "ed": 126, "text": "vanishing gradient problem"}, {"st": 146, "ed": 148, "text": "highly competitive"}, {"st": 148, "ed": 150, "text": "object recognition"}, {"st": 150, "ed": 152, "text": "benchmark tasks"}, {"st": 152, "ed": 156, "text": "cifar 10 cifar 100"}, {"st": 162, "ed": 164, "text": "significant improvements"}, {"st": 179, "ed": 181, "text": "achieve high"}, {"st": 184, "ed": 186, "text": "pre trained"}, {"st": 188, "ed": 192, "text": "available at https github.com"}]
[{"st": 53, "ed": 56, "text": "fully connected layers"}, {"st": 66, "ed": 68, "text": "convolutional layers"}, {"st": 112, "ed": 114, "text": "feature maps"}, {"st": 170, "ed": 172, "text": "vgg 16"}]
[{"st": 63, "ed": 66, "text": "deep neural networks"}, {"st": 81, "ed": 83, "text": "theoretical framework"}, {"st": 85, "ed": 87, "text": "graph based"}, {"st": 89, "ed": 91, "text": "empirically evaluate"}]
[{"st": 5, "ed": 7, "text": "multi label"}, {"st": 117, "ed": 119, "text": "video classification"}, {"st": 160, "ed": 163, "text": "the research community"}, {"st": 166, "ed": 168, "text": "feature extraction"}, {"st": 175, "ed": 177, "text": "machine learning"}, {"st": 179, "ed": 181, "text": "related tasks"}]
[{"st": 84, "ed": 86, "text": "automatic segmentation"}, {"st": 130, "ed": 132, "text": "real life"}]
[{"st": 6, "ed": 8, "text": "frame prediction"}, {"st": 13, "ed": 16, "text": "convolutional neural network"}, {"st": 48, "ed": 50, "text": "frame prediction"}, {"st": 115, "ed": 117, "text": "proposed approach"}, {"st": 132, "ed": 134, "text": "proposed method"}, {"st": 143, "ed": 145, "text": "existing methods"}]
[{"st": 7, "ed": 9, "text": "prediction problems"}, {"st": 10, "ed": 12, "text": "low level"}, {"st": 12, "ed": 14, "text": "edge detection"}, {"st": 15, "ed": 17, "text": "mid level"}, {"st": 30, "ed": 33, "text": "fully convolutional network"}, {"st": 36, "ed": 38, "text": "remarkable success"}, {"st": 50, "ed": 52, "text": "computationally efficient"}, {"st": 60, "ed": 62, "text": "statistically efficient"}, {"st": 79, "ed": 81, "text": "stratified sampling"}, {"st": 92, "ed": 94, "text": "multi scale"}, {"st": 103, "ed": 106, "text": "fully connected layers"}, {"st": 160, "ed": 162, "text": "edge detection"}]
[{"st": 9, "ed": 11, "text": "latent factor"}, {"st": 14, "ed": 16, "text": "training data"}, {"st": 28, "ed": 30, "text": "main goal"}, {"st": 54, "ed": 56, "text": "training set"}]
[{"st": 5, "ed": 7, "text": "neural network"}, {"st": 19, "ed": 21, "text": "low power"}, {"st": 31, "ed": 33, "text": "haar wavelet"}, {"st": 36, "ed": 40, "text": "convolutional neural network cnn"}, {"st": 46, "ed": 49, "text": "radial basis function"}, {"st": 49, "ed": 51, "text": "kernel approximation"}, {"st": 67, "ed": 69, "text": "linear classifier"}]
[{"st": 8, "ed": 10, "text": "low rank"}, {"st": 36, "ed": 38, "text": "low rank"}, {"st": 49, "ed": 51, "text": "proposed algorithm"}, {"st": 51, "ed": 53, "text": "simultaneously learns"}, {"st": 54, "ed": 56, "text": "low rank"}, {"st": 64, "ed": 67, "text": "synthetic and real"}, {"st": 67, "ed": 69, "text": "brain imaging"}, {"st": 71, "ed": 74, "text": "unsupervised and supervised"}, {"st": 74, "ed": 76, "text": "classification tasks"}]
[{"st": 1, "ed": 3, "text": "object tracking"}, {"st": 10, "ed": 12, "text": "computer vision"}, {"st": 60, "ed": 62, "text": "hyper parameters"}, {"st": 84, "ed": 86, "text": "neural network"}, {"st": 104, "ed": 106, "text": "competitive performance"}, {"st": 108, "ed": 111, "text": "speed and accuracy"}]
[{"st": 3, "ed": 6, "text": "low computational cost"}, {"st": 9, "ed": 11, "text": "metric learning"}, {"st": 12, "ed": 14, "text": "large scale"}, {"st": 20, "ed": 22, "text": "closed form"}, {"st": 69, "ed": 71, "text": "closed form"}, {"st": 131, "ed": 134, "text": "cold start problem"}, {"st": 150, "ed": 152, "text": "tasks including"}, {"st": 155, "ed": 157, "text": "face verification"}, {"st": 159, "ed": 161, "text": "event detection"}, {"st": 180, "ed": 182, "text": "feature extraction"}, {"st": 184, "ed": 186, "text": "hand crafted"}, {"st": 200, "ed": 202, "text": "promising performance"}, {"st": 215, "ed": 217, "text": "cold start"}]
[{"st": 7, "ed": 9, "text": "dictionary learning"}, {"st": 15, "ed": 17, "text": "local geometry"}, {"st": 24, "ed": 26, "text": "graph based"}, {"st": 28, "ed": 31, "text": "takes into account"}, {"st": 65, "ed": 67, "text": "training data"}, {"st": 73, "ed": 75, "text": "dictionary learning"}, {"st": 84, "ed": 86, "text": "discriminative power"}, {"st": 89, "ed": 91, "text": "sparse representations"}, {"st": 98, "ed": 100, "text": "proposed method"}, {"st": 111, "ed": 114, "text": "multi label classification"}]
[{"st": 15, "ed": 17, "text": "joint distribution"}, {"st": 29, "ed": 31, "text": "neural architecture"}]
[{"st": 0, "ed": 2, "text": "face recognition"}, {"st": 28, "ed": 30, "text": "face recognition"}, {"st": 39, "ed": 41, "text": "face images"}, {"st": 58, "ed": 60, "text": "low resolution"}, {"st": 60, "ed": 62, "text": "face images"}, {"st": 83, "ed": 85, "text": "low resolution"}, {"st": 88, "ed": 90, "text": "face images"}, {"st": 100, "ed": 103, "text": "support vector machine"}, {"st": 134, "ed": 136, "text": "kernel learning"}, {"st": 152, "ed": 155, "text": "multiple kernel learning"}, {"st": 167, "ed": 170, "text": "unsupervised domain adaptation"}, {"st": 173, "ed": 177, "text": "reproducing kernel hilbert space"}, {"st": 191, "ed": 193, "text": "real world"}, {"st": 208, "ed": 210, "text": "recognition accuracy"}, {"st": 215, "ed": 217, "text": "proposed method"}]
[{"st": 4, "ed": 6, "text": "residual network"}, {"st": 18, "ed": 21, "text": "image to image"}, {"st": 56, "ed": 58, "text": "quadratic loss"}]
[{"st": 12, "ed": 14, "text": "connectivity patterns"}, {"st": 27, "ed": 30, "text": "bag of words"}, {"st": 75, "ed": 79, "text": "gaussian mixture model gmm"}, {"st": 100, "ed": 102, "text": "connectivity patterns"}, {"st": 135, "ed": 138, "text": "support vector machines"}, {"st": 173, "ed": 175, "text": "mixture models"}]
[{"st": 15, "ed": 17, "text": "proposed approach"}, {"st": 23, "ed": 25, "text": "random sampling"}, {"st": 30, "ed": 32, "text": "machine learning"}, {"st": 43, "ed": 46, "text": "nonnegative matrix factorization"}, {"st": 50, "ed": 52, "text": "proposed framework"}, {"st": 96, "ed": 98, "text": "medium sized"}, {"st": 132, "ed": 134, "text": "parameter estimation"}]
[{"st": 10, "ed": 12, "text": "visual object"}, {"st": 17, "ed": 19, "text": "reinforcement learning"}, {"st": 21, "ed": 23, "text": "neural network"}, {"st": 45, "ed": 47, "text": "randomly generated"}]
[{"st": 20, "ed": 22, "text": "large scale"}, {"st": 36, "ed": 40, "text": "convolutional neural network cnn"}, {"st": 41, "ed": 43, "text": "time series"}, {"st": 63, "ed": 65, "text": "comparative study"}]
[{"st": 15, "ed": 18, "text": "generative adversarial network"}, {"st": 30, "ed": 32, "text": "recent advances"}, {"st": 34, "ed": 36, "text": "convolutional networks"}, {"st": 83, "ed": 85, "text": "low dimensional"}, {"st": 133, "ed": 135, "text": "experiments demonstrate"}, {"st": 137, "ed": 139, "text": "method generates"}, {"st": 146, "ed": 148, "text": "learned features"}, {"st": 149, "ed": 151, "text": "impressive performance"}, {"st": 153, "ed": 155, "text": "object recognition"}, {"st": 159, "ed": 161, "text": "supervised learning"}]
[{"st": 34, "ed": 36, "text": "higher level"}, {"st": 55, "ed": 57, "text": "deep network"}]
[{"st": 2, "ed": 4, "text": "embedding methods"}, {"st": 5, "ed": 7, "text": "vision tasks"}, {"st": 29, "ed": 32, "text": "effective and efficient"}, {"st": 63, "ed": 65, "text": "feature space"}, {"st": 103, "ed": 105, "text": "similarity metric"}, {"st": 107, "ed": 109, "text": "local feature"}, {"st": 122, "ed": 124, "text": "local neighborhood"}, {"st": 148, "ed": 150, "text": "convolutional networks"}, {"st": 160, "ed": 162, "text": "feature embedding"}, {"st": 165, "ed": 167, "text": "faster convergence"}, {"st": 173, "ed": 175, "text": "image retrieval"}, {"st": 177, "ed": 179, "text": "large margin"}, {"st": 190, "ed": 192, "text": "open set"}, {"st": 194, "ed": 196, "text": "transfer learning"}, {"st": 197, "ed": 200, "text": "zero shot learning"}]
[{"st": 4, "ed": 7, "text": "recurrent neural network"}, {"st": 15, "ed": 18, "text": "unsupervised feature learning"}, {"st": 19, "ed": 21, "text": "multi dimensional"}, {"st": 28, "ed": 30, "text": "object recognition"}, {"st": 51, "ed": 53, "text": "previously proposed"}, {"st": 71, "ed": 75, "text": "trained end to end"}, {"st": 83, "ed": 85, "text": "previously proposed"}]
[{"st": 0, "ed": 3, "text": "person re identification"}, {"st": 32, "ed": 34, "text": "feature space"}, {"st": 37, "ed": 41, "text": "convolutional neural networks cnn"}, {"st": 69, "ed": 71, "text": "embedding methods"}, {"st": 112, "ed": 115, "text": "point of view"}, {"st": 119, "ed": 121, "text": "intra class"}, {"st": 121, "ed": 123, "text": "training samples"}, {"st": 140, "ed": 142, "text": "intra class"}, {"st": 160, "ed": 163, "text": "person re identification"}, {"st": 205, "ed": 208, "text": "person re identification"}, {"st": 211, "ed": 213, "text": "deep model"}, {"st": 213, "ed": 215, "text": "significantly outperforms"}, {"st": 243, "ed": 245, "text": "deep models"}]
[{"st": 43, "ed": 45, "text": "residual networks"}, {"st": 89, "ed": 91, "text": "residual network"}, {"st": 121, "ed": 123, "text": "fully connected"}, {"st": 134, "ed": 136, "text": "deep models"}, {"st": 170, "ed": 175, "text": "cifar 10 and cifar 100"}]
[{"st": 1, "ed": 3, "text": "recent years"}, {"st": 3, "ed": 6, "text": "deep neural networks"}, {"st": 6, "ed": 8, "text": "dnn based"}, {"st": 34, "ed": 36, "text": "dnn based"}, {"st": 52, "ed": 54, "text": "embedded systems"}, {"st": 65, "ed": 67, "text": "fixed point"}, {"st": 76, "ed": 78, "text": "computational complexity"}, {"st": 109, "ed": 111, "text": "extensive experiments"}, {"st": 112, "ed": 114, "text": "large scale"}, {"st": 114, "ed": 116, "text": "imagenet classification"}]
[{"st": 6, "ed": 8, "text": "domain adaptation"}, {"st": 11, "ed": 13, "text": "maximum margin"}, {"st": 13, "ed": 15, "text": "domain transfer"}, {"st": 97, "ed": 99, "text": "computational costs"}, {"st": 111, "ed": 113, "text": "image datasets"}, {"st": 129, "ed": 132, "text": "support vector machines"}, {"st": 137, "ed": 139, "text": "image features"}, {"st": 146, "ed": 148, "text": "training samples"}]
[{"st": 6, "ed": 8, "text": "feature importance"}, {"st": 9, "ed": 11, "text": "machine learning"}, {"st": 15, "ed": 17, "text": "deep networks"}, {"st": 84, "ed": 86, "text": "object recognition"}, {"st": 127, "ed": 129, "text": "wide variety"}, {"st": 130, "ed": 132, "text": "deep networks"}, {"st": 139, "ed": 141, "text": "feature importance"}, {"st": 163, "ed": 165, "text": "previous methods"}]
[{"st": 7, "ed": 9, "text": "object detection"}, {"st": 14, "ed": 17, "text": "deep reinforcement learning"}, {"st": 19, "ed": 21, "text": "key idea"}, {"st": 41, "ed": 43, "text": "intelligent agent"}, {"st": 102, "ed": 105, "text": "convolutional neural network"}, {"st": 115, "ed": 117, "text": "feature maps"}, {"st": 128, "ed": 130, "text": "feature maps"}, {"st": 160, "ed": 162, "text": "image features"}, {"st": 196, "ed": 198, "text": "reinforcement learning"}]
[{"st": 0, "ed": 2, "text": "low rank"}, {"st": 4, "ed": 7, "text": "attracted much attention"}, {"st": 17, "ed": 19, "text": "real world"}, {"st": 21, "ed": 23, "text": "subspace segmentation"}, {"st": 27, "ed": 29, "text": "low rank"}, {"st": 34, "ed": 36, "text": "low dimensional"}, {"st": 38, "ed": 40, "text": "supervised learning"}, {"st": 53, "ed": 55, "text": "low rank"}, {"st": 77, "ed": 79, "text": "rank minimization"}, {"st": 83, "ed": 85, "text": "least squares"}, {"st": 98, "ed": 100, "text": "low dimensional"}, {"st": 106, "ed": 108, "text": "robust subspace"}, {"st": 113, "ed": 115, "text": "low rank"}, {"st": 118, "ed": 120, "text": "low dimensional"}, {"st": 148, "ed": 150, "text": "objective function"}, {"st": 155, "ed": 157, "text": "nuclear norm"}, {"st": 157, "ed": 159, "text": "minimization problem"}, {"st": 167, "ed": 169, "text": "lagrange multiplier"}, {"st": 170, "ed": 172, "text": "extensive experiments"}, {"st": 173, "ed": 175, "text": "image classification"}, {"st": 175, "ed": 178, "text": "human pose estimation"}]
[{"st": 1, "ed": 3, "text": "pooling layers"}, {"st": 4, "ed": 6, "text": "max pooling"}, {"st": 7, "ed": 11, "text": "convolutional neural networks cnns"}, {"st": 53, "ed": 55, "text": "feature map"}, {"st": 61, "ed": 63, "text": "spatial resolution"}, {"st": 90, "ed": 92, "text": "starting point"}, {"st": 112, "ed": 114, "text": "signal processing"}, {"st": 214, "ed": 216, "text": "proposed approach"}, {"st": 218, "ed": 220, "text": "extensive experiments"}, {"st": 223, "ed": 225, "text": "image classification"}, {"st": 235, "ed": 239, "text": "available at https github.com"}]
[{"st": 4, "ed": 6, "text": "computer vision"}, {"st": 14, "ed": 16, "text": "abstract concepts"}, {"st": 45, "ed": 47, "text": "low level"}, {"st": 62, "ed": 64, "text": "object localization"}, {"st": 87, "ed": 89, "text": "kernel based"}, {"st": 149, "ed": 151, "text": "image features"}, {"st": 152, "ed": 154, "text": "contextual information"}, {"st": 199, "ed": 201, "text": "multipole expansion"}, {"st": 204, "ed": 207, "text": "proof of concept"}, {"st": 241, "ed": 243, "text": "object localization"}, {"st": 257, "ed": 259, "text": "machine learning"}, {"st": 266, "ed": 268, "text": "partially observed"}]
[{"st": 0, "ed": 3, "text": "multi task learning"}, {"st": 6, "ed": 8, "text": "generalization performance"}, {"st": 10, "ed": 12, "text": "prediction tasks"}, {"st": 15, "ed": 17, "text": "relevant information"}, {"st": 23, "ed": 26, "text": "deep neural networks"}, {"st": 32, "ed": 34, "text": "hand designed"}, {"st": 34, "ed": 36, "text": "network architectures"}, {"st": 47, "ed": 49, "text": "task specific"}, {"st": 55, "ed": 57, "text": "multi task"}, {"st": 57, "ed": 59, "text": "deep architectures"}, {"st": 85, "ed": 87, "text": "error prone"}, {"st": 95, "ed": 97, "text": "principled approach"}, {"st": 100, "ed": 102, "text": "multi task"}, {"st": 102, "ed": 104, "text": "deep learning"}, {"st": 139, "ed": 141, "text": "classification tasks"}, {"st": 153, "ed": 155, "text": "proposed method"}, {"st": 171, "ed": 173, "text": "strong baselines"}]
[{"st": 4, "ed": 6, "text": "matching problem"}, {"st": 8, "ed": 10, "text": "optical flow"}, {"st": 34, "ed": 37, "text": "deep neural network"}, {"st": 39, "ed": 41, "text": "image patches"}, {"st": 52, "ed": 54, "text": "metric learning"}, {"st": 58, "ed": 60, "text": "negative samples"}, {"st": 98, "ed": 100, "text": "optical flow"}]
[{"st": 0, "ed": 4, "text": "generative adversarial networks gans"}, {"st": 20, "ed": 22, "text": "latent space"}, {"st": 49, "ed": 51, "text": "latent variables"}, {"st": 65, "ed": 67, "text": "image retrieval"}, {"st": 68, "ed": 70, "text": "image classification"}, {"st": 80, "ed": 82, "text": "latent space"}, {"st": 100, "ed": 102, "text": "multiple layers"}, {"st": 122, "ed": 124, "text": "latent space"}, {"st": 126, "ed": 128, "text": "pre trained"}, {"st": 160, "ed": 162, "text": "latent space"}, {"st": 199, "ed": 201, "text": "latent space"}]
[{"st": 6, "ed": 8, "text": "generative model"}, {"st": 9, "ed": 12, "text": "generative adversarial nets"}, {"st": 28, "ed": 30, "text": "unlike existing"}, {"st": 30, "ed": 33, "text": "generative adversarial nets"}, {"st": 34, "ed": 36, "text": "based methods"}, {"st": 69, "ed": 71, "text": "latent variable"}, {"st": 78, "ed": 80, "text": "latent variables"}, {"st": 99, "ed": 101, "text": "latent variables"}, {"st": 119, "ed": 121, "text": "recently proposed"}, {"st": 122, "ed": 124, "text": "wasserstein gan"}, {"st": 135, "ed": 138, "text": "end to end"}]
[{"st": 0, "ed": 3, "text": "deep neural networks"}, {"st": 11, "ed": 13, "text": "large scale"}, {"st": 13, "ed": 15, "text": "computer vision"}, {"st": 27, "ed": 29, "text": "matrix multiplications"}, {"st": 48, "ed": 50, "text": "neural networks"}, {"st": 73, "ed": 76, "text": "spike and slab"}, {"st": 86, "ed": 88, "text": "large networks"}, {"st": 98, "ed": 100, "text": "neural network"}]
[{"st": 1, "ed": 4, "text": "k means clustering"}, {"st": 15, "ed": 18, "text": "number of clusters"}, {"st": 33, "ed": 35, "text": "local minima"}, {"st": 36, "ed": 38, "text": "randomly selected"}, {"st": 184, "ed": 186, "text": "k means"}, {"st": 194, "ed": 196, "text": "similarity measures"}, {"st": 215, "ed": 217, "text": "k means"}, {"st": 224, "ed": 226, "text": "clustering methods"}, {"st": 235, "ed": 237, "text": "synthetic data"}, {"st": 248, "ed": 250, "text": "ground truth"}, {"st": 261, "ed": 264, "text": "real world data"}, {"st": 273, "ed": 275, "text": "face database"}]
[{"st": 11, "ed": 13, "text": "feature vector"}, {"st": 29, "ed": 31, "text": "feature vector"}, {"st": 69, "ed": 71, "text": "dimensionality reduction"}, {"st": 77, "ed": 79, "text": "feature vector"}, {"st": 98, "ed": 100, "text": "computational complexity"}, {"st": 114, "ed": 116, "text": "dimensionality reduction"}, {"st": 171, "ed": 173, "text": "dimensionality reduction"}, {"st": 175, "ed": 177, "text": "feature vector"}, {"st": 187, "ed": 189, "text": "dimensionality reduction"}, {"st": 191, "ed": 193, "text": "feature vector"}, {"st": 248, "ed": 251, "text": "support vector machine"}, {"st": 265, "ed": 267, "text": "gesture recognition"}, {"st": 275, "ed": 277, "text": "motion capture"}, {"st": 302, "ed": 304, "text": "significantly reduce"}, {"st": 309, "ed": 311, "text": "feature vector"}, {"st": 336, "ed": 338, "text": "a level"}, {"st": 355, "ed": 357, "text": "computational complexity"}]
[{"st": 0, "ed": 2, "text": "real estate"}, {"st": 12, "ed": 14, "text": "real estate"}, {"st": 36, "ed": 38, "text": "widely adopted"}, {"st": 40, "ed": 42, "text": "real estate"}, {"st": 63, "ed": 65, "text": "real estate"}, {"st": 74, "ed": 76, "text": "real estate"}, {"st": 86, "ed": 88, "text": "real estate"}, {"st": 128, "ed": 130, "text": "computer vision"}, {"st": 135, "ed": 137, "text": "visual content"}, {"st": 144, "ed": 148, "text": "recurrent neural network rnn"}, {"st": 150, "ed": 152, "text": "real estate"}]
[{"st": 10, "ed": 12, "text": "neural network"}, {"st": 24, "ed": 26, "text": "faster training"}, {"st": 33, "ed": 35, "text": "previous attempts"}, {"st": 41, "ed": 43, "text": "performance degradation"}]
[{"st": 19, "ed": 21, "text": "machine learning"}, {"st": 23, "ed": 25, "text": "deep learning"}, {"st": 26, "ed": 28, "text": "resource constrained"}, {"st": 29, "ed": 31, "text": "embedded devices"}, {"st": 32, "ed": 34, "text": "limited memory"}, {"st": 39, "ed": 43, "text": "convolutional neural networks cnns"}, {"st": 49, "ed": 51, "text": "computational power"}, {"st": 56, "ed": 58, "text": "embedded devices"}, {"st": 71, "ed": 73, "text": "edge weights"}, {"st": 95, "ed": 97, "text": "computational efficiency"}, {"st": 105, "ed": 107, "text": "classification accuracy"}]
[{"st": 9, "ed": 11, "text": "kernel machines"}, {"st": 24, "ed": 26, "text": "training set"}, {"st": 44, "ed": 46, "text": "kernel machines"}, {"st": 81, "ed": 83, "text": "gaussian processes"}, {"st": 96, "ed": 98, "text": "kernel methods"}, {"st": 99, "ed": 101, "text": "gaussian process"}, {"st": 138, "ed": 141, "text": "human pose estimation"}]
[{"st": 33, "ed": 36, "text": "image to image"}, {"st": 37, "ed": 40, "text": "problem and propose"}, {"st": 48, "ed": 50, "text": "deep convolutional"}, {"st": 51, "ed": 55, "text": "conditional generative adversarial networks"}, {"st": 79, "ed": 81, "text": "unsupervised learning"}, {"st": 110, "ed": 112, "text": "prior works"}]
[{"st": 1, "ed": 3, "text": "neural networks"}, {"st": 9, "ed": 11, "text": "deep learning"}, {"st": 15, "ed": 17, "text": "significantly lower"}, {"st": 35, "ed": 37, "text": "fine grained"}, {"st": 60, "ed": 62, "text": "recently proposed"}, {"st": 67, "ed": 71, "text": "field programmable gate array"}, {"st": 92, "ed": 94, "text": "fully connected"}, {"st": 98, "ed": 100, "text": "pooling layers"}, {"st": 220, "ed": 222, "text": "cifar 10"}]
[{"st": 32, "ed": 34, "text": "gaussian curvature"}, {"st": 37, "ed": 39, "text": "closely related"}, {"st": 43, "ed": 45, "text": "haar wavelet"}, {"st": 59, "ed": 61, "text": "object classification"}]
[{"st": 3, "ed": 5, "text": "complex systems"}, {"st": 48, "ed": 50, "text": "signal processing"}, {"st": 79, "ed": 81, "text": "signal processing"}]
[{"st": 11, "ed": 13, "text": "frame prediction"}, {"st": 68, "ed": 70, "text": "frame prediction"}, {"st": 89, "ed": 91, "text": "ground truth"}, {"st": 123, "ed": 125, "text": "proposed approach"}]
[{"st": 3, "ed": 7, "text": "deep convolutional neural networks"}, {"st": 8, "ed": 10, "text": "image classification"}, {"st": 11, "ed": 13, "text": "recognition tasks"}, {"st": 35, "ed": 37, "text": "neural network"}, {"st": 40, "ed": 42, "text": "multi class"}, {"st": 71, "ed": 73, "text": "imbalanced data"}, {"st": 80, "ed": 82, "text": "convolutional networks"}, {"st": 113, "ed": 115, "text": "low level"}, {"st": 141, "ed": 143, "text": "multi class"}, {"st": 162, "ed": 164, "text": "x ray"}, {"st": 166, "ed": 168, "text": "ground truth"}, {"st": 184, "ed": 186, "text": "loss function"}]
[{"st": 31, "ed": 34, "text": "theoretical and empirical"}, {"st": 88, "ed": 90, "text": "pooling layer"}, {"st": 110, "ed": 112, "text": "object categorization"}, {"st": 160, "ed": 162, "text": "object categorization"}, {"st": 164, "ed": 166, "text": "large scale"}]
[{"st": 4, "ed": 6, "text": "sensory data"}, {"st": 9, "ed": 11, "text": "statistical properties"}, {"st": 13, "ed": 15, "text": "real data"}, {"st": 23, "ed": 25, "text": "big data"}, {"st": 27, "ed": 29, "text": "synthetic data"}, {"st": 37, "ed": 39, "text": "real data"}, {"st": 74, "ed": 76, "text": "synthetic data"}, {"st": 80, "ed": 82, "text": "statistical properties"}, {"st": 153, "ed": 155, "text": "sensory data"}, {"st": 159, "ed": 161, "text": "deep learning"}, {"st": 174, "ed": 176, "text": "deep learning"}, {"st": 195, "ed": 200, "text": "long short term memory lstm"}, {"st": 210, "ed": 212, "text": "lstm network"}, {"st": 243, "ed": 245, "text": "deep learning"}]
[{"st": 7, "ed": 10, "text": "end to end"}, {"st": 12, "ed": 14, "text": "visual tracking"}, {"st": 21, "ed": 23, "text": "bounding box"}, {"st": 44, "ed": 47, "text": "sequential decision making"}, {"st": 53, "ed": 55, "text": "relevant information"}, {"st": 69, "ed": 72, "text": "convolutional neural network"}, {"st": 86, "ed": 88, "text": "reinforcement learning"}, {"st": 146, "ed": 148, "text": "neural network"}, {"st": 151, "ed": 154, "text": "convolutional and recurrent"}]
[{"st": 6, "ed": 9, "text": "convolutional neural network"}, {"st": 14, "ed": 16, "text": "optic disc"}]
[{"st": 5, "ed": 7, "text": "widely applied"}, {"st": 8, "ed": 11, "text": "approximate nearest neighbor"}, {"st": 13, "ed": 15, "text": "large scale"}, {"st": 25, "ed": 27, "text": "deep learning"}, {"st": 34, "ed": 37, "text": "end to end"}, {"st": 37, "ed": 39, "text": "representation learning"}, {"st": 43, "ed": 46, "text": "received increasing attention"}, {"st": 50, "ed": 52, "text": "ill posed"}, {"st": 61, "ed": 63, "text": "deep learning"}, {"st": 76, "ed": 78, "text": "hash codes"}, {"st": 97, "ed": 99, "text": "deep architecture"}, {"st": 100, "ed": 102, "text": "deep learning"}, {"st": 108, "ed": 110, "text": "convergence guarantees"}, {"st": 114, "ed": 116, "text": "hash codes"}, {"st": 121, "ed": 123, "text": "key idea"}, {"st": 127, "ed": 129, "text": "ill posed"}, {"st": 133, "ed": 135, "text": "deep networks"}, {"st": 154, "ed": 156, "text": "activation function"}, {"st": 175, "ed": 177, "text": "deep network"}, {"st": 183, "ed": 185, "text": "empirical evidence"}, {"st": 192, "ed": 194, "text": "hash codes"}, {"st": 201, "ed": 203, "text": "retrieval performance"}]
[{"st": 5, "ed": 7, "text": "super resolution"}, {"st": 19, "ed": 21, "text": "low resolution"}, {"st": 33, "ed": 35, "text": "super resolution"}, {"st": 61, "ed": 63, "text": "conditional distribution"}, {"st": 73, "ed": 75, "text": "image pixels"}, {"st": 78, "ed": 80, "text": "low resolution"}, {"st": 92, "ed": 94, "text": "natural images"}, {"st": 116, "ed": 118, "text": "photo realistic"}]
[{"st": 0, "ed": 3, "text": "principal component analysis"}, {"st": 11, "ed": 13, "text": "linear subspace"}, {"st": 45, "ed": 47, "text": "linear subspaces"}, {"st": 95, "ed": 97, "text": "principal components"}, {"st": 122, "ed": 124, "text": "efficient online"}, {"st": 132, "ed": 134, "text": "linear complexity"}, {"st": 154, "ed": 156, "text": "robust subspace"}, {"st": 158, "ed": 160, "text": "shows significant"}, {"st": 168, "ed": 170, "text": "recently published"}, {"st": 179, "ed": 181, "text": "competitive performance"}, {"st": 193, "ed": 195, "text": "real data"}, {"st": 202, "ed": 204, "text": "proposed method"}]
[{"st": 0, "ed": 2, "text": "deep learning"}, {"st": 17, "ed": 19, "text": "practical problems"}, {"st": 20, "ed": 22, "text": "imbalanced data"}, {"st": 52, "ed": 54, "text": "existing solutions"}, {"st": 76, "ed": 78, "text": "specifically designed"}, {"st": 131, "ed": 133, "text": "multi class"}, {"st": 136, "ed": 138, "text": "pre trained"}, {"st": 144, "ed": 146, "text": "multi class"}, {"st": 146, "ed": 148, "text": "neural network"}, {"st": 162, "ed": 164, "text": "unlabeled data"}, {"st": 172, "ed": 174, "text": "multi class"}]
[{"st": 101, "ed": 103, "text": "optimization techniques"}, {"st": 129, "ed": 131, "text": "real life"}]
[{"st": 91, "ed": 93, "text": "neural network"}, {"st": 106, "ed": 108, "text": "object relations"}, {"st": 136, "ed": 138, "text": "deep representations"}]
[{"st": 10, "ed": 12, "text": "colorectal cancer"}, {"st": 49, "ed": 51, "text": "convolutional networks"}]
[{"st": 7, "ed": 9, "text": "domain adaptation"}, {"st": 13, "ed": 17, "text": "supervised and semi supervised"}, {"st": 30, "ed": 32, "text": "domain invariant"}, {"st": 44, "ed": 46, "text": "jointly learn"}, {"st": 48, "ed": 50, "text": "auto encoders"}, {"st": 62, "ed": 64, "text": "domain invariant"}, {"st": 79, "ed": 82, "text": "maximum mean discrepancy"}, {"st": 84, "ed": 87, "text": "source and target"}, {"st": 107, "ed": 109, "text": "closed form"}, {"st": 144, "ed": 146, "text": "iterative procedure"}, {"st": 158, "ed": 160, "text": "regularization terms"}, {"st": 167, "ed": 169, "text": "domain adaptation"}, {"st": 180, "ed": 182, "text": "benchmark datasets"}, {"st": 186, "ed": 188, "text": "domain adaptation"}]
[{"st": 7, "ed": 9, "text": "learning framework"}, {"st": 47, "ed": 49, "text": "representation learning"}, {"st": 52, "ed": 54, "text": "deep features"}, {"st": 56, "ed": 59, "text": "end to end"}, {"st": 72, "ed": 74, "text": "learning algorithm"}, {"st": 77, "ed": 79, "text": "decision trees"}, {"st": 84, "ed": 86, "text": "decision trees"}, {"st": 109, "ed": 111, "text": "decision trees"}, {"st": 122, "ed": 124, "text": "loss function"}, {"st": 133, "ed": 135, "text": "learned jointly"}, {"st": 152, "ed": 154, "text": "loss function"}, {"st": 174, "ed": 176, "text": "computer vision"}, {"st": 178, "ed": 180, "text": "significant improvements"}]
[{"st": 0, "ed": 2, "text": "object recognition"}, {"st": 7, "ed": 9, "text": "computer vision"}, {"st": 18, "ed": 21, "text": "end to end"}, {"st": 21, "ed": 23, "text": "scene recognition"}, {"st": 26, "ed": 28, "text": "feature extraction"}, {"st": 37, "ed": 39, "text": "feature descriptors"}, {"st": 42, "ed": 44, "text": "local feature"}, {"st": 51, "ed": 53, "text": "feature descriptor"}, {"st": 69, "ed": 71, "text": "local feature"}, {"st": 94, "ed": 96, "text": "visual words"}, {"st": 101, "ed": 103, "text": "k means"}, {"st": 135, "ed": 137, "text": "multi class"}, {"st": 144, "ed": 146, "text": "cross validation"}, {"st": 177, "ed": 180, "text": "training and testing"}, {"st": 208, "ed": 210, "text": "open source"}]
[{"st": 7, "ed": 9, "text": "fundamental problems"}, {"st": 10, "ed": 13, "text": "deep convolutional networks"}, {"st": 32, "ed": 34, "text": "convolutional networks"}, {"st": 50, "ed": 52, "text": "linear transformations"}, {"st": 116, "ed": 118, "text": "fundamental problems"}, {"st": 119, "ed": 121, "text": "deep learning"}]
[{"st": 4, "ed": 6, "text": "gained increasing"}, {"st": 53, "ed": 55, "text": "practical problems"}, {"st": 62, "ed": 64, "text": "latent features"}, {"st": 72, "ed": 74, "text": "generative model"}, {"st": 98, "ed": 100, "text": "underlying structure"}, {"st": 148, "ed": 150, "text": "real world"}]
[{"st": 0, "ed": 3, "text": "deep residual networks"}, {"st": 12, "ed": 14, "text": "image processing"}, {"st": 73, "ed": 75, "text": "convolutional layers"}, {"st": 87, "ed": 89, "text": "multiple times"}, {"st": 153, "ed": 155, "text": "residual network"}, {"st": 166, "ed": 168, "text": "deep residual"}, {"st": 174, "ed": 176, "text": "convolutional layers"}]
[{"st": 2, "ed": 4, "text": "invariant representations"}, {"st": 5, "ed": 7, "text": "visual data"}, {"st": 14, "ed": 17, "text": "deep convolutional networks"}, {"st": 22, "ed": 25, "text": "image and video"}, {"st": 92, "ed": 94, "text": "pooling layers"}, {"st": 95, "ed": 97, "text": "deep networks"}, {"st": 104, "ed": 106, "text": "pooling layers"}, {"st": 143, "ed": 145, "text": "invariance properties"}]
[{"st": 38, "ed": 43, "text": "deep convolutional neural networks cnns"}, {"st": 133, "ed": 135, "text": "classification accuracy"}, {"st": 139, "ed": 143, "text": "signal to noise ratio"}]
[{"st": 50, "ed": 52, "text": "representation space"}, {"st": 80, "ed": 82, "text": "error rate"}]
[{"st": 6, "ed": 8, "text": "image generation"}, {"st": 17, "ed": 19, "text": "unlike previous"}, {"st": 19, "ed": 23, "text": "generative adversarial networks gans"}, {"st": 77, "ed": 80, "text": "end to end"}, {"st": 86, "ed": 88, "text": "experiments demonstrate"}, {"st": 94, "ed": 96, "text": "natural images"}]
[{"st": 9, "ed": 12, "text": "convolutional neural networks"}, {"st": 35, "ed": 37, "text": "hyper parameters"}, {"st": 49, "ed": 51, "text": "max pooling"}, {"st": 79, "ed": 81, "text": "deep network"}, {"st": 90, "ed": 92, "text": "point wise"}, {"st": 130, "ed": 132, "text": "nearest neighbor"}, {"st": 135, "ed": 137, "text": "deep features"}, {"st": 156, "ed": 158, "text": "support vectors"}]
[{"st": 10, "ed": 12, "text": "computer vision"}, {"st": 13, "ed": 15, "text": "pattern recognition"}, {"st": 35, "ed": 37, "text": "similarity metric"}, {"st": 44, "ed": 46, "text": "highly dependent"}, {"st": 58, "ed": 60, "text": "metric learning"}, {"st": 71, "ed": 74, "text": "convolutional neural networks"}, {"st": 78, "ed": 80, "text": "spectral graph"}, {"st": 123, "ed": 125, "text": "similarity function"}, {"st": 151, "ed": 153, "text": "similarity metric"}, {"st": 164, "ed": 166, "text": "k nn"}]
[{"st": 0, "ed": 3, "text": "generative adversarial nets"}, {"st": 8, "ed": 10, "text": "image generation"}, {"st": 11, "ed": 14, "text": "semi supervised learning"}, {"st": 93, "ed": 95, "text": "generative adversarial"}, {"st": 117, "ed": 119, "text": "conditional distributions"}, {"st": 178, "ed": 180, "text": "classification results"}, {"st": 181, "ed": 184, "text": "deep generative models"}, {"st": 205, "ed": 207, "text": "latent space"}]
[{"st": 76, "ed": 78, "text": "multi dimensional"}, {"st": 79, "ed": 81, "text": "time series"}, {"st": 136, "ed": 138, "text": "success rate"}, {"st": 148, "ed": 150, "text": "success rate"}]
[{"st": 22, "ed": 24, "text": "social media"}, {"st": 37, "ed": 42, "text": "computer vision and machine learning"}, {"st": 56, "ed": 58, "text": "content analysis"}, {"st": 58, "ed": 60, "text": "face recognition"}, {"st": 61, "ed": 63, "text": "object detection"}, {"st": 127, "ed": 129, "text": "deep learning"}, {"st": 143, "ed": 145, "text": "machine learning"}, {"st": 157, "ed": 159, "text": "transfer learning"}, {"st": 168, "ed": 170, "text": "deep learning"}, {"st": 172, "ed": 174, "text": "face recognition"}, {"st": 178, "ed": 180, "text": "classification techniques"}, {"st": 189, "ed": 191, "text": "highly accurate"}, {"st": 191, "ed": 193, "text": "classification results"}]
[{"st": 0, "ed": 5, "text": "deep convolutional neural network cnn"}, {"st": 30, "ed": 32, "text": "low precision"}, {"st": 32, "ed": 34, "text": "fixed point"}, {"st": 49, "ed": 51, "text": "fixed point"}, {"st": 71, "ed": 73, "text": "floating point"}, {"st": 79, "ed": 81, "text": "fixed point"}, {"st": 90, "ed": 92, "text": "floating point"}, {"st": 99, "ed": 101, "text": "fixed point"}, {"st": 112, "ed": 114, "text": "large scale"}, {"st": 156, "ed": 158, "text": "power consumption"}]
[{"st": 41, "ed": 44, "text": "convolutional neural network"}, {"st": 62, "ed": 64, "text": "feature extraction"}, {"st": 71, "ed": 73, "text": "parameter estimation"}, {"st": 84, "ed": 86, "text": "network parameters"}, {"st": 97, "ed": 99, "text": "manual annotation"}, {"st": 106, "ed": 108, "text": "generalization capabilities"}, {"st": 123, "ed": 125, "text": "instance level"}]
[{"st": 50, "ed": 52, "text": "big data"}, {"st": 66, "ed": 68, "text": "extracting information"}, {"st": 74, "ed": 76, "text": "time consuming"}, {"st": 88, "ed": 90, "text": "deep learning"}, {"st": 99, "ed": 103, "text": "deep convolutional neural networks"}, {"st": 122, "ed": 125, "text": "deep neural networks"}, {"st": 125, "ed": 127, "text": "automatically identify"}, {"st": 217, "ed": 220, "text": "deep neural networks"}]
[{"st": 55, "ed": 57, "text": "unsupervised learning"}, {"st": 61, "ed": 63, "text": "imaging data"}, {"st": 71, "ed": 73, "text": "deep convolutional"}, {"st": 73, "ed": 76, "text": "generative adversarial network"}, {"st": 95, "ed": 97, "text": "image space"}, {"st": 111, "ed": 113, "text": "image patches"}, {"st": 122, "ed": 125, "text": "optical coherence tomography"}]
[{"st": 8, "ed": 11, "text": "efficient and accurate"}, {"st": 46, "ed": 48, "text": "detection algorithms"}, {"st": 59, "ed": 62, "text": "convolutional neural network"}, {"st": 66, "ed": 68, "text": "image segmentation"}]
[{"st": 4, "ed": 6, "text": "active learning"}, {"st": 25, "ed": 27, "text": "active learning"}, {"st": 55, "ed": 57, "text": "deep generative"}, {"st": 70, "ed": 72, "text": "decision boundary"}, {"st": 95, "ed": 97, "text": "active learning"}, {"st": 107, "ed": 109, "text": "decision boundary"}, {"st": 119, "ed": 121, "text": "decision boundary"}]
[{"st": 7, "ed": 10, "text": "deep neural network"}, {"st": 19, "ed": 22, "text": "2d and 3d"}, {"st": 22, "ed": 24, "text": "face alignment"}, {"st": 43, "ed": 45, "text": "strong baseline"}, {"st": 74, "ed": 76, "text": "facial landmark"}, {"st": 85, "ed": 87, "text": "facial landmark"}, {"st": 122, "ed": 124, "text": "facial landmark"}, {"st": 136, "ed": 138, "text": "neural network"}, {"st": 140, "ed": 142, "text": "face alignment"}, {"st": 163, "ed": 165, "text": "face alignment"}, {"st": 188, "ed": 191, "text": "2d and 3d"}, {"st": 191, "ed": 193, "text": "face alignment"}, {"st": 208, "ed": 211, "text": "training and testing"}, {"st": 223, "ed": 225, "text": "face alignment"}]
[{"st": 30, "ed": 32, "text": "autonomous driving"}, {"st": 35, "ed": 37, "text": "visual scene"}, {"st": 106, "ed": 109, "text": "convolutional neural network"}, {"st": 139, "ed": 141, "text": "prediction results"}]
[{"st": 2, "ed": 4, "text": "clustering methods"}, {"st": 9, "ed": 11, "text": "feature learning"}, {"st": 15, "ed": 17, "text": "recent research"}, {"st": 26, "ed": 28, "text": "unified framework"}, {"st": 43, "ed": 45, "text": "fully convolutional"}, {"st": 45, "ed": 47, "text": "auto encoders"}, {"st": 49, "ed": 51, "text": "feature learning"}, {"st": 60, "ed": 62, "text": "image representations"}, {"st": 63, "ed": 65, "text": "cluster centers"}, {"st": 69, "ed": 71, "text": "fully convolutional"}, {"st": 71, "ed": 73, "text": "auto encoder"}, {"st": 75, "ed": 77, "text": "k means"}, {"st": 90, "ed": 92, "text": "auto encoder"}, {"st": 142, "ed": 144, "text": "benchmark datasets"}]
[{"st": 0, "ed": 4, "text": "deep convolutional neural networks"}, {"st": 35, "ed": 38, "text": "convolutional neural networks"}, {"st": 53, "ed": 55, "text": "convolutional networks"}, {"st": 67, "ed": 69, "text": "convolutional layers"}, {"st": 72, "ed": 74, "text": "convolutional layers"}, {"st": 88, "ed": 90, "text": "classification performance"}, {"st": 133, "ed": 135, "text": "convolutional networks"}]
[{"st": 20, "ed": 23, "text": "each data point"}, {"st": 81, "ed": 83, "text": "valuable information"}, {"st": 152, "ed": 155, "text": "computer aided diagnosis"}, {"st": 192, "ed": 194, "text": "real world"}]
[{"st": 5, "ed": 7, "text": "multiple views"}, {"st": 67, "ed": 69, "text": "machine learning"}, {"st": 72, "ed": 75, "text": "convolutional neural networks"}, {"st": 86, "ed": 88, "text": "image patches"}, {"st": 96, "ed": 98, "text": "multi view"}]
[{"st": 41, "ed": 43, "text": "pre defined"}, {"st": 56, "ed": 58, "text": "x 1"}, {"st": 104, "ed": 106, "text": "top 5"}, {"st": 131, "ed": 133, "text": "excellent performance"}, {"st": 135, "ed": 137, "text": "small sample"}, {"st": 140, "ed": 143, "text": "end to end"}, {"st": 158, "ed": 160, "text": "cifar 10"}]
[{"st": 14, "ed": 16, "text": "super resolution"}, {"st": 20, "ed": 22, "text": "spatial resolution"}, {"st": 24, "ed": 26, "text": "input image"}, {"st": 41, "ed": 43, "text": "spatial resolution"}, {"st": 61, "ed": 63, "text": "natural images"}, {"st": 79, "ed": 81, "text": "low resolution"}, {"st": 94, "ed": 96, "text": "lighting conditions"}, {"st": 127, "ed": 129, "text": "super resolution"}, {"st": 150, "ed": 152, "text": "natural image"}, {"st": 176, "ed": 180, "text": "convolutional neural network cnn"}, {"st": 187, "ed": 190, "text": "end to end"}, {"st": 194, "ed": 196, "text": "rgb image"}, {"st": 207, "ed": 209, "text": "super resolution"}]
[{"st": 13, "ed": 15, "text": "assisted living"}, {"st": 49, "ed": 52, "text": "recurrent neural networks"}, {"st": 53, "ed": 56, "text": "convolutional neural networks"}, {"st": 61, "ed": 64, "text": "gated recurrent unit"}, {"st": 65, "ed": 67, "text": "neural networks"}]
[{"st": 8, "ed": 10, "text": "image classification"}, {"st": 59, "ed": 61, "text": "relevant information"}, {"st": 222, "ed": 224, "text": "frame rate"}]
[{"st": 0, "ed": 2, "text": "deep neural"}, {"st": 92, "ed": 94, "text": "visual attention"}, {"st": 100, "ed": 103, "text": "end to end"}, {"st": 109, "ed": 111, "text": "attention model"}, {"st": 112, "ed": 114, "text": "image regions"}, {"st": 190, "ed": 193, "text": "end to end"}]
[{"st": 9, "ed": 11, "text": "current methods"}, {"st": 27, "ed": 29, "text": "deep networks"}, {"st": 57, "ed": 59, "text": "type ii"}, {"st": 140, "ed": 142, "text": "adversarial examples"}]
[{"st": 25, "ed": 27, "text": "geometric structure"}, {"st": 40, "ed": 42, "text": "metric tensor"}, {"st": 44, "ed": 46, "text": "fisher information"}, {"st": 56, "ed": 58, "text": "geometric structure"}, {"st": 62, "ed": 64, "text": "dissimilarity measure"}]
[{"st": 10, "ed": 12, "text": "deep architectures"}, {"st": 14, "ed": 17, "text": "end to end"}, {"st": 26, "ed": 28, "text": "continuous relaxation"}, {"st": 49, "ed": 51, "text": "image compression"}, {"st": 52, "ed": 54, "text": "neural network"}]
[{"st": 12, "ed": 15, "text": "accuracy and speed"}, {"st": 35, "ed": 37, "text": "deep model"}, {"st": 58, "ed": 60, "text": "feed forward"}, {"st": 80, "ed": 82, "text": "proposed method"}, {"st": 121, "ed": 124, "text": "training and testing"}, {"st": 125, "ed": 127, "text": "deep network"}, {"st": 143, "ed": 146, "text": "end to end"}, {"st": 149, "ed": 151, "text": "joint learning"}, {"st": 160, "ed": 162, "text": "pascal voc"}]
[{"st": 0, "ed": 2, "text": "breast cancer"}, {"st": 21, "ed": 23, "text": "image segmentation"}, {"st": 61, "ed": 63, "text": "image segmentation"}, {"st": 99, "ed": 101, "text": "previous approaches"}]
[{"st": 1, "ed": 3, "text": "hash function"}, {"st": 3, "ed": 5, "text": "learning algorithms"}, {"st": 6, "ed": 9, "text": "achieved great success"}, {"st": 10, "ed": 12, "text": "recent years"}, {"st": 17, "ed": 19, "text": "off line"}, {"st": 52, "ed": 54, "text": "loss function"}, {"st": 84, "ed": 86, "text": "theoretical analysis"}, {"st": 150, "ed": 152, "text": "extensive experiments"}, {"st": 154, "ed": 156, "text": "large scale"}]
[{"st": 11, "ed": 13, "text": "time series"}, {"st": 35, "ed": 37, "text": "open question"}, {"st": 39, "ed": 41, "text": "remote sensing"}, {"st": 43, "ed": 45, "text": "deep learning"}, {"st": 51, "ed": 53, "text": "remote sensing"}, {"st": 56, "ed": 58, "text": "scene classification"}, {"st": 59, "ed": 63, "text": "convolutional neural networks cnns"}, {"st": 74, "ed": 76, "text": "deep learning"}, {"st": 78, "ed": 82, "text": "recurrent neural networks rnns"}, {"st": 85, "ed": 87, "text": "remote sensing"}, {"st": 97, "ed": 100, "text": "recurrent neural networks"}, {"st": 104, "ed": 107, "text": "short term memory"}, {"st": 111, "ed": 113, "text": "land cover"}, {"st": 122, "ed": 124, "text": "time series"}, {"st": 148, "ed": 151, "text": "recurrent neural networks"}, {"st": 182, "ed": 184, "text": "feature representation"}]
[{"st": 8, "ed": 10, "text": "image processing"}, {"st": 18, "ed": 20, "text": "sparse representation"}, {"st": 68, "ed": 70, "text": "image data"}, {"st": 89, "ed": 91, "text": "dictionary learning"}, {"st": 94, "ed": 96, "text": "image representations"}, {"st": 110, "ed": 112, "text": "multi dimensional"}, {"st": 138, "ed": 140, "text": "sparse representation"}, {"st": 152, "ed": 154, "text": "spatial frequency"}, {"st": 224, "ed": 226, "text": "image representations"}, {"st": 236, "ed": 238, "text": "promising results"}, {"st": 239, "ed": 241, "text": "image denoising"}]
[{"st": 3, "ed": 6, "text": "fast and accurate"}, {"st": 6, "ed": 8, "text": "image reconstruction"}, {"st": 38, "ed": 40, "text": "highly efficient"}, {"st": 49, "ed": 51, "text": "image reconstruction"}, {"st": 55, "ed": 60, "text": "deep convolutional neural network cnn"}, {"st": 68, "ed": 70, "text": "image reconstruction"}, {"st": 101, "ed": 103, "text": "u net"}, {"st": 109, "ed": 111, "text": "image reconstruction"}, {"st": 112, "ed": 114, "text": "deep learning"}, {"st": 135, "ed": 137, "text": "iterative reconstruction"}, {"st": 152, "ed": 154, "text": "numerical results"}, {"st": 158, "ed": 160, "text": "deep learning"}]
[{"st": 1, "ed": 4, "text": "generative adversarial networks"}, {"st": 25, "ed": 27, "text": "generated data"}, {"st": 42, "ed": 44, "text": "wasserstein distance"}, {"st": 61, "ed": 63, "text": "mode collapse"}, {"st": 89, "ed": 91, "text": "margin based"}, {"st": 119, "ed": 121, "text": "maximum margin"}, {"st": 121, "ed": 123, "text": "ranking loss"}, {"st": 163, "ed": 165, "text": "generated data"}, {"st": 205, "ed": 207, "text": "cifar 10"}]
[{"st": 2, "ed": 4, "text": "recently introduced"}, {"st": 114, "ed": 116, "text": "extensive experiments"}, {"st": 121, "ed": 123, "text": "natural images"}, {"st": 135, "ed": 137, "text": "imagenet dataset"}, {"st": 191, "ed": 193, "text": "input images"}, {"st": 225, "ed": 227, "text": "image analysis"}]
[{"st": 0, "ed": 3, "text": "stochastic gradient descent"}, {"st": 6, "ed": 8, "text": "successfully applied"}, {"st": 9, "ed": 12, "text": "support vector machines"}, {"st": 21, "ed": 24, "text": "stochastic gradient descent"}, {"st": 29, "ed": 32, "text": "support vector machines"}, {"st": 42, "ed": 45, "text": "support vector machines"}, {"st": 52, "ed": 55, "text": "stochastic gradient descent"}, {"st": 80, "ed": 83, "text": "support vector machines"}, {"st": 119, "ed": 121, "text": "large scale"}]
[{"st": 7, "ed": 9, "text": "input image"}, {"st": 73, "ed": 76, "text": "unsupervised domain adaptation"}, {"st": 77, "ed": 79, "text": "cross domain"}, {"st": 83, "ed": 85, "text": "generalization bound"}]
[{"st": 4, "ed": 7, "text": "end to end"}, {"st": 7, "ed": 9, "text": "deep learning"}, {"st": 22, "ed": 24, "text": "passive radar"}, {"st": 64, "ed": 66, "text": "source data"}, {"st": 80, "ed": 83, "text": "convolutional neural network"}, {"st": 87, "ed": 89, "text": "time series"}, {"st": 102, "ed": 104, "text": "random forest"}, {"st": 137, "ed": 139, "text": "deep learning"}, {"st": 145, "ed": 147, "text": "great potential"}, {"st": 148, "ed": 151, "text": "end to end"}]
[{"st": 23, "ed": 25, "text": "object tracking"}, {"st": 46, "ed": 48, "text": "previous works"}, {"st": 86, "ed": 88, "text": "closed form"}, {"st": 101, "ed": 103, "text": "deep features"}]
[{"st": 8, "ed": 10, "text": "based method"}, {"st": 15, "ed": 17, "text": "brain tumor"}, {"st": 23, "ed": 25, "text": "learned features"}, {"st": 26, "ed": 28, "text": "fully convolutional"}, {"st": 28, "ed": 30, "text": "neural network"}, {"st": 32, "ed": 34, "text": "hand designed"}, {"st": 56, "ed": 58, "text": "feature map"}, {"st": 71, "ed": 73, "text": "learned features"}, {"st": 112, "ed": 114, "text": "random forest"}, {"st": 121, "ed": 123, "text": "learned features"}, {"st": 127, "ed": 129, "text": "hand designed"}, {"st": 142, "ed": 144, "text": "brain tumor"}, {"st": 146, "ed": 148, "text": "ground truth"}]
[{"st": 12, "ed": 14, "text": "lung cancer"}, {"st": 41, "ed": 45, "text": "convolutional neural network cnn"}, {"st": 61, "ed": 63, "text": "ct scan"}, {"st": 72, "ed": 74, "text": "cnn based"}, {"st": 86, "ed": 88, "text": "training data"}, {"st": 93, "ed": 95, "text": "transfer learning"}, {"st": 107, "ed": 109, "text": "feature representation"}, {"st": 118, "ed": 120, "text": "complementary information"}, {"st": 122, "ed": 125, "text": "multi task learning"}, {"st": 152, "ed": 154, "text": "proposed approach"}]
[{"st": 11, "ed": 13, "text": "gravitational wave"}, {"st": 19, "ed": 21, "text": "gravitational wave"}, {"st": 31, "ed": 33, "text": "multi view"}, {"st": 33, "ed": 36, "text": "convolutional neural network"}, {"st": 77, "ed": 79, "text": "image classification"}, {"st": 88, "ed": 90, "text": "multi view"}, {"st": 90, "ed": 93, "text": "deep neural network"}]
[{"st": 70, "ed": 72, "text": "multi layer"}, {"st": 110, "ed": 112, "text": "pre processing"}, {"st": 113, "ed": 115, "text": "natural images"}, {"st": 149, "ed": 151, "text": "super resolution"}, {"st": 162, "ed": 164, "text": "low resolution"}, {"st": 180, "ed": 182, "text": "training set"}]
[{"st": 6, "ed": 11, "text": "convolutional neural network cnn architecture"}, {"st": 19, "ed": 21, "text": "deep cnn"}, {"st": 21, "ed": 23, "text": "based methods"}, {"st": 24, "ed": 26, "text": "method achieves"}, {"st": 31, "ed": 33, "text": "training examples"}, {"st": 67, "ed": 69, "text": "method outperforms"}]
[{"st": 0, "ed": 2, "text": "multi class"}, {"st": 2, "ed": 4, "text": "supervised learning"}, {"st": 38, "ed": 40, "text": "incremental learning"}, {"st": 86, "ed": 88, "text": "incremental learning"}, {"st": 106, "ed": 108, "text": "generative models"}, {"st": 139, "ed": 141, "text": "catastrophic forgetting"}, {"st": 149, "ed": 152, "text": "deep neural networks"}, {"st": 167, "ed": 169, "text": "multi class"}, {"st": 169, "ed": 171, "text": "incremental learning"}, {"st": 177, "ed": 179, "text": "benchmark datasets"}, {"st": 187, "ed": 189, "text": "incremental learning"}, {"st": 203, "ed": 205, "text": "cross domain"}, {"st": 221, "ed": 223, "text": "continual learning"}]
[{"st": 22, "ed": 24, "text": "pre processing"}, {"st": 28, "ed": 30, "text": "false positive"}, {"st": 54, "ed": 56, "text": "convolutional networks"}, {"st": 77, "ed": 79, "text": "pre processing"}, {"st": 85, "ed": 87, "text": "feature extraction"}]
[{"st": 10, "ed": 12, "text": "machine learning"}, {"st": 22, "ed": 24, "text": "pre trained"}, {"st": 24, "ed": 26, "text": "deep convolutional"}, {"st": 31, "ed": 33, "text": "pre trained"}, {"st": 35, "ed": 37, "text": "feature extractors"}, {"st": 42, "ed": 44, "text": "convolutional layers"}, {"st": 69, "ed": 71, "text": "method named"}, {"st": 100, "ed": 102, "text": "empirical studies"}, {"st": 117, "ed": 119, "text": "consistently outperforms"}, {"st": 134, "ed": 136, "text": "generalization ability"}]
[{"st": 0, "ed": 3, "text": "the paper presents"}, {"st": 25, "ed": 27, "text": "pre processing"}, {"st": 49, "ed": 51, "text": "multi class"}, {"st": 51, "ed": 53, "text": "object recognition"}]
[{"st": 3, "ed": 6, "text": "trained neural network"}, {"st": 21, "ed": 23, "text": "existing approaches"}, {"st": 26, "ed": 28, "text": "optimal solutions"}, {"st": 29, "ed": 31, "text": "joint training"}, {"st": 55, "ed": 57, "text": "method called"}, {"st": 69, "ed": 71, "text": "linear combinations"}, {"st": 89, "ed": 91, "text": "network architecture"}, {"st": 99, "ed": 101, "text": "fine tuning"}, {"st": 121, "ed": 123, "text": "network quantization"}, {"st": 153, "ed": 155, "text": "learned representations"}, {"st": 168, "ed": 171, "text": "conduct extensive experiments"}, {"st": 181, "ed": 183, "text": "image classification"}]
[{"st": 21, "ed": 23, "text": "optical flow"}, {"st": 30, "ed": 32, "text": "inference tasks"}, {"st": 34, "ed": 36, "text": "machine learning"}, {"st": 58, "ed": 60, "text": "neural network"}, {"st": 79, "ed": 81, "text": "non negativity"}, {"st": 99, "ed": 101, "text": "neural networks"}]
[{"st": 17, "ed": 19, "text": "optimization problem"}, {"st": 23, "ed": 25, "text": "low level"}, {"st": 37, "ed": 39, "text": "weakly supervised"}, {"st": 43, "ed": 46, "text": "multi task learning"}, {"st": 53, "ed": 55, "text": "shared representation"}, {"st": 56, "ed": 58, "text": "proposed framework"}, {"st": 73, "ed": 75, "text": "bounding box"}, {"st": 112, "ed": 114, "text": "unified framework"}, {"st": 115, "ed": 117, "text": "strong baseline"}, {"st": 126, "ed": 128, "text": "large margin"}, {"st": 135, "ed": 137, "text": "competitive results"}, {"st": 148, "ed": 150, "text": "benchmark datasets"}]
[{"st": 1, "ed": 5, "text": "deep convolutional neural network"}, {"st": 8, "ed": 10, "text": "remarkable success"}, {"st": 17, "ed": 19, "text": "natural image"}, {"st": 23, "ed": 25, "text": "natural image"}, {"st": 28, "ed": 30, "text": "remote sensing"}, {"st": 48, "ed": 50, "text": "remote sensing"}, {"st": 67, "ed": 69, "text": "remote sensing"}, {"st": 69, "ed": 71, "text": "scene recognition"}, {"st": 78, "ed": 80, "text": "object recognition"}, {"st": 86, "ed": 88, "text": "remote sensing"}, {"st": 88, "ed": 90, "text": "scene understanding"}, {"st": 106, "ed": 108, "text": "remote sensing"}, {"st": 108, "ed": 110, "text": "scene understanding"}, {"st": 136, "ed": 138, "text": "visualization method"}, {"st": 139, "ed": 142, "text": "qualitatively and quantitatively"}, {"st": 149, "ed": 151, "text": "remote sensing"}, {"st": 157, "ed": 159, "text": "multi objective"}]
[{"st": 0, "ed": 3, "text": "variational auto encoder"}, {"st": 7, "ed": 9, "text": "unsupervised learning"}, {"st": 47, "ed": 49, "text": "decoder network"}, {"st": 54, "ed": 56, "text": "skip connections"}, {"st": 75, "ed": 77, "text": "multi stage"}, {"st": 88, "ed": 90, "text": "multi stage"}, {"st": 129, "ed": 131, "text": "loss functions"}, {"st": 140, "ed": 142, "text": "proposed framework"}, {"st": 152, "ed": 154, "text": "experiment results"}, {"st": 164, "ed": 166, "text": "multi stage"}]
[{"st": 32, "ed": 34, "text": "low resolution"}, {"st": 48, "ed": 50, "text": "low resolution"}, {"st": 60, "ed": 62, "text": "low resolution"}, {"st": 89, "ed": 91, "text": "approach produces"}, {"st": 97, "ed": 99, "text": "existing methods"}, {"st": 107, "ed": 109, "text": "turing test"}]
[{"st": 4, "ed": 6, "text": "neural network"}, {"st": 23, "ed": 25, "text": "feed forward"}, {"st": 62, "ed": 65, "text": "training and testing"}, {"st": 76, "ed": 78, "text": "benchmark datasets"}, {"st": 78, "ed": 81, "text": "mnist cifar 10"}, {"st": 81, "ed": 83, "text": "cifar 100"}, {"st": 96, "ed": 98, "text": "local minimum"}]
[{"st": 9, "ed": 11, "text": "deep learning"}, {"st": 42, "ed": 44, "text": "residual networks"}, {"st": 53, "ed": 55, "text": "published results"}, {"st": 56, "ed": 61, "text": "cifar 10 and cifar 100"}, {"st": 74, "ed": 76, "text": "skip connections"}, {"st": 77, "ed": 79, "text": "batch normalization"}, {"st": 80, "ed": 82, "text": "encouraging results"}, {"st": 94, "ed": 98, "text": "available at https github.com"}]
[{"st": 27, "ed": 29, "text": "similar performance"}, {"st": 31, "ed": 33, "text": "small scale"}, {"st": 56, "ed": 59, "text": "orders of magnitude"}, {"st": 106, "ed": 108, "text": "benchmark datasets"}, {"st": 109, "ed": 111, "text": "pre training"}, {"st": 138, "ed": 140, "text": "image classification"}, {"st": 151, "ed": 153, "text": "spatio temporal"}, {"st": 153, "ed": 155, "text": "feature extractors"}, {"st": 171, "ed": 173, "text": "pre training"}]
[{"st": 1, "ed": 4, "text": "generative adversarial networks"}, {"st": 17, "ed": 19, "text": "lower dimensional"}, {"st": 30, "ed": 32, "text": "generated samples"}, {"st": 58, "ed": 60, "text": "low dimensional"}, {"st": 114, "ed": 116, "text": "higher quality"}]
[{"st": 50, "ed": 52, "text": "audio visual"}, {"st": 53, "ed": 55, "text": "learning task"}, {"st": 69, "ed": 71, "text": "additional supervision"}, {"st": 107, "ed": 109, "text": "classification benchmarks"}, {"st": 120, "ed": 122, "text": "supervised approaches"}, {"st": 143, "ed": 145, "text": "fine grained"}]
[{"st": 1, "ed": 3, "text": "structure learning"}, {"st": 6, "ed": 8, "text": "time series"}, {"st": 23, "ed": 25, "text": "time series"}, {"st": 34, "ed": 36, "text": "recent advances"}, {"st": 44, "ed": 47, "text": "recurrent neural networks"}, {"st": 52, "ed": 55, "text": "hidden markov models"}, {"st": 67, "ed": 69, "text": "sequential data"}, {"st": 93, "ed": 95, "text": "neural network"}, {"st": 103, "ed": 106, "text": "recurrent neural networks"}, {"st": 121, "ed": 123, "text": "efficient inference"}, {"st": 126, "ed": 128, "text": "bi directional"}, {"st": 128, "ed": 130, "text": "inference network"}, {"st": 141, "ed": 143, "text": "gumbel softmax"}, {"st": 161, "ed": 163, "text": "tasks including"}, {"st": 165, "ed": 167, "text": "automatic segmentation"}, {"st": 187, "ed": 189, "text": "significant improvement"}]
[{"st": 32, "ed": 34, "text": "image classification"}, {"st": 37, "ed": 40, "text": "convolutional neural network"}, {"st": 49, "ed": 52, "text": "deep neural network"}, {"st": 59, "ed": 61, "text": "classification models"}, {"st": 68, "ed": 70, "text": "dental plaque"}, {"st": 77, "ed": 79, "text": "multi channel"}, {"st": 85, "ed": 87, "text": "improved performance"}]
[{"st": 7, "ed": 10, "text": "attracted much attention"}, {"st": 12, "ed": 14, "text": "machine learning"}, {"st": 30, "ed": 33, "text": "taking advantage of"}, {"st": 58, "ed": 60, "text": "training samples"}, {"st": 69, "ed": 71, "text": "auto encoder"}, {"st": 103, "ed": 105, "text": "feature space"}, {"st": 114, "ed": 116, "text": "method called"}, {"st": 117, "ed": 119, "text": "auto encoders"}, {"st": 179, "ed": 181, "text": "real world"}, {"st": 184, "ed": 186, "text": "proposed method"}]
[{"st": 1, "ed": 3, "text": "recent works"}, {"st": 5, "ed": 8, "text": "deep neural networks"}, {"st": 14, "ed": 16, "text": "adversarial attacks"}, {"st": 31, "ed": 33, "text": "dnn training"}, {"st": 36, "ed": 38, "text": "adversarial examples"}, {"st": 50, "ed": 52, "text": "adversarial attacks"}, {"st": 66, "ed": 68, "text": "adversarial examples"}, {"st": 85, "ed": 87, "text": "adversarial training"}, {"st": 92, "ed": 94, "text": "adversarial training"}, {"st": 135, "ed": 137, "text": "deep learning"}, {"st": 139, "ed": 141, "text": "adversarial attacks"}, {"st": 142, "ed": 145, "text": "mnist cifar 10"}, {"st": 145, "ed": 147, "text": "cifar 100"}]
[{"st": 0, "ed": 2, "text": "sparse coding"}, {"st": 43, "ed": 45, "text": "total number"}, {"st": 55, "ed": 57, "text": "sparse recovery"}, {"st": 87, "ed": 89, "text": "sparse representation"}, {"st": 93, "ed": 95, "text": "image patches"}, {"st": 121, "ed": 125, "text": "synthetic and real data"}, {"st": 129, "ed": 131, "text": "proposed framework"}, {"st": 133, "ed": 135, "text": "image representation"}]
[{"st": 5, "ed": 7, "text": "image fusion"}, {"st": 48, "ed": 50, "text": "based approaches"}, {"st": 56, "ed": 58, "text": "feature space"}, {"st": 77, "ed": 79, "text": "image fusion"}, {"st": 96, "ed": 98, "text": "training set"}, {"st": 148, "ed": 150, "text": "jointly learning"}, {"st": 158, "ed": 160, "text": "sparse representations"}, {"st": 165, "ed": 167, "text": "feature spaces"}, {"st": 196, "ed": 198, "text": "sparse representation"}, {"st": 208, "ed": 210, "text": "extensive experimental"}, {"st": 218, "ed": 220, "text": "image fusion"}]
[{"st": 5, "ed": 7, "text": "challenging task"}, {"st": 37, "ed": 39, "text": "promising results"}, {"st": 40, "ed": 42, "text": "prediction accuracy"}, {"st": 85, "ed": 87, "text": "high level"}, {"st": 93, "ed": 97, "text": "deep convolutional neural network"}, {"st": 105, "ed": 107, "text": "spatio temporal"}, {"st": 110, "ed": 112, "text": "deep features"}, {"st": 130, "ed": 132, "text": "predictive model"}, {"st": 144, "ed": 146, "text": "patient data"}, {"st": 150, "ed": 152, "text": "spatio temporal"}, {"st": 159, "ed": 161, "text": "imaging data"}, {"st": 175, "ed": 177, "text": "method achieves"}, {"st": 212, "ed": 214, "text": "model based"}]
[{"st": 2, "ed": 7, "text": "deep convolutional neural network cnn"}, {"st": 18, "ed": 21, "text": "labeled training data"}, {"st": 36, "ed": 38, "text": "fine tune"}, {"st": 43, "ed": 45, "text": "pre trained"}, {"st": 86, "ed": 88, "text": "image analysis"}, {"st": 93, "ed": 95, "text": "pre trained"}, {"st": 95, "ed": 97, "text": "deep cnns"}, {"st": 99, "ed": 101, "text": "fine tuning"}, {"st": 107, "ed": 109, "text": "deep cnn"}, {"st": 131, "ed": 134, "text": "detection and segmentation"}, {"st": 145, "ed": 147, "text": "deep cnns"}, {"st": 147, "ed": 150, "text": "trained from scratch"}, {"st": 153, "ed": 155, "text": "pre trained"}, {"st": 156, "ed": 158, "text": "fine tuned"}, {"st": 160, "ed": 162, "text": "layer wise"}, {"st": 173, "ed": 176, "text": "pre trained cnn"}, {"st": 178, "ed": 180, "text": "fine tuning"}, {"st": 184, "ed": 186, "text": "worst case"}, {"st": 192, "ed": 195, "text": "trained from scratch"}, {"st": 196, "ed": 198, "text": "fine tuned"}, {"st": 206, "ed": 208, "text": "training sets"}, {"st": 210, "ed": 213, "text": "trained from scratch"}, {"st": 231, "ed": 233, "text": "layer wise"}, {"st": 233, "ed": 235, "text": "fine tuning"}]
[{"st": 0, "ed": 2, "text": "cardiovascular disease"}, {"st": 118, "ed": 120, "text": "time consuming"}, {"st": 149, "ed": 151, "text": "extensive experiments"}, {"st": 156, "ed": 158, "text": "significantly outperforms"}, {"st": 171, "ed": 173, "text": "unified framework"}, {"st": 175, "ed": 179, "text": "convolutional neural networks cnns"}, {"st": 183, "ed": 185, "text": "image representation"}, {"st": 187, "ed": 189, "text": "post processing"}]
[{"st": 2, "ed": 4, "text": "deep learning"}, {"st": 10, "ed": 12, "text": "prediction accuracy"}, {"st": 31, "ed": 33, "text": "real world"}, {"st": 35, "ed": 37, "text": "recent advances"}, {"st": 38, "ed": 40, "text": "deep learning"}, {"st": 85, "ed": 87, "text": "multi class"}, {"st": 93, "ed": 95, "text": "don t"}, {"st": 108, "ed": 110, "text": "pre trained"}, {"st": 124, "ed": 126, "text": "based methods"}, {"st": 151, "ed": 153, "text": "real world"}, {"st": 171, "ed": 173, "text": "image classification"}, {"st": 190, "ed": 192, "text": "prediction task"}, {"st": 195, "ed": 197, "text": "large scale"}, {"st": 197, "ed": 199, "text": "autonomous driving"}]
[{"st": 2, "ed": 4, "text": "real world"}, {"st": 5, "ed": 7, "text": "labeled data"}, {"st": 10, "ed": 12, "text": "machine learning"}, {"st": 17, "ed": 19, "text": "semi supervised"}, {"st": 26, "ed": 28, "text": "unlabeled data"}, {"st": 41, "ed": 43, "text": "semi supervised"}, {"st": 45, "ed": 48, "text": "deep neural networks"}, {"st": 59, "ed": 61, "text": "labeled samples"}, {"st": 110, "ed": 113, "text": "end to end"}, {"st": 132, "ed": 134, "text": "improve performance"}, {"st": 135, "ed": 137, "text": "classification tasks"}, {"st": 152, "ed": 154, "text": "labeled data"}, {"st": 155, "ed": 157, "text": "training scheme"}]
[{"st": 0, "ed": 2, "text": "pattern recognition"}, {"st": 21, "ed": 23, "text": "class classification"}, {"st": 32, "ed": 34, "text": "majority class"}, {"st": 36, "ed": 39, "text": "the minority class"}, {"st": 90, "ed": 92, "text": "ensemble learning"}, {"st": 122, "ed": 124, "text": "base classifiers"}, {"st": 139, "ed": 141, "text": "validation set"}, {"st": 195, "ed": 197, "text": "weight update"}, {"st": 233, "ed": 235, "text": "proposed approach"}, {"st": 240, "ed": 242, "text": "synthetic data"}]
[{"st": 3, "ed": 6, "text": "end to end"}, {"st": 7, "ed": 10, "text": "fully convolutional network"}, {"st": 66, "ed": 68, "text": "generation process"}, {"st": 91, "ed": 93, "text": "fine tune"}, {"st": 101, "ed": 103, "text": "semi supervised"}, {"st": 109, "ed": 111, "text": "network architecture"}, {"st": 120, "ed": 122, "text": "synthetic data"}]
[{"st": 18, "ed": 20, "text": "feature representation"}, {"st": 61, "ed": 64, "text": "recurrent neural network"}, {"st": 75, "ed": 77, "text": "input images"}, {"st": 103, "ed": 106, "text": "short term memory"}, {"st": 151, "ed": 153, "text": "extensive experiments"}, {"st": 160, "ed": 162, "text": "great potential"}]
[{"st": 6, "ed": 8, "text": "visual attention"}, {"st": 12, "ed": 14, "text": "attention based"}, {"st": 24, "ed": 26, "text": "fully differentiable"}, {"st": 31, "ed": 34, "text": "end to end"}, {"st": 36, "ed": 39, "text": "stochastic gradient descent"}, {"st": 42, "ed": 44, "text": "spatial transformer"}, {"st": 48, "ed": 50, "text": "visual attention"}, {"st": 67, "ed": 69, "text": "spatial transformer"}, {"st": 72, "ed": 74, "text": "recurrent architecture"}, {"st": 89, "ed": 92, "text": "publicly available datasets"}, {"st": 105, "ed": 107, "text": "real world"}]
[{"st": 1, "ed": 3, "text": "recent successes"}, {"st": 9, "ed": 11, "text": "generative models"}, {"st": 25, "ed": 27, "text": "latent space"}, {"st": 34, "ed": 36, "text": "variational autoencoders"}, {"st": 39, "ed": 41, "text": "recurrent connections"}, {"st": 51, "ed": 53, "text": "high level"}, {"st": 61, "ed": 64, "text": "coarse to fine"}, {"st": 67, "ed": 69, "text": "adversarial loss"}, {"st": 102, "ed": 104, "text": "latent representations"}, {"st": 106, "ed": 108, "text": "downstream tasks"}, {"st": 128, "ed": 130, "text": "mutual information"}, {"st": 133, "ed": 135, "text": "latent variables"}]
[{"st": 11, "ed": 15, "text": "convolutional neural networks cnn"}, {"st": 20, "ed": 23, "text": "received increasing attention"}, {"st": 52, "ed": 54, "text": "deep cnns"}, {"st": 106, "ed": 108, "text": "deep cnns"}, {"st": 137, "ed": 139, "text": "k 1"}, {"st": 166, "ed": 168, "text": "deep cnns"}, {"st": 219, "ed": 221, "text": "feature maps"}, {"st": 230, "ed": 232, "text": "feature maps"}]
[{"st": 0, "ed": 4, "text": "convolutional neural networks cnns"}, {"st": 6, "ed": 8, "text": "great success"}, {"st": 9, "ed": 11, "text": "computer vision"}, {"st": 40, "ed": 42, "text": "learned features"}, {"st": 97, "ed": 99, "text": "learned representations"}, {"st": 108, "ed": 110, "text": "improve performance"}, {"st": 115, "ed": 117, "text": "object recognition"}]
[{"st": 4, "ed": 6, "text": "remote sensing"}, {"st": 9, "ed": 11, "text": "supervised classification"}, {"st": 14, "ed": 17, "text": "support vector machines"}, {"st": 19, "ed": 21, "text": "training samples"}, {"st": 70, "ed": 72, "text": "remote sensing"}, {"st": 86, "ed": 90, "text": "number of training samples"}, {"st": 124, "ed": 126, "text": "fine tuned"}, {"st": 143, "ed": 145, "text": "image data"}, {"st": 149, "ed": 153, "text": "number of training samples"}, {"st": 181, "ed": 183, "text": "classification accuracies"}, {"st": 194, "ed": 196, "text": "training data"}, {"st": 200, "ed": 202, "text": "classification accuracy"}]
[{"st": 0, "ed": 3, "text": "classification and clustering"}, {"st": 8, "ed": 10, "text": "machine learning"}, {"st": 19, "ed": 21, "text": "deep learning"}, {"st": 25, "ed": 27, "text": "vision problems"}, {"st": 28, "ed": 30, "text": "object recognition"}, {"st": 44, "ed": 46, "text": "clustering algorithms"}, {"st": 74, "ed": 77, "text": "convolutional neural networks"}, {"st": 114, "ed": 116, "text": "real world"}, {"st": 125, "ed": 129, "text": "trained end to end"}, {"st": 130, "ed": 132, "text": "back propagation"}, {"st": 133, "ed": 135, "text": "noisy labels"}, {"st": 186, "ed": 188, "text": "clustering algorithms"}, {"st": 190, "ed": 192, "text": "learning representations"}, {"st": 194, "ed": 196, "text": "unsupervised manner"}, {"st": 226, "ed": 228, "text": "spectral clustering"}]
[{"st": 7, "ed": 10, "text": "deep neural networks"}, {"st": 58, "ed": 60, "text": "performance degradation"}, {"st": 116, "ed": 118, "text": "matrix multiplications"}]
[{"st": 0, "ed": 2, "text": "conventional methods"}, {"st": 5, "ed": 7, "text": "generative modeling"}, {"st": 11, "ed": 13, "text": "deep networks"}, {"st": 35, "ed": 37, "text": "3d shapes"}, {"st": 53, "ed": 55, "text": "generative modeling"}]
[{"st": 1, "ed": 3, "text": "neural networks"}, {"st": 53, "ed": 55, "text": "neural networks"}, {"st": 141, "ed": 143, "text": "training speed"}, {"st": 148, "ed": 151, "text": "convolutional neural networks"}, {"st": 157, "ed": 159, "text": "standard datasets"}, {"st": 173, "ed": 175, "text": "top 5"}, {"st": 175, "ed": 177, "text": "error rate"}]
[{"st": 37, "ed": 40, "text": "convolutional neural network"}, {"st": 46, "ed": 48, "text": "input image"}, {"st": 56, "ed": 59, "text": "end to end"}, {"st": 118, "ed": 120, "text": "challenging task"}, {"st": 121, "ed": 123, "text": "scene graph"}]
[{"st": 0, "ed": 2, "text": "multi label"}, {"st": 5, "ed": 7, "text": "training instances"}, {"st": 13, "ed": 15, "text": "multi label"}, {"st": 55, "ed": 57, "text": "multi label"}, {"st": 153, "ed": 155, "text": "multi label"}, {"st": 166, "ed": 168, "text": "multi label"}, {"st": 168, "ed": 170, "text": "learning framework"}, {"st": 174, "ed": 176, "text": "multi label"}, {"st": 193, "ed": 195, "text": "extensive experiments"}]
[{"st": 22, "ed": 24, "text": "existing methods"}, {"st": 31, "ed": 33, "text": "false positives"}, {"st": 72, "ed": 74, "text": "generative adversarial"}]
[{"st": 29, "ed": 31, "text": "collective behavior"}, {"st": 62, "ed": 64, "text": "proximity sensor"}, {"st": 115, "ed": 117, "text": "probabilistic model"}, {"st": 133, "ed": 135, "text": "preliminary results"}, {"st": 159, "ed": 161, "text": "pattern recognition"}]
[{"st": 42, "ed": 44, "text": "large scale"}, {"st": 50, "ed": 52, "text": "recent years"}, {"st": 52, "ed": 54, "text": "computer vision"}, {"st": 65, "ed": 67, "text": "large scale"}, {"st": 74, "ed": 76, "text": "based approaches"}, {"st": 78, "ed": 81, "text": "received much attention"}, {"st": 136, "ed": 138, "text": "image processing"}, {"st": 144, "ed": 146, "text": "computer vision"}, {"st": 180, "ed": 182, "text": "computer vision"}]
[{"st": 10, "ed": 12, "text": "representation learning"}, {"st": 30, "ed": 32, "text": "deep autoencoder"}, {"st": 45, "ed": 47, "text": "learned representations"}, {"st": 47, "ed": 49, "text": "outperform existing"}, {"st": 52, "ed": 54, "text": "recognition tasks"}, {"st": 80, "ed": 82, "text": "generative models"}, {"st": 88, "ed": 90, "text": "point clouds"}, {"st": 90, "ed": 92, "text": "significantly improved"}, {"st": 97, "ed": 99, "text": "latent space"}, {"st": 102, "ed": 105, "text": "gaussian mixture models"}, {"st": 109, "ed": 111, "text": "quantitative evaluation"}, {"st": 142, "ed": 144, "text": "latent space"}]
[{"st": 12, "ed": 14, "text": "deep learning"}, {"st": 27, "ed": 30, "text": "end to end"}, {"st": 38, "ed": 40, "text": "conventional methods"}, {"st": 51, "ed": 53, "text": "labelled data"}, {"st": 59, "ed": 61, "text": "proposed method"}, {"st": 69, "ed": 71, "text": "proposed method"}]
[{"st": 8, "ed": 10, "text": "traffic congestion"}, {"st": 19, "ed": 21, "text": "traffic flow"}, {"st": 33, "ed": 35, "text": "time series"}, {"st": 35, "ed": 37, "text": "multivariate data"}, {"st": 41, "ed": 43, "text": "traffic flow"}, {"st": 60, "ed": 63, "text": "support vector machine"}, {"st": 83, "ed": 85, "text": "time series"}, {"st": 90, "ed": 93, "text": "conditional random field"}, {"st": 97, "ed": 100, "text": "probabilistic graphical model"}, {"st": 111, "ed": 113, "text": "sequential data"}, {"st": 117, "ed": 119, "text": "time series"}, {"st": 137, "ed": 139, "text": "approach called"}, {"st": 162, "ed": 164, "text": "result shows"}]
[{"st": 4, "ed": 6, "text": "adversarial examples"}, {"st": 14, "ed": 16, "text": "worst case"}, {"st": 25, "ed": 27, "text": "generalization performance"}, {"st": 79, "ed": 81, "text": "neural network"}, {"st": 95, "ed": 97, "text": "generalization performance"}, {"st": 98, "ed": 102, "text": "supervised and semi supervised"}, {"st": 112, "ed": 114, "text": "trained model"}, {"st": 117, "ed": 119, "text": "performance improvement"}, {"st": 129, "ed": 131, "text": "neural networks"}]
[{"st": 5, "ed": 7, "text": "visual scene"}, {"st": 27, "ed": 29, "text": "existing algorithms"}, {"st": 43, "ed": 45, "text": "neural network"}, {"st": 70, "ed": 74, "text": "deep neural network architecture"}, {"st": 114, "ed": 116, "text": "comparable results"}]
[{"st": 1, "ed": 3, "text": "electron microscopy"}, {"st": 70, "ed": 72, "text": "machine learning"}, {"st": 72, "ed": 74, "text": "based method"}, {"st": 91, "ed": 93, "text": "supervised learning"}, {"st": 96, "ed": 98, "text": "sampling algorithm"}, {"st": 99, "ed": 101, "text": "neural networks"}]
[{"st": 6, "ed": 8, "text": "face images"}, {"st": 26, "ed": 28, "text": "challenging problem"}, {"st": 33, "ed": 35, "text": "deep learning"}, {"st": 86, "ed": 88, "text": "ground truth"}, {"st": 94, "ed": 96, "text": "imitation learning"}, {"st": 104, "ed": 107, "text": "inverse reinforcement learning"}, {"st": 113, "ed": 115, "text": "clustering approaches"}, {"st": 123, "ed": 125, "text": "sequential decision"}, {"st": 131, "ed": 133, "text": "two face"}, {"st": 142, "ed": 144, "text": "extensive experiments"}, {"st": 146, "ed": 148, "text": "benchmark datasets"}, {"st": 153, "ed": 156, "text": "unsupervised and supervised"}]
[{"st": 8, "ed": 10, "text": "activation functions"}, {"st": 20, "ed": 22, "text": "standard classification"}, {"st": 78, "ed": 80, "text": "faster convergence"}, {"st": 85, "ed": 87, "text": "classification tasks"}, {"st": 87, "ed": 89, "text": "image classification"}, {"st": 90, "ed": 92, "text": "cifar 10"}, {"st": 111, "ed": 113, "text": "neural network"}]
[{"st": 7, "ed": 9, "text": "semi supervised"}, {"st": 14, "ed": 16, "text": "image synthesis"}, {"st": 33, "ed": 35, "text": "semantically meaningful"}, {"st": 40, "ed": 42, "text": "latent variables"}, {"st": 52, "ed": 55, "text": "generative adversarial networks"}, {"st": 67, "ed": 69, "text": "achieves higher"}, {"st": 86, "ed": 88, "text": "labeled data"}, {"st": 99, "ed": 101, "text": "latent variables"}, {"st": 120, "ed": 122, "text": "mutual information"}]
[{"st": 0, "ed": 3, "text": "deep residual networks"}, {"st": 14, "ed": 16, "text": "image classification"}, {"st": 50, "ed": 52, "text": "residual network"}, {"st": 92, "ed": 96, "text": "cifar 10 cifar 100"}, {"st": 101, "ed": 103, "text": "classification accuracy"}]
[{"st": 7, "ed": 9, "text": "applications including"}, {"st": 24, "ed": 26, "text": "machine learning"}, {"st": 43, "ed": 47, "text": "conditional generative adversarial networks"}, {"st": 55, "ed": 57, "text": "neural network"}, {"st": 79, "ed": 82, "text": "effective and efficient"}, {"st": 82, "ed": 84, "text": "spatial information"}, {"st": 103, "ed": 105, "text": "residual network"}, {"st": 140, "ed": 142, "text": "proposed method"}, {"st": 146, "ed": 148, "text": "quantitative results"}, {"st": 164, "ed": 166, "text": "simulated data"}, {"st": 183, "ed": 185, "text": "simulated data"}]
[{"st": 5, "ed": 7, "text": "real estate"}, {"st": 29, "ed": 31, "text": "real estate"}, {"st": 102, "ed": 106, "text": "deep convolutional neural networks"}, {"st": 127, "ed": 129, "text": "real estate"}, {"st": 161, "ed": 163, "text": "proposed method"}, {"st": 171, "ed": 173, "text": "real estate"}]
[{"st": 4, "ed": 6, "text": "weakly supervised"}, {"st": 10, "ed": 12, "text": "context specific"}, {"st": 16, "ed": 18, "text": "linear combination"}, {"st": 58, "ed": 60, "text": "similarity measure"}, {"st": 72, "ed": 74, "text": "optimal solutions"}, {"st": 86, "ed": 88, "text": "hand crafted"}, {"st": 113, "ed": 115, "text": "learning algorithm"}, {"st": 127, "ed": 129, "text": "objective function"}, {"st": 134, "ed": 136, "text": "special case"}, {"st": 138, "ed": 140, "text": "convex function"}, {"st": 142, "ed": 144, "text": "convex function"}, {"st": 154, "ed": 157, "text": "proof of concept"}]
[{"st": 0, "ed": 2, "text": "visual recognition"}, {"st": 31, "ed": 33, "text": "convolutional network"}, {"st": 80, "ed": 82, "text": "skip connections"}]
[{"st": 0, "ed": 2, "text": "texture classification"}, {"st": 6, "ed": 8, "text": "challenging problem"}, {"st": 10, "ed": 12, "text": "image processing"}, {"st": 14, "ed": 18, "text": "convolutional neural networks cnns"}, {"st": 22, "ed": 24, "text": "image classification"}, {"st": 24, "ed": 26, "text": "texture classification"}, {"st": 44, "ed": 46, "text": "image processing"}, {"st": 46, "ed": 48, "text": "texture classification"}, {"st": 102, "ed": 104, "text": "spectral analysis"}, {"st": 111, "ed": 113, "text": "pooling layer"}, {"st": 139, "ed": 141, "text": "spectral analysis"}, {"st": 164, "ed": 166, "text": "experiments demonstrate"}, {"st": 173, "ed": 175, "text": "texture classification"}, {"st": 185, "ed": 187, "text": "significantly fewer"}]
[{"st": 3, "ed": 5, "text": "breast cancer"}, {"st": 32, "ed": 34, "text": "breast cancer"}, {"st": 80, "ed": 82, "text": "classification results"}]
[{"st": 11, "ed": 13, "text": "virtual reality"}, {"st": 35, "ed": 37, "text": "optimal solutions"}, {"st": 58, "ed": 60, "text": "image classification"}, {"st": 62, "ed": 65, "text": "taking into account"}, {"st": 79, "ed": 81, "text": "deep learning"}, {"st": 96, "ed": 98, "text": "convolutional filters"}, {"st": 119, "ed": 121, "text": "proposed method"}, {"st": 127, "ed": 129, "text": "image classification"}]
[{"st": 2, "ed": 4, "text": "imagenet dataset"}, {"st": 7, "ed": 9, "text": "large scale"}, {"st": 27, "ed": 29, "text": "hyperparameter tuning"}, {"st": 159, "ed": 161, "text": "https github.com"}]
[{"st": 8, "ed": 10, "text": "computer vision"}, {"st": 69, "ed": 71, "text": "generative framework"}, {"st": 74, "ed": 76, "text": "variational autoencoders"}, {"st": 142, "ed": 144, "text": "promising results"}, {"st": 155, "ed": 157, "text": "rgb d"}]
[{"st": 0, "ed": 2, "text": "real world"}, {"st": 2, "ed": 4, "text": "face detection"}, {"st": 9, "ed": 11, "text": "discriminative model"}, {"st": 22, "ed": 24, "text": "deep learning"}, {"st": 26, "ed": 29, "text": "convolutional neural networks"}, {"st": 30, "ed": 32, "text": "face detection"}, {"st": 38, "ed": 40, "text": "recent studies"}, {"st": 45, "ed": 47, "text": "face detection"}, {"st": 72, "ed": 74, "text": "higher quality"}, {"st": 74, "ed": 76, "text": "training data"}, {"st": 77, "ed": 80, "text": "end to end"}, {"st": 96, "ed": 98, "text": "experiments demonstrate"}]
[{"st": 3, "ed": 5, "text": "deep learning"}, {"st": 5, "ed": 7, "text": "pose estimation"}, {"st": 9, "ed": 11, "text": "vision based"}, {"st": 63, "ed": 65, "text": "deep learning"}, {"st": 65, "ed": 67, "text": "pose estimation"}, {"st": 120, "ed": 122, "text": "feature sets"}, {"st": 200, "ed": 202, "text": "deep learning"}, {"st": 203, "ed": 205, "text": "vision based"}, {"st": 212, "ed": 214, "text": "promising performance"}, {"st": 219, "ed": 221, "text": "deep learning"}, {"st": 235, "ed": 237, "text": "computer vision"}, {"st": 238, "ed": 240, "text": "deep learning"}]
[{"st": 5, "ed": 9, "text": "convolutional neural networks cnns"}, {"st": 26, "ed": 28, "text": "autonomous driving"}, {"st": 50, "ed": 52, "text": "recent advances"}, {"st": 53, "ed": 55, "text": "computer graphics"}, {"st": 63, "ed": 65, "text": "photo realistic"}, {"st": 65, "ed": 67, "text": "synthetic data"}, {"st": 78, "ed": 80, "text": "real images"}, {"st": 82, "ed": 84, "text": "synthetic data"}, {"st": 107, "ed": 109, "text": "domain adaptation"}, {"st": 161, "ed": 163, "text": "spatial relations"}, {"st": 195, "ed": 197, "text": "significantly outperforms"}]
[{"st": 11, "ed": 13, "text": "feature representation"}, {"st": 19, "ed": 21, "text": "group level"}, {"st": 35, "ed": 38, "text": "convolutional neural networks"}, {"st": 43, "ed": 45, "text": "training set"}, {"st": 55, "ed": 58, "text": "recurrent neural network"}, {"st": 73, "ed": 75, "text": "feature representation"}, {"st": 98, "ed": 100, "text": "support vector"}, {"st": 108, "ed": 110, "text": "extensive experiments"}, {"st": 136, "ed": 139, "text": "mean square error"}, {"st": 141, "ed": 143, "text": "validation set"}]
[{"st": 2, "ed": 4, "text": "pattern recognition"}, {"st": 6, "ed": 8, "text": "deep learning"}, {"st": 13, "ed": 16, "text": "synthetic aperture radar"}, {"st": 29, "ed": 31, "text": "deep learning"}, {"st": 36, "ed": 38, "text": "learning process"}, {"st": 71, "ed": 73, "text": "classification accuracy"}, {"st": 94, "ed": 97, "text": "short term memory"}, {"st": 98, "ed": 101, "text": "recurrent neural networks"}, {"st": 125, "ed": 127, "text": "gabor filter"}, {"st": 130, "ed": 132, "text": "local binary"}, {"st": 141, "ed": 143, "text": "spatial features"}, {"st": 145, "ed": 147, "text": "dimensionality reduction"}, {"st": 149, "ed": 153, "text": "multi layer perceptron mlp"}, {"st": 158, "ed": 160, "text": "bidirectional lstm"}, {"st": 160, "ed": 163, "text": "recurrent neural network"}, {"st": 184, "ed": 186, "text": "proposed method"}, {"st": 208, "ed": 211, "text": "deep learning based"}]
[{"st": 105, "ed": 107, "text": "f measure"}]
[{"st": 3, "ed": 5, "text": "encoder decoder"}, {"st": 19, "ed": 21, "text": "attention mechanisms"}, {"st": 22, "ed": 24, "text": "neural networks"}, {"st": 47, "ed": 49, "text": "spatial regions"}, {"st": 89, "ed": 91, "text": "noise levels"}, {"st": 102, "ed": 104, "text": "visual tasks"}, {"st": 110, "ed": 113, "text": "wall street journal"}, {"st": 120, "ed": 122, "text": "network architecture"}, {"st": 162, "ed": 164, "text": "attention mechanism"}, {"st": 187, "ed": 190, "text": "wall street journal"}]
[{"st": 12, "ed": 14, "text": "object detectors"}, {"st": 30, "ed": 32, "text": "pre trained"}, {"st": 33, "ed": 35, "text": "large scale"}, {"st": 50, "ed": 52, "text": "loss functions"}, {"st": 62, "ed": 64, "text": "fine tuning"}, {"st": 66, "ed": 68, "text": "detection task"}, {"st": 80, "ed": 82, "text": "pre trained"}, {"st": 112, "ed": 114, "text": "object detectors"}, {"st": 133, "ed": 135, "text": "loss functions"}, {"st": 137, "ed": 139, "text": "training data"}, {"st": 153, "ed": 155, "text": "object detectors"}, {"st": 169, "ed": 171, "text": "layer wise"}, {"st": 198, "ed": 201, "text": "pascal voc 2007"}, {"st": 203, "ed": 205, "text": "ms coco"}, {"st": 259, "ed": 263, "text": "available at https github.com"}]
[{"st": 20, "ed": 22, "text": "recently introduced"}, {"st": 25, "ed": 27, "text": "visual representation"}, {"st": 50, "ed": 52, "text": "previous works"}, {"st": 54, "ed": 56, "text": "bounding boxes"}, {"st": 61, "ed": 64, "text": "training and test"}, {"st": 85, "ed": 87, "text": "bounding boxes"}, {"st": 107, "ed": 109, "text": "bounding boxes"}, {"st": 119, "ed": 122, "text": "end to end"}, {"st": 141, "ed": 143, "text": "spatial transformer"}, {"st": 153, "ed": 155, "text": "large scale"}, {"st": 187, "ed": 189, "text": "extensive experiments"}, {"st": 217, "ed": 219, "text": "real world"}]
[{"st": 9, "ed": 13, "text": "generative adversarial networks gans"}, {"st": 17, "ed": 20, "text": "generative adversarial network"}, {"st": 26, "ed": 28, "text": "image quality"}, {"st": 74, "ed": 76, "text": "image space"}]
[{"st": 1, "ed": 3, "text": "neural networks"}, {"st": 14, "ed": 16, "text": "previous works"}, {"st": 92, "ed": 94, "text": "extensive experiments"}, {"st": 113, "ed": 115, "text": "significantly improved"}, {"st": 115, "ed": 117, "text": "prediction accuracy"}]
[{"st": 0, "ed": 3, "text": "convolutional neural networks"}, {"st": 6, "ed": 8, "text": "remarkable success"}, {"st": 14, "ed": 16, "text": "network architectures"}, {"st": 17, "ed": 19, "text": "hand crafted"}, {"st": 48, "ed": 50, "text": "learning paradigm"}, {"st": 51, "ed": 53, "text": "epsilon greedy"}, {"st": 63, "ed": 65, "text": "learning agent"}, {"st": 87, "ed": 89, "text": "generation process"}, {"st": 111, "ed": 113, "text": "competitive results"}, {"st": 117, "ed": 119, "text": "hand crafted"}, {"st": 125, "ed": 127, "text": "image classification"}, {"st": 138, "ed": 140, "text": "error rate"}, {"st": 141, "ed": 143, "text": "cifar 10"}, {"st": 160, "ed": 162, "text": "search space"}, {"st": 191, "ed": 193, "text": "larger scale"}]
[{"st": 0, "ed": 3, "text": "human activity recognition"}, {"st": 20, "ed": 22, "text": "deep learning"}, {"st": 32, "ed": 35, "text": "deep network architecture"}, {"st": 39, "ed": 42, "text": "short term memory"}, {"st": 74, "ed": 76, "text": "residual connections"}, {"st": 116, "ed": 118, "text": "residual connections"}, {"st": 152, "ed": 154, "text": "previously reported"}]
[{"st": 11, "ed": 13, "text": "land cover"}, {"st": 15, "ed": 17, "text": "remote sensing"}, {"st": 21, "ed": 23, "text": "challenging task"}, {"st": 82, "ed": 86, "text": "deep convolutional neural network"}, {"st": 99, "ed": 101, "text": "deep cnns"}, {"st": 103, "ed": 105, "text": "remote sensing"}, {"st": 120, "ed": 122, "text": "classification accuracy"}, {"st": 157, "ed": 159, "text": "land cover"}]
[{"st": 4, "ed": 6, "text": "deep learning"}, {"st": 9, "ed": 11, "text": "image reconstruction"}, {"st": 26, "ed": 30, "text": "convolutional neural network cnn"}, {"st": 37, "ed": 39, "text": "least squares"}, {"st": 83, "ed": 85, "text": "proximal gradient"}, {"st": 95, "ed": 97, "text": "deep cnn"}, {"st": 118, "ed": 120, "text": "image quality"}, {"st": 126, "ed": 128, "text": "image reconstruction"}, {"st": 135, "ed": 137, "text": "iterative method"}, {"st": 142, "ed": 144, "text": "image reconstruction"}, {"st": 153, "ed": 155, "text": "image reconstruction"}, {"st": 156, "ed": 159, "text": "qualitative and quantitative"}]
[{"st": 28, "ed": 31, "text": "human activity recognition"}, {"st": 76, "ed": 78, "text": "intra class"}, {"st": 80, "ed": 82, "text": "class imbalance"}, {"st": 98, "ed": 100, "text": "multi label"}, {"st": 116, "ed": 118, "text": "computer vision"}, {"st": 128, "ed": 130, "text": "multi label"}, {"st": 134, "ed": 136, "text": "multi label"}, {"st": 136, "ed": 138, "text": "class imbalanced"}, {"st": 178, "ed": 180, "text": "originally designed"}, {"st": 189, "ed": 191, "text": "proposed approach"}]
[{"st": 14, "ed": 16, "text": "real world"}, {"st": 24, "ed": 26, "text": "existing methods"}, {"st": 27, "ed": 29, "text": "weighted average"}, {"st": 125, "ed": 127, "text": "proposed method"}, {"st": 133, "ed": 135, "text": "ensemble methods"}]
[{"st": 140, "ed": 144, "text": "deep convolutional neural network"}, {"st": 149, "ed": 151, "text": "feature representation"}, {"st": 201, "ed": 203, "text": "classification accuracy"}]
[{"st": 3, "ed": 5, "text": "machine learning"}, {"st": 6, "ed": 9, "text": "extreme learning machine"}, {"st": 21, "ed": 23, "text": "directly applied"}, {"st": 30, "ed": 32, "text": "recognition rate"}, {"st": 43, "ed": 45, "text": "spatial information"}, {"st": 72, "ed": 75, "text": "loopy belief propagation"}, {"st": 147, "ed": 149, "text": "proposed method"}]
[{"st": 7, "ed": 9, "text": "patient specific"}, {"st": 33, "ed": 35, "text": "conditional distribution"}, {"st": 44, "ed": 46, "text": "neural network"}, {"st": 53, "ed": 55, "text": "random noise"}, {"st": 59, "ed": 61, "text": "generative network"}, {"st": 71, "ed": 73, "text": "neural network"}, {"st": 76, "ed": 78, "text": "generated samples"}, {"st": 94, "ed": 96, "text": "neural network"}, {"st": 100, "ed": 102, "text": "input image"}, {"st": 105, "ed": 107, "text": "patient specific"}, {"st": 114, "ed": 116, "text": "generative models"}, {"st": 122, "ed": 124, "text": "training data"}, {"st": 138, "ed": 140, "text": "mr images"}, {"st": 152, "ed": 154, "text": "finite element"}, {"st": 171, "ed": 173, "text": "patient specific"}, {"st": 178, "ed": 181, "text": "ability to capture"}, {"st": 201, "ed": 203, "text": "a 10"}, {"st": 203, "ed": 206, "text": "fold cross validation"}, {"st": 230, "ed": 232, "text": "machine learning"}, {"st": 242, "ed": 244, "text": "shows significant"}]
[{"st": 4, "ed": 6, "text": "neural networks"}, {"st": 14, "ed": 16, "text": "neural networks"}, {"st": 106, "ed": 108, "text": "floating point"}, {"st": 146, "ed": 148, "text": "empirical evidence"}, {"st": 154, "ed": 156, "text": "efficient inference"}, {"st": 163, "ed": 165, "text": "limited memory"}, {"st": 177, "ed": 179, "text": "mnist dataset"}, {"st": 213, "ed": 215, "text": "source code"}, {"st": 227, "ed": 229, "text": "learning task"}]
[{"st": 0, "ed": 2, "text": "sparse coding"}, {"st": 17, "ed": 19, "text": "excellent performance"}, {"st": 21, "ed": 23, "text": "signal processing"}, {"st": 27, "ed": 29, "text": "sparse coding"}, {"st": 41, "ed": 43, "text": "local minima"}, {"st": 62, "ed": 64, "text": "unified framework"}, {"st": 65, "ed": 67, "text": "self paced"}, {"st": 67, "ed": 69, "text": "sparse coding"}, {"st": 86, "ed": 89, "text": "self paced learning"}, {"st": 105, "ed": 108, "text": "real world data"}]
[{"st": 6, "ed": 8, "text": "natural image"}, {"st": 18, "ed": 20, "text": "natural image"}, {"st": 29, "ed": 31, "text": "image restoration"}, {"st": 43, "ed": 45, "text": "image restoration"}, {"st": 57, "ed": 59, "text": "mean shift"}, {"st": 62, "ed": 64, "text": "natural image"}, {"st": 70, "ed": 72, "text": "mean shift"}, {"st": 75, "ed": 77, "text": "denoising autoencoders"}, {"st": 92, "ed": 94, "text": "competitive results"}, {"st": 98, "ed": 100, "text": "super resolution"}]
[{"st": 17, "ed": 19, "text": "neural network"}, {"st": 25, "ed": 27, "text": "neural network"}, {"st": 28, "ed": 30, "text": "image denoising"}, {"st": 35, "ed": 37, "text": "supervised learning"}, {"st": 54, "ed": 56, "text": "neural network"}, {"st": 77, "ed": 79, "text": "supervised training"}, {"st": 84, "ed": 86, "text": "labeled training"}, {"st": 89, "ed": 91, "text": "fine tuning"}, {"st": 93, "ed": 95, "text": "network parameters"}, {"st": 115, "ed": 117, "text": "loss function"}, {"st": 153, "ed": 155, "text": "fine tuning"}, {"st": 167, "ed": 169, "text": "supervised learning"}, {"st": 171, "ed": 173, "text": "image denoising"}, {"st": 176, "ed": 178, "text": "trained model"}, {"st": 198, "ed": 200, "text": "fine tuning"}]
[{"st": 2, "ed": 4, "text": "image classification"}, {"st": 14, "ed": 16, "text": "transfer learning"}, {"st": 39, "ed": 41, "text": "classification problem"}, {"st": 58, "ed": 60, "text": "depth estimation"}, {"st": 69, "ed": 71, "text": "image classification"}, {"st": 81, "ed": 83, "text": "domain knowledge"}, {"st": 84, "ed": 86, "text": "depth estimation"}, {"st": 89, "ed": 91, "text": "image classification"}, {"st": 112, "ed": 114, "text": "image classification"}, {"st": 121, "ed": 123, "text": "neural networks"}]
[{"st": 1, "ed": 3, "text": "event detection"}, {"st": 4, "ed": 6, "text": "time series"}, {"st": 13, "ed": 15, "text": "event detection"}, {"st": 19, "ed": 21, "text": "event detection"}, {"st": 25, "ed": 27, "text": "detection methods"}, {"st": 49, "ed": 51, "text": "recent years"}, {"st": 56, "ed": 58, "text": "computational power"}, {"st": 58, "ed": 60, "text": "machine learning"}, {"st": 74, "ed": 77, "text": "deep learning based"}, {"st": 84, "ed": 86, "text": "time series"}, {"st": 98, "ed": 100, "text": "object detection"}, {"st": 139, "ed": 142, "text": "convolutional neural network"}, {"st": 151, "ed": 153, "text": "contextual information"}, {"st": 164, "ed": 166, "text": "generalization performance"}, {"st": 193, "ed": 195, "text": "detection problem"}, {"st": 207, "ed": 209, "text": "detection methods"}, {"st": 225, "ed": 227, "text": "rock mechanics"}, {"st": 254, "ed": 256, "text": "deep learning"}, {"st": 257, "ed": 259, "text": "detection methods"}, {"st": 262, "ed": 264, "text": "powerful tools"}, {"st": 268, "ed": 270, "text": "time series"}]
[{"st": 9, "ed": 11, "text": "convolutional networks"}, {"st": 34, "ed": 36, "text": "recent studies"}, {"st": 60, "ed": 62, "text": "significant improvements"}, {"st": 158, "ed": 160, "text": "loss function"}, {"st": 175, "ed": 177, "text": "multi class"}, {"st": 177, "ed": 179, "text": "image classification"}, {"st": 187, "ed": 189, "text": "higher accuracy"}]
[{"st": 11, "ed": 13, "text": "affinity matrix"}, {"st": 30, "ed": 32, "text": "transformation matrix"}, {"st": 35, "ed": 37, "text": "affinity matrix"}, {"st": 63, "ed": 65, "text": "transformation matrix"}, {"st": 74, "ed": 76, "text": "deep cnn"}, {"st": 82, "ed": 84, "text": "affinity matrix"}, {"st": 88, "ed": 90, "text": "task specific"}, {"st": 90, "ed": 92, "text": "pairwise similarity"}, {"st": 101, "ed": 103, "text": "image features"}, {"st": 125, "ed": 127, "text": "generic framework"}, {"st": 134, "ed": 136, "text": "related tasks"}, {"st": 162, "ed": 164, "text": "vision tasks"}, {"st": 172, "ed": 175, "text": "deep neural network"}, {"st": 186, "ed": 188, "text": "image segmentation"}, {"st": 196, "ed": 199, "text": "pascal voc 2012"}, {"st": 211, "ed": 214, "text": "effective and efficient"}]
[{"st": 22, "ed": 25, "text": "curse of dimensionality"}, {"st": 62, "ed": 64, "text": "open problem"}, {"st": 85, "ed": 87, "text": "multi level"}]
[{"st": 9, "ed": 12, "text": "deep neural networks"}, {"st": 18, "ed": 20, "text": "regularization methods"}, {"st": 27, "ed": 29, "text": "hidden units"}, {"st": 125, "ed": 128, "text": "stochastic gradient descent"}, {"st": 138, "ed": 140, "text": "computer vision"}]
[{"st": 0, "ed": 4, "text": "convolutional neural networks cnns"}, {"st": 9, "ed": 11, "text": "image recognition"}, {"st": 31, "ed": 33, "text": "training process"}, {"st": 84, "ed": 86, "text": "visual analytics"}, {"st": 94, "ed": 96, "text": "trained cnn"}, {"st": 140, "ed": 142, "text": "case study"}]
[{"st": 0, "ed": 3, "text": "person re identification"}, {"st": 38, "ed": 41, "text": "self paced learning"}, {"st": 52, "ed": 54, "text": "self paced"}, {"st": 62, "ed": 64, "text": "distance metric"}, {"st": 66, "ed": 69, "text": "deep neural network"}, {"st": 76, "ed": 78, "text": "discriminative features"}, {"st": 102, "ed": 104, "text": "training loss"}, {"st": 144, "ed": 147, "text": "self paced learning"}, {"st": 181, "ed": 183, "text": "back propagation"}, {"st": 187, "ed": 189, "text": "distance metric"}, {"st": 195, "ed": 197, "text": "intra class"}, {"st": 201, "ed": 203, "text": "inter class"}, {"st": 214, "ed": 217, "text": "deep neural network"}, {"st": 232, "ed": 234, "text": "convolutional layers"}, {"st": 246, "ed": 248, "text": "benchmark datasets"}]
[{"st": 8, "ed": 10, "text": "deep learning"}, {"st": 16, "ed": 18, "text": "computed tomography"}, {"st": 24, "ed": 26, "text": "training examples"}, {"st": 27, "ed": 29, "text": "deep networks"}, {"st": 44, "ed": 46, "text": "semantically meaningful"}, {"st": 67, "ed": 69, "text": "deep networks"}, {"st": 90, "ed": 92, "text": "classification accuracy"}, {"st": 100, "ed": 102, "text": "deep networks"}]
[{"st": 0, "ed": 2, "text": "image classification"}, {"st": 9, "ed": 11, "text": "input image"}, {"st": 76, "ed": 78, "text": "visual data"}, {"st": 138, "ed": 140, "text": "visual recognition"}, {"st": 147, "ed": 149, "text": "image classification"}, {"st": 188, "ed": 190, "text": "classification task"}, {"st": 301, "ed": 303, "text": "task specific"}]
[{"st": 1, "ed": 3, "text": "computer vision"}, {"st": 7, "ed": 9, "text": "spatio temporal"}, {"st": 16, "ed": 20, "text": "restricted boltzmann machines rbms"}, {"st": 35, "ed": 37, "text": "temporal patterns"}, {"st": 46, "ed": 48, "text": "local spatial"}, {"st": 66, "ed": 68, "text": "explicitly model"}, {"st": 69, "ed": 71, "text": "local spatial"}, {"st": 84, "ed": 86, "text": "efficient learning"}, {"st": 97, "ed": 99, "text": "multi class"}, {"st": 118, "ed": 120, "text": "generative model"}, {"st": 131, "ed": 133, "text": "computer vision"}, {"st": 134, "ed": 136, "text": "facial expression"}]
[{"st": 0, "ed": 5, "text": "deep convolutional neural networks cnns"}, {"st": 6, "ed": 8, "text": "recently achieved"}, {"st": 8, "ed": 10, "text": "great success"}, {"st": 12, "ed": 14, "text": "visual recognition"}, {"st": 17, "ed": 21, "text": "deep convolutional neural network"}, {"st": 23, "ed": 25, "text": "computationally expensive"}, {"st": 56, "ed": 58, "text": "deep networks"}, {"st": 105, "ed": 107, "text": "low rank"}, {"st": 110, "ed": 112, "text": "convolutional filters"}]
[{"st": 8, "ed": 10, "text": "training process"}, {"st": 12, "ed": 14, "text": "deep learning"}, {"st": 21, "ed": 23, "text": "training process"}, {"st": 24, "ed": 26, "text": "deep learning"}, {"st": 50, "ed": 52, "text": "automatically generate"}, {"st": 54, "ed": 56, "text": "training samples"}, {"st": 74, "ed": 76, "text": "training samples"}, {"st": 88, "ed": 90, "text": "training samples"}, {"st": 105, "ed": 107, "text": "generative model"}, {"st": 127, "ed": 129, "text": "training points"}, {"st": 150, "ed": 152, "text": "theoretically sound"}, {"st": 154, "ed": 156, "text": "monte carlo"}, {"st": 168, "ed": 172, "text": "generative adversarial network gan"}, {"st": 173, "ed": 175, "text": "classification results"}, {"st": 176, "ed": 179, "text": "mnist cifar 10"}, {"st": 180, "ed": 182, "text": "cifar 100"}, {"st": 188, "ed": 190, "text": "proposed method"}, {"st": 206, "ed": 208, "text": "approach produces"}, {"st": 209, "ed": 211, "text": "classification results"}]
[{"st": 13, "ed": 16, "text": "field of view"}, {"st": 66, "ed": 69, "text": "deep reinforcement learning"}]
[{"st": 0, "ed": 2, "text": "skip connections"}, {"st": 6, "ed": 9, "text": "deep neural networks"}, {"st": 60, "ed": 62, "text": "fully convolutional"}, {"st": 79, "ed": 81, "text": "skip connections"}, {"st": 97, "ed": 99, "text": "skip connections"}, {"st": 154, "ed": 156, "text": "l 2"}, {"st": 184, "ed": 186, "text": "tabula rasa"}, {"st": 189, "ed": 191, "text": "competitive results"}]
[{"st": 0, "ed": 2, "text": "image analysis"}, {"st": 8, "ed": 10, "text": "multi modal"}, {"st": 51, "ed": 53, "text": "unified framework"}, {"st": 68, "ed": 70, "text": "image fusion"}, {"st": 74, "ed": 76, "text": "image analysis"}, {"st": 79, "ed": 81, "text": "feature level"}, {"st": 90, "ed": 92, "text": "decision making"}, {"st": 101, "ed": 103, "text": "deep learning"}, {"st": 104, "ed": 106, "text": "natural image"}, {"st": 111, "ed": 113, "text": "image fusion"}, {"st": 118, "ed": 122, "text": "convolutional neural network cnn"}, {"st": 133, "ed": 135, "text": "image segmentation"}, {"st": 158, "ed": 160, "text": "soft tissue"}, {"st": 165, "ed": 169, "text": "magnetic resonance imaging mri"}, {"st": 169, "ed": 171, "text": "computed tomography"}, {"st": 198, "ed": 200, "text": "feature level"}, {"st": 210, "ed": 213, "text": "accuracy and computational"}]
[{"st": 5, "ed": 7, "text": "multitask learning"}, {"st": 10, "ed": 12, "text": "deep learning"}, {"st": 20, "ed": 22, "text": "multiple tasks"}, {"st": 43, "ed": 45, "text": "object detection"}, {"st": 93, "ed": 95, "text": "training process"}]
[{"st": 5, "ed": 7, "text": "conditional generative"}, {"st": 14, "ed": 16, "text": "existing models"}, {"st": 23, "ed": 25, "text": "labeled instances"}, {"st": 41, "ed": 44, "text": "generative adversarial networks"}, {"st": 46, "ed": 48, "text": "semi supervised"}, {"st": 48, "ed": 50, "text": "conditional generative"}, {"st": 62, "ed": 64, "text": "latent variables"}, {"st": 142, "ed": 144, "text": "based methods"}, {"st": 170, "ed": 172, "text": "disentangled representations"}, {"st": 181, "ed": 183, "text": "multiple datasets"}, {"st": 186, "ed": 188, "text": "semi supervised"}, {"st": 188, "ed": 190, "text": "image classification"}, {"st": 193, "ed": 195, "text": "error rates"}, {"st": 196, "ed": 198, "text": "mnist svhn"}, {"st": 199, "ed": 201, "text": "cifar 10"}, {"st": 224, "ed": 226, "text": "visual quality"}]
[{"st": 9, "ed": 11, "text": "challenging task"}, {"st": 24, "ed": 26, "text": "weakly supervised"}, {"st": 35, "ed": 37, "text": "higher level"}, {"st": 50, "ed": 53, "text": "convolutional neural network"}, {"st": 112, "ed": 114, "text": "cross entropy"}, {"st": 142, "ed": 144, "text": "fully automatic"}, {"st": 163, "ed": 165, "text": "neural network"}, {"st": 170, "ed": 173, "text": "global and local"}, {"st": 174, "ed": 176, "text": "experiment results"}, {"st": 189, "ed": 191, "text": "magnetic resonance"}, {"st": 198, "ed": 200, "text": "prostate cancer"}]
[{"st": 15, "ed": 18, "text": "fully connected layers"}, {"st": 20, "ed": 22, "text": "neural network"}, {"st": 24, "ed": 26, "text": "pre defined"}, {"st": 42, "ed": 45, "text": "convolutional neural networks"}, {"st": 107, "ed": 110, "text": "proof of concept"}, {"st": 122, "ed": 124, "text": "morse code"}]
[{"st": 66, "ed": 68, "text": "joint training"}, {"st": 87, "ed": 89, "text": "benchmark datasets"}, {"st": 89, "ed": 91, "text": "cifar 100"}, {"st": 108, "ed": 110, "text": "cifar 100"}, {"st": 120, "ed": 122, "text": "fine grained"}, {"st": 137, "ed": 140, "text": "pascal voc 2012"}]
[{"st": 3, "ed": 5, "text": "deep generative"}, {"st": 18, "ed": 20, "text": "existing methods"}, {"st": 42, "ed": 44, "text": "class specific"}, {"st": 44, "ed": 46, "text": "latent space"}, {"st": 54, "ed": 56, "text": "latent space"}, {"st": 63, "ed": 65, "text": "variational autoencoder"}, {"st": 72, "ed": 74, "text": "feature representations"}, {"st": 82, "ed": 85, "text": "end to end"}, {"st": 127, "ed": 129, "text": "semi supervised"}, {"st": 139, "ed": 141, "text": "unsupervised learning"}, {"st": 144, "ed": 147, "text": "few shot learning"}]
[{"st": 40, "ed": 42, "text": "recent advances"}, {"st": 46, "ed": 48, "text": "deep learning"}, {"st": 51, "ed": 53, "text": "great potential"}, {"st": 61, "ed": 63, "text": "deep learning"}, {"st": 68, "ed": 70, "text": "structured data"}, {"st": 73, "ed": 75, "text": "deep learning"}, {"st": 91, "ed": 93, "text": "deep learning"}, {"st": 122, "ed": 124, "text": "neural network"}, {"st": 166, "ed": 168, "text": "breast cancer"}, {"st": 169, "ed": 171, "text": "classification task"}, {"st": 173, "ed": 175, "text": "breast cancer"}, {"st": 188, "ed": 190, "text": "survival analysis"}, {"st": 191, "ed": 193, "text": "proposed method"}, {"st": 208, "ed": 210, "text": "starting point"}]
[{"st": 71, "ed": 73, "text": "object detection"}, {"st": 110, "ed": 112, "text": "post processing"}, {"st": 153, "ed": 155, "text": "experiments confirm"}, {"st": 166, "ed": 168, "text": "object detection"}]
[{"st": 4, "ed": 7, "text": "deep neural network"}, {"st": 17, "ed": 19, "text": "challenging task"}, {"st": 20, "ed": 23, "text": "deep neural networks"}, {"st": 24, "ed": 26, "text": "previously learned"}, {"st": 52, "ed": 54, "text": "domain adaptation"}, {"st": 124, "ed": 126, "text": "image classification"}]
[{"st": 5, "ed": 7, "text": "image synthesis"}, {"st": 48, "ed": 50, "text": "unseen data"}, {"st": 140, "ed": 144, "text": "generative adversarial networks gans"}, {"st": 162, "ed": 164, "text": "real world"}, {"st": 165, "ed": 168, "text": "qualitative and quantitative"}, {"st": 174, "ed": 176, "text": "semantically meaningful"}, {"st": 182, "ed": 184, "text": "face verification"}]
[{"st": 1, "ed": 3, "text": "recent years"}, {"st": 3, "ed": 6, "text": "deep neural networks"}, {"st": 12, "ed": 14, "text": "low level"}, {"st": 27, "ed": 29, "text": "deep networks"}, {"st": 75, "ed": 77, "text": "image restoration"}, {"st": 96, "ed": 98, "text": "nonlinear function"}, {"st": 114, "ed": 116, "text": "significantly smaller"}, {"st": 146, "ed": 148, "text": "super resolution"}]
[{"st": 9, "ed": 11, "text": "point cloud"}, {"st": 17, "ed": 22, "text": "deep convolutional neural network cnn"}, {"st": 24, "ed": 26, "text": "proposed method"}, {"st": 33, "ed": 35, "text": "pooling layer"}, {"st": 49, "ed": 51, "text": "point cloud"}, {"st": 62, "ed": 64, "text": "pooling layer"}, {"st": 87, "ed": 89, "text": "object detection"}, {"st": 94, "ed": 96, "text": "autonomous driving"}, {"st": 99, "ed": 101, "text": "deep cnn"}, {"st": 110, "ed": 112, "text": "object detection"}, {"st": 116, "ed": 118, "text": "bounding boxes"}, {"st": 142, "ed": 144, "text": "object detection"}]
[{"st": 12, "ed": 14, "text": "deep learning"}, {"st": 59, "ed": 61, "text": "network architectures"}, {"st": 62, "ed": 64, "text": "image segmentation"}]
[{"st": 2, "ed": 4, "text": "machine learning"}, {"st": 23, "ed": 27, "text": "convolutional neural networks cnns"}, {"st": 70, "ed": 72, "text": "efficiently compute"}, {"st": 100, "ed": 102, "text": "weight initialization"}, {"st": 110, "ed": 112, "text": "linear combination"}, {"st": 118, "ed": 120, "text": "numerical experiments"}, {"st": 126, "ed": 128, "text": "sample complexity"}, {"st": 147, "ed": 149, "text": "proposed approach"}]
[{"st": 41, "ed": 43, "text": "neural networks"}, {"st": 121, "ed": 123, "text": "style transfer"}, {"st": 146, "ed": 148, "text": "neural networks"}]
[{"st": 1, "ed": 5, "text": "deep convolutional neural networks"}, {"st": 18, "ed": 20, "text": "real world"}, {"st": 35, "ed": 37, "text": "deep network"}, {"st": 56, "ed": 58, "text": "residual networks"}, {"st": 87, "ed": 89, "text": "policy network"}, {"st": 92, "ed": 94, "text": "reinforcement learning"}, {"st": 111, "ed": 114, "text": "conduct extensive experiments"}, {"st": 122, "ed": 125, "text": "quantitative and qualitative"}, {"st": 147, "ed": 149, "text": "method achieves"}]
[{"st": 8, "ed": 10, "text": "recently shown"}, {"st": 48, "ed": 50, "text": "computational complexity"}, {"st": 80, "ed": 83, "text": "end to end"}, {"st": 100, "ed": 103, "text": "efficient and accurate"}, {"st": 113, "ed": 115, "text": "loss function"}, {"st": 124, "ed": 126, "text": "retrieval performance"}, {"st": 148, "ed": 150, "text": "cross domain"}]
[{"st": 4, "ed": 6, "text": "neural networks"}, {"st": 22, "ed": 24, "text": "significant improvement"}, {"st": 28, "ed": 30, "text": "neural networks"}, {"st": 36, "ed": 38, "text": "generative modeling"}, {"st": 53, "ed": 56, "text": "generative adversarial networks"}, {"st": 69, "ed": 71, "text": "wasserstein distance"}, {"st": 90, "ed": 92, "text": "a 20"}, {"st": 106, "ed": 108, "text": "supervised classification"}, {"st": 122, "ed": 124, "text": "adversarial examples"}, {"st": 140, "ed": 142, "text": "encouraging results"}, {"st": 143, "ed": 145, "text": "unsupervised learning"}, {"st": 145, "ed": 147, "text": "problems including"}, {"st": 156, "ed": 158, "text": "supervised classification"}]
[{"st": 0, "ed": 4, "text": "deep convolutional neural networks"}, {"st": 23, "ed": 25, "text": "invariance properties"}, {"st": 26, "ed": 28, "text": "deep networks"}, {"st": 55, "ed": 57, "text": "deep networks"}, {"st": 62, "ed": 64, "text": "worst case"}, {"st": 74, "ed": 77, "text": "extensive experimental results"}, {"st": 116, "ed": 118, "text": "adversarial training"}, {"st": 127, "ed": 129, "text": "invariance properties"}]
[{"st": 16, "ed": 18, "text": "temporal patterns"}, {"st": 36, "ed": 38, "text": "temporal information"}, {"st": 47, "ed": 50, "text": "achieve competitive results"}, {"st": 62, "ed": 64, "text": "attention based"}, {"st": 64, "ed": 66, "text": "local feature"}, {"st": 80, "ed": 82, "text": "local feature"}, {"st": 107, "ed": 109, "text": "attention mechanisms"}, {"st": 138, "ed": 140, "text": "real world"}, {"st": 146, "ed": 148, "text": "competitive results"}, {"st": 156, "ed": 158, "text": "large scale"}, {"st": 182, "ed": 184, "text": "top 5"}]
[{"st": 1, "ed": 3, "text": "convolutional networks"}, {"st": 7, "ed": 9, "text": "feed forward"}, {"st": 12, "ed": 14, "text": "neural network"}, {"st": 35, "ed": 37, "text": "network structure"}, {"st": 59, "ed": 61, "text": "fine grained"}, {"st": 76, "ed": 78, "text": "convolutional networks"}, {"st": 89, "ed": 91, "text": "residual networks"}, {"st": 119, "ed": 121, "text": "cifar 10"}, {"st": 129, "ed": 131, "text": "computational budget"}, {"st": 145, "ed": 147, "text": "top 5"}, {"st": 147, "ed": 149, "text": "error rate"}, {"st": 189, "ed": 191, "text": "adversarial attacks"}]
[{"st": 0, "ed": 3, "text": "deep neural network"}, {"st": 7, "ed": 9, "text": "labeled datasets"}, {"st": 20, "ed": 22, "text": "computer vision"}, {"st": 27, "ed": 29, "text": "labeled data"}, {"st": 36, "ed": 38, "text": "time consuming"}, {"st": 38, "ed": 40, "text": "manual annotation"}, {"st": 43, "ed": 45, "text": "unlabeled data"}, {"st": 61, "ed": 63, "text": "unlabeled data"}, {"st": 65, "ed": 68, "text": "deep generative models"}, {"st": 70, "ed": 73, "text": "labeled and unlabeled"}, {"st": 84, "ed": 86, "text": "unlabeled data"}, {"st": 95, "ed": 97, "text": "generative models"}, {"st": 100, "ed": 102, "text": "labeled samples"}, {"st": 107, "ed": 109, "text": "fully supervised"}, {"st": 109, "ed": 111, "text": "generative models"}, {"st": 140, "ed": 142, "text": "large scale"}, {"st": 146, "ed": 148, "text": "two face"}]
[{"st": 0, "ed": 2, "text": "deep learning"}, {"st": 23, "ed": 25, "text": "sample size"}, {"st": 36, "ed": 38, "text": "outperforms previous"}, {"st": 38, "ed": 40, "text": "machine learning"}, {"st": 48, "ed": 50, "text": "neural networks"}, {"st": 77, "ed": 79, "text": "feed forward"}, {"st": 163, "ed": 165, "text": "proposed approach"}, {"st": 188, "ed": 190, "text": "prediction tasks"}]
[{"st": 8, "ed": 10, "text": "convolutional autoencoder"}, {"st": 14, "ed": 16, "text": "face image"}, {"st": 29, "ed": 31, "text": "face image"}, {"st": 41, "ed": 43, "text": "face recognition"}, {"st": 58, "ed": 60, "text": "training scheme"}, {"st": 64, "ed": 66, "text": "adversarial training"}, {"st": 94, "ed": 96, "text": "objective function"}, {"st": 114, "ed": 116, "text": "face image"}, {"st": 135, "ed": 137, "text": "recognition performance"}, {"st": 145, "ed": 147, "text": "extensive experiments"}]
[{"st": 3, "ed": 5, "text": "probabilistic model"}, {"st": 7, "ed": 9, "text": "latent variables"}, {"st": 15, "ed": 17, "text": "deep learning"}, {"st": 27, "ed": 29, "text": "latent variables"}, {"st": 47, "ed": 50, "text": "a posteriori map"}, {"st": 51, "ed": 53, "text": "map inference"}, {"st": 63, "ed": 65, "text": "recently proposed"}, {"st": 74, "ed": 76, "text": "ad hoc"}]
[{"st": 0, "ed": 3, "text": "deep convolutional networks"}, {"st": 5, "ed": 7, "text": "image segmentation"}, {"st": 7, "ed": 9, "text": "typically require"}, {"st": 9, "ed": 11, "text": "large scale"}, {"st": 11, "ed": 13, "text": "labeled data"}, {"st": 16, "ed": 18, "text": "ms coco"}, {"st": 31, "ed": 33, "text": "recently proposed"}, {"st": 34, "ed": 36, "text": "pre train"}, {"st": 88, "ed": 90, "text": "image segmentation"}, {"st": 128, "ed": 130, "text": "proposed approach"}, {"st": 159, "ed": 161, "text": "image segmentation"}, {"st": 164, "ed": 166, "text": "fully supervised"}, {"st": 166, "ed": 168, "text": "pre trained"}, {"st": 237, "ed": 239, "text": "fine tuning"}, {"st": 275, "ed": 277, "text": "achieve comparable"}, {"st": 285, "ed": 287, "text": "pre trained"}]
[{"st": 27, "ed": 29, "text": "features extracted"}, {"st": 31, "ed": 33, "text": "variational autoencoder"}, {"st": 97, "ed": 100, "text": "quantitatively and qualitatively"}, {"st": 101, "ed": 103, "text": "similar performance"}]
[{"st": 5, "ed": 7, "text": "remote sensing"}, {"st": 35, "ed": 37, "text": "remote sensing"}, {"st": 37, "ed": 39, "text": "deep learning"}, {"st": 56, "ed": 58, "text": "deep learning"}, {"st": 63, "ed": 65, "text": "land cover"}, {"st": 116, "ed": 118, "text": "ground truth"}, {"st": 122, "ed": 124, "text": "spatial resolution"}, {"st": 126, "ed": 129, "text": "training and testing"}]
[{"st": 1, "ed": 3, "text": "deep learning"}, {"st": 7, "ed": 9, "text": "significant improvement"}, {"st": 13, "ed": 15, "text": "image reconstruction"}, {"st": 37, "ed": 40, "text": "fully convolutional network"}, {"st": 104, "ed": 106, "text": "proposed method"}, {"st": 108, "ed": 110, "text": "existing methods"}]
[{"st": 15, "ed": 17, "text": "feature maps"}, {"st": 32, "ed": 35, "text": "person re identification"}, {"st": 41, "ed": 43, "text": "max pooling"}, {"st": 51, "ed": 53, "text": "receptive field"}, {"st": 95, "ed": 97, "text": "convolutional networks"}, {"st": 108, "ed": 110, "text": "convolutional architecture"}, {"st": 113, "ed": 115, "text": "convolutional networks"}, {"st": 127, "ed": 129, "text": "feature maps"}, {"st": 131, "ed": 133, "text": "spatio temporal"}, {"st": 150, "ed": 152, "text": "feature maps"}, {"st": 161, "ed": 163, "text": "multi scale"}, {"st": 184, "ed": 186, "text": "feature extraction"}]
[{"st": 3, "ed": 7, "text": "deep convolutional neural networks"}, {"st": 13, "ed": 15, "text": "labeled data"}, {"st": 24, "ed": 26, "text": "object recognition"}, {"st": 28, "ed": 30, "text": "impressive results"}]
[{"st": 11, "ed": 13, "text": "deep learning"}, {"st": 27, "ed": 30, "text": "convolutional auto encoder"}, {"st": 46, "ed": 48, "text": "auto encoders"}, {"st": 99, "ed": 101, "text": "auto encoders"}, {"st": 164, "ed": 166, "text": "representation space"}, {"st": 170, "ed": 173, "text": "convolutional auto encoder"}, {"st": 222, "ed": 224, "text": "pre training"}]
[{"st": 49, "ed": 52, "text": "detection and classification"}, {"st": 98, "ed": 101, "text": "detection and classification"}, {"st": 112, "ed": 115, "text": "detection and classification"}, {"st": 126, "ed": 130, "text": "deep convolutional neural networks"}, {"st": 136, "ed": 138, "text": "open access"}]
[{"st": 1, "ed": 3, "text": "conditional generative"}, {"st": 45, "ed": 47, "text": "neural networks"}, {"st": 67, "ed": 69, "text": "generated images"}, {"st": 74, "ed": 76, "text": "conditional generative"}, {"st": 88, "ed": 90, "text": "multi layered"}, {"st": 90, "ed": 92, "text": "non linearity"}, {"st": 105, "ed": 107, "text": "natural images"}, {"st": 123, "ed": 125, "text": "real images"}, {"st": 133, "ed": 135, "text": "generated data"}, {"st": 137, "ed": 139, "text": "real data"}, {"st": 151, "ed": 153, "text": "reconstructed images"}, {"st": 157, "ed": 160, "text": "quantitatively and qualitatively"}]
[{"st": 5, "ed": 7, "text": "class imbalanced"}, {"st": 21, "ed": 23, "text": "class imbalanced"}, {"st": 32, "ed": 34, "text": "majority class"}, {"st": 43, "ed": 46, "text": "convolutional neural networks"}, {"st": 54, "ed": 56, "text": "class imbalanced"}, {"st": 76, "ed": 79, "text": "convolutional neural network"}, {"st": 91, "ed": 94, "text": "convolutional neural network"}, {"st": 109, "ed": 111, "text": "proposed method"}, {"st": 122, "ed": 124, "text": "false positives"}]
[{"st": 16, "ed": 18, "text": "receptive fields"}, {"st": 105, "ed": 107, "text": "parameter sharing"}, {"st": 139, "ed": 141, "text": "theoretical analysis"}]
[{"st": 15, "ed": 17, "text": "recent years"}, {"st": 17, "ed": 21, "text": "deep convolutional neural networks"}, {"st": 26, "ed": 28, "text": "multiple tasks"}, {"st": 37, "ed": 39, "text": "visual recognition"}, {"st": 97, "ed": 99, "text": "training loss"}, {"st": 101, "ed": 103, "text": "fine tuning"}, {"st": 119, "ed": 121, "text": "vgg 16"}, {"st": 124, "ed": 127, "text": "pascal voc 2012"}, {"st": 141, "ed": 143, "text": "loss functions"}, {"st": 175, "ed": 177, "text": "https github.com"}]
[{"st": 10, "ed": 13, "text": "deep neural networks"}, {"st": 16, "ed": 18, "text": "matrix multiplications"}, {"st": 27, "ed": 30, "text": "end to end"}, {"st": 32, "ed": 34, "text": "low cost"}, {"st": 36, "ed": 38, "text": "matrix multiplications"}, {"st": 43, "ed": 45, "text": "matrix multiplications"}, {"st": 48, "ed": 52, "text": "sum product networks spns"}, {"st": 58, "ed": 60, "text": "edge weights"}, {"st": 91, "ed": 93, "text": "image classification"}, {"st": 120, "ed": 122, "text": "predictive performance"}, {"st": 132, "ed": 134, "text": "proposed framework"}, {"st": 139, "ed": 141, "text": "s matrix"}]
[{"st": 11, "ed": 13, "text": "recognition systems"}, {"st": 18, "ed": 20, "text": "deep architectures"}, {"st": 21, "ed": 23, "text": "representation learning"}, {"st": 51, "ed": 53, "text": "deep networks"}, {"st": 55, "ed": 57, "text": "global optimality"}]
[]
[{"st": 0, "ed": 2, "text": "deep learning"}, {"st": 8, "ed": 10, "text": "point clouds"}, {"st": 24, "ed": 26, "text": "point clouds"}, {"st": 27, "ed": 30, "text": "convolutional neural network"}, {"st": 44, "ed": 47, "text": "convolutional neural network"}, {"st": 51, "ed": 53, "text": "object recognition"}, {"st": 64, "ed": 66, "text": "point wise"}, {"st": 82, "ed": 85, "text": "fully convolutional network"}, {"st": 93, "ed": 95, "text": "competitive accuracy"}, {"st": 100, "ed": 102, "text": "object recognition"}]
[{"st": 9, "ed": 11, "text": "batch normalization"}, {"st": 18, "ed": 20, "text": "exploding gradient"}, {"st": 40, "ed": 42, "text": "exploding gradients"}, {"st": 65, "ed": 67, "text": "exploding gradients"}, {"st": 85, "ed": 87, "text": "significantly lower"}, {"st": 93, "ed": 95, "text": "exploding gradient"}, {"st": 119, "ed": 121, "text": "neural network"}, {"st": 123, "ed": 125, "text": "residual network"}, {"st": 134, "ed": 136, "text": "skip connections"}]
[{"st": 0, "ed": 3, "text": "recurrent neural networks"}, {"st": 5, "ed": 7, "text": "attention mechanisms"}, {"st": 19, "ed": 21, "text": "classification performance"}, {"st": 80, "ed": 82, "text": "attention mechanism"}]
[{"st": 3, "ed": 5, "text": "input samples"}, {"st": 26, "ed": 28, "text": "machine learning"}, {"st": 83, "ed": 85, "text": "similarity measure"}, {"st": 110, "ed": 112, "text": "without compromising"}, {"st": 118, "ed": 120, "text": "jointly learning"}, {"st": 202, "ed": 205, "text": "support vector machines"}, {"st": 207, "ed": 209, "text": "ridge regression"}]
[{"st": 17, "ed": 19, "text": "common space"}, {"st": 23, "ed": 25, "text": "cross modal"}, {"st": 56, "ed": 58, "text": "audio visual"}, {"st": 69, "ed": 71, "text": "cross modal"}, {"st": 81, "ed": 83, "text": "network architectures"}, {"st": 95, "ed": 97, "text": "cross modal"}, {"st": 166, "ed": 168, "text": "optical flow"}, {"st": 195, "ed": 197, "text": "cautionary tale"}]
[{"st": 28, "ed": 30, "text": "challenging task"}, {"st": 33, "ed": 35, "text": "intra class"}, {"st": 53, "ed": 56, "text": "convolutional neural network"}, {"st": 65, "ed": 68, "text": "end to end"}, {"st": 71, "ed": 73, "text": "image level"}, {"st": 104, "ed": 106, "text": "manual annotation"}, {"st": 157, "ed": 159, "text": "proposed approach"}, {"st": 167, "ed": 169, "text": "standard benchmarks"}]
[{"st": 1, "ed": 3, "text": "recent progress"}, {"st": 5, "ed": 7, "text": "machine learning"}, {"st": 9, "ed": 11, "text": "deep learning"}, {"st": 12, "ed": 14, "text": "image recognition"}, {"st": 26, "ed": 28, "text": "x ray"}, {"st": 49, "ed": 51, "text": "deep learning"}, {"st": 61, "ed": 63, "text": "lung cancer"}, {"st": 119, "ed": 121, "text": "pre processing"}]
[{"st": 0, "ed": 2, "text": "transfer learning"}, {"st": 14, "ed": 16, "text": "annotated data"}, {"st": 21, "ed": 23, "text": "visual content"}, {"st": 25, "ed": 29, "text": "convolutional neural network cnn"}, {"st": 93, "ed": 95, "text": "unified framework"}]
[{"st": 1, "ed": 3, "text": "activity recognition"}, {"st": 8, "ed": 10, "text": "time consuming"}, {"st": 19, "ed": 21, "text": "transfer learning"}, {"st": 23, "ed": 25, "text": "labeled samples"}, {"st": 27, "ed": 29, "text": "source domain"}, {"st": 40, "ed": 42, "text": "existing approaches"}, {"st": 47, "ed": 49, "text": "domain shift"}, {"st": 73, "ed": 75, "text": "cross domain"}, {"st": 75, "ed": 77, "text": "learning framework"}, {"st": 87, "ed": 89, "text": "intra class"}, {"st": 92, "ed": 94, "text": "proposed framework"}, {"st": 98, "ed": 100, "text": "transfer learning"}, {"st": 105, "ed": 107, "text": "classification accuracy"}, {"st": 108, "ed": 110, "text": "cross domain"}, {"st": 123, "ed": 125, "text": "majority voting"}, {"st": 129, "ed": 131, "text": "intra class"}, {"st": 168, "ed": 170, "text": "activity recognition"}, {"st": 181, "ed": 183, "text": "significantly outperforms"}, {"st": 190, "ed": 192, "text": "classification accuracy"}]
[{"st": 11, "ed": 13, "text": "network architectures"}, {"st": 20, "ed": 22, "text": "neural networks"}, {"st": 35, "ed": 37, "text": "training examples"}, {"st": 38, "ed": 40, "text": "achieve high"}, {"st": 40, "ed": 42, "text": "prediction accuracy"}, {"st": 48, "ed": 50, "text": "proposed method"}, {"st": 55, "ed": 57, "text": "benchmark datasets"}, {"st": 60, "ed": 62, "text": "performance gain"}, {"st": 70, "ed": 73, "text": "mnist cifar 10"}, {"st": 85, "ed": 87, "text": "training sample"}, {"st": 127, "ed": 129, "text": "training examples"}, {"st": 130, "ed": 132, "text": "achieve high"}]
[{"st": 1, "ed": 4, "text": "k nearest neighbor"}, {"st": 8, "ed": 10, "text": "conceptually simple"}, {"st": 11, "ed": 13, "text": "widely applied"}, {"st": 32, "ed": 34, "text": "optimal solution"}, {"st": 90, "ed": 92, "text": "similarity function"}, {"st": 113, "ed": 115, "text": "classification tasks"}, {"st": 121, "ed": 123, "text": "classification task"}, {"st": 125, "ed": 128, "text": "tens of thousands"}, {"st": 172, "ed": 174, "text": "significantly reduced"}]
[{"st": 3, "ed": 5, "text": "adversarial examples"}, {"st": 6, "ed": 8, "text": "deep learning"}]
[{"st": 7, "ed": 9, "text": "visual object"}, {"st": 20, "ed": 22, "text": "meta learning"}, {"st": 22, "ed": 24, "text": "based method"}, {"st": 28, "ed": 30, "text": "deep networks"}, {"st": 37, "ed": 39, "text": "meta learning"}, {"st": 45, "ed": 47, "text": "deep networks"}, {"st": 72, "ed": 74, "text": "future frames"}, {"st": 75, "ed": 77, "text": "avoid overfitting"}, {"st": 96, "ed": 98, "text": "meta learning"}, {"st": 129, "ed": 131, "text": "standard benchmarks"}]
[{"st": 0, "ed": 3, "text": "deep neural networks"}, {"st": 6, "ed": 8, "text": "auto encoders"}, {"st": 26, "ed": 28, "text": "key challenge"}, {"st": 53, "ed": 55, "text": "latent image"}, {"st": 79, "ed": 81, "text": "image compression"}, {"st": 84, "ed": 86, "text": "main idea"}, {"st": 94, "ed": 96, "text": "latent representation"}, {"st": 107, "ed": 109, "text": "conditional probability"}, {"st": 112, "ed": 114, "text": "latent distribution"}, {"st": 121, "ed": 123, "text": "auto encoder"}, {"st": 159, "ed": 161, "text": "approach yields"}, {"st": 166, "ed": 168, "text": "image compression"}]
[{"st": 4, "ed": 6, "text": "machine learning"}, {"st": 25, "ed": 27, "text": "pre processing"}, {"st": 122, "ed": 124, "text": "high dimensional"}, {"st": 130, "ed": 132, "text": "latent space"}, {"st": 138, "ed": 140, "text": "latent space"}, {"st": 168, "ed": 170, "text": "linear model"}, {"st": 173, "ed": 175, "text": "latent spaces"}, {"st": 199, "ed": 202, "text": "qualitative and quantitative"}, {"st": 206, "ed": 208, "text": "proposed method"}]
[{"st": 11, "ed": 14, "text": "deep neural networks"}, {"st": 24, "ed": 26, "text": "fully connected"}, {"st": 30, "ed": 32, "text": "main contribution"}, {"st": 64, "ed": 66, "text": "sectional curvature"}, {"st": 75, "ed": 77, "text": "geometric properties"}, {"st": 96, "ed": 98, "text": "features extracted"}]
[{"st": 3, "ed": 5, "text": "computer vision"}, {"st": 20, "ed": 22, "text": "image level"}, {"st": 85, "ed": 87, "text": "convolutional filters"}, {"st": 94, "ed": 96, "text": "differential equations"}, {"st": 104, "ed": 106, "text": "unlike traditional"}, {"st": 106, "ed": 108, "text": "convolutional networks"}, {"st": 122, "ed": 124, "text": "feature learning"}, {"st": 124, "ed": 126, "text": "takes place"}, {"st": 177, "ed": 179, "text": "computer vision"}]
[{"st": 4, "ed": 8, "text": "convolutional neural network cnn"}, {"st": 11, "ed": 14, "text": "multi task learning"}, {"st": 18, "ed": 20, "text": "related tasks"}, {"st": 53, "ed": 55, "text": "convolutional layer"}, {"st": 58, "ed": 60, "text": "task specific"}, {"st": 72, "ed": 74, "text": "final performance"}, {"st": 127, "ed": 129, "text": "dimensionality reduction"}, {"st": 135, "ed": 137, "text": "batch normalization"}, {"st": 138, "ed": 140, "text": "weight decay"}, {"st": 150, "ed": 152, "text": "dimensionality reduction"}, {"st": 175, "ed": 177, "text": "network structures"}, {"st": 183, "ed": 185, "text": "promising performance"}]
[{"st": 12, "ed": 14, "text": "visual recognition"}, {"st": 47, "ed": 49, "text": "visual recognition"}, {"st": 94, "ed": 96, "text": "neural network"}, {"st": 99, "ed": 101, "text": "boolean algebra"}, {"st": 161, "ed": 163, "text": "visual concepts"}, {"st": 170, "ed": 172, "text": "visual recognition"}, {"st": 177, "ed": 179, "text": "qualitative analysis"}]
[{"st": 8, "ed": 10, "text": "image data"}, {"st": 28, "ed": 30, "text": "fully differentiable"}, {"st": 32, "ed": 34, "text": "clustering approach"}, {"st": 40, "ed": 43, "text": "end to end"}, {"st": 86, "ed": 88, "text": "feature representation"}, {"st": 98, "ed": 100, "text": "promising results"}, {"st": 103, "ed": 105, "text": "computer vision"}]
[{"st": 6, "ed": 10, "text": "deep convolutional neural network"}, {"st": 41, "ed": 43, "text": "domain transfer"}, {"st": 51, "ed": 53, "text": "pre trained"}, {"st": 57, "ed": 59, "text": "imagenet dataset"}, {"st": 81, "ed": 83, "text": "domain transfer"}, {"st": 90, "ed": 92, "text": "deep learning"}, {"st": 110, "ed": 113, "text": "deep neural network"}, {"st": 115, "ed": 117, "text": "proposed method"}, {"st": 131, "ed": 133, "text": "image dataset"}]
[{"st": 19, "ed": 21, "text": "challenging problems"}, {"st": 22, "ed": 24, "text": "computer vision"}, {"st": 25, "ed": 27, "text": "pattern recognition"}, {"st": 39, "ed": 42, "text": "bag of words"}, {"st": 45, "ed": 49, "text": "convolutional neural networks cnns"}, {"st": 51, "ed": 53, "text": "extensively studied"}, {"st": 102, "ed": 104, "text": "recent advances"}, {"st": 115, "ed": 117, "text": "based methods"}, {"st": 122, "ed": 124, "text": "benchmark datasets"}, {"st": 145, "ed": 148, "text": "directions for future"}]
[{"st": 14, "ed": 16, "text": "accurately predict"}, {"st": 43, "ed": 46, "text": "real world data"}, {"st": 57, "ed": 59, "text": "demonstrate empirically"}, {"st": 93, "ed": 95, "text": "simulated data"}, {"st": 116, "ed": 118, "text": "predictive model"}, {"st": 131, "ed": 133, "text": "importance sampling"}, {"st": 151, "ed": 153, "text": "neural network"}, {"st": 153, "ed": 155, "text": "domain adaptation"}, {"st": 161, "ed": 163, "text": "simulated data"}, {"st": 173, "ed": 175, "text": "simulated data"}]
[{"st": 1, "ed": 3, "text": "recent years"}, {"st": 21, "ed": 24, "text": "large data sets"}, {"st": 24, "ed": 27, "text": "corrupted by noise"}, {"st": 42, "ed": 45, "text": "low dimensional manifold"}, {"st": 115, "ed": 118, "text": "low dimensional manifold"}, {"st": 121, "ed": 123, "text": "convergence rate"}, {"st": 145, "ed": 147, "text": "image data"}, {"st": 148, "ed": 151, "text": "corrupted by noise"}, {"st": 184, "ed": 186, "text": "image data"}]
[{"st": 28, "ed": 30, "text": "hand crafted"}, {"st": 45, "ed": 47, "text": "expert knowledge"}, {"st": 49, "ed": 51, "text": "hyper parameter"}]
[{"st": 12, "ed": 14, "text": "image recognition"}, {"st": 40, "ed": 42, "text": "city university"}, {"st": 86, "ed": 88, "text": "pre trained"}, {"st": 89, "ed": 91, "text": "artificial intelligence"}, {"st": 153, "ed": 155, "text": "image recognition"}]
[{"st": 36, "ed": 38, "text": "inverse problem"}, {"st": 56, "ed": 58, "text": "non invasive"}, {"st": 66, "ed": 68, "text": "inverse problem"}, {"st": 72, "ed": 74, "text": "inverse problem"}, {"st": 97, "ed": 100, "text": "artificial neural network"}, {"st": 113, "ed": 115, "text": "search space"}, {"st": 133, "ed": 135, "text": "real world"}]
[{"st": 0, "ed": 4, "text": "convolutional neural network cnn"}, {"st": 5, "ed": 7, "text": "machine learning"}, {"st": 12, "ed": 14, "text": "feature extraction"}, {"st": 15, "ed": 17, "text": "image recognition"}, {"st": 56, "ed": 58, "text": "current approaches"}, {"st": 60, "ed": 62, "text": "low resolution"}, {"st": 113, "ed": 116, "text": "space filling curve"}, {"st": 156, "ed": 158, "text": "experiments demonstrate"}, {"st": 235, "ed": 237, "text": "structural biology"}, {"st": 237, "ed": 239, "text": "classification task"}, {"st": 269, "ed": 271, "text": "dimensionality reduction"}, {"st": 279, "ed": 281, "text": "deep learning"}]
[{"st": 31, "ed": 33, "text": "labeled dataset"}, {"st": 40, "ed": 42, "text": "training samples"}, {"st": 47, "ed": 49, "text": "unlabeled data"}, {"st": 52, "ed": 54, "text": "semi supervised"}, {"st": 71, "ed": 73, "text": "visual recognition"}, {"st": 82, "ed": 84, "text": "labeled datasets"}, {"st": 144, "ed": 146, "text": "cosine similarity"}, {"st": 157, "ed": 159, "text": "fully convolutional"}, {"st": 168, "ed": 170, "text": "creative commons"}, {"st": 184, "ed": 187, "text": "approximate nearest neighbor"}, {"st": 197, "ed": 199, "text": "labeled instances"}, {"st": 207, "ed": 209, "text": "significantly improve"}, {"st": 209, "ed": 211, "text": "object categorization"}, {"st": 216, "ed": 218, "text": "ms coco"}]
[{"st": 53, "ed": 55, "text": "approach called"}, {"st": 82, "ed": 84, "text": "cifar 10"}]
[{"st": 8, "ed": 10, "text": "neural networks"}, {"st": 11, "ed": 13, "text": "resource constrained"}, {"st": 73, "ed": 75, "text": "approximation error"}]
[{"st": 4, "ed": 6, "text": "weight pruning"}, {"st": 8, "ed": 11, "text": "deep neural networks"}, {"st": 14, "ed": 20, "text": "alternating direction method of multipliers admm"}, {"st": 25, "ed": 27, "text": "weight pruning"}, {"st": 33, "ed": 35, "text": "nonconvex optimization"}, {"st": 54, "ed": 56, "text": "weight pruning"}, {"st": 59, "ed": 61, "text": "computational efficiency"}, {"st": 68, "ed": 70, "text": "compression ratio"}, {"st": 83, "ed": 85, "text": "faster convergence"}]
[{"st": 17, "ed": 20, "text": "deep neural networks"}, {"st": 30, "ed": 32, "text": "image segmentation"}, {"st": 41, "ed": 43, "text": "feature maps"}, {"st": 56, "ed": 58, "text": "object category"}, {"st": 66, "ed": 68, "text": "intra class"}, {"st": 82, "ed": 84, "text": "feature maps"}, {"st": 156, "ed": 159, "text": "pascal voc 2012"}, {"st": 163, "ed": 165, "text": "improved results"}, {"st": 168, "ed": 170, "text": "proposed approach"}]
[{"st": 1, "ed": 4, "text": "convolutional neural network"}, {"st": 5, "ed": 7, "text": "image classification"}, {"st": 21, "ed": 23, "text": "visual cortex"}, {"st": 37, "ed": 40, "text": "point of view"}, {"st": 77, "ed": 79, "text": "independent variable"}, {"st": 85, "ed": 88, "text": "ordinary differential equations"}, {"st": 112, "ed": 114, "text": "convolutional networks"}, {"st": 136, "ed": 139, "text": "convolutional neural network"}, {"st": 146, "ed": 148, "text": "convolutional networks"}, {"st": 164, "ed": 166, "text": "benchmark datasets"}, {"st": 201, "ed": 204, "text": "convolutional neural networks"}, {"st": 205, "ed": 207, "text": "image classification"}]
[{"st": 11, "ed": 13, "text": "deep learning"}, {"st": 30, "ed": 35, "text": "deep convolutional neural network cnn"}, {"st": 105, "ed": 107, "text": "training data"}, {"st": 137, "ed": 139, "text": "classification accuracies"}, {"st": 156, "ed": 158, "text": "classification accuracies"}, {"st": 161, "ed": 163, "text": "training instances"}, {"st": 209, "ed": 211, "text": "classification accuracies"}, {"st": 249, "ed": 251, "text": "deep learning"}, {"st": 258, "ed": 260, "text": "prediction tasks"}]
[{"st": 0, "ed": 2, "text": "generative models"}, {"st": 4, "ed": 7, "text": "deep neural networks"}, {"st": 12, "ed": 14, "text": "natural image"}, {"st": 30, "ed": 32, "text": "log likelihood"}, {"st": 58, "ed": 60, "text": "inverse problems"}, {"st": 83, "ed": 85, "text": "auto encoder"}]
[{"st": 3, "ed": 7, "text": "deep neural network dnn"}, {"st": 8, "ed": 10, "text": "face recognition"}, {"st": 25, "ed": 27, "text": "adversarial examples"}, {"st": 39, "ed": 41, "text": "recent studies"}, {"st": 53, "ed": 55, "text": "real world"}, {"st": 128, "ed": 130, "text": "resource constrained"}, {"st": 201, "ed": 203, "text": "image representations"}, {"st": 213, "ed": 215, "text": "input images"}, {"st": 233, "ed": 235, "text": "significantly improve"}]
[{"st": 103, "ed": 106, "text": "ann arbor michigan"}, {"st": 107, "ed": 109, "text": "k means"}, {"st": 110, "ed": 114, "text": "dynamic time warping dtw"}]
[{"st": 0, "ed": 2, "text": "weakly supervised"}, {"st": 12, "ed": 15, "text": "deep neural network"}, {"st": 17, "ed": 19, "text": "attention maps"}, {"st": 24, "ed": 26, "text": "attention maps"}, {"st": 35, "ed": 37, "text": "object localization"}, {"st": 49, "ed": 51, "text": "previous approaches"}, {"st": 54, "ed": 56, "text": "attention maps"}, {"st": 61, "ed": 63, "text": "attention maps"}, {"st": 70, "ed": 73, "text": "end to end"}, {"st": 130, "ed": 133, "text": "pascal voc 2012"}, {"st": 139, "ed": 141, "text": "proposed framework"}, {"st": 162, "ed": 165, "text": "under mild assumptions"}, {"st": 177, "ed": 179, "text": "weakly supervised"}]
[{"st": 9, "ed": 11, "text": "optic nerve"}, {"st": 28, "ed": 31, "text": "optical coherence tomography"}, {"st": 48, "ed": 50, "text": "deep learning"}, {"st": 72, "ed": 74, "text": "contextual information"}]
[{"st": 18, "ed": 20, "text": "decision making"}, {"st": 38, "ed": 40, "text": "machine learning"}, {"st": 41, "ed": 43, "text": "computer vision"}, {"st": 44, "ed": 46, "text": "remote sensing"}]
[{"st": 10, "ed": 12, "text": "computed tomography"}, {"st": 40, "ed": 42, "text": "machine learning"}, {"st": 42, "ed": 44, "text": "based method"}, {"st": 46, "ed": 48, "text": "multi step"}, {"st": 79, "ed": 81, "text": "x ray"}, {"st": 93, "ed": 95, "text": "proposed approach"}]
[{"st": 8, "ed": 10, "text": "neural network"}, {"st": 19, "ed": 21, "text": "recent methods"}, {"st": 39, "ed": 41, "text": "ad hoc"}, {"st": 71, "ed": 73, "text": "loss functions"}, {"st": 87, "ed": 89, "text": "transfer learning"}, {"st": 95, "ed": 97, "text": "transfer learning"}, {"st": 106, "ed": 108, "text": "image datasets"}]
[{"st": 0, "ed": 3, "text": "unsupervised domain adaptation"}, {"st": 20, "ed": 22, "text": "labeled data"}, {"st": 29, "ed": 31, "text": "multiple sources"}, {"st": 96, "ed": 98, "text": "theoretical results"}, {"st": 118, "ed": 121, "text": "unsupervised domain adaptation"}, {"st": 148, "ed": 150, "text": "multiple source"}, {"st": 216, "ed": 218, "text": "domain adaptation"}]
[{"st": 13, "ed": 15, "text": "high dimensional"}, {"st": 19, "ed": 21, "text": "clustering algorithm"}, {"st": 24, "ed": 26, "text": "dimensionality reduction"}, {"st": 35, "ed": 37, "text": "lower dimensional"}, {"st": 65, "ed": 67, "text": "prior knowledge"}, {"st": 71, "ed": 73, "text": "ground truth"}, {"st": 76, "ed": 78, "text": "dimensionality reduction"}, {"st": 106, "ed": 108, "text": "multiple domains"}, {"st": 118, "ed": 120, "text": "clustering schemes"}, {"st": 121, "ed": 123, "text": "recent methods"}]
[{"st": 10, "ed": 12, "text": "internal representations"}, {"st": 14, "ed": 16, "text": "low level"}, {"st": 28, "ed": 30, "text": "existing methods"}, {"st": 110, "ed": 112, "text": "conduct experiments"}, {"st": 119, "ed": 121, "text": "features extracted"}, {"st": 126, "ed": 130, "text": "deep convolutional neural networks"}, {"st": 181, "ed": 183, "text": "future directions"}]
[{"st": 31, "ed": 35, "text": "generative adversarial network gan"}, {"st": 38, "ed": 40, "text": "spatial transformer"}, {"st": 48, "ed": 50, "text": "spatial transformer"}, {"st": 80, "ed": 82, "text": "training strategy"}]
[{"st": 0, "ed": 2, "text": "feed forward"}, {"st": 10, "ed": 12, "text": "loss functions"}, {"st": 18, "ed": 20, "text": "generated image"}, {"st": 28, "ed": 30, "text": "loss functions"}, {"st": 60, "ed": 62, "text": "loss function"}]
[{"st": 15, "ed": 17, "text": "real world"}, {"st": 30, "ed": 35, "text": "deep convolutional neural network cnn"}, {"st": 37, "ed": 39, "text": "residual network"}, {"st": 46, "ed": 48, "text": "image classification"}, {"st": 86, "ed": 88, "text": "stereo camera"}, {"st": 123, "ed": 126, "text": "support vector machine"}, {"st": 220, "ed": 222, "text": "false positive"}]
[{"st": 65, "ed": 69, "text": "convolutional neural network cnn"}, {"st": 71, "ed": 74, "text": "end to end"}, {"st": 105, "ed": 107, "text": "training data"}, {"st": 136, "ed": 138, "text": "land cover"}, {"st": 153, "ed": 155, "text": "quantitative evaluations"}, {"st": 159, "ed": 161, "text": "ground truth"}]
[{"st": 3, "ed": 5, "text": "convolutional network"}, {"st": 74, "ed": 76, "text": "bounding box"}]
[{"st": 0, "ed": 2, "text": "feature learning"}, {"st": 3, "ed": 5, "text": "point clouds"}, {"st": 7, "ed": 9, "text": "great promise"}, {"st": 16, "ed": 18, "text": "deep learning"}, {"st": 61, "ed": 63, "text": "spectral graph"}, {"st": 85, "ed": 87, "text": "nearest neighbor"}, {"st": 104, "ed": 106, "text": "max pooling"}, {"st": 140, "ed": 142, "text": "extensive experiments"}, {"st": 156, "ed": 158, "text": "point set"}]
[{"st": 0, "ed": 2, "text": "deep learning"}, {"st": 5, "ed": 7, "text": "de facto"}, {"st": 14, "ed": 16, "text": "problems including"}, {"st": 35, "ed": 37, "text": "deep learning"}, {"st": 44, "ed": 46, "text": "previous research"}, {"st": 52, "ed": 54, "text": "deep learning"}, {"st": 67, "ed": 69, "text": "deep learning"}, {"st": 81, "ed": 84, "text": "convolutional neural network"}, {"st": 91, "ed": 93, "text": "input image"}, {"st": 105, "ed": 107, "text": "neural network"}, {"st": 123, "ed": 125, "text": "mnist dataset"}]
[{"st": 35, "ed": 37, "text": "magnetic resonance"}, {"st": 37, "ed": 39, "text": "mr images"}, {"st": 51, "ed": 53, "text": "deep learning"}, {"st": 61, "ed": 63, "text": "multi modal"}, {"st": 77, "ed": 80, "text": "fully convolutional network"}, {"st": 92, "ed": 94, "text": "network architecture"}, {"st": 104, "ed": 106, "text": "multi modal"}, {"st": 106, "ed": 108, "text": "mr images"}, {"st": 124, "ed": 126, "text": "false negative"}, {"st": 130, "ed": 132, "text": "false negatives"}]
[{"st": 11, "ed": 13, "text": "optical microscopy"}, {"st": 21, "ed": 24, "text": "point spread function"}, {"st": 43, "ed": 45, "text": "computationally tractable"}, {"st": 49, "ed": 52, "text": "convolutional neural network"}]
[{"st": 4, "ed": 7, "text": "convolutional neural networks"}, {"st": 28, "ed": 30, "text": "residual learning"}, {"st": 37, "ed": 39, "text": "attention mechanism"}, {"st": 58, "ed": 60, "text": "sampling strategy"}, {"st": 63, "ed": 65, "text": "receptive fields"}, {"st": 79, "ed": 81, "text": "feature map"}, {"st": 112, "ed": 116, "text": "trained end to end"}, {"st": 132, "ed": 134, "text": "prediction tasks"}, {"st": 135, "ed": 137, "text": "object detection"}, {"st": 163, "ed": 165, "text": "flow estimation"}]
[{"st": 4, "ed": 8, "text": "deep convolutional neural networks"}, {"st": 14, "ed": 16, "text": "computer vision"}, {"st": 39, "ed": 42, "text": "massive amounts of"}, {"st": 43, "ed": 45, "text": "labeled data"}, {"st": 56, "ed": 58, "text": "feature learning"}, {"st": 62, "ed": 64, "text": "manual annotation"}, {"st": 78, "ed": 80, "text": "visual data"}, {"st": 91, "ed": 93, "text": "image features"}, {"st": 115, "ed": 118, "text": "qualitatively and quantitatively"}, {"st": 141, "ed": 144, "text": "unsupervised feature learning"}, {"st": 174, "ed": 177, "text": "unsupervised representation learning"}, {"st": 190, "ed": 193, "text": "pascal voc 2007"}, {"st": 193, "ed": 195, "text": "detection task"}, {"st": 197, "ed": 199, "text": "pre trained"}, {"st": 208, "ed": 210, "text": "unsupervised methods"}, {"st": 233, "ed": 235, "text": "learned features"}, {"st": 241, "ed": 243, "text": "imagenet classification"}, {"st": 248, "ed": 250, "text": "cifar 10"}, {"st": 262, "ed": 264, "text": "https github.com"}]
[{"st": 9, "ed": 12, "text": "convolutional neural networks"}, {"st": 17, "ed": 19, "text": "input signal"}, {"st": 29, "ed": 31, "text": "fine tuned"}, {"st": 35, "ed": 37, "text": "pre trained"}, {"st": 50, "ed": 52, "text": "fine tuned"}, {"st": 60, "ed": 62, "text": "resnet 50"}, {"st": 76, "ed": 78, "text": "input space"}, {"st": 114, "ed": 116, "text": "total order"}, {"st": 125, "ed": 127, "text": "input signal"}, {"st": 133, "ed": 135, "text": "classification accuracy"}, {"st": 149, "ed": 151, "text": "mutual information"}, {"st": 160, "ed": 162, "text": "fine tuned"}, {"st": 172, "ed": 174, "text": "neural network"}, {"st": 204, "ed": 206, "text": "input signal"}]
[{"st": 0, "ed": 2, "text": "batch normalization"}, {"st": 11, "ed": 13, "text": "deep learning"}, {"st": 33, "ed": 35, "text": "batch size"}, {"st": 56, "ed": 58, "text": "computer vision"}, {"st": 58, "ed": 60, "text": "tasks including"}, {"st": 109, "ed": 111, "text": "batch sizes"}, {"st": 124, "ed": 126, "text": "resnet 50"}, {"st": 141, "ed": 143, "text": "batch size"}, {"st": 148, "ed": 150, "text": "batch sizes"}, {"st": 168, "ed": 170, "text": "pre training"}, {"st": 184, "ed": 186, "text": "object detection"}]
[{"st": 2, "ed": 4, "text": "computational mechanics"}, {"st": 4, "ed": 6, "text": "structural analysis"}, {"st": 9, "ed": 11, "text": "cellular automata"}, {"st": 19, "ed": 21, "text": "change point"}, {"st": 23, "ed": 25, "text": "time series"}, {"st": 69, "ed": 71, "text": "regular language"}, {"st": 79, "ed": 81, "text": "ideal solution"}, {"st": 86, "ed": 88, "text": "worst case"}, {"st": 160, "ed": 162, "text": "finite state"}]
[{"st": 72, "ed": 74, "text": "real valued"}, {"st": 83, "ed": 85, "text": "real valued"}, {"st": 91, "ed": 93, "text": "continuum hypothesis"}, {"st": 94, "ed": 96, "text": "pure mathematics"}, {"st": 127, "ed": 129, "text": "speech production"}]
[{"st": 2, "ed": 4, "text": "based approach"}, {"st": 14, "ed": 16, "text": "finite state"}]
[{"st": 14, "ed": 16, "text": "multi agent"}, {"st": 95, "ed": 97, "text": "multi agent"}]
[]
[{"st": 5, "ed": 8, "text": "artificial neural network"}, {"st": 32, "ed": 34, "text": "neural networks"}]
[{"st": 5, "ed": 8, "text": "artificial neural network"}, {"st": 39, "ed": 41, "text": "neural network"}, {"st": 76, "ed": 78, "text": "anton chekhov"}]
[{"st": 4, "ed": 6, "text": "recently proposed"}, {"st": 9, "ed": 11, "text": "neural network"}, {"st": 29, "ed": 31, "text": "neural network"}, {"st": 65, "ed": 67, "text": "neural network"}, {"st": 80, "ed": 82, "text": "neural network"}]
[{"st": 16, "ed": 18, "text": "natural language"}]
[{"st": 13, "ed": 15, "text": "question answering"}, {"st": 66, "ed": 68, "text": "predicate logic"}]
[{"st": 0, "ed": 3, "text": "dynamic bayesian networks"}, {"st": 18, "ed": 20, "text": "existing algorithms"}, {"st": 22, "ed": 25, "text": "learning and inference"}, {"st": 40, "ed": 42, "text": "natural language"}, {"st": 49, "ed": 51, "text": "information extraction"}, {"st": 71, "ed": 74, "text": "named entity recognition"}, {"st": 86, "ed": 88, "text": "method outperforms"}, {"st": 88, "ed": 90, "text": "previously published"}]
[{"st": 11, "ed": 13, "text": "natural language"}, {"st": 20, "ed": 22, "text": "multi dimensional"}, {"st": 22, "ed": 24, "text": "type theory"}, {"st": 123, "ed": 125, "text": "sequent calculus"}]
[{"st": 26, "ed": 28, "text": "case study"}, {"st": 62, "ed": 64, "text": "sentence level"}, {"st": 88, "ed": 90, "text": "pre processing"}, {"st": 213, "ed": 215, "text": "case study"}]
[{"st": 8, "ed": 10, "text": "domain knowledge"}, {"st": 54, "ed": 56, "text": "domain knowledge"}, {"st": 68, "ed": 70, "text": "domain knowledge"}]
[]
[{"st": 6, "ed": 8, "text": "parameter free"}, {"st": 9, "ed": 11, "text": "distance measures"}, {"st": 15, "ed": 17, "text": "pattern recognition"}, {"st": 135, "ed": 137, "text": "distance measures"}, {"st": 182, "ed": 184, "text": "distance measures"}, {"st": 187, "ed": 189, "text": "distance measures"}]
[]
[{"st": 10, "ed": 12, "text": "functional analysis"}, {"st": 60, "ed": 62, "text": "self organizing"}, {"st": 83, "ed": 85, "text": "proposed approach"}]
[{"st": 16, "ed": 18, "text": "natural language"}, {"st": 28, "ed": 30, "text": "natural language"}, {"st": 50, "ed": 52, "text": "without compromising"}, {"st": 72, "ed": 74, "text": "real life"}, {"st": 74, "ed": 76, "text": "natural language"}]
[{"st": 7, "ed": 9, "text": "textual entailment"}, {"st": 20, "ed": 22, "text": "web based"}, {"st": 27, "ed": 29, "text": "natural language"}, {"st": 35, "ed": 37, "text": "textual entailment"}, {"st": 94, "ed": 96, "text": "pattern recognition"}, {"st": 103, "ed": 105, "text": "space complexity"}]
[{"st": 98, "ed": 100, "text": "natural language"}, {"st": 174, "ed": 177, "text": "open source software"}]
[{"st": 2, "ed": 4, "text": "topic model"}, {"st": 47, "ed": 49, "text": "latent topic"}, {"st": 53, "ed": 55, "text": "level features"}, {"st": 66, "ed": 68, "text": "latent topics"}, {"st": 70, "ed": 72, "text": "topic models"}, {"st": 82, "ed": 84, "text": "parse tree"}, {"st": 96, "ed": 98, "text": "latent state"}, {"st": 129, "ed": 131, "text": "posterior inference"}, {"st": 138, "ed": 141, "text": "qualitative and quantitative"}, {"st": 144, "ed": 146, "text": "synthetic data"}, {"st": 158, "ed": 160, "text": "predictive model"}]
[{"st": 10, "ed": 12, "text": "speech signals"}, {"st": 86, "ed": 88, "text": "machine learning"}]
[{"st": 129, "ed": 131, "text": "english language"}]
[{"st": 25, "ed": 27, "text": "answer set"}, {"st": 46, "ed": 48, "text": "lambda calculus"}, {"st": 48, "ed": 50, "text": "based approach"}]
[]
[{"st": 1, "ed": 3, "text": "article presents"}, {"st": 50, "ed": 52, "text": "word order"}]
[{"st": 34, "ed": 36, "text": "topic modeling"}]
[{"st": 0, "ed": 2, "text": "text mining"}, {"st": 6, "ed": 8, "text": "web 2.0"}, {"st": 20, "ed": 22, "text": "text mining"}, {"st": 26, "ed": 28, "text": "text mining"}, {"st": 35, "ed": 37, "text": "natural language"}, {"st": 40, "ed": 42, "text": "machine learning"}, {"st": 46, "ed": 48, "text": "text mining"}, {"st": 73, "ed": 75, "text": "text mining"}, {"st": 104, "ed": 106, "text": "standard datasets"}, {"st": 113, "ed": 115, "text": "standard datasets"}, {"st": 118, "ed": 120, "text": "text mining"}, {"st": 138, "ed": 140, "text": "web 2.0"}, {"st": 202, "ed": 204, "text": "text mining"}]
[{"st": 11, "ed": 13, "text": "experiments conducted"}, {"st": 63, "ed": 65, "text": "natural language"}]
[{"st": 46, "ed": 48, "text": "task oriented"}, {"st": 61, "ed": 63, "text": "lexical analysis"}]
[{"st": 125, "ed": 127, "text": "artificial intelligence"}]
[{"st": 28, "ed": 31, "text": "words and phrases"}]
[{"st": 14, "ed": 16, "text": "feature based"}, {"st": 30, "ed": 32, "text": "feature based"}]
[{"st": 14, "ed": 16, "text": "distributional semantics"}, {"st": 64, "ed": 66, "text": "finite dimensional"}, {"st": 159, "ed": 161, "text": "inner product"}]
[{"st": 17, "ed": 19, "text": "inner product"}, {"st": 53, "ed": 55, "text": "post processing"}, {"st": 72, "ed": 74, "text": "text analysis"}]
[{"st": 0, "ed": 2, "text": "financial statements"}, {"st": 24, "ed": 27, "text": "qualitative and quantitative"}, {"st": 49, "ed": 52, "text": "conditional random field"}]
[{"st": 0, "ed": 2, "text": "concept map"}, {"st": 19, "ed": 21, "text": "knowledge management"}, {"st": 61, "ed": 63, "text": "concept map"}, {"st": 82, "ed": 84, "text": "semi automatic"}, {"st": 95, "ed": 97, "text": "concept map"}, {"st": 122, "ed": 124, "text": "croatian language"}, {"st": 128, "ed": 130, "text": "proposed method"}, {"st": 133, "ed": 135, "text": "data mining"}, {"st": 141, "ed": 143, "text": "minor adjustments"}, {"st": 150, "ed": 152, "text": "concept map"}]
[{"st": 133, "ed": 135, "text": "lexical semantics"}, {"st": 147, "ed": 149, "text": "framework called"}, {"st": 206, "ed": 208, "text": "lambda calculus"}]
[]
[{"st": 24, "ed": 26, "text": "real world"}]
[{"st": 1, "ed": 3, "text": "categorical compositional"}, {"st": 34, "ed": 36, "text": "theoretical framework"}, {"st": 92, "ed": 94, "text": "additional information"}, {"st": 104, "ed": 106, "text": "finite dimensional"}, {"st": 116, "ed": 118, "text": "distributional semantics"}]
[{"st": 1, "ed": 3, "text": "open domain"}]
[{"st": 3, "ed": 5, "text": "natural language"}, {"st": 29, "ed": 31, "text": "cognitive linguistics"}, {"st": 39, "ed": 41, "text": "construction grammar"}, {"st": 52, "ed": 54, "text": "fine grained"}, {"st": 100, "ed": 103, "text": "proof of concept"}]
[{"st": 163, "ed": 165, "text": "fock space"}]
[{"st": 8, "ed": 10, "text": "cognitive science"}]
[{"st": 12, "ed": 14, "text": "qa systems"}, {"st": 19, "ed": 21, "text": "complex questions"}, {"st": 181, "ed": 183, "text": "f measure"}, {"st": 233, "ed": 235, "text": "f measure"}]
[{"st": 16, "ed": 18, "text": "computational linguistics"}, {"st": 87, "ed": 89, "text": "category theory"}]
[{"st": 8, "ed": 10, "text": "quantum information"}, {"st": 30, "ed": 32, "text": "natural language"}]
[{"st": 33, "ed": 35, "text": "machine learning"}, {"st": 36, "ed": 38, "text": "data mining"}, {"st": 68, "ed": 70, "text": "natural language"}, {"st": 88, "ed": 90, "text": "world knowledge"}, {"st": 91, "ed": 93, "text": "empirical evaluation"}]
[{"st": 53, "ed": 55, "text": "drug metabolism"}, {"st": 56, "ed": 58, "text": "signal transduction"}, {"st": 125, "ed": 127, "text": "answer questions"}, {"st": 130, "ed": 132, "text": "existing models"}, {"st": 199, "ed": 201, "text": "main goal"}, {"st": 273, "ed": 275, "text": "real world"}]
[{"st": 9, "ed": 11, "text": "large scale"}, {"st": 12, "ed": 14, "text": "text classification"}, {"st": 22, "ed": 24, "text": "generative models"}, {"st": 29, "ed": 31, "text": "base classifiers"}, {"st": 45, "ed": 47, "text": "pre processing"}, {"st": 50, "ed": 52, "text": "tf idf"}, {"st": 64, "ed": 66, "text": "random search"}, {"st": 100, "ed": 102, "text": "base classifier"}, {"st": 169, "ed": 171, "text": "open source"}, {"st": 173, "ed": 175, "text": "gnu gpl"}]
[{"st": 2, "ed": 4, "text": "quantum mechanics"}, {"st": 5, "ed": 7, "text": "corpus linguistics"}, {"st": 47, "ed": 49, "text": "natural language"}, {"st": 138, "ed": 140, "text": "machine learning"}]
[{"st": 7, "ed": 9, "text": "social media"}, {"st": 10, "ed": 12, "text": "instant messaging"}, {"st": 80, "ed": 82, "text": "real world"}, {"st": 95, "ed": 97, "text": "social media"}, {"st": 106, "ed": 108, "text": "machine learning"}]
[{"st": 0, "ed": 3, "text": "electronic health records"}, {"st": 15, "ed": 17, "text": "structured data"}, {"st": 25, "ed": 27, "text": "unstructured data"}, {"st": 64, "ed": 66, "text": "empirical study"}, {"st": 73, "ed": 75, "text": "structured data"}, {"st": 86, "ed": 88, "text": "clinical trials"}, {"st": 89, "ed": 92, "text": "chronic lymphocytic leukemia"}, {"st": 96, "ed": 98, "text": "unstructured data"}, {"st": 112, "ed": 114, "text": "prostate cancer"}]
[{"st": 71, "ed": 74, "text": "latent semantic analysis"}, {"st": 142, "ed": 144, "text": "f measure"}]
[{"st": 7, "ed": 9, "text": "dialogue systems"}, {"st": 19, "ed": 21, "text": "word level"}, {"st": 23, "ed": 25, "text": "reinforcement learning"}, {"st": 55, "ed": 57, "text": "time consuming"}, {"st": 67, "ed": 69, "text": "turn based"}, {"st": 127, "ed": 130, "text": "question answer pairs"}, {"st": 168, "ed": 170, "text": "machine learning"}]
[{"st": 111, "ed": 113, "text": "improve performance"}, {"st": 122, "ed": 124, "text": "domain specific"}]
[{"st": 38, "ed": 40, "text": "natural language"}, {"st": 70, "ed": 72, "text": "large scale"}, {"st": 72, "ed": 74, "text": "text processing"}, {"st": 77, "ed": 79, "text": "statistical inference"}, {"st": 85, "ed": 87, "text": "front end"}, {"st": 130, "ed": 132, "text": "great potential"}]
[{"st": 12, "ed": 14, "text": "web based"}, {"st": 40, "ed": 42, "text": "web based"}, {"st": 42, "ed": 44, "text": "quantitative evaluation"}, {"st": 78, "ed": 80, "text": "search engine"}, {"st": 100, "ed": 102, "text": "main idea"}, {"st": 124, "ed": 126, "text": "search engines"}, {"st": 188, "ed": 190, "text": "sentiment analysis"}]
[{"st": 55, "ed": 57, "text": "higher levels"}]
[]
[{"st": 13, "ed": 15, "text": "textual entailment"}, {"st": 64, "ed": 66, "text": "text analysis"}, {"st": 88, "ed": 90, "text": "statistical models"}, {"st": 115, "ed": 117, "text": "automated reasoning"}, {"st": 125, "ed": 127, "text": "background knowledge"}, {"st": 157, "ed": 159, "text": "common sense"}, {"st": 164, "ed": 166, "text": "fine grained"}]
[{"st": 0, "ed": 2, "text": "topic models"}, {"st": 23, "ed": 26, "text": "latent dirichlet allocation"}, {"st": 27, "ed": 29, "text": "topic model"}, {"st": 67, "ed": 69, "text": "clustering methods"}, {"st": 70, "ed": 73, "text": "k means clustering"}, {"st": 76, "ed": 78, "text": "mixture model"}]
[{"st": 0, "ed": 2, "text": "web services"}, {"st": 25, "ed": 27, "text": "web services"}, {"st": 36, "ed": 38, "text": "web services"}, {"st": 54, "ed": 56, "text": "web services"}, {"st": 65, "ed": 67, "text": "web services"}, {"st": 91, "ed": 93, "text": "web services"}, {"st": 104, "ed": 106, "text": "web service"}, {"st": 126, "ed": 128, "text": "web service"}, {"st": 153, "ed": 155, "text": "web services"}, {"st": 170, "ed": 172, "text": "web services"}, {"st": 191, "ed": 193, "text": "main goal"}, {"st": 206, "ed": 208, "text": "web services"}]
[{"st": 22, "ed": 24, "text": "simplified chinese"}, {"st": 34, "ed": 37, "text": "named entity recognition"}, {"st": 74, "ed": 76, "text": "machine learning"}, {"st": 96, "ed": 98, "text": "traditional chinese"}]
[{"st": 19, "ed": 21, "text": "proposed framework"}, {"st": 48, "ed": 50, "text": "facial expressions"}, {"st": 61, "ed": 63, "text": "topic modeling"}, {"st": 75, "ed": 77, "text": "ground truth"}, {"st": 83, "ed": 85, "text": "weighted average"}]
[]
[{"st": 0, "ed": 2, "text": "social media"}, {"st": 5, "ed": 7, "text": "increasingly important"}, {"st": 35, "ed": 37, "text": "social media"}, {"st": 45, "ed": 47, "text": "based methods"}, {"st": 63, "ed": 65, "text": "automatically identify"}, {"st": 80, "ed": 82, "text": "based methods"}, {"st": 101, "ed": 103, "text": "machine learning"}, {"st": 188, "ed": 190, "text": "controlled vocabulary"}, {"st": 211, "ed": 213, "text": "machine learning"}, {"st": 213, "ed": 216, "text": "approach significantly outperforms"}]
[{"st": 6, "ed": 8, "text": "text summarization"}]
[{"st": 100, "ed": 103, "text": "training and test"}, {"st": 131, "ed": 134, "text": "k nearest neighbors"}]
[{"st": 5, "ed": 7, "text": "automatically generate"}, {"st": 12, "ed": 14, "text": "natural language"}, {"st": 19, "ed": 21, "text": "supervised learning"}, {"st": 28, "ed": 30, "text": "weakly supervised"}]
[{"st": 18, "ed": 20, "text": "word embedding"}, {"st": 22, "ed": 25, "text": "deep neural networks"}, {"st": 52, "ed": 54, "text": "significantly improve"}, {"st": 56, "ed": 58, "text": "prediction accuracy"}]
[{"st": 94, "ed": 96, "text": "natural language"}, {"st": 102, "ed": 104, "text": "relevant information"}, {"st": 111, "ed": 113, "text": "social sciences"}]
[]
[{"st": 32, "ed": 34, "text": "natural language"}]
[{"st": 29, "ed": 31, "text": "case studies"}]
[{"st": 30, "ed": 32, "text": "compositional distributional"}]
[]
[{"st": 78, "ed": 80, "text": "rule based"}, {"st": 89, "ed": 91, "text": "feature sets"}, {"st": 115, "ed": 117, "text": "multi label"}, {"st": 125, "ed": 127, "text": "relation classification"}, {"st": 128, "ed": 130, "text": "rule based"}, {"st": 158, "ed": 160, "text": "feature sets"}, {"st": 175, "ed": 177, "text": "relation classification"}, {"st": 190, "ed": 192, "text": "analysis shows"}, {"st": 215, "ed": 218, "text": "multi label classification"}, {"st": 230, "ed": 232, "text": "statistically significant"}]
[{"st": 14, "ed": 16, "text": "natural language"}]
[{"st": 38, "ed": 40, "text": "social networks"}, {"st": 42, "ed": 44, "text": "sentiment analysis"}, {"st": 51, "ed": 53, "text": "key idea"}, {"st": 75, "ed": 77, "text": "attention based"}, {"st": 77, "ed": 79, "text": "neural network"}, {"st": 110, "ed": 112, "text": "social network"}, {"st": 128, "ed": 130, "text": "labeled data"}, {"st": 135, "ed": 137, "text": "significantly improves"}, {"st": 140, "ed": 142, "text": "sentiment analysis"}]
[{"st": 70, "ed": 72, "text": "natural language"}, {"st": 79, "ed": 81, "text": "theoretical foundation"}, {"st": 81, "ed": 83, "text": "statistical modeling"}, {"st": 90, "ed": 93, "text": "real world data"}, {"st": 116, "ed": 118, "text": "jeff hawkins"}, {"st": 158, "ed": 160, "text": "reference frame"}, {"st": 229, "ed": 231, "text": "similarity function"}, {"st": 236, "ed": 238, "text": "practical problems"}, {"st": 252, "ed": 255, "text": "precision and recall"}]
[{"st": 36, "ed": 38, "text": "structured prediction"}, {"st": 73, "ed": 75, "text": "based approach"}, {"st": 108, "ed": 110, "text": "error reduction"}]
[{"st": 10, "ed": 12, "text": "co occurrence"}, {"st": 17, "ed": 19, "text": "compositional distributional"}, {"st": 30, "ed": 32, "text": "categorical compositional"}, {"st": 32, "ed": 34, "text": "distributional semantics"}, {"st": 71, "ed": 73, "text": "sentence level"}, {"st": 94, "ed": 96, "text": "sentence level"}]
[{"st": 34, "ed": 36, "text": "natural language"}, {"st": 42, "ed": 45, "text": "the research community"}, {"st": 70, "ed": 72, "text": "closed set"}]
[{"st": 39, "ed": 41, "text": "natural language"}, {"st": 54, "ed": 56, "text": "named entity"}, {"st": 94, "ed": 96, "text": "distance based"}, {"st": 277, "ed": 279, "text": "based approaches"}, {"st": 301, "ed": 303, "text": "shows significant"}]
[]
[{"st": 15, "ed": 17, "text": "multi agent"}]
[{"st": 13, "ed": 15, "text": "artificial language"}, {"st": 37, "ed": 39, "text": "weighted sum"}, {"st": 53, "ed": 55, "text": "multi agent"}, {"st": 129, "ed": 131, "text": "steady state"}]
[{"st": 7, "ed": 9, "text": "unsupervised learning"}, {"st": 24, "ed": 26, "text": "continuous speech"}, {"st": 41, "ed": 43, "text": "prior knowledge"}, {"st": 75, "ed": 77, "text": "proposed method"}, {"st": 78, "ed": 80, "text": "nonparametric bayesian"}, {"st": 88, "ed": 90, "text": "generative model"}, {"st": 102, "ed": 104, "text": "latent variables"}, {"st": 112, "ed": 114, "text": "proposed method"}, {"st": 127, "ed": 129, "text": "mobile robot"}, {"st": 135, "ed": 137, "text": "conducted experiments"}]
[{"st": 0, "ed": 2, "text": "categorical compositional"}, {"st": 2, "ed": 4, "text": "distributional semantics"}, {"st": 8, "ed": 10, "text": "natural language"}, {"st": 35, "ed": 37, "text": "natural language"}, {"st": 56, "ed": 58, "text": "generative grammar"}, {"st": 63, "ed": 65, "text": "categorical compositional"}, {"st": 79, "ed": 81, "text": "finite dimensional"}]
[{"st": 27, "ed": 29, "text": "text mining"}, {"st": 52, "ed": 54, "text": "machine learning"}, {"st": 60, "ed": 62, "text": "text data"}, {"st": 67, "ed": 69, "text": "text mining"}, {"st": 77, "ed": 79, "text": "text mining"}, {"st": 79, "ed": 81, "text": "generative models"}, {"st": 130, "ed": 132, "text": "text mining"}, {"st": 148, "ed": 150, "text": "computational complexity"}, {"st": 153, "ed": 155, "text": "text mining"}, {"st": 160, "ed": 162, "text": "probabilistic models"}, {"st": 174, "ed": 176, "text": "generative models"}, {"st": 179, "ed": 181, "text": "text mining"}, {"st": 190, "ed": 192, "text": "text classification"}, {"st": 208, "ed": 210, "text": "task specific"}]
[{"st": 44, "ed": 46, "text": "pharmaceutical industry"}, {"st": 54, "ed": 56, "text": "social media"}, {"st": 99, "ed": 101, "text": "social media"}]
[{"st": 33, "ed": 35, "text": "topic model"}, {"st": 56, "ed": 58, "text": "natural language"}, {"st": 87, "ed": 89, "text": "automatically generated"}]
[{"st": 0, "ed": 2, "text": "novelty detection"}, {"st": 31, "ed": 33, "text": "large data"}, {"st": 53, "ed": 55, "text": "event detection"}, {"st": 65, "ed": 67, "text": "tf idf"}, {"st": 70, "ed": 73, "text": "locality sensitive hashing"}, {"st": 106, "ed": 108, "text": "novelty detection"}, {"st": 120, "ed": 122, "text": "benchmark dataset"}]
[{"st": 8, "ed": 10, "text": "machine learning"}, {"st": 12, "ed": 15, "text": "latent semantic analysis"}, {"st": 26, "ed": 28, "text": "concept map"}, {"st": 29, "ed": 31, "text": "domain knowledge"}, {"st": 80, "ed": 82, "text": "input data"}, {"st": 92, "ed": 94, "text": "computer graphics"}, {"st": 104, "ed": 107, "text": "java programming language"}, {"st": 119, "ed": 121, "text": "computer graphics"}]
[{"st": 105, "ed": 107, "text": "signal processing"}, {"st": 108, "ed": 110, "text": "sentiment analysis"}, {"st": 111, "ed": 113, "text": "machine learning"}, {"st": 143, "ed": 145, "text": "animated cartoon"}, {"st": 161, "ed": 163, "text": "deep learning"}, {"st": 186, "ed": 188, "text": "deep learning"}]
[{"st": 9, "ed": 11, "text": "natural language"}, {"st": 27, "ed": 29, "text": "natural language"}, {"st": 44, "ed": 46, "text": "natural language"}]
[{"st": 12, "ed": 14, "text": "social media"}, {"st": 30, "ed": 32, "text": "social networks"}]
[{"st": 10, "ed": 12, "text": "sentiment analysis"}, {"st": 96, "ed": 98, "text": "distant supervision"}, {"st": 155, "ed": 157, "text": "n grams"}, {"st": 176, "ed": 178, "text": "contextual information"}, {"st": 182, "ed": 184, "text": "sentiment classification"}]
[{"st": 27, "ed": 29, "text": "cross language"}, {"st": 58, "ed": 60, "text": "source language"}, {"st": 73, "ed": 75, "text": "low dimensional"}, {"st": 93, "ed": 95, "text": "transformation matrix"}, {"st": 118, "ed": 120, "text": "target language"}, {"st": 157, "ed": 159, "text": "proposed method"}, {"st": 161, "ed": 163, "text": "word embedding"}, {"st": 176, "ed": 178, "text": "proposed method"}, {"st": 184, "ed": 186, "text": "machine translation"}]
[{"st": 3, "ed": 5, "text": "cause effect"}, {"st": 7, "ed": 9, "text": "natural language"}, {"st": 13, "ed": 15, "text": "open problem"}, {"st": 36, "ed": 38, "text": "domain specific"}, {"st": 45, "ed": 47, "text": "big data"}, {"st": 58, "ed": 60, "text": "machine learning"}, {"st": 80, "ed": 82, "text": "labelled data"}, {"st": 100, "ed": 102, "text": "relation extraction"}, {"st": 110, "ed": 113, "text": "strengths and weaknesses"}]
[{"st": 112, "ed": 114, "text": "distributional semantics"}]
[{"st": 1, "ed": 3, "text": "open problem"}, {"st": 4, "ed": 6, "text": "categorical compositional"}, {"st": 6, "ed": 8, "text": "distributional semantics"}, {"st": 72, "ed": 74, "text": "finite dimensional"}]
[{"st": 4, "ed": 6, "text": "big data"}, {"st": 26, "ed": 28, "text": "natural language"}, {"st": 39, "ed": 41, "text": "natural language"}, {"st": 46, "ed": 48, "text": "complex network"}, {"st": 126, "ed": 128, "text": "sentiment analysis"}, {"st": 131, "ed": 133, "text": "transfer learning"}]
[{"st": 39, "ed": 41, "text": "textual description"}, {"st": 44, "ed": 46, "text": "feature vector"}, {"st": 62, "ed": 64, "text": "distributional semantics"}, {"st": 76, "ed": 78, "text": "ensemble approach"}, {"st": 95, "ed": 97, "text": "error reduction"}, {"st": 113, "ed": 115, "text": "ensemble approach"}, {"st": 121, "ed": 123, "text": "e commerce"}, {"st": 130, "ed": 132, "text": "evaluation metrics"}]
[{"st": 0, "ed": 2, "text": "conversational agents"}, {"st": 56, "ed": 58, "text": "deep structured"}, {"st": 59, "ed": 61, "text": "neural network"}, {"st": 92, "ed": 94, "text": "context sensitive"}, {"st": 98, "ed": 100, "text": "deep structured"}, {"st": 106, "ed": 108, "text": "significantly outperforms"}]
[{"st": 26, "ed": 28, "text": "rule based"}, {"st": 40, "ed": 42, "text": "supervised classification"}, {"st": 49, "ed": 52, "text": "support vector machines"}, {"st": 67, "ed": 69, "text": "training set"}, {"st": 88, "ed": 90, "text": "text processing"}]
[{"st": 69, "ed": 71, "text": "linked data"}, {"st": 131, "ed": 133, "text": "fully automated"}, {"st": 168, "ed": 170, "text": "empirically evaluated"}]
[{"st": 7, "ed": 10, "text": "word sense disambiguation"}, {"st": 14, "ed": 17, "text": "evolutionary game theory"}, {"st": 99, "ed": 102, "text": "word sense disambiguation"}, {"st": 105, "ed": 107, "text": "constraint satisfaction"}, {"st": 171, "ed": 174, "text": "word sense disambiguation"}, {"st": 196, "ed": 198, "text": "similarity measures"}]
[{"st": 9, "ed": 11, "text": "web search"}, {"st": 73, "ed": 75, "text": "search engines"}, {"st": 117, "ed": 119, "text": "large data"}, {"st": 135, "ed": 137, "text": "contextual information"}, {"st": 147, "ed": 149, "text": "large scale"}, {"st": 180, "ed": 183, "text": "cold start problem"}, {"st": 207, "ed": 210, "text": "approach significantly outperforms"}, {"st": 221, "ed": 223, "text": "open source"}]
[{"st": 60, "ed": 62, "text": "pairwise similarity"}, {"st": 106, "ed": 108, "text": "disjoint sets"}, {"st": 184, "ed": 186, "text": "proposed method"}, {"st": 191, "ed": 193, "text": "document clustering"}]
[{"st": 1, "ed": 3, "text": "sentiment analysis"}, {"st": 89, "ed": 91, "text": "important role"}, {"st": 113, "ed": 115, "text": "meta data"}, {"st": 145, "ed": 147, "text": "text categorization"}]
[{"st": 0, "ed": 2, "text": "recent years"}, {"st": 5, "ed": 7, "text": "market penetration"}, {"st": 77, "ed": 79, "text": "spoken language"}, {"st": 93, "ed": 95, "text": "spoken language"}]
[{"st": 0, "ed": 2, "text": "word embedding"}, {"st": 8, "ed": 10, "text": "vector representations"}, {"st": 13, "ed": 15, "text": "neural networks"}, {"st": 30, "ed": 32, "text": "similarity measures"}, {"st": 49, "ed": 51, "text": "based methods"}, {"st": 70, "ed": 72, "text": "recent studies"}, {"st": 106, "ed": 108, "text": "word representations"}, {"st": 130, "ed": 132, "text": "word representations"}, {"st": 140, "ed": 142, "text": "word embedding"}, {"st": 161, "ed": 163, "text": "word embedding"}, {"st": 182, "ed": 184, "text": "empirical evaluations"}, {"st": 191, "ed": 193, "text": "representation learning"}]
[{"st": 21, "ed": 23, "text": "natural language"}, {"st": 36, "ed": 38, "text": "autonomous systems"}, {"st": 39, "ed": 41, "text": "natural language"}, {"st": 55, "ed": 57, "text": "autonomous systems"}]
[{"st": 59, "ed": 61, "text": "knowledge engineering"}, {"st": 66, "ed": 68, "text": "qualitative analysis"}]
[{"st": 9, "ed": 11, "text": "answer set"}, {"st": 81, "ed": 83, "text": "existing literature"}, {"st": 108, "ed": 110, "text": "real world"}, {"st": 132, "ed": 135, "text": "theory and practice"}]
[{"st": 96, "ed": 98, "text": "unsupervised methods"}]
[{"st": 19, "ed": 21, "text": "cognitive science"}, {"st": 42, "ed": 44, "text": "quantum physics"}, {"st": 71, "ed": 73, "text": "natural language"}, {"st": 75, "ed": 77, "text": "cognitive science"}]
[{"st": 4, "ed": 6, "text": "categorical compositional"}, {"st": 32, "ed": 34, "text": "categorical compositional"}]
[{"st": 13, "ed": 15, "text": "natural language"}, {"st": 16, "ed": 18, "text": "categorical compositional"}, {"st": 18, "ed": 20, "text": "distributional semantics"}]
[{"st": 12, "ed": 14, "text": "categorical compositional"}, {"st": 22, "ed": 24, "text": "categorical compositional"}, {"st": 154, "ed": 156, "text": "quantum mechanics"}]
[{"st": 86, "ed": 88, "text": "social media"}, {"st": 94, "ed": 96, "text": "natural language"}]
[{"st": 12, "ed": 14, "text": "automatically generated"}, {"st": 45, "ed": 47, "text": "proposed method"}, {"st": 48, "ed": 50, "text": "content analysis"}, {"st": 51, "ed": 53, "text": "natural language"}]
[{"st": 4, "ed": 6, "text": "latent vector"}, {"st": 9, "ed": 11, "text": "jointly learns"}, {"st": 12, "ed": 14, "text": "latent representations"}, {"st": 16, "ed": 18, "text": "e commerce"}, {"st": 58, "ed": 60, "text": "latent vector"}, {"st": 74, "ed": 77, "text": "learning to rank"}, {"st": 79, "ed": 81, "text": "latent vector"}, {"st": 102, "ed": 105, "text": "representations of words"}]
[{"st": 1, "ed": 3, "text": "topic modeling"}, {"st": 39, "ed": 41, "text": "training data"}, {"st": 71, "ed": 73, "text": "text mining"}, {"st": 103, "ed": 105, "text": "software engineering"}, {"st": 113, "ed": 115, "text": "differential evolution"}, {"st": 153, "ed": 155, "text": "scikit learn"}, {"st": 171, "ed": 173, "text": "gibbs sampling"}, {"st": 181, "ed": 183, "text": "text mining"}]
[{"st": 25, "ed": 27, "text": "knowledge graph"}, {"st": 108, "ed": 110, "text": "knowledge graph"}, {"st": 116, "ed": 118, "text": "real world"}, {"st": 165, "ed": 167, "text": "compact representation"}, {"st": 185, "ed": 187, "text": "natural language"}, {"st": 188, "ed": 190, "text": "anomaly detection"}, {"st": 204, "ed": 206, "text": "main contribution"}, {"st": 218, "ed": 220, "text": "knowledge graph"}]
[{"st": 2, "ed": 4, "text": "text corpora"}, {"st": 23, "ed": 26, "text": "artificial intelligence ai"}, {"st": 28, "ed": 30, "text": "new york"}]
[{"st": 1, "ed": 4, "text": "end to end"}, {"st": 7, "ed": 10, "text": "shown promising results"}, {"st": 11, "ed": 13, "text": "question answering"}, {"st": 21, "ed": 23, "text": "explicit memory"}, {"st": 42, "ed": 44, "text": "sentence level"}, {"st": 56, "ed": 58, "text": "attention mechanism"}, {"st": 100, "ed": 102, "text": "sentence level"}, {"st": 104, "ed": 106, "text": "word level"}, {"st": 110, "ed": 112, "text": "max pooling"}, {"st": 119, "ed": 121, "text": "sentence level"}, {"st": 137, "ed": 139, "text": "attention mechanism"}, {"st": 141, "ed": 143, "text": "word level"}, {"st": 163, "ed": 165, "text": "sentence level"}, {"st": 169, "ed": 171, "text": "word level"}]
[{"st": 1, "ed": 3, "text": "recent years"}, {"st": 15, "ed": 17, "text": "sensitive information"}, {"st": 30, "ed": 32, "text": "edward snowden"}, {"st": 84, "ed": 86, "text": "sensitive information"}, {"st": 154, "ed": 156, "text": "sensitive information"}, {"st": 226, "ed": 228, "text": "sensitive information"}]
[{"st": 14, "ed": 16, "text": "computational linguistics"}, {"st": 34, "ed": 36, "text": "cognitive science"}, {"st": 43, "ed": 45, "text": "cognitive science"}, {"st": 47, "ed": 49, "text": "computational linguistics"}]
[{"st": 0, "ed": 2, "text": "question answering"}, {"st": 23, "ed": 26, "text": "question answering qa"}, {"st": 40, "ed": 42, "text": "qa systems"}, {"st": 61, "ed": 63, "text": "qa systems"}, {"st": 69, "ed": 71, "text": "time consuming"}, {"st": 84, "ed": 86, "text": "qa systems"}, {"st": 107, "ed": 109, "text": "web based"}, {"st": 109, "ed": 111, "text": "qa systems"}, {"st": 146, "ed": 148, "text": "qa systems"}]
[{"st": 48, "ed": 50, "text": "attention based"}, {"st": 50, "ed": 52, "text": "encoder decoder"}, {"st": 63, "ed": 65, "text": "rule based"}, {"st": 69, "ed": 71, "text": "neural networks"}, {"st": 100, "ed": 103, "text": "end to end"}]
[{"st": 0, "ed": 2, "text": "recent years"}, {"st": 20, "ed": 22, "text": "humanoid robot"}, {"st": 42, "ed": 44, "text": "social media"}, {"st": 69, "ed": 71, "text": "autonomous systems"}, {"st": 82, "ed": 84, "text": "tasks involving"}, {"st": 103, "ed": 106, "text": "artificial intelligence ai"}, {"st": 121, "ed": 123, "text": "rule based"}, {"st": 151, "ed": 153, "text": "level design"}, {"st": 180, "ed": 182, "text": "control theory"}, {"st": 216, "ed": 218, "text": "generative model"}]
[{"st": 18, "ed": 20, "text": "natural language"}, {"st": 117, "ed": 119, "text": "industrial robot"}, {"st": 142, "ed": 144, "text": "experiment results"}]
[{"st": 16, "ed": 18, "text": "sentiment analysis"}, {"st": 32, "ed": 34, "text": "previously proposed"}, {"st": 39, "ed": 41, "text": "gain insight"}]
[{"st": 49, "ed": 51, "text": "learning algorithms"}, {"st": 57, "ed": 59, "text": "unstructured text"}, {"st": 85, "ed": 87, "text": "plain text"}, {"st": 108, "ed": 110, "text": "topic model"}, {"st": 139, "ed": 141, "text": "proposed method"}, {"st": 151, "ed": 153, "text": "domain specific"}, {"st": 164, "ed": 166, "text": "real world"}, {"st": 166, "ed": 168, "text": "text corpora"}, {"st": 170, "ed": 172, "text": "bbc news"}, {"st": 180, "ed": 182, "text": "proposed method"}, {"st": 186, "ed": 188, "text": "existing methods"}, {"st": 207, "ed": 209, "text": "topic modeling"}]
[{"st": 68, "ed": 70, "text": "search engines"}, {"st": 135, "ed": 137, "text": "data mining"}, {"st": 187, "ed": 190, "text": "point of view"}, {"st": 201, "ed": 203, "text": "linear combination"}, {"st": 206, "ed": 208, "text": "mutual information"}]
[{"st": 60, "ed": 62, "text": "main goal"}]
[{"st": 0, "ed": 2, "text": "natural language"}, {"st": 9, "ed": 11, "text": "natural language"}, {"st": 137, "ed": 139, "text": "future research"}]
[{"st": 0, "ed": 2, "text": "natural language"}, {"st": 10, "ed": 12, "text": "natural language"}, {"st": 114, "ed": 117, "text": "advantages and disadvantages"}]
[{"st": 2, "ed": 4, "text": "word vectors"}, {"st": 20, "ed": 22, "text": "pre trained"}, {"st": 27, "ed": 29, "text": "linear transformation"}, {"st": 42, "ed": 44, "text": "linear transformation"}]
[{"st": 0, "ed": 2, "text": "task oriented"}, {"st": 16, "ed": 18, "text": "service providers"}, {"st": 91, "ed": 93, "text": "reinforcement learning"}, {"st": 122, "ed": 124, "text": "proposed framework"}, {"st": 149, "ed": 151, "text": "success rate"}, {"st": 162, "ed": 164, "text": "task oriented"}]
[{"st": 120, "ed": 122, "text": "co occurrence"}, {"st": 156, "ed": 158, "text": "natural language"}]
[{"st": 22, "ed": 24, "text": "previous works"}, {"st": 69, "ed": 71, "text": "word embeddings"}, {"st": 109, "ed": 111, "text": "text based"}, {"st": 124, "ed": 126, "text": "embedding space"}]
[{"st": 6, "ed": 8, "text": "word embeddings"}, {"st": 44, "ed": 46, "text": "distributed representations"}, {"st": 49, "ed": 51, "text": "method outperforms"}, {"st": 60, "ed": 62, "text": "benchmark tasks"}]
[{"st": 58, "ed": 60, "text": "traditional approaches"}]
[{"st": 56, "ed": 58, "text": "text categorization"}, {"st": 66, "ed": 68, "text": "challenging problem"}, {"st": 75, "ed": 77, "text": "real world"}, {"st": 96, "ed": 99, "text": "scale to large"}, {"st": 132, "ed": 134, "text": "extraction task"}, {"st": 184, "ed": 186, "text": "significantly faster"}]
[{"st": 162, "ed": 164, "text": "previous studies"}, {"st": 169, "ed": 171, "text": "quantum physics"}, {"st": 201, "ed": 203, "text": "co occurrences"}]
[{"st": 4, "ed": 6, "text": "real world"}, {"st": 19, "ed": 21, "text": "natural language"}, {"st": 42, "ed": 44, "text": "link prediction"}, {"st": 67, "ed": 69, "text": "embedding models"}, {"st": 84, "ed": 87, "text": "standard benchmark datasets"}]
[{"st": 129, "ed": 131, "text": "real world"}]
[{"st": 23, "ed": 25, "text": "low level"}, {"st": 34, "ed": 36, "text": "low level"}, {"st": 48, "ed": 50, "text": "low level"}, {"st": 52, "ed": 54, "text": "higher level"}, {"st": 103, "ed": 105, "text": "higher level"}, {"st": 125, "ed": 127, "text": "higher quality"}, {"st": 139, "ed": 141, "text": "preliminary results"}, {"st": 143, "ed": 145, "text": "real life"}]
[{"st": 48, "ed": 50, "text": "dialogue systems"}, {"st": 63, "ed": 66, "text": "amazon mechanical turk"}, {"st": 77, "ed": 79, "text": "proposed approach"}]
[{"st": 6, "ed": 9, "text": "online learning algorithm"}, {"st": 26, "ed": 28, "text": "nonparametric bayesian"}, {"st": 46, "ed": 48, "text": "theoretical framework"}, {"st": 54, "ed": 56, "text": "proposed method"}, {"st": 57, "ed": 59, "text": "simultaneously learn"}, {"st": 71, "ed": 73, "text": "proposed method"}, {"st": 75, "ed": 77, "text": "image features"}, {"st": 89, "ed": 91, "text": "online learning"}, {"st": 116, "ed": 118, "text": "online learning"}]
[{"st": 22, "ed": 24, "text": "input data"}, {"st": 63, "ed": 65, "text": "word embedding"}, {"st": 67, "ed": 69, "text": "gaussian processes"}, {"st": 84, "ed": 86, "text": "model achieves"}]
[{"st": 3, "ed": 6, "text": "simple yet effective"}, {"st": 27, "ed": 29, "text": "character level"}, {"st": 46, "ed": 49, "text": "approach significantly outperforms"}]
[{"st": 4, "ed": 6, "text": "semi supervised"}, {"st": 20, "ed": 22, "text": "previous approaches"}, {"st": 35, "ed": 37, "text": "probabilistic framework"}, {"st": 58, "ed": 60, "text": "carefully designed"}, {"st": 68, "ed": 70, "text": "approach outperforms"}, {"st": 104, "ed": 106, "text": "previous approaches"}]
[{"st": 23, "ed": 25, "text": "mental health"}, {"st": 27, "ed": 29, "text": "time consuming"}, {"st": 39, "ed": 41, "text": "mental health"}, {"st": 61, "ed": 63, "text": "social media"}, {"st": 88, "ed": 90, "text": "public health"}, {"st": 100, "ed": 102, "text": "mental health"}, {"st": 114, "ed": 116, "text": "representation learning"}, {"st": 122, "ed": 124, "text": "social media"}, {"st": 161, "ed": 163, "text": "mental health"}, {"st": 183, "ed": 187, "text": "post traumatic stress disorder"}, {"st": 207, "ed": 210, "text": "ability to capture"}, {"st": 215, "ed": 217, "text": "mental health"}, {"st": 224, "ed": 226, "text": "mental health"}, {"st": 226, "ed": 228, "text": "prediction models"}]
[{"st": 1, "ed": 3, "text": "recent studies"}, {"st": 6, "ed": 9, "text": "recurrent neural networks"}, {"st": 56, "ed": 59, "text": "recurrent neural networks"}, {"st": 60, "ed": 62, "text": "natural language"}, {"st": 85, "ed": 87, "text": "neural network"}]
[{"st": 0, "ed": 2, "text": "unsupervised learning"}, {"st": 3, "ed": 5, "text": "low dimensional"}, {"st": 6, "ed": 9, "text": "representations of words"}, {"st": 12, "ed": 14, "text": "recently gained"}, {"st": 31, "ed": 33, "text": "previously published"}, {"st": 44, "ed": 46, "text": "representation learning"}, {"st": 47, "ed": 49, "text": "fine grained"}, {"st": 64, "ed": 66, "text": "existing models"}]
[{"st": 4, "ed": 6, "text": "embedding model"}]
[{"st": 0, "ed": 2, "text": "sentiment analysis"}, {"st": 4, "ed": 6, "text": "natural language"}, {"st": 12, "ed": 15, "text": "detection and classification"}, {"st": 84, "ed": 86, "text": "sentiment analysis"}, {"st": 111, "ed": 114, "text": "positive or negative"}, {"st": 119, "ed": 121, "text": "machine learning"}, {"st": 130, "ed": 132, "text": "social media"}, {"st": 135, "ed": 137, "text": "sentiment analysis"}, {"st": 151, "ed": 153, "text": "sentiment analysis"}, {"st": 156, "ed": 158, "text": "social media"}, {"st": 163, "ed": 165, "text": "pre processing"}, {"st": 171, "ed": 173, "text": "multilayer perceptron"}]
[{"st": 17, "ed": 19, "text": "unstructured text"}, {"st": 28, "ed": 31, "text": "efficient and effective"}, {"st": 40, "ed": 42, "text": "text mining"}, {"st": 69, "ed": 71, "text": "text mining"}, {"st": 76, "ed": 78, "text": "pre processing"}, {"st": 85, "ed": 87, "text": "text mining"}]
[{"st": 0, "ed": 2, "text": "machine learning"}, {"st": 13, "ed": 15, "text": "deep learning"}]
[{"st": 7, "ed": 9, "text": "natural language"}, {"st": 13, "ed": 15, "text": "resource constrained"}, {"st": 26, "ed": 28, "text": "user generated"}, {"st": 51, "ed": 53, "text": "real world"}]
[{"st": 14, "ed": 16, "text": "formal language"}, {"st": 74, "ed": 76, "text": "context sensitive"}, {"st": 81, "ed": 83, "text": "finite state"}, {"st": 97, "ed": 99, "text": "finite state"}, {"st": 120, "ed": 122, "text": "finite state"}, {"st": 136, "ed": 138, "text": "finite state"}]
[{"st": 28, "ed": 30, "text": "natural language"}, {"st": 54, "ed": 56, "text": "natural language"}, {"st": 62, "ed": 64, "text": "neural network"}, {"st": 68, "ed": 70, "text": "natural language"}, {"st": 123, "ed": 125, "text": "key feature"}, {"st": 139, "ed": 141, "text": "text descriptions"}, {"st": 147, "ed": 149, "text": "natural language"}, {"st": 151, "ed": 153, "text": "preliminary results"}, {"st": 185, "ed": 187, "text": "previously unseen"}]
[{"st": 38, "ed": 40, "text": "finite dimensional"}, {"st": 70, "ed": 72, "text": "unsupervised manner"}, {"st": 138, "ed": 140, "text": "distributional semantics"}, {"st": 141, "ed": 143, "text": "topic models"}, {"st": 145, "ed": 147, "text": "neural networks"}]
[{"st": 2, "ed": 4, "text": "e commerce"}, {"st": 57, "ed": 59, "text": "user experience"}, {"st": 76, "ed": 78, "text": "existing models"}, {"st": 92, "ed": 94, "text": "latent factors"}, {"st": 100, "ed": 102, "text": "existing works"}, {"st": 111, "ed": 113, "text": "deep learning"}, {"st": 114, "ed": 116, "text": "framework named"}, {"st": 132, "ed": 134, "text": "user experience"}, {"st": 140, "ed": 142, "text": "gated recurrent"}, {"st": 142, "ed": 144, "text": "neural networks"}, {"st": 151, "ed": 153, "text": "latent representations"}, {"st": 157, "ed": 159, "text": "extensive experiments"}, {"st": 160, "ed": 162, "text": "benchmark datasets"}, {"st": 169, "ed": 171, "text": "significant improvements"}, {"st": 186, "ed": 188, "text": "user experience"}]
[{"st": 6, "ed": 8, "text": "machine translation"}, {"st": 32, "ed": 34, "text": "bandit feedback"}, {"st": 66, "ed": 69, "text": "neural machine translation"}, {"st": 78, "ed": 80, "text": "reinforcement learning"}, {"st": 86, "ed": 88, "text": "bandit feedback"}, {"st": 90, "ed": 92, "text": "domain adaptation"}]
[{"st": 1, "ed": 3, "text": "statistical models"}, {"st": 35, "ed": 38, "text": "hidden markov models"}, {"st": 40, "ed": 43, "text": "context free grammar"}, {"st": 45, "ed": 47, "text": "latent variables"}, {"st": 55, "ed": 57, "text": "unsupervised learning"}, {"st": 77, "ed": 79, "text": "predictive power"}]
[]
[{"st": 21, "ed": 23, "text": "search engines"}, {"st": 62, "ed": 64, "text": "attention mechanism"}, {"st": 96, "ed": 98, "text": "noisy data"}, {"st": 175, "ed": 177, "text": "evaluation metrics"}, {"st": 183, "ed": 185, "text": "generative models"}, {"st": 191, "ed": 195, "text": "extensive set of experiments"}]
[{"st": 8, "ed": 10, "text": "challenging research"}, {"st": 13, "ed": 15, "text": "text summarization"}, {"st": 70, "ed": 72, "text": "clustering approaches"}, {"st": 90, "ed": 92, "text": "based clustering"}, {"st": 106, "ed": 108, "text": "mutual information"}]
[{"st": 41, "ed": 43, "text": "text summarization"}]
[{"st": 17, "ed": 19, "text": "real world"}, {"st": 28, "ed": 30, "text": "cross lingual"}, {"st": 35, "ed": 37, "text": "machine translation"}, {"st": 69, "ed": 71, "text": "machine translation"}, {"st": 72, "ed": 74, "text": "cross lingual"}, {"st": 93, "ed": 95, "text": "embedding model"}, {"st": 96, "ed": 98, "text": "cross lingual"}, {"st": 128, "ed": 130, "text": "real world"}, {"st": 134, "ed": 137, "text": "approach significantly outperforms"}, {"st": 145, "ed": 147, "text": "cross lingual"}]
[{"st": 1, "ed": 4, "text": "short term memory"}, {"st": 8, "ed": 11, "text": "recurrent neural networks"}, {"st": 16, "ed": 19, "text": "automatic speech recognition"}, {"st": 20, "ed": 22, "text": "residual learning"}, {"st": 28, "ed": 30, "text": "neural networks"}, {"st": 67, "ed": 69, "text": "error rate"}]
[{"st": 16, "ed": 18, "text": "large scale"}, {"st": 18, "ed": 20, "text": "social media"}, {"st": 29, "ed": 32, "text": "number of clusters"}, {"st": 143, "ed": 145, "text": "monte carlo"}, {"st": 156, "ed": 158, "text": "highly scalable"}, {"st": 205, "ed": 207, "text": "predictive performance"}]
[{"st": 1, "ed": 4, "text": "electronic health record"}, {"st": 10, "ed": 12, "text": "multi dimensional"}, {"st": 24, "ed": 26, "text": "previous studies"}, {"st": 43, "ed": 45, "text": "deep learning"}, {"st": 53, "ed": 55, "text": "neural attention"}, {"st": 64, "ed": 68, "text": "convolutional neural networks cnn"}, {"st": 77, "ed": 79, "text": "computed tomography"}, {"st": 104, "ed": 106, "text": "attention models"}]
[{"st": 49, "ed": 51, "text": "probabilistic modeling"}, {"st": 56, "ed": 58, "text": "multiple sources"}]
[]
[{"st": 0, "ed": 2, "text": "recommendation systems"}, {"st": 18, "ed": 20, "text": "news uk"}, {"st": 78, "ed": 80, "text": "collaborative filtering"}]
[{"st": 4, "ed": 6, "text": "knowledge organization"}, {"st": 16, "ed": 18, "text": "linked data"}, {"st": 21, "ed": 23, "text": "knowledge organization"}, {"st": 115, "ed": 117, "text": "natural language"}]
[{"st": 39, "ed": 41, "text": "doctoral advisor"}, {"st": 64, "ed": 66, "text": "natural language"}, {"st": 68, "ed": 70, "text": "previous approaches"}]
[{"st": 39, "ed": 41, "text": "autonomous robot"}, {"st": 69, "ed": 71, "text": "approach produces"}]
[{"st": 14, "ed": 16, "text": "natural language"}, {"st": 19, "ed": 22, "text": "conditional random fields"}, {"st": 32, "ed": 34, "text": "rule based"}, {"st": 51, "ed": 54, "text": "undirected graphical model"}, {"st": 147, "ed": 149, "text": "natural language"}]
[{"st": 16, "ed": 18, "text": "dialogue systems"}, {"st": 19, "ed": 21, "text": "conversational agents"}, {"st": 32, "ed": 34, "text": "recent research"}, {"st": 82, "ed": 84, "text": "recent research"}, {"st": 115, "ed": 118, "text": "locality sensitive hashing"}, {"st": 122, "ed": 125, "text": "approximate nearest neighbor"}, {"st": 163, "ed": 165, "text": "selection method"}, {"st": 166, "ed": 168, "text": "based approaches"}, {"st": 174, "ed": 177, "text": "future research directions"}]
[{"st": 8, "ed": 11, "text": "graphical user interface"}, {"st": 50, "ed": 52, "text": "dialogue systems"}, {"st": 72, "ed": 74, "text": "wide variety"}]
[{"st": 37, "ed": 39, "text": "elastic net"}, {"st": 41, "ed": 43, "text": "logistic regression"}, {"st": 96, "ed": 98, "text": "positive rate"}, {"st": 102, "ed": 105, "text": "false positive rate"}]
[{"st": 3, "ed": 6, "text": "automatic speech recognition"}, {"st": 10, "ed": 12, "text": "multiple languages"}, {"st": 120, "ed": 122, "text": "recognition performance"}, {"st": 153, "ed": 155, "text": "improve performance"}]
[{"st": 36, "ed": 38, "text": "potential applications"}, {"st": 39, "ed": 41, "text": "fuzzy logic"}]
[{"st": 4, "ed": 6, "text": "natural language"}, {"st": 7, "ed": 10, "text": "a long standing"}, {"st": 10, "ed": 12, "text": "open problem"}, {"st": 24, "ed": 26, "text": "de facto"}, {"st": 91, "ed": 93, "text": "reinforcement learning"}, {"st": 112, "ed": 114, "text": "reinforcement learning"}, {"st": 150, "ed": 152, "text": "based approach"}, {"st": 192, "ed": 194, "text": "attention mechanism"}, {"st": 215, "ed": 217, "text": "prior art"}]
[{"st": 84, "ed": 86, "text": "proposed framework"}, {"st": 103, "ed": 105, "text": "feature vectors"}, {"st": 119, "ed": 121, "text": "feature vectors"}, {"st": 126, "ed": 129, "text": "support vector machine"}, {"st": 142, "ed": 144, "text": "prediction performance"}, {"st": 151, "ed": 153, "text": "word embedding"}, {"st": 159, "ed": 161, "text": "natural language"}, {"st": 198, "ed": 200, "text": "feature representation"}, {"st": 206, "ed": 208, "text": "baseline methods"}]
[{"st": 1, "ed": 5, "text": "automatic speech recognition asr"}, {"st": 13, "ed": 15, "text": "target language"}, {"st": 67, "ed": 69, "text": "asr systems"}, {"st": 71, "ed": 75, "text": "recurrent neural networks rnns"}, {"st": 132, "ed": 134, "text": "feature vectors"}, {"st": 166, "ed": 168, "text": "proposed approach"}]
[{"st": 10, "ed": 14, "text": "recurrent neural networks rnns"}, {"st": 39, "ed": 41, "text": "feature vectors"}, {"st": 63, "ed": 65, "text": "feature level"}, {"st": 99, "ed": 101, "text": "hidden layers"}, {"st": 126, "ed": 128, "text": "error rates"}]
[{"st": 0, "ed": 2, "text": "search engines"}, {"st": 6, "ed": 8, "text": "based approaches"}, {"st": 35, "ed": 37, "text": "word order"}, {"st": 73, "ed": 75, "text": "based approaches"}, {"st": 104, "ed": 106, "text": "based approaches"}, {"st": 135, "ed": 138, "text": "and vice versa"}, {"st": 207, "ed": 209, "text": "latent vector"}]
[{"st": 13, "ed": 15, "text": "fine grained"}, {"st": 23, "ed": 25, "text": "recurrent models"}, {"st": 29, "ed": 32, "text": "short term memory"}, {"st": 49, "ed": 51, "text": "important role"}, {"st": 54, "ed": 56, "text": "learned representations"}, {"st": 83, "ed": 85, "text": "learned representations"}, {"st": 98, "ed": 100, "text": "convolutional layers"}, {"st": 117, "ed": 119, "text": "conceptually simple"}, {"st": 126, "ed": 128, "text": "competitive performance"}, {"st": 140, "ed": 142, "text": "learning representations"}, {"st": 163, "ed": 165, "text": "extensive experiments"}, {"st": 184, "ed": 186, "text": "competitive performance"}]
[{"st": 131, "ed": 133, "text": "approach achieves"}]
[{"st": 0, "ed": 2, "text": "attention based"}, {"st": 7, "ed": 10, "text": "automatic speech recognition"}, {"st": 53, "ed": 55, "text": "log linear"}, {"st": 117, "ed": 120, "text": "word error rate"}, {"st": 125, "ed": 127, "text": "attention based"}]
[{"st": 15, "ed": 17, "text": "natural language"}, {"st": 24, "ed": 26, "text": "text mining"}, {"st": 26, "ed": 28, "text": "text summarization"}, {"st": 82, "ed": 84, "text": "user generated"}, {"st": 103, "ed": 106, "text": "deep neural network"}, {"st": 106, "ed": 108, "text": "based approach"}, {"st": 111, "ed": 113, "text": "coarse grained"}, {"st": 117, "ed": 120, "text": "convolutional neural network"}, {"st": 123, "ed": 126, "text": "short term memory"}, {"st": 131, "ed": 133, "text": "fine grained"}, {"st": 133, "ed": 135, "text": "word level"}, {"st": 144, "ed": 146, "text": "proposed approach"}, {"st": 146, "ed": 148, "text": "outperforms existing"}, {"st": 154, "ed": 156, "text": "user generated"}, {"st": 157, "ed": 159, "text": "social media"}, {"st": 166, "ed": 168, "text": "highly competitive"}]
[{"st": 68, "ed": 70, "text": "highly correlated"}, {"st": 88, "ed": 90, "text": "promising performance"}, {"st": 105, "ed": 107, "text": "domain specific"}, {"st": 118, "ed": 120, "text": "human trafficking"}, {"st": 157, "ed": 159, "text": "machine learning"}, {"st": 168, "ed": 170, "text": "real world"}]
[{"st": 85, "ed": 87, "text": "neural network"}, {"st": 88, "ed": 91, "text": "convolutional and recurrent"}, {"st": 91, "ed": 93, "text": "neural networks"}, {"st": 102, "ed": 104, "text": "neural networks"}, {"st": 128, "ed": 130, "text": "neural network"}, {"st": 147, "ed": 150, "text": "mean squared error"}]
[{"st": 23, "ed": 25, "text": "question answering"}, {"st": 36, "ed": 38, "text": "extracting information"}, {"st": 45, "ed": 47, "text": "important information"}, {"st": 73, "ed": 75, "text": "question answering"}, {"st": 129, "ed": 131, "text": "relation extraction"}, {"st": 146, "ed": 148, "text": "semi supervised"}, {"st": 222, "ed": 225, "text": "future research directions"}]
[{"st": 2, "ed": 4, "text": "sentiment analysis"}, {"st": 22, "ed": 24, "text": "neural network"}, {"st": 37, "ed": 39, "text": "sentiment analysis"}, {"st": 156, "ed": 158, "text": "neural attention"}, {"st": 163, "ed": 166, "text": "end to end"}, {"st": 187, "ed": 189, "text": "benchmark datasets"}]
[{"st": 1, "ed": 4, "text": "end to end"}, {"st": 14, "ed": 17, "text": "hand crafted features"}, {"st": 27, "ed": 29, "text": "typically require"}, {"st": 40, "ed": 42, "text": "previous studies"}, {"st": 54, "ed": 56, "text": "domain specific"}, {"st": 77, "ed": 79, "text": "recent developments"}, {"st": 83, "ed": 85, "text": "continual learning"}, {"st": 102, "ed": 104, "text": "continual learning"}, {"st": 134, "ed": 136, "text": "continual learning"}, {"st": 165, "ed": 167, "text": "customer support"}]
[{"st": 18, "ed": 20, "text": "look alike"}, {"st": 45, "ed": 47, "text": "predictive performance"}, {"st": 49, "ed": 51, "text": "look alike"}, {"st": 90, "ed": 92, "text": "app store"}, {"st": 93, "ed": 95, "text": "google play"}, {"st": 171, "ed": 174, "text": "supervised machine learning"}, {"st": 176, "ed": 178, "text": "look alike"}, {"st": 197, "ed": 199, "text": "look alike"}, {"st": 208, "ed": 210, "text": "training process"}]
[{"st": 4, "ed": 6, "text": "word embeddings"}, {"st": 21, "ed": 23, "text": "word embeddings"}, {"st": 27, "ed": 29, "text": "word embedding"}, {"st": 40, "ed": 42, "text": "current methods"}, {"st": 66, "ed": 68, "text": "large scale"}, {"st": 77, "ed": 79, "text": "skip gram"}, {"st": 97, "ed": 99, "text": "embedding models"}, {"st": 134, "ed": 136, "text": "benchmark datasets"}, {"st": 152, "ed": 154, "text": "case study"}, {"st": 171, "ed": 173, "text": "competitive accuracy"}, {"st": 194, "ed": 196, "text": "error prone"}]
[{"st": 12, "ed": 14, "text": "artificial intelligence"}, {"st": 33, "ed": 35, "text": "machine learning"}, {"st": 36, "ed": 38, "text": "reinforcement learning"}, {"st": 95, "ed": 97, "text": "reinforcement learning"}, {"st": 104, "ed": 106, "text": "machine learning"}]
[{"st": 39, "ed": 41, "text": "low latency"}, {"st": 42, "ed": 44, "text": "low cost"}, {"st": 105, "ed": 107, "text": "without compromising"}, {"st": 160, "ed": 162, "text": "open domain"}]
[{"st": 12, "ed": 14, "text": "recent years"}, {"st": 15, "ed": 17, "text": "question answering"}, {"st": 33, "ed": 35, "text": "processing step"}, {"st": 36, "ed": 38, "text": "question answering"}, {"st": 60, "ed": 62, "text": "neural network"}, {"st": 78, "ed": 80, "text": "question answering"}, {"st": 83, "ed": 86, "text": "significant performance gains"}]
[{"st": 6, "ed": 8, "text": "recommender systems"}, {"st": 25, "ed": 27, "text": "neural architecture"}, {"st": 82, "ed": 84, "text": "learning scheme"}, {"st": 114, "ed": 116, "text": "word level"}, {"st": 119, "ed": 121, "text": "based method"}, {"st": 125, "ed": 127, "text": "gumbel softmax"}, {"st": 165, "ed": 167, "text": "learning scheme"}, {"st": 171, "ed": 173, "text": "multiple views"}, {"st": 189, "ed": 191, "text": "extensive experiments"}, {"st": 194, "ed": 196, "text": "benchmark datasets"}, {"st": 200, "ed": 202, "text": "empirical results"}, {"st": 205, "ed": 208, "text": "approach significantly outperforms"}, {"st": 219, "ed": 221, "text": "relative improvement"}]
[{"st": 53, "ed": 55, "text": "question answering"}, {"st": 58, "ed": 60, "text": "inference tasks"}, {"st": 198, "ed": 200, "text": "training data"}]
[{"st": 12, "ed": 14, "text": "natural language"}, {"st": 20, "ed": 22, "text": "input output"}, {"st": 28, "ed": 30, "text": "deep learning"}, {"st": 37, "ed": 40, "text": "domain specific language"}, {"st": 44, "ed": 46, "text": "search algorithm"}, {"st": 80, "ed": 82, "text": "significantly outperforms"}]
[{"st": 6, "ed": 8, "text": "weakly supervised"}, {"st": 30, "ed": 32, "text": "attention based"}, {"st": 32, "ed": 34, "text": "deep learning"}, {"st": 36, "ed": 38, "text": "jointly learns"}, {"st": 71, "ed": 73, "text": "topic models"}, {"st": 85, "ed": 87, "text": "attention weights"}]
[{"st": 7, "ed": 9, "text": "recent years"}, {"st": 100, "ed": 102, "text": "open world"}, {"st": 105, "ed": 108, "text": "problem and propose"}, {"st": 110, "ed": 112, "text": "technique called"}, {"st": 113, "ed": 115, "text": "interactive learning"}]
[]
[{"st": 6, "ed": 8, "text": "wide variety"}]
[{"st": 103, "ed": 105, "text": "user study"}, {"st": 122, "ed": 124, "text": "visualization method"}]
[{"st": 24, "ed": 26, "text": "automatic evaluation"}, {"st": 41, "ed": 43, "text": "n gram"}]
[{"st": 0, "ed": 2, "text": "autonomous systems"}, {"st": 36, "ed": 38, "text": "natural language"}, {"st": 103, "ed": 105, "text": "mental model"}]
[{"st": 55, "ed": 57, "text": "content analysis"}, {"st": 63, "ed": 65, "text": "content analysis"}, {"st": 108, "ed": 110, "text": "latent semantic"}, {"st": 133, "ed": 135, "text": "network structure"}, {"st": 141, "ed": 143, "text": "previous studies"}, {"st": 186, "ed": 188, "text": "proposed framework"}, {"st": 191, "ed": 193, "text": "proposed method"}, {"st": 195, "ed": 197, "text": "large scale"}, {"st": 210, "ed": 212, "text": "experiment results"}, {"st": 215, "ed": 217, "text": "proposed framework"}, {"st": 217, "ed": 219, "text": "outperforms existing"}]
[{"st": 10, "ed": 12, "text": "hate speech"}, {"st": 20, "ed": 22, "text": "natural language"}, {"st": 49, "ed": 52, "text": "qualitative and quantitative"}]
[{"st": 6, "ed": 8, "text": "text corpus"}, {"st": 130, "ed": 132, "text": "significantly outperform"}, {"st": 165, "ed": 167, "text": "baseline models"}]
[{"st": 1, "ed": 3, "text": "increasing attention"}, {"st": 88, "ed": 90, "text": "previous works"}, {"st": 94, "ed": 96, "text": "proposed method"}, {"st": 96, "ed": 99, "text": "takes advantage of"}, {"st": 138, "ed": 140, "text": "proposed method"}]
[]
[{"st": 8, "ed": 10, "text": "low dimensional"}, {"st": 10, "ed": 12, "text": "vector representations"}, {"st": 20, "ed": 22, "text": "machine learning"}, {"st": 23, "ed": 25, "text": "natural language"}, {"st": 40, "ed": 43, "text": "named entity recognition"}, {"st": 43, "ed": 45, "text": "sentiment analysis"}, {"st": 45, "ed": 47, "text": "machine translation"}, {"st": 65, "ed": 67, "text": "multi core"}]
[{"st": 4, "ed": 6, "text": "statistical model"}, {"st": 152, "ed": 154, "text": "sentence level"}, {"st": 159, "ed": 161, "text": "context free"}, {"st": 169, "ed": 171, "text": "closely related"}, {"st": 172, "ed": 174, "text": "context free"}]
[{"st": 14, "ed": 16, "text": "similarity measure"}, {"st": 54, "ed": 56, "text": "proposed method"}, {"st": 86, "ed": 88, "text": "outperform traditional"}]
[{"st": 13, "ed": 15, "text": "machine learning"}, {"st": 57, "ed": 59, "text": "numerical analysis"}, {"st": 63, "ed": 65, "text": "authorship attribution"}]
[{"st": 8, "ed": 10, "text": "natural language"}, {"st": 144, "ed": 146, "text": "open source"}, {"st": 150, "ed": 152, "text": "100 000"}, {"st": 156, "ed": 158, "text": "highly effective"}, {"st": 185, "ed": 187, "text": "missing entries"}]
[]
[{"st": 3, "ed": 5, "text": "large scale"}, {"st": 50, "ed": 52, "text": "cluster analysis"}, {"st": 68, "ed": 70, "text": "spanish language"}]
[{"st": 103, "ed": 105, "text": "social networking"}, {"st": 111, "ed": 113, "text": "computational linguistics"}, {"st": 127, "ed": 129, "text": "baseline models"}, {"st": 137, "ed": 139, "text": "large data"}, {"st": 147, "ed": 149, "text": "multi level"}]
[{"st": 36, "ed": 38, "text": "football soccer"}, {"st": 64, "ed": 66, "text": "predictive models"}, {"st": 74, "ed": 77, "text": "english premier league"}, {"st": 93, "ed": 95, "text": "predictive models"}, {"st": 98, "ed": 100, "text": "historical data"}, {"st": 116, "ed": 118, "text": "final results"}, {"st": 199, "ed": 201, "text": "football soccer"}]
[{"st": 12, "ed": 14, "text": "applications including"}, {"st": 14, "ed": 16, "text": "cross lingual"}, {"st": 51, "ed": 53, "text": "parallel corpora"}, {"st": 118, "ed": 120, "text": "machine translation"}, {"st": 126, "ed": 128, "text": "additional information"}]
[{"st": 15, "ed": 18, "text": "statistical machine translation"}, {"st": 28, "ed": 30, "text": "text corpora"}, {"st": 92, "ed": 94, "text": "hierarchical models"}]
[{"st": 50, "ed": 52, "text": "quality assessment"}]
[{"st": 29, "ed": 31, "text": "significant improvements"}, {"st": 45, "ed": 47, "text": "nearest neighbor"}, {"st": 64, "ed": 67, "text": "linear discriminant analysis"}, {"st": 71, "ed": 73, "text": "class conditional"}, {"st": 88, "ed": 91, "text": "automatic speech recognition"}, {"st": 102, "ed": 105, "text": "deep neural network"}, {"st": 113, "ed": 115, "text": "output units"}, {"st": 120, "ed": 122, "text": "frame level"}, {"st": 164, "ed": 166, "text": "relative improvement"}, {"st": 195, "ed": 197, "text": "speaker recognition"}, {"st": 204, "ed": 206, "text": "output units"}, {"st": 215, "ed": 217, "text": "set size"}]
[{"st": 32, "ed": 34, "text": "time consuming"}, {"st": 87, "ed": 90, "text": "optical character recognition"}, {"st": 229, "ed": 231, "text": "larger scale"}, {"st": 237, "ed": 239, "text": "mechanical turk"}]
[{"st": 8, "ed": 10, "text": "social networking"}, {"st": 30, "ed": 32, "text": "text mining"}, {"st": 96, "ed": 98, "text": "textual data"}, {"st": 117, "ed": 119, "text": "cosine similarity"}, {"st": 120, "ed": 123, "text": "k means clustering"}, {"st": 132, "ed": 135, "text": "latent dirichlet allocation"}, {"st": 138, "ed": 140, "text": "topic modeling"}, {"st": 157, "ed": 159, "text": "textual data"}, {"st": 173, "ed": 175, "text": "latent dirichlet"}, {"st": 182, "ed": 184, "text": "latent topics"}]
[{"st": 72, "ed": 74, "text": "multiple times"}, {"st": 75, "ed": 77, "text": "large scale"}, {"st": 102, "ed": 104, "text": "fully automated"}, {"st": 107, "ed": 109, "text": "annotated data"}, {"st": 149, "ed": 151, "text": "important role"}]
[{"st": 3, "ed": 5, "text": "recent advances"}, {"st": 31, "ed": 33, "text": "nearest neighbor"}, {"st": 61, "ed": 64, "text": "automatic speech recognition"}, {"st": 83, "ed": 85, "text": "output units"}, {"st": 90, "ed": 92, "text": "frame level"}]
[{"st": 68, "ed": 70, "text": "topic models"}, {"st": 72, "ed": 74, "text": "large corpora"}, {"st": 96, "ed": 99, "text": "ability to capture"}, {"st": 121, "ed": 125, "text": "united states of america"}, {"st": 150, "ed": 152, "text": "whooping cough"}, {"st": 176, "ed": 178, "text": "time series"}]
[{"st": 23, "ed": 25, "text": "machine learning"}, {"st": 44, "ed": 46, "text": "trained models"}, {"st": 170, "ed": 172, "text": "meta data"}, {"st": 216, "ed": 218, "text": "sensitivity analysis"}]
[{"st": 10, "ed": 12, "text": "topic models"}, {"st": 14, "ed": 16, "text": "highly successful"}, {"st": 24, "ed": 26, "text": "text documents"}, {"st": 150, "ed": 153, "text": "quantitatively and qualitatively"}]
[{"st": 55, "ed": 57, "text": "training data"}, {"st": 61, "ed": 63, "text": "meta data"}]
[{"st": 1, "ed": 3, "text": "topic modeling"}, {"st": 41, "ed": 43, "text": "higher order"}, {"st": 57, "ed": 59, "text": "higher order"}, {"st": 86, "ed": 88, "text": "topic modeling"}, {"st": 102, "ed": 104, "text": "proposed approach"}, {"st": 145, "ed": 148, "text": "easy to implement"}, {"st": 181, "ed": 183, "text": "clustering accuracy"}]
[{"st": 7, "ed": 9, "text": "neural network"}, {"st": 37, "ed": 39, "text": "computation graph"}, {"st": 67, "ed": 69, "text": "computation graph"}, {"st": 83, "ed": 85, "text": "network outputs"}, {"st": 93, "ed": 95, "text": "network structures"}, {"st": 107, "ed": 109, "text": "network architectures"}, {"st": 112, "ed": 114, "text": "specifically designed"}, {"st": 130, "ed": 132, "text": "programming language"}, {"st": 145, "ed": 147, "text": "symbolic computation"}, {"st": 190, "ed": 192, "text": "significantly faster"}, {"st": 201, "ed": 203, "text": "open source"}]
[{"st": 3, "ed": 5, "text": "case study"}, {"st": 47, "ed": 49, "text": "distance based"}, {"st": 104, "ed": 106, "text": "hierarchical models"}, {"st": 115, "ed": 117, "text": "relative clause"}]
[{"st": 0, "ed": 2, "text": "word embedding"}, {"st": 16, "ed": 19, "text": "word co occurrence"}, {"st": 22, "ed": 24, "text": "word vectors"}, {"st": 46, "ed": 48, "text": "open research"}, {"st": 70, "ed": 73, "text": "low dimensional latent"}, {"st": 77, "ed": 79, "text": "synthetic data"}, {"st": 81, "ed": 83, "text": "generative process"}, {"st": 111, "ed": 113, "text": "substantial improvements"}]
[{"st": 25, "ed": 27, "text": "meta analysis"}, {"st": 208, "ed": 210, "text": "finite mixture"}, {"st": 218, "ed": 220, "text": "hierarchical bayesian"}, {"st": 220, "ed": 222, "text": "mixture models"}, {"st": 241, "ed": 243, "text": "mixture distribution"}]
[{"st": 10, "ed": 12, "text": "collaborative filtering"}, {"st": 18, "ed": 20, "text": "user preferences"}, {"st": 31, "ed": 33, "text": "minimization problem"}, {"st": 35, "ed": 37, "text": "random variables"}, {"st": 40, "ed": 42, "text": "theoretical analysis"}, {"st": 48, "ed": 51, "text": "empirical risk minimization"}, {"st": 53, "ed": 55, "text": "worst case"}, {"st": 63, "ed": 66, "text": "positive and negative"}, {"st": 71, "ed": 73, "text": "neural network"}, {"st": 75, "ed": 77, "text": "jointly learns"}, {"st": 102, "ed": 104, "text": "learning objective"}, {"st": 138, "ed": 140, "text": "dot product"}, {"st": 170, "ed": 172, "text": "extensive experiments"}, {"st": 174, "ed": 176, "text": "real world"}, {"st": 214, "ed": 216, "text": "collaborative filtering"}]
[{"st": 6, "ed": 9, "text": "electronic health records"}, {"st": 51, "ed": 53, "text": "recent years"}, {"st": 56, "ed": 58, "text": "text processing"}, {"st": 77, "ed": 79, "text": "topic modeling"}, {"st": 92, "ed": 95, "text": "latent semantic analysis"}, {"st": 100, "ed": 102, "text": "topic modeling"}, {"st": 125, "ed": 127, "text": "quantitative evaluations"}, {"st": 136, "ed": 139, "text": "latent dirichlet allocation"}]
[{"st": 20, "ed": 22, "text": "topic models"}, {"st": 25, "ed": 27, "text": "machine learning"}, {"st": 51, "ed": 54, "text": "latent dirichlet allocation"}, {"st": 56, "ed": 58, "text": "numerous applications"}, {"st": 63, "ed": 65, "text": "topic models"}, {"st": 73, "ed": 75, "text": "practical problems"}, {"st": 86, "ed": 88, "text": "statistical properties"}, {"st": 112, "ed": 114, "text": "text corpora"}, {"st": 142, "ed": 144, "text": "based approach"}, {"st": 152, "ed": 154, "text": "topic modeling"}, {"st": 174, "ed": 177, "text": "number of topics"}, {"st": 199, "ed": 201, "text": "approach outperforms"}, {"st": 205, "ed": 207, "text": "statistical model"}]
[{"st": 20, "ed": 22, "text": "lower house"}, {"st": 27, "ed": 29, "text": "irish parliament"}]
[{"st": 9, "ed": 11, "text": "speech recognition"}, {"st": 153, "ed": 156, "text": "word error rate"}, {"st": 169, "ed": 171, "text": "analysis shows"}]
[{"st": 23, "ed": 25, "text": "method works"}, {"st": 114, "ed": 116, "text": "co occurrence"}, {"st": 149, "ed": 151, "text": "learning process"}, {"st": 195, "ed": 197, "text": "speech recognition"}, {"st": 202, "ed": 204, "text": "encouraging results"}]
[{"st": 0, "ed": 2, "text": "attention based"}, {"st": 2, "ed": 4, "text": "encoder decoder"}, {"st": 23, "ed": 26, "text": "automatic speech recognition"}, {"st": 48, "ed": 50, "text": "asr systems"}, {"st": 66, "ed": 68, "text": "challenging tasks"}, {"st": 89, "ed": 91, "text": "significantly improve"}, {"st": 140, "ed": 143, "text": "word error rate"}, {"st": 163, "ed": 165, "text": "a 12"}, {"st": 168, "ed": 170, "text": "search task"}]
[{"st": 11, "ed": 13, "text": "online fashion"}, {"st": 96, "ed": 98, "text": "trained model"}, {"st": 131, "ed": 133, "text": "search task"}]
[{"st": 6, "ed": 8, "text": "attention based"}, {"st": 10, "ed": 13, "text": "automatic speech recognition"}, {"st": 20, "ed": 22, "text": "cross entropy"}, {"st": 28, "ed": 30, "text": "log likelihood"}, {"st": 42, "ed": 46, "text": "word error rate wer"}, {"st": 50, "ed": 52, "text": "asr systems"}, {"st": 72, "ed": 74, "text": "closely related"}, {"st": 85, "ed": 87, "text": "attention based"}, {"st": 98, "ed": 100, "text": "loss functions"}, {"st": 132, "ed": 134, "text": "sampling based"}, {"st": 143, "ed": 145, "text": "training procedure"}, {"st": 145, "ed": 147, "text": "improves performance"}, {"st": 165, "ed": 167, "text": "attention based"}]
[{"st": 2, "ed": 4, "text": "context dependent"}, {"st": 18, "ed": 20, "text": "status quo"}, {"st": 27, "ed": 30, "text": "end to end"}, {"st": 96, "ed": 99, "text": "end to end"}, {"st": 115, "ed": 117, "text": "probabilistic model"}, {"st": 120, "ed": 122, "text": "joint learning"}, {"st": 153, "ed": 156, "text": "end to end"}, {"st": 161, "ed": 164, "text": "end to end"}, {"st": 174, "ed": 176, "text": "large vocabulary"}, {"st": 178, "ed": 180, "text": "search task"}, {"st": 195, "ed": 197, "text": "based approaches"}]
[{"st": 1, "ed": 3, "text": "article presents"}, {"st": 37, "ed": 39, "text": "donald trump"}, {"st": 51, "ed": 53, "text": "meta data"}]
[{"st": 80, "ed": 82, "text": "latent representation"}, {"st": 94, "ed": 96, "text": "latent variable"}, {"st": 107, "ed": 109, "text": "information science"}, {"st": 116, "ed": 118, "text": "inference problem"}, {"st": 146, "ed": 148, "text": "proposed approach"}]
[{"st": 8, "ed": 10, "text": "maximum likelihood"}, {"st": 12, "ed": 15, "text": "end to end"}, {"st": 15, "ed": 17, "text": "speech recognition"}, {"st": 27, "ed": 29, "text": "maximum likelihood"}, {"st": 31, "ed": 33, "text": "performance metric"}, {"st": 35, "ed": 37, "text": "speech recognition"}, {"st": 38, "ed": 42, "text": "word error rate wer"}, {"st": 50, "ed": 52, "text": "objective function"}, {"st": 69, "ed": 71, "text": "maximum likelihood"}, {"st": 83, "ed": 85, "text": "directly optimize"}, {"st": 95, "ed": 97, "text": "joint training"}, {"st": 106, "ed": 109, "text": "end to end"}, {"st": 126, "ed": 129, "text": "wall street journal"}]
[{"st": 4, "ed": 7, "text": "end to end"}, {"st": 28, "ed": 31, "text": "end to end"}, {"st": 42, "ed": 45, "text": "end to end"}, {"st": 56, "ed": 59, "text": "end to end"}, {"st": 61, "ed": 63, "text": "speech recognition"}, {"st": 111, "ed": 113, "text": "performance improvement"}, {"st": 115, "ed": 118, "text": "wall street journal"}, {"st": 134, "ed": 137, "text": "end to end"}]
[{"st": 2, "ed": 5, "text": "named entity recognition"}, {"st": 23, "ed": 25, "text": "handcrafted features"}, {"st": 25, "ed": 27, "text": "specifically designed"}, {"st": 35, "ed": 37, "text": "generation process"}, {"st": 59, "ed": 61, "text": "recent studies"}, {"st": 63, "ed": 65, "text": "neural network"}, {"st": 91, "ed": 94, "text": "multi task learning"}, {"st": 101, "ed": 103, "text": "neural network"}, {"st": 137, "ed": 139, "text": "benchmark datasets"}, {"st": 155, "ed": 157, "text": "neural network"}, {"st": 160, "ed": 162, "text": "large margin"}, {"st": 166, "ed": 168, "text": "training data"}, {"st": 171, "ed": 173, "text": "analysis shows"}, {"st": 176, "ed": 178, "text": "performance gains"}, {"st": 183, "ed": 185, "text": "word level"}, {"st": 196, "ed": 198, "text": "text mining"}]
[{"st": 7, "ed": 9, "text": "great promise"}, {"st": 20, "ed": 22, "text": "social science"}, {"st": 33, "ed": 35, "text": "conceptual framework"}, {"st": 65, "ed": 67, "text": "text based"}, {"st": 73, "ed": 75, "text": "text based"}, {"st": 80, "ed": 82, "text": "latent representation"}, {"st": 98, "ed": 100, "text": "latent representation"}, {"st": 149, "ed": 151, "text": "text based"}]
[{"st": 27, "ed": 29, "text": "error rates"}, {"st": 40, "ed": 42, "text": "speech synthesis"}, {"st": 57, "ed": 59, "text": "examples include"}, {"st": 79, "ed": 81, "text": "low quality"}, {"st": 89, "ed": 92, "text": "generative adversarial network"}, {"st": 93, "ed": 95, "text": "speech enhancement"}, {"st": 101, "ed": 103, "text": "speech data"}, {"st": 118, "ed": 121, "text": "text to speech"}, {"st": 144, "ed": 146, "text": "significantly improved"}, {"st": 149, "ed": 151, "text": "low quality"}, {"st": 161, "ed": 163, "text": "significantly improved"}, {"st": 189, "ed": 191, "text": "low quality"}]
[{"st": 6, "ed": 8, "text": "neural network"}, {"st": 9, "ed": 11, "text": "recently proposed"}, {"st": 30, "ed": 32, "text": "neural network"}, {"st": 37, "ed": 39, "text": "maximum likelihood"}, {"st": 82, "ed": 84, "text": "maximum likelihood"}, {"st": 99, "ed": 101, "text": "hamming distance"}]
[{"st": 2, "ed": 4, "text": "binary data"}, {"st": 31, "ed": 33, "text": "detection theory"}, {"st": 36, "ed": 38, "text": "neural network"}]
[{"st": 5, "ed": 7, "text": "machine learning"}, {"st": 18, "ed": 20, "text": "global optimal"}, {"st": 26, "ed": 28, "text": "optimization problems"}, {"st": 31, "ed": 33, "text": "machine learning"}, {"st": 39, "ed": 41, "text": "machine learning"}, {"st": 68, "ed": 70, "text": "recently developed"}, {"st": 103, "ed": 105, "text": "based methods"}, {"st": 110, "ed": 112, "text": "machine learning"}, {"st": 146, "ed": 149, "text": "synthetic and real"}, {"st": 181, "ed": 184, "text": "global and local"}, {"st": 199, "ed": 201, "text": "evolutionary algorithms"}]
[{"st": 28, "ed": 30, "text": "input variables"}, {"st": 38, "ed": 40, "text": "input variables"}, {"st": 78, "ed": 80, "text": "cellular automata"}, {"st": 83, "ed": 85, "text": "classification task"}, {"st": 94, "ed": 96, "text": "genetic programming"}, {"st": 116, "ed": 118, "text": "special case"}, {"st": 130, "ed": 132, "text": "cellular automata"}]
[{"st": 2, "ed": 4, "text": "recent advances"}, {"st": 5, "ed": 7, "text": "synthetic biology"}, {"st": 8, "ed": 10, "text": "artificial life"}, {"st": 17, "ed": 19, "text": "hot topic"}, {"st": 220, "ed": 222, "text": "natural selection"}]
[{"st": 7, "ed": 9, "text": "machine learning"}, {"st": 90, "ed": 92, "text": "feature engineering"}, {"st": 131, "ed": 133, "text": "learning machines"}]
[{"st": 49, "ed": 51, "text": "monte carlo"}]
[{"st": 9, "ed": 11, "text": "recent progress"}, {"st": 15, "ed": 17, "text": "deep learning"}, {"st": 59, "ed": 61, "text": "predictive models"}, {"st": 73, "ed": 75, "text": "deep learning"}, {"st": 95, "ed": 97, "text": "predictive model"}, {"st": 119, "ed": 121, "text": "predictive models"}, {"st": 142, "ed": 145, "text": "deep generative models"}]
[]
[]
[{"st": 50, "ed": 52, "text": "low resolution"}]
[{"st": 29, "ed": 31, "text": "spatial information"}, {"st": 34, "ed": 36, "text": "temporal information"}, {"st": 62, "ed": 64, "text": "spatial temporal"}, {"st": 105, "ed": 107, "text": "generative model"}, {"st": 131, "ed": 133, "text": "probabilistic model"}, {"st": 167, "ed": 169, "text": "joint inference"}, {"st": 189, "ed": 191, "text": "proposed framework"}, {"st": 211, "ed": 214, "text": "bag of words"}, {"st": 223, "ed": 225, "text": "spatial temporal"}, {"st": 251, "ed": 253, "text": "text descriptions"}, {"st": 267, "ed": 269, "text": "empirically evaluated"}, {"st": 275, "ed": 277, "text": "ground truth"}]
[{"st": 47, "ed": 49, "text": "text based"}, {"st": 81, "ed": 83, "text": "ground truth"}, {"st": 110, "ed": 112, "text": "ground truth"}, {"st": 126, "ed": 128, "text": "computer vision"}]
[{"st": 4, "ed": 7, "text": "optical character recognition"}, {"st": 34, "ed": 36, "text": "real applications"}, {"st": 104, "ed": 106, "text": "character set"}, {"st": 122, "ed": 125, "text": "support vector machine"}, {"st": 128, "ed": 130, "text": "feature vectors"}]
[{"st": 15, "ed": 17, "text": "natural language"}, {"st": 74, "ed": 76, "text": "natural language"}, {"st": 125, "ed": 127, "text": "fundamental problems"}, {"st": 144, "ed": 146, "text": "object detection"}]
[{"st": 4, "ed": 7, "text": "end to end"}, {"st": 9, "ed": 11, "text": "bidirectional lstm"}, {"st": 12, "ed": 15, "text": "short term memory"}, {"st": 24, "ed": 29, "text": "deep convolutional neural network cnn"}, {"st": 41, "ed": 43, "text": "visual language"}, {"st": 51, "ed": 53, "text": "context information"}, {"st": 81, "ed": 83, "text": "visual language"}, {"st": 91, "ed": 93, "text": "multi scale"}, {"st": 99, "ed": 101, "text": "prevent overfitting"}, {"st": 110, "ed": 112, "text": "bidirectional lstm"}, {"st": 132, "ed": 134, "text": "caption generation"}, {"st": 137, "ed": 139, "text": "retrieval tasks"}, {"st": 141, "ed": 143, "text": "benchmark datasets"}, {"st": 151, "ed": 153, "text": "bidirectional lstm"}, {"st": 155, "ed": 157, "text": "highly competitive"}, {"st": 166, "ed": 168, "text": "caption generation"}, {"st": 174, "ed": 176, "text": "object detection"}, {"st": 176, "ed": 178, "text": "attention model"}, {"st": 180, "ed": 182, "text": "significantly outperform"}, {"st": 182, "ed": 184, "text": "recent methods"}]
[{"st": 45, "ed": 47, "text": "spatial relations"}, {"st": 78, "ed": 80, "text": "event recognition"}]
[{"st": 10, "ed": 12, "text": "distance based"}, {"st": 14, "ed": 17, "text": "world wide web"}, {"st": 28, "ed": 30, "text": "search engine"}]
[{"st": 48, "ed": 50, "text": "automatically generated"}]
[{"st": 8, "ed": 11, "text": "a long standing"}, {"st": 13, "ed": 15, "text": "computer vision"}, {"st": 16, "ed": 18, "text": "natural language"}, {"st": 21, "ed": 23, "text": "recent progress"}, {"st": 24, "ed": 26, "text": "object detection"}, {"st": 72, "ed": 74, "text": "based method"}, {"st": 131, "ed": 133, "text": "image description"}, {"st": 158, "ed": 160, "text": "ms coco"}]
[{"st": 0, "ed": 3, "text": "descriptive video service"}, {"st": 38, "ed": 40, "text": "computer vision"}]
[{"st": 42, "ed": 44, "text": "visual concepts"}, {"st": 97, "ed": 99, "text": "large scale"}, {"st": 100, "ed": 102, "text": "visual sentiment"}, {"st": 144, "ed": 146, "text": "prediction task"}, {"st": 167, "ed": 169, "text": "visual concepts"}]
[{"st": 5, "ed": 7, "text": "challenging research"}, {"st": 66, "ed": 68, "text": "pre training"}, {"st": 74, "ed": 76, "text": "recent approaches"}, {"st": 113, "ed": 115, "text": "n grams"}, {"st": 183, "ed": 185, "text": "domain adaptation"}, {"st": 201, "ed": 203, "text": "vision based"}]
[{"st": 44, "ed": 46, "text": "natural language"}, {"st": 80, "ed": 82, "text": "learning framework"}, {"st": 92, "ed": 95, "text": "structure and parameters"}, {"st": 110, "ed": 112, "text": "rgb d"}, {"st": 124, "ed": 126, "text": "natural language"}, {"st": 142, "ed": 144, "text": "word embeddings"}, {"st": 191, "ed": 193, "text": "learning framework"}]
[{"st": 55, "ed": 57, "text": "weakly supervised"}, {"st": 120, "ed": 122, "text": "textual description"}, {"st": 197, "ed": 199, "text": "machine learning"}]
[{"st": 44, "ed": 46, "text": "humanoid robot"}, {"st": 78, "ed": 80, "text": "lambda calculus"}, {"st": 90, "ed": 92, "text": "lambda calculus"}, {"st": 128, "ed": 130, "text": "propositional logic"}, {"st": 134, "ed": 136, "text": "experiments conducted"}, {"st": 146, "ed": 148, "text": "theoretical framework"}]
[{"st": 30, "ed": 32, "text": "visual question"}, {"st": 37, "ed": 40, "text": "visual question answering"}, {"st": 43, "ed": 46, "text": "recurrent neural networks"}, {"st": 48, "ed": 52, "text": "convolutional neural network cnn"}, {"st": 110, "ed": 113, "text": "amazon mechanical turk"}]
[{"st": 4, "ed": 6, "text": "open source"}, {"st": 11, "ed": 14, "text": "bag of words"}, {"st": 31, "ed": 33, "text": "document classification"}, {"st": 47, "ed": 49, "text": "low level"}, {"st": 63, "ed": 65, "text": "input features"}, {"st": 114, "ed": 116, "text": "continuous speech"}, {"st": 120, "ed": 122, "text": "sentiment analysis"}, {"st": 125, "ed": 127, "text": "improved results"}, {"st": 129, "ed": 131, "text": "feature representation"}]
[{"st": 18, "ed": 20, "text": "visual sentiment"}, {"st": 37, "ed": 39, "text": "visual concepts"}, {"st": 72, "ed": 74, "text": "fine tuning"}, {"st": 80, "ed": 84, "text": "extensive set of experiments"}, {"st": 84, "ed": 86, "text": "parameter tuning"}, {"st": 93, "ed": 95, "text": "higher accuracy"}, {"st": 117, "ed": 119, "text": "modern architecture"}, {"st": 204, "ed": 206, "text": "trained models"}]
[{"st": 23, "ed": 25, "text": "computational linguistics"}, {"st": 37, "ed": 39, "text": "visual concepts"}, {"st": 65, "ed": 67, "text": "word embeddings"}, {"st": 74, "ed": 76, "text": "low dimensional"}, {"st": 114, "ed": 116, "text": "visual semantic"}, {"st": 123, "ed": 125, "text": "clustering schemes"}, {"st": 128, "ed": 130, "text": "visual concepts"}, {"st": 145, "ed": 147, "text": "visual semantic"}]
[{"st": 22, "ed": 24, "text": "social media"}, {"st": 46, "ed": 48, "text": "contextual information"}, {"st": 64, "ed": 66, "text": "social media"}, {"st": 109, "ed": 111, "text": "social media"}, {"st": 183, "ed": 185, "text": "neural network"}]
[{"st": 7, "ed": 11, "text": "visual question answering vqa"}, {"st": 14, "ed": 16, "text": "deep learning"}, {"st": 23, "ed": 28, "text": "long short term memory lstm"}, {"st": 31, "ed": 33, "text": "natural language"}, {"st": 37, "ed": 39, "text": "question answering"}, {"st": 39, "ed": 41, "text": "text based"}, {"st": 66, "ed": 68, "text": "vgg 16"}, {"st": 71, "ed": 74, "text": "convolutional neural networks"}, {"st": 76, "ed": 78, "text": "visual features"}, {"st": 86, "ed": 88, "text": "word embedding"}, {"st": 107, "ed": 110, "text": "visual question answering"}]
[{"st": 9, "ed": 11, "text": "encoder decoder"}, {"st": 16, "ed": 18, "text": "neural networks"}, {"st": 32, "ed": 34, "text": "attention mechanisms"}, {"st": 68, "ed": 70, "text": "spatial regions"}, {"st": 81, "ed": 83, "text": "attention model"}, {"st": 92, "ed": 94, "text": "spatial regions"}, {"st": 102, "ed": 104, "text": "image features"}, {"st": 104, "ed": 106, "text": "motion features"}, {"st": 113, "ed": 115, "text": "attention mechanism"}, {"st": 165, "ed": 167, "text": "significantly outperforms"}]
[{"st": 5, "ed": 7, "text": "social media"}, {"st": 78, "ed": 80, "text": "social media"}, {"st": 156, "ed": 159, "text": "visual and textual"}, {"st": 181, "ed": 184, "text": "deep neural network"}, {"st": 184, "ed": 186, "text": "visual features"}]
[{"st": 86, "ed": 88, "text": "visual semantic"}, {"st": 88, "ed": 90, "text": "embedding space"}]
[{"st": 135, "ed": 137, "text": "fine grained"}]
[{"st": 1, "ed": 3, "text": "cross modal"}, {"st": 30, "ed": 32, "text": "cross modal"}, {"st": 37, "ed": 40, "text": "image and text"}, {"st": 73, "ed": 75, "text": "existing works"}, {"st": 77, "ed": 80, "text": "deep neural network"}, {"st": 84, "ed": 86, "text": "common space"}, {"st": 106, "ed": 108, "text": "existing works"}, {"st": 112, "ed": 114, "text": "cross modal"}, {"st": 128, "ed": 131, "text": "end to end"}, {"st": 137, "ed": 139, "text": "cross modal"}, {"st": 173, "ed": 175, "text": "attention based"}, {"st": 181, "ed": 183, "text": "attention weights"}, {"st": 186, "ed": 188, "text": "fine grained"}, {"st": 188, "ed": 190, "text": "cross modal"}, {"st": 222, "ed": 224, "text": "cross modal"}, {"st": 227, "ed": 229, "text": "cross modal"}, {"st": 245, "ed": 247, "text": "large scale"}, {"st": 254, "ed": 256, "text": "proposed approach"}]
[{"st": 14, "ed": 16, "text": "deep learning"}, {"st": 20, "ed": 22, "text": "input features"}, {"st": 31, "ed": 34, "text": "word error rate"}, {"st": 50, "ed": 52, "text": "auto encoder"}, {"st": 90, "ed": 92, "text": "auto encoder"}]
[{"st": 3, "ed": 5, "text": "important applications"}, {"st": 28, "ed": 30, "text": "spoken language"}, {"st": 41, "ed": 43, "text": "spoken language"}, {"st": 60, "ed": 62, "text": "text descriptions"}, {"st": 90, "ed": 92, "text": "task specific"}, {"st": 92, "ed": 94, "text": "vision language"}, {"st": 103, "ed": 105, "text": "method outperforms"}, {"st": 105, "ed": 107, "text": "competing methods"}, {"st": 127, "ed": 129, "text": "vision language"}]
[{"st": 0, "ed": 2, "text": "sentiment analysis"}, {"st": 19, "ed": 21, "text": "potential applications"}, {"st": 30, "ed": 32, "text": "existing methods"}, {"st": 38, "ed": 40, "text": "visual data"}, {"st": 72, "ed": 75, "text": "visual and textual"}, {"st": 82, "ed": 85, "text": "end to end"}, {"st": 87, "ed": 90, "text": "convolutional neural network"}, {"st": 91, "ed": 93, "text": "jointly learn"}, {"st": 95, "ed": 97, "text": "visual sentiment"}, {"st": 110, "ed": 112, "text": "pooling layer"}, {"st": 115, "ed": 118, "text": "fully connected layers"}, {"st": 126, "ed": 128, "text": "proposed approach"}, {"st": 138, "ed": 140, "text": "method achieves"}]
[{"st": 0, "ed": 2, "text": "social media"}, {"st": 90, "ed": 92, "text": "deep learning"}, {"st": 96, "ed": 99, "text": "image and text"}, {"st": 100, "ed": 102, "text": "personality trait"}, {"st": 111, "ed": 113, "text": "highly correlated"}, {"st": 119, "ed": 121, "text": "key contribution"}, {"st": 127, "ed": 129, "text": "personality trait"}, {"st": 170, "ed": 173, "text": "deep neural networks"}, {"st": 193, "ed": 195, "text": "classification results"}]
[{"st": 13, "ed": 17, "text": "visual question answering vqa"}, {"st": 28, "ed": 30, "text": "goal oriented"}, {"st": 125, "ed": 127, "text": "visual question"}]
[{"st": 0, "ed": 2, "text": "sentiment analysis"}, {"st": 45, "ed": 47, "text": "deep learning"}, {"st": 51, "ed": 53, "text": "sentiment classification"}, {"st": 64, "ed": 66, "text": "multiple datasets"}, {"st": 81, "ed": 83, "text": "sentiment analysis"}, {"st": 110, "ed": 112, "text": "sentiment analysis"}, {"st": 120, "ed": 122, "text": "future research"}, {"st": 151, "ed": 153, "text": "visual features"}, {"st": 153, "ed": 155, "text": "cross modal"}]
[{"st": 18, "ed": 20, "text": "machine learning"}, {"st": 80, "ed": 83, "text": "goodness of fit"}, {"st": 102, "ed": 104, "text": "low resolution"}, {"st": 135, "ed": 137, "text": "surface gravity"}, {"st": 156, "ed": 160, "text": "signal to noise ratio"}]
[{"st": 5, "ed": 7, "text": "widely studied"}, {"st": 101, "ed": 103, "text": "optimal solution"}, {"st": 108, "ed": 110, "text": "optimal solutions"}, {"st": 135, "ed": 137, "text": "optimal solutions"}, {"st": 168, "ed": 170, "text": "optimal solution"}, {"st": 198, "ed": 200, "text": "swarm intelligence"}, {"st": 218, "ed": 220, "text": "simulated annealing"}, {"st": 237, "ed": 239, "text": "decision making"}]
[{"st": 80, "ed": 82, "text": "activation function"}, {"st": 88, "ed": 90, "text": "recurrent networks"}, {"st": 100, "ed": 102, "text": "sample based"}]
[{"st": 10, "ed": 12, "text": "probability distributions"}, {"st": 14, "ed": 16, "text": "exponential family"}, {"st": 17, "ed": 19, "text": "sufficient statistics"}, {"st": 69, "ed": 71, "text": "linear map"}, {"st": 124, "ed": 127, "text": "restricted boltzmann machine"}]
[]
[{"st": 26, "ed": 28, "text": "theoretical framework"}, {"st": 37, "ed": 39, "text": "sample based"}, {"st": 193, "ed": 195, "text": "membrane potential"}, {"st": 245, "ed": 247, "text": "logistic function"}, {"st": 276, "ed": 278, "text": "low power"}, {"st": 283, "ed": 285, "text": "machine learning"}]
[{"st": 54, "ed": 56, "text": "activation function"}, {"st": 61, "ed": 63, "text": "parameter space"}, {"st": 106, "ed": 108, "text": "recurrent networks"}, {"st": 118, "ed": 120, "text": "sample based"}]
[{"st": 31, "ed": 33, "text": "object recognition"}, {"st": 57, "ed": 59, "text": "object classification"}, {"st": 99, "ed": 101, "text": "linear classification"}, {"st": 163, "ed": 165, "text": "finite sample"}, {"st": 213, "ed": 215, "text": "numerical simulations"}, {"st": 216, "ed": 218, "text": "recently developed"}, {"st": 221, "ed": 223, "text": "maximum margin"}, {"st": 240, "ed": 242, "text": "statistical mechanics"}, {"st": 243, "ed": 245, "text": "linear classification"}, {"st": 259, "ed": 261, "text": "deep networks"}, {"st": 263, "ed": 265, "text": "object recognition"}]
[{"st": 96, "ed": 98, "text": "experiments conducted"}, {"st": 103, "ed": 105, "text": "content based"}, {"st": 113, "ed": 116, "text": "world wide web"}, {"st": 132, "ed": 134, "text": "natural language"}, {"st": 137, "ed": 139, "text": "machine learning"}]
[{"st": 10, "ed": 12, "text": "physical review"}, {"st": 72, "ed": 74, "text": "physical review"}]
[{"st": 47, "ed": 49, "text": "naive bayesian"}, {"st": 85, "ed": 87, "text": "cost sensitive"}, {"st": 97, "ed": 99, "text": "set size"}, {"st": 117, "ed": 119, "text": "naive bayesian"}]
[{"st": 6, "ed": 8, "text": "machine learning"}, {"st": 56, "ed": 58, "text": "naive bayesian"}, {"st": 83, "ed": 85, "text": "naive bayesian"}, {"st": 105, "ed": 107, "text": "naive bayesian"}, {"st": 118, "ed": 120, "text": "cost sensitive"}]
[]
[{"st": 1, "ed": 3, "text": "academic journals"}, {"st": 46, "ed": 48, "text": "wide variety"}, {"st": 73, "ed": 75, "text": "supervised learning"}, {"st": 87, "ed": 89, "text": "learning algorithm"}, {"st": 94, "ed": 97, "text": "positive or negative"}, {"st": 108, "ed": 110, "text": "decision tree"}, {"st": 165, "ed": 167, "text": "domain knowledge"}, {"st": 207, "ed": 209, "text": "wide variety"}]
[{"st": 5, "ed": 7, "text": "unsupervised learning"}, {"st": 86, "ed": 88, "text": "mutual information"}, {"st": 98, "ed": 100, "text": "mutual information"}]
[{"st": 5, "ed": 7, "text": "unsupervised learning"}, {"st": 29, "ed": 31, "text": "mutual information"}, {"st": 47, "ed": 49, "text": "empirically evaluated"}, {"st": 100, "ed": 103, "text": "latent semantic analysis"}, {"st": 119, "ed": 121, "text": "potential applications"}, {"st": 124, "ed": 126, "text": "unsupervised learning"}, {"st": 137, "ed": 139, "text": "latent semantic"}]
[{"st": 8, "ed": 10, "text": "text based"}, {"st": 12, "ed": 15, "text": "vector space model"}, {"st": 23, "ed": 25, "text": "analogy questions"}, {"st": 63, "ed": 65, "text": "analogy questions"}, {"st": 67, "ed": 69, "text": "word pair"}, {"st": 80, "ed": 82, "text": "word pair"}, {"st": 103, "ed": 105, "text": "analogy questions"}, {"st": 122, "ed": 124, "text": "cognitive science"}, {"st": 135, "ed": 137, "text": "natural language"}, {"st": 176, "ed": 178, "text": "nearest neighbour"}, {"st": 217, "ed": 219, "text": "learning algorithm"}]
[{"st": 158, "ed": 160, "text": "experiments demonstrate"}, {"st": 175, "ed": 177, "text": "domain specific"}, {"st": 188, "ed": 190, "text": "computer science"}]
[{"st": 35, "ed": 38, "text": "positive or negative"}, {"st": 55, "ed": 57, "text": "text classification"}, {"st": 95, "ed": 98, "text": "positive and negative"}, {"st": 117, "ed": 119, "text": "mutual information"}, {"st": 121, "ed": 124, "text": "latent semantic analysis"}]
[{"st": 4, "ed": 6, "text": "natural language"}, {"st": 34, "ed": 36, "text": "ensemble methods"}, {"st": 58, "ed": 60, "text": "probability distributions"}, {"st": 93, "ed": 95, "text": "lexical semantics"}, {"st": 125, "ed": 127, "text": "statistically significant"}]
[{"st": 8, "ed": 11, "text": "word sense disambiguation"}, {"st": 33, "ed": 36, "text": "supervised machine learning"}, {"st": 44, "ed": 46, "text": "machine learning"}, {"st": 50, "ed": 52, "text": "rule based"}, {"st": 61, "ed": 63, "text": "feature vectors"}, {"st": 98, "ed": 100, "text": "co occurrence"}]
[{"st": 15, "ed": 17, "text": "potential applications"}, {"st": 23, "ed": 26, "text": "word sense disambiguation"}, {"st": 26, "ed": 28, "text": "machine translation"}, {"st": 83, "ed": 85, "text": "word pair"}, {"st": 97, "ed": 99, "text": "semantic similarity"}, {"st": 109, "ed": 112, "text": "vector space model"}, {"st": 138, "ed": 140, "text": "multiple choice"}, {"st": 191, "ed": 195, "text": "singular value decomposition svd"}, {"st": 209, "ed": 212, "text": "latent semantic analysis"}, {"st": 214, "ed": 216, "text": "automatically generated"}, {"st": 232, "ed": 234, "text": "analogy questions"}, {"st": 247, "ed": 250, "text": "problem of classifying"}]
[{"st": 4, "ed": 6, "text": "natural language"}, {"st": 34, "ed": 36, "text": "ensemble methods"}, {"st": 56, "ed": 58, "text": "probability distributions"}, {"st": 89, "ed": 91, "text": "lexical semantics"}, {"st": 121, "ed": 123, "text": "statistically significant"}]
[{"st": 55, "ed": 57, "text": "cognitive science"}, {"st": 74, "ed": 77, "text": "vector space model"}, {"st": 139, "ed": 142, "text": "singular value decomposition"}, {"st": 176, "ed": 178, "text": "multiple choice"}, {"st": 179, "ed": 181, "text": "analogy questions"}, {"st": 202, "ed": 204, "text": "analogy questions"}]
[{"st": 8, "ed": 10, "text": "text based"}, {"st": 12, "ed": 15, "text": "vector space model"}, {"st": 23, "ed": 25, "text": "analogy questions"}, {"st": 62, "ed": 64, "text": "analogy questions"}, {"st": 66, "ed": 68, "text": "word pair"}, {"st": 79, "ed": 81, "text": "word pair"}, {"st": 102, "ed": 104, "text": "analogy questions"}, {"st": 114, "ed": 117, "text": "senior high school"}, {"st": 135, "ed": 137, "text": "natural language"}, {"st": 176, "ed": 178, "text": "nearest neighbour"}, {"st": 217, "ed": 219, "text": "learning algorithm"}]
[{"st": 60, "ed": 62, "text": "word pair"}, {"st": 86, "ed": 88, "text": "potential applications"}, {"st": 94, "ed": 97, "text": "word sense disambiguation"}, {"st": 102, "ed": 105, "text": "vector space model"}, {"st": 128, "ed": 130, "text": "multiple choice"}, {"st": 177, "ed": 181, "text": "singular value decomposition svd"}, {"st": 190, "ed": 192, "text": "automatically generated"}, {"st": 208, "ed": 210, "text": "analogy questions"}, {"st": 223, "ed": 226, "text": "problem of classifying"}]
[{"st": 28, "ed": 30, "text": "wide variety"}, {"st": 54, "ed": 56, "text": "ad hoc"}, {"st": 104, "ed": 106, "text": "machine learning"}, {"st": 110, "ed": 112, "text": "word pairs"}, {"st": 119, "ed": 121, "text": "multiple choice"}, {"st": 122, "ed": 124, "text": "analogy questions"}]
[{"st": 36, "ed": 39, "text": "named entity recognition"}, {"st": 72, "ed": 74, "text": "natural language"}, {"st": 111, "ed": 113, "text": "classification performance"}, {"st": 113, "ed": 115, "text": "significantly higher"}, {"st": 146, "ed": 149, "text": "named entity recognition"}, {"st": 153, "ed": 155, "text": "substantial improvement"}, {"st": 174, "ed": 176, "text": "linear classifier"}, {"st": 231, "ed": 233, "text": "detection method"}]
[{"st": 53, "ed": 55, "text": "approximate inference"}, {"st": 60, "ed": 63, "text": "large data sets"}]
[{"st": 7, "ed": 9, "text": "plain text"}, {"st": 128, "ed": 130, "text": "prediction tasks"}, {"st": 209, "ed": 211, "text": "small scale"}, {"st": 216, "ed": 218, "text": "real world"}, {"st": 233, "ed": 235, "text": "automatically learn"}]
[{"st": 13, "ed": 15, "text": "environmental change"}, {"st": 45, "ed": 47, "text": "machine reading"}, {"st": 64, "ed": 66, "text": "performs comparably"}, {"st": 69, "ed": 71, "text": "complex data"}, {"st": 73, "ed": 75, "text": "inference tasks"}, {"st": 81, "ed": 83, "text": "unlike traditional"}, {"st": 120, "ed": 122, "text": "machine reading"}]
[{"st": 2, "ed": 4, "text": "topic modeling"}, {"st": 6, "ed": 8, "text": "text corpora"}, {"st": 38, "ed": 40, "text": "post processing"}, {"st": 47, "ed": 49, "text": "topic models"}, {"st": 52, "ed": 54, "text": "n gram"}, {"st": 61, "ed": 63, "text": "low quality"}, {"st": 83, "ed": 85, "text": "computationally efficient"}, {"st": 108, "ed": 110, "text": "topic model"}, {"st": 130, "ed": 133, "text": "bag of words"}, {"st": 133, "ed": 135, "text": "topic model"}]
[{"st": 0, "ed": 2, "text": "sentiment analysis"}, {"st": 8, "ed": 10, "text": "open research"}, {"st": 16, "ed": 18, "text": "web 2.0"}, {"st": 27, "ed": 29, "text": "social media"}, {"st": 36, "ed": 38, "text": "sentiment analysis"}, {"st": 51, "ed": 53, "text": "text classification"}, {"st": 57, "ed": 59, "text": "text classification"}, {"st": 70, "ed": 72, "text": "feature selection"}, {"st": 75, "ed": 77, "text": "sentiment analysis"}, {"st": 92, "ed": 94, "text": "sentiment analysis"}, {"st": 100, "ed": 102, "text": "feature selection"}, {"st": 104, "ed": 106, "text": "sentiment analysis"}, {"st": 120, "ed": 122, "text": "feature selection"}, {"st": 129, "ed": 131, "text": "chi squared"}, {"st": 175, "ed": 177, "text": "feature selection"}]
[{"st": 4, "ed": 6, "text": "speech perception"}, {"st": 76, "ed": 78, "text": "speech perception"}, {"st": 250, "ed": 252, "text": "speech perception"}]
[{"st": 11, "ed": 14, "text": "restricted boltzmann machine"}, {"st": 16, "ed": 18, "text": "multivariate gaussian"}, {"st": 18, "ed": 21, "text": "restricted boltzmann machine"}, {"st": 74, "ed": 77, "text": "deep neural network"}]
[{"st": 85, "ed": 87, "text": "classification algorithm"}]
[{"st": 2, "ed": 4, "text": "iq test"}, {"st": 61, "ed": 63, "text": "artificial intelligence"}, {"st": 66, "ed": 68, "text": "deep learning"}, {"st": 71, "ed": 73, "text": "recently developed"}, {"st": 74, "ed": 76, "text": "successfully applied"}, {"st": 96, "ed": 98, "text": "word embedding"}, {"st": 155, "ed": 157, "text": "distributed representations"}, {"st": 165, "ed": 167, "text": "word embedding"}, {"st": 204, "ed": 206, "text": "word representations"}, {"st": 215, "ed": 217, "text": "proposed framework"}, {"st": 220, "ed": 222, "text": "outperform existing"}, {"st": 236, "ed": 239, "text": "amazon mechanical turk"}, {"st": 253, "ed": 255, "text": "deep learning"}]
[{"st": 62, "ed": 64, "text": "supervised methods"}, {"st": 83, "ed": 87, "text": "unsupervised and semi supervised"}, {"st": 108, "ed": 112, "text": "unsupervised and semi supervised"}, {"st": 118, "ed": 120, "text": "training data"}, {"st": 124, "ed": 126, "text": "without compromising"}, {"st": 129, "ed": 131, "text": "fine grained"}, {"st": 149, "ed": 152, "text": "hidden markov models"}, {"st": 165, "ed": 167, "text": "probability distributions"}, {"st": 171, "ed": 173, "text": "empirical evaluations"}]
[{"st": 2, "ed": 4, "text": "sentence level"}, {"st": 5, "ed": 7, "text": "topic model"}, {"st": 10, "ed": 12, "text": "topic model"}, {"st": 45, "ed": 47, "text": "topic models"}, {"st": 68, "ed": 72, "text": "recurrent neural networks rnn"}, {"st": 82, "ed": 84, "text": "strong baselines"}, {"st": 90, "ed": 92, "text": "automatically generate"}, {"st": 106, "ed": 108, "text": "real world"}]
[{"st": 43, "ed": 45, "text": "natural language"}, {"st": 62, "ed": 64, "text": "joint learning"}, {"st": 129, "ed": 131, "text": "task performance"}, {"st": 133, "ed": 135, "text": "latent variable"}]
[{"st": 3, "ed": 5, "text": "text classification"}, {"st": 49, "ed": 51, "text": "cross language"}, {"st": 62, "ed": 64, "text": "multi view"}, {"st": 67, "ed": 69, "text": "cross language"}, {"st": 76, "ed": 78, "text": "parallel corpora"}, {"st": 86, "ed": 88, "text": "training error"}, {"st": 106, "ed": 108, "text": "empirical study"}, {"st": 113, "ed": 115, "text": "cross language"}, {"st": 115, "ed": 117, "text": "text classification"}, {"st": 120, "ed": 122, "text": "proposed method"}, {"st": 122, "ed": 124, "text": "consistently outperforms"}, {"st": 129, "ed": 131, "text": "domain adaptation"}, {"st": 133, "ed": 135, "text": "multi view"}]
[{"st": 11, "ed": 14, "text": "naive bayes classifier"}, {"st": 28, "ed": 30, "text": "n grams"}, {"st": 31, "ed": 33, "text": "feature selection"}, {"st": 34, "ed": 36, "text": "mutual information"}, {"st": 39, "ed": 41, "text": "significant improvement"}, {"st": 47, "ed": 49, "text": "highly accurate"}, {"st": 59, "ed": 61, "text": "naive bayes"}, {"st": 65, "ed": 68, "text": "training and testing"}]
[{"st": 5, "ed": 8, "text": "support vector machine"}, {"st": 11, "ed": 14, "text": "automatic speech recognition"}, {"st": 15, "ed": 17, "text": "front end"}, {"st": 53, "ed": 55, "text": "ensemble methods"}, {"st": 59, "ed": 61, "text": "front end"}, {"st": 76, "ed": 78, "text": "additive noise"}, {"st": 87, "ed": 89, "text": "classification task"}, {"st": 98, "ed": 100, "text": "front end"}, {"st": 105, "ed": 107, "text": "front end"}, {"st": 116, "ed": 120, "text": "signal to noise ratio"}, {"st": 129, "ed": 131, "text": "front end"}, {"st": 134, "ed": 136, "text": "front end"}]
[{"st": 0, "ed": 2, "text": "complex questions"}, {"st": 22, "ed": 24, "text": "document summarization"}, {"st": 61, "ed": 64, "text": "statistical machine learning"}, {"st": 65, "ed": 67, "text": "k means"}, {"st": 68, "ed": 71, "text": "expectation maximization em"}, {"st": 91, "ed": 93, "text": "approach outperforms"}, {"st": 114, "ed": 116, "text": "feature set"}, {"st": 145, "ed": 147, "text": "cosine similarity"}, {"st": 150, "ed": 152, "text": "kernel based"}, {"st": 165, "ed": 167, "text": "local search"}, {"st": 186, "ed": 188, "text": "kernel functions"}, {"st": 195, "ed": 197, "text": "complex tasks"}, {"st": 220, "ed": 222, "text": "complex questions"}, {"st": 233, "ed": 235, "text": "k means"}, {"st": 249, "ed": 252, "text": "bag of words"}]
[{"st": 5, "ed": 7, "text": "topic model"}, {"st": 22, "ed": 24, "text": "latent topic"}, {"st": 111, "ed": 113, "text": "substantial improvements"}, {"st": 116, "ed": 118, "text": "previously proposed"}]
[{"st": 43, "ed": 45, "text": "clustering algorithm"}, {"st": 82, "ed": 84, "text": "similarity metric"}, {"st": 86, "ed": 88, "text": "manually annotated"}, {"st": 98, "ed": 100, "text": "feature space"}, {"st": 118, "ed": 120, "text": "fine tuning"}, {"st": 121, "ed": 123, "text": "similarity function"}, {"st": 126, "ed": 128, "text": "relevant features"}, {"st": 137, "ed": 139, "text": "clustering algorithm"}]
[{"st": 1, "ed": 3, "text": "recent years"}, {"st": 6, "ed": 8, "text": "machine learning"}, {"st": 28, "ed": 30, "text": "text classification"}, {"st": 85, "ed": 87, "text": "random sampling"}, {"st": 98, "ed": 101, "text": "support vector machine"}, {"st": 104, "ed": 106, "text": "method outperforms"}, {"st": 109, "ed": 111, "text": "sampling methods"}]
[{"st": 1, "ed": 3, "text": "collaborative filtering"}, {"st": 67, "ed": 69, "text": "electronic commerce"}, {"st": 109, "ed": 112, "text": "latent dirichlet allocation"}, {"st": 141, "ed": 143, "text": "latent space"}, {"st": 159, "ed": 161, "text": "similarity measure"}, {"st": 170, "ed": 172, "text": "latent topic"}, {"st": 207, "ed": 210, "text": "approach significantly outperforms"}, {"st": 221, "ed": 223, "text": "classification accuracy"}, {"st": 226, "ed": 228, "text": "precision recall"}]
[{"st": 3, "ed": 6, "text": "unsupervised feature learning"}, {"st": 9, "ed": 11, "text": "latent representations"}, {"st": 38, "ed": 40, "text": "latent spaces"}, {"st": 59, "ed": 61, "text": "continuous space"}, {"st": 61, "ed": 63, "text": "word representations"}, {"st": 75, "ed": 77, "text": "learned features"}]
[{"st": 16, "ed": 18, "text": "knowledge engineering"}, {"st": 43, "ed": 46, "text": "divide and conquer"}, {"st": 80, "ed": 82, "text": "real world"}, {"st": 102, "ed": 105, "text": "orders of magnitude"}]
[{"st": 0, "ed": 2, "text": "topic models"}]
[{"st": 7, "ed": 9, "text": "structural similarity"}, {"st": 47, "ed": 49, "text": "kernel based"}, {"st": 57, "ed": 59, "text": "similarity measure"}, {"st": 77, "ed": 79, "text": "similarity measures"}, {"st": 81, "ed": 83, "text": "unlabeled data"}, {"st": 88, "ed": 90, "text": "learning task"}, {"st": 101, "ed": 103, "text": "promising results"}]
[{"st": 7, "ed": 10, "text": "a long standing"}, {"st": 52, "ed": 54, "text": "machine learning"}, {"st": 118, "ed": 120, "text": "rule based"}, {"st": 142, "ed": 144, "text": "inference tasks"}, {"st": 148, "ed": 151, "text": "orders of magnitude"}]
[{"st": 0, "ed": 2, "text": "topic modeling"}, {"st": 16, "ed": 19, "text": "bag of words"}, {"st": 55, "ed": 57, "text": "gaussian mixture"}, {"st": 58, "ed": 60, "text": "topic model"}, {"st": 86, "ed": 88, "text": "multi dimensional"}, {"st": 127, "ed": 129, "text": "gaussian mixture"}, {"st": 142, "ed": 144, "text": "extensive experiments"}, {"st": 167, "ed": 169, "text": "topic modeling"}]
[{"st": 35, "ed": 37, "text": "clinical practice"}, {"st": 53, "ed": 55, "text": "supervised learning"}, {"st": 64, "ed": 66, "text": "manually annotated"}, {"st": 100, "ed": 102, "text": "training data"}, {"st": 106, "ed": 108, "text": "low quality"}, {"st": 109, "ed": 111, "text": "training data"}, {"st": 134, "ed": 136, "text": "annotated data"}, {"st": 173, "ed": 175, "text": "training data"}, {"st": 202, "ed": 204, "text": "training data"}, {"st": 224, "ed": 226, "text": "training set"}]
[{"st": 14, "ed": 16, "text": "existing methods"}, {"st": 25, "ed": 27, "text": "feature set"}, {"st": 46, "ed": 49, "text": "deep neural network"}, {"st": 75, "ed": 77, "text": "proposed method"}, {"st": 90, "ed": 92, "text": "significantly outperforms"}, {"st": 100, "ed": 102, "text": "proposed method"}, {"st": 108, "ed": 110, "text": "prediction error"}, {"st": 131, "ed": 133, "text": "feature set"}]
[{"st": 16, "ed": 18, "text": "large scale"}, {"st": 28, "ed": 31, "text": "hundreds of thousands"}, {"st": 63, "ed": 65, "text": "evaluation measures"}]
[{"st": 16, "ed": 18, "text": "character level"}, {"st": 47, "ed": 49, "text": "constraint based"}, {"st": 55, "ed": 57, "text": "domain knowledge"}, {"st": 63, "ed": 66, "text": "deep neural networks"}, {"st": 94, "ed": 96, "text": "constraint based"}]
[{"st": 7, "ed": 11, "text": "deep recurrent neural networks"}, {"st": 42, "ed": 44, "text": "context information"}, {"st": 64, "ed": 66, "text": "fine tune"}, {"st": 68, "ed": 71, "text": "automatic speech recognition"}]
[{"st": 0, "ed": 2, "text": "higher order"}, {"st": 9, "ed": 13, "text": "singular value decomposition svd"}, {"st": 27, "ed": 29, "text": "powerful tool"}, {"st": 31, "ed": 33, "text": "achieved impressive"}, {"st": 37, "ed": 39, "text": "collaborative filtering"}, {"st": 39, "ed": 41, "text": "computational linguistics"}, {"st": 60, "ed": 62, "text": "potential applications"}, {"st": 69, "ed": 71, "text": "higher order"}, {"st": 79, "ed": 81, "text": "higher order"}, {"st": 83, "ed": 85, "text": "higher order"}, {"st": 85, "ed": 88, "text": "singular value decomposition"}, {"st": 90, "ed": 92, "text": "higher order"}]
[{"st": 123, "ed": 125, "text": "open source"}]
[{"st": 22, "ed": 24, "text": "visualization techniques"}, {"st": 52, "ed": 58, "text": "experiments on synthetic and real world"}]
[{"st": 3, "ed": 5, "text": "open domain"}]
[{"st": 7, "ed": 9, "text": "relation extraction"}, {"st": 32, "ed": 34, "text": "scoring functions"}, {"st": 38, "ed": 40, "text": "low dimensional"}, {"st": 56, "ed": 58, "text": "new york"}, {"st": 90, "ed": 92, "text": "existing methods"}]
[]
[{"st": 0, "ed": 2, "text": "authorship attribution"}, {"st": 10, "ed": 12, "text": "authorship attribution"}, {"st": 29, "ed": 31, "text": "statistical methods"}, {"st": 114, "ed": 116, "text": "pre processing"}]
[{"st": 11, "ed": 13, "text": "distributional semantics"}, {"st": 42, "ed": 44, "text": "high dimensional"}, {"st": 128, "ed": 130, "text": "compositional distributional"}, {"st": 132, "ed": 134, "text": "natural language"}, {"st": 150, "ed": 152, "text": "competing approaches"}, {"st": 156, "ed": 158, "text": "natural language"}, {"st": 165, "ed": 167, "text": "computational linguistics"}, {"st": 202, "ed": 204, "text": "compositional distributional"}, {"st": 224, "ed": 226, "text": "compositional distributional"}, {"st": 239, "ed": 241, "text": "category theory"}, {"st": 262, "ed": 265, "text": "directions for future"}]
[{"st": 5, "ed": 7, "text": "web 2.0"}, {"st": 23, "ed": 25, "text": "feature based"}, {"st": 77, "ed": 80, "text": "supervised machine learning"}, {"st": 81, "ed": 83, "text": "rule based"}, {"st": 102, "ed": 104, "text": "proposed approach"}, {"st": 105, "ed": 108, "text": "supervised machine learning"}, {"st": 125, "ed": 127, "text": "rule based"}]
[{"st": 0, "ed": 2, "text": "social media"}, {"st": 49, "ed": 51, "text": "social media"}, {"st": 56, "ed": 58, "text": "individual level"}, {"st": 115, "ed": 117, "text": "text analysis"}, {"st": 117, "ed": 119, "text": "anomaly detection"}, {"st": 120, "ed": 122, "text": "social network"}]
[{"st": 0, "ed": 2, "text": "topic modeling"}, {"st": 14, "ed": 16, "text": "text corpus"}, {"st": 37, "ed": 39, "text": "topic modeling"}, {"st": 58, "ed": 61, "text": "number of topics"}, {"st": 119, "ed": 122, "text": "number of topics"}, {"st": 133, "ed": 135, "text": "topic modeling"}, {"st": 138, "ed": 140, "text": "matrix factorization"}, {"st": 155, "ed": 157, "text": "model selection"}]
[{"st": 14, "ed": 16, "text": "relational database"}, {"st": 114, "ed": 116, "text": "probabilistic inference"}, {"st": 223, "ed": 225, "text": "feature engineering"}, {"st": 237, "ed": 240, "text": "end to end"}]
[{"st": 3, "ed": 5, "text": "document classification"}, {"st": 42, "ed": 44, "text": "variational bayesian"}, {"st": 45, "ed": 48, "text": "nonnegative matrix factorization"}, {"st": 64, "ed": 66, "text": "latent semantic"}, {"st": 69, "ed": 71, "text": "tf idf"}]
[{"st": 11, "ed": 13, "text": "topic models"}, {"st": 17, "ed": 20, "text": "number of topics"}, {"st": 39, "ed": 41, "text": "statistical model"}, {"st": 43, "ed": 45, "text": "cross validation"}, {"st": 49, "ed": 51, "text": "bayesian nonparametric"}, {"st": 53, "ed": 56, "text": "hierarchical dirichlet process"}, {"st": 66, "ed": 68, "text": "inference algorithm"}, {"st": 75, "ed": 77, "text": "parameter values"}, {"st": 84, "ed": 86, "text": "big data"}, {"st": 129, "ed": 132, "text": "latent semantic analysis"}, {"st": 135, "ed": 137, "text": "inference procedure"}, {"st": 171, "ed": 174, "text": "number of topics"}, {"st": 182, "ed": 185, "text": "goodness of fit"}, {"st": 200, "ed": 202, "text": "parameter free"}, {"st": 215, "ed": 217, "text": "weak supervision"}, {"st": 227, "ed": 229, "text": "parameter free"}, {"st": 229, "ed": 231, "text": "topic model"}, {"st": 234, "ed": 237, "text": "number of topics"}, {"st": 261, "ed": 265, "text": "synthetic and real data"}, {"st": 268, "ed": 270, "text": "parameter free"}, {"st": 270, "ed": 272, "text": "topic model"}, {"st": 281, "ed": 283, "text": "topic models"}, {"st": 284, "ed": 286, "text": "manual transmission"}, {"st": 297, "ed": 299, "text": "bayesian nonparametric"}]
[{"st": 10, "ed": 12, "text": "statistically significant"}, {"st": 44, "ed": 46, "text": "meta analysis"}, {"st": 49, "ed": 51, "text": "time series"}, {"st": 59, "ed": 62, "text": "change point detection"}, {"st": 75, "ed": 77, "text": "increasing complexity"}, {"st": 82, "ed": 84, "text": "time series"}, {"st": 97, "ed": 99, "text": "recently proposed"}, {"st": 99, "ed": 101, "text": "deep neural"}, {"st": 106, "ed": 108, "text": "vector representations"}, {"st": 129, "ed": 131, "text": "distance based"}, {"st": 132, "ed": 134, "text": "time series"}, {"st": 188, "ed": 190, "text": "analysis reveals"}]
[{"st": 18, "ed": 20, "text": "natural language"}, {"st": 24, "ed": 26, "text": "ensemble method"}, {"st": 31, "ed": 33, "text": "multiple classifiers"}, {"st": 47, "ed": 49, "text": "integer programming"}, {"st": 54, "ed": 56, "text": "published results"}]
[{"st": 6, "ed": 8, "text": "multi task"}]
[{"st": 30, "ed": 32, "text": "computer vision"}, {"st": 58, "ed": 60, "text": "time consuming"}]
[{"st": 27, "ed": 29, "text": "wide variety"}, {"st": 37, "ed": 39, "text": "automatically identify"}, {"st": 105, "ed": 107, "text": "feature selection"}, {"st": 108, "ed": 111, "text": "supervised machine learning"}, {"st": 169, "ed": 171, "text": "h index"}, {"st": 178, "ed": 180, "text": "h index"}]
[{"st": 0, "ed": 2, "text": "social media"}, {"st": 24, "ed": 27, "text": "problem of classifying"}, {"st": 34, "ed": 36, "text": "supervised learning"}, {"st": 38, "ed": 41, "text": "supervised and unsupervised"}, {"st": 41, "ed": 43, "text": "domain adaptation"}, {"st": 63, "ed": 66, "text": "multi task learning"}]
[{"st": 72, "ed": 75, "text": "probabilistic topic models"}, {"st": 88, "ed": 90, "text": "unsupervised learning"}, {"st": 95, "ed": 97, "text": "embedding space"}, {"st": 124, "ed": 126, "text": "embedding space"}, {"st": 193, "ed": 195, "text": "approach outperforms"}]
[{"st": 1, "ed": 3, "text": "text summarization"}, {"st": 18, "ed": 20, "text": "text summarization"}, {"st": 30, "ed": 32, "text": "large scale"}, {"st": 46, "ed": 48, "text": "short text"}, {"st": 56, "ed": 58, "text": "sina weibo"}, {"st": 113, "ed": 116, "text": "recurrent neural network"}, {"st": 122, "ed": 124, "text": "promising results"}, {"st": 135, "ed": 137, "text": "short text"}]
[{"st": 9, "ed": 11, "text": "question answering"}, {"st": 29, "ed": 31, "text": "recurrent architecture"}, {"st": 38, "ed": 40, "text": "neural networks"}, {"st": 44, "ed": 46, "text": "joint representation"}, {"st": 47, "ed": 49, "text": "question answer"}, {"st": 55, "ed": 57, "text": "joint representation"}, {"st": 62, "ed": 65, "text": "short term memory"}, {"st": 82, "ed": 84, "text": "experiments conducted"}]
[{"st": 7, "ed": 9, "text": "textual data"}, {"st": 34, "ed": 36, "text": "prior knowledge"}, {"st": 43, "ed": 45, "text": "open ended"}, {"st": 55, "ed": 57, "text": "topic modeling"}, {"st": 71, "ed": 73, "text": "latent topics"}, {"st": 93, "ed": 95, "text": "topic modeling"}, {"st": 105, "ed": 107, "text": "distributional semantics"}, {"st": 148, "ed": 150, "text": "important role"}]
[{"st": 8, "ed": 10, "text": "topic model"}, {"st": 20, "ed": 22, "text": "topic model"}, {"st": 29, "ed": 31, "text": "multiple languages"}, {"st": 69, "ed": 71, "text": "social science"}]
[{"st": 120, "ed": 122, "text": "learning algorithms"}, {"st": 183, "ed": 185, "text": "class labels"}, {"st": 195, "ed": 197, "text": "classifier performance"}, {"st": 253, "ed": 255, "text": "active learning"}]
[{"st": 15, "ed": 17, "text": "large scale"}, {"st": 30, "ed": 32, "text": "co occurrence"}, {"st": 82, "ed": 84, "text": "extensively evaluate"}, {"st": 91, "ed": 94, "text": "qualitatively and quantitatively"}]
[{"st": 0, "ed": 2, "text": "negative transfer"}, {"st": 8, "ed": 11, "text": "automatic speech recognition"}, {"st": 32, "ed": 34, "text": "negative transfer"}, {"st": 38, "ed": 40, "text": "speech data"}, {"st": 55, "ed": 57, "text": "submodular function"}, {"st": 101, "ed": 103, "text": "experiments demonstrate"}, {"st": 124, "ed": 126, "text": "relative improvement"}]
[{"st": 1, "ed": 4, "text": "the past decade"}, {"st": 7, "ed": 9, "text": "exponential growth"}, {"st": 17, "ed": 19, "text": "social media"}, {"st": 69, "ed": 71, "text": "sentiment analysis"}, {"st": 89, "ed": 92, "text": "positive and negative"}, {"st": 101, "ed": 103, "text": "unsupervised learning"}, {"st": 107, "ed": 110, "text": "k means clustering"}, {"st": 117, "ed": 119, "text": "supervised learning"}, {"st": 122, "ed": 124, "text": "decision trees"}, {"st": 125, "ed": 128, "text": "support vector machines"}]
[{"st": 28, "ed": 30, "text": "topic modeling"}, {"st": 37, "ed": 39, "text": "topic models"}, {"st": 87, "ed": 89, "text": "topic modeling"}, {"st": 99, "ed": 101, "text": "topic models"}, {"st": 135, "ed": 137, "text": "processing units"}, {"st": 222, "ed": 224, "text": "medium sized"}]
[{"st": 10, "ed": 12, "text": "statistically significant"}, {"st": 17, "ed": 19, "text": "meta analysis"}, {"st": 21, "ed": 23, "text": "statistical properties"}, {"st": 31, "ed": 33, "text": "statistical methods"}, {"st": 41, "ed": 43, "text": "previous approaches"}, {"st": 65, "ed": 67, "text": "recently developed"}, {"st": 69, "ed": 72, "text": "neural language models"}, {"st": 74, "ed": 76, "text": "word representations"}, {"st": 175, "ed": 177, "text": "analysis reveals"}, {"st": 183, "ed": 185, "text": "multiple scales"}, {"st": 212, "ed": 214, "text": "american english"}]
[{"st": 15, "ed": 17, "text": "latent variable"}, {"st": 33, "ed": 36, "text": "latent dirichlet allocation"}, {"st": 77, "ed": 79, "text": "achieve high"}, {"st": 121, "ed": 123, "text": "latent variable"}]
[{"st": 4, "ed": 6, "text": "empirical study"}, {"st": 9, "ed": 11, "text": "deep learning"}, {"st": 12, "ed": 14, "text": "question answering"}, {"st": 48, "ed": 50, "text": "message passing"}, {"st": 54, "ed": 56, "text": "convergence speed"}]
[{"st": 7, "ed": 9, "text": "building blocks"}, {"st": 13, "ed": 15, "text": "social networks"}, {"st": 48, "ed": 51, "text": "conditional random field"}, {"st": 51, "ed": 53, "text": "based methods"}, {"st": 146, "ed": 148, "text": "harvard university"}]
[{"st": 15, "ed": 17, "text": "large scale"}, {"st": 17, "ed": 19, "text": "text data"}, {"st": 26, "ed": 28, "text": "topic modeling"}, {"st": 35, "ed": 37, "text": "generative models"}, {"st": 47, "ed": 49, "text": "topic modeling"}, {"st": 51, "ed": 53, "text": "recent research"}, {"st": 69, "ed": 71, "text": "generative models"}, {"st": 75, "ed": 78, "text": "number of topics"}, {"st": 124, "ed": 126, "text": "method named"}, {"st": 126, "ed": 128, "text": "hierarchical latent"}, {"st": 148, "ed": 150, "text": "latent topic"}]
[]
[{"st": 15, "ed": 17, "text": "real applications"}, {"st": 32, "ed": 34, "text": "existing approaches"}, {"st": 42, "ed": 44, "text": "topic modeling"}, {"st": 74, "ed": 76, "text": "pre defined"}, {"st": 82, "ed": 84, "text": "sentiment classification"}, {"st": 87, "ed": 91, "text": "convolutional neural network cnn"}, {"st": 91, "ed": 93, "text": "based methods"}, {"st": 157, "ed": 159, "text": "based methods"}]
[{"st": 5, "ed": 7, "text": "great importance"}, {"st": 8, "ed": 10, "text": "discourse analysis"}, {"st": 63, "ed": 65, "text": "topic modeling"}, {"st": 115, "ed": 118, "text": "unsupervised and supervised"}]
[{"st": 0, "ed": 2, "text": "social network"}, {"st": 25, "ed": 27, "text": "recommendation systems"}, {"st": 40, "ed": 42, "text": "recommendation systems"}, {"st": 50, "ed": 52, "text": "based methods"}, {"st": 52, "ed": 54, "text": "machine learning"}, {"st": 54, "ed": 56, "text": "based methods"}, {"st": 57, "ed": 59, "text": "matrix factorization"}, {"st": 71, "ed": 73, "text": "natural language"}, {"st": 80, "ed": 82, "text": "recommendation systems"}, {"st": 83, "ed": 85, "text": "unlike previous"}]
[{"st": 10, "ed": 12, "text": "neural network"}, {"st": 18, "ed": 20, "text": "hidden unit"}, {"st": 28, "ed": 30, "text": "hidden units"}, {"st": 83, "ed": 85, "text": "feature extractors"}, {"st": 104, "ed": 106, "text": "speech recognition"}, {"st": 133, "ed": 136, "text": "word error rate"}, {"st": 153, "ed": 156, "text": "training and test"}]
[{"st": 0, "ed": 2, "text": "attention mechanisms"}, {"st": 3, "ed": 5, "text": "neural networks"}, {"st": 13, "ed": 16, "text": "input and output"}, {"st": 56, "ed": 58, "text": "neural network"}, {"st": 78, "ed": 80, "text": "context dependent"}, {"st": 92, "ed": 94, "text": "source code"}, {"st": 115, "ed": 117, "text": "attention mechanisms"}, {"st": 127, "ed": 129, "text": "attention weights"}, {"st": 154, "ed": 156, "text": "neural network"}]
[{"st": 1, "ed": 4, "text": "linear discriminant analysis"}, {"st": 18, "ed": 20, "text": "speaker recognition"}, {"st": 23, "ed": 25, "text": "performance degradation"}, {"st": 62, "ed": 64, "text": "unlike traditional"}, {"st": 74, "ed": 76, "text": "proposed method"}, {"st": 88, "ed": 90, "text": "experiments conducted"}, {"st": 97, "ed": 99, "text": "improved accuracy"}]
[{"st": 13, "ed": 17, "text": "computer assisted language learning"}, {"st": 30, "ed": 33, "text": "component analysis pca"}, {"st": 71, "ed": 73, "text": "feature vectors"}, {"st": 118, "ed": 121, "text": "hidden markov models"}, {"st": 134, "ed": 137, "text": "efficient and effective"}, {"st": 138, "ed": 140, "text": "training data"}]
[{"st": 3, "ed": 5, "text": "large vocabulary"}, {"st": 5, "ed": 7, "text": "speech recognition"}, {"st": 12, "ed": 14, "text": "low latency"}, {"st": 32, "ed": 34, "text": "nexus 5"}, {"st": 41, "ed": 44, "text": "short term memory"}, {"st": 108, "ed": 110, "text": "context dependent"}, {"st": 131, "ed": 134, "text": "word error rate"}, {"st": 136, "ed": 138, "text": "open ended"}, {"st": 148, "ed": 151, "text": "times faster than"}]
[{"st": 3, "ed": 5, "text": "sentiment analysis"}, {"st": 15, "ed": 17, "text": "user generated"}, {"st": 25, "ed": 27, "text": "previous studies"}, {"st": 54, "ed": 56, "text": "neural networks"}, {"st": 57, "ed": 60, "text": "conditional random fields"}, {"st": 62, "ed": 64, "text": "unified framework"}, {"st": 78, "ed": 80, "text": "discriminative features"}, {"st": 96, "ed": 99, "text": "hand crafted features"}, {"st": 127, "ed": 129, "text": "baseline methods"}]
[{"st": 22, "ed": 24, "text": "recently shown"}, {"st": 97, "ed": 100, "text": "multi class classification"}, {"st": 105, "ed": 107, "text": "prediction models"}, {"st": 110, "ed": 112, "text": "feature extraction"}, {"st": 121, "ed": 123, "text": "latent semantic"}, {"st": 126, "ed": 128, "text": "machine learning"}, {"st": 130, "ed": 132, "text": "logistic regression"}, {"st": 133, "ed": 135, "text": "naive bayes"}, {"st": 141, "ed": 143, "text": "support vector"}, {"st": 175, "ed": 178, "text": "training and testing"}]
[{"st": 0, "ed": 3, "text": "probabilistic topic models"}, {"st": 4, "ed": 6, "text": "generative models"}, {"st": 15, "ed": 17, "text": "latent topics"}, {"st": 97, "ed": 99, "text": "text classification"}]
[{"st": 4, "ed": 6, "text": "topic modeling"}, {"st": 10, "ed": 12, "text": "n grams"}, {"st": 21, "ed": 23, "text": "n grams"}, {"st": 32, "ed": 34, "text": "topic modeling"}, {"st": 70, "ed": 72, "text": "n grams"}, {"st": 149, "ed": 151, "text": "latent topics"}, {"st": 163, "ed": 165, "text": "inference process"}, {"st": 183, "ed": 185, "text": "latent topics"}, {"st": 212, "ed": 215, "text": "latent dirichlet allocation"}]
[{"st": 12, "ed": 14, "text": "question answer"}, {"st": 79, "ed": 81, "text": "real world"}]
[{"st": 21, "ed": 23, "text": "sentiment analysis"}, {"st": 29, "ed": 31, "text": "sentiment classification"}, {"st": 54, "ed": 56, "text": "feature sets"}, {"st": 83, "ed": 85, "text": "evaluation measures"}, {"st": 115, "ed": 117, "text": "ensemble learning"}, {"st": 125, "ed": 127, "text": "linear models"}, {"st": 168, "ed": 170, "text": "https github.com"}]
[{"st": 15, "ed": 17, "text": "speech signals"}, {"st": 89, "ed": 91, "text": "proposed method"}, {"st": 95, "ed": 97, "text": "semi supervised"}, {"st": 100, "ed": 103, "text": "deep neural network"}, {"st": 107, "ed": 110, "text": "continuous speech recognition"}]
[{"st": 1, "ed": 3, "text": "systematic review"}, {"st": 20, "ed": 22, "text": "evidence based"}, {"st": 56, "ed": 58, "text": "natural language"}, {"st": 60, "ed": 62, "text": "machine learning"}, {"st": 85, "ed": 87, "text": "manual annotation"}, {"st": 110, "ed": 112, "text": "systematic reviews"}, {"st": 145, "ed": 148, "text": "support vector machine"}, {"st": 154, "ed": 156, "text": "labeled data"}, {"st": 167, "ed": 169, "text": "conducted experiments"}, {"st": 172, "ed": 174, "text": "systematic reviews"}, {"st": 176, "ed": 179, "text": "congestive heart failure"}, {"st": 188, "ed": 190, "text": "empirical results"}, {"st": 227, "ed": 229, "text": "empirical results"}, {"st": 235, "ed": 237, "text": "valuable information"}, {"st": 245, "ed": 247, "text": "time consuming"}]
[{"st": 3, "ed": 6, "text": "automatic speech recognition"}, {"st": 15, "ed": 17, "text": "speech enhancement"}, {"st": 51, "ed": 53, "text": "training strategy"}, {"st": 61, "ed": 63, "text": "multi stage"}, {"st": 68, "ed": 72, "text": "signal to noise ratio"}, {"st": 102, "ed": 104, "text": "method called"}, {"st": 112, "ed": 114, "text": "training samples"}, {"st": 139, "ed": 142, "text": "end to end"}, {"st": 142, "ed": 144, "text": "speech recognition"}, {"st": 147, "ed": 150, "text": "wall street journal"}, {"st": 155, "ed": 159, "text": "word error rate wer"}]
[{"st": 4, "ed": 6, "text": "deep learning"}, {"st": 18, "ed": 20, "text": "speech separation"}, {"st": 24, "ed": 26, "text": "cocktail party"}, {"st": 36, "ed": 38, "text": "speech separation"}, {"st": 40, "ed": 42, "text": "multi class"}, {"st": 47, "ed": 49, "text": "clustering technique"}, {"st": 55, "ed": 57, "text": "clustering problem"}, {"st": 86, "ed": 88, "text": "deep learning"}, {"st": 119, "ed": 121, "text": "cocktail party"}, {"st": 124, "ed": 126, "text": "real world"}, {"st": 133, "ed": 135, "text": "multi party"}]
[{"st": 7, "ed": 9, "text": "textual data"}, {"st": 17, "ed": 19, "text": "text documents"}, {"st": 21, "ed": 24, "text": "real world applications"}, {"st": 44, "ed": 46, "text": "hierarchical bayesian"}, {"st": 46, "ed": 48, "text": "topic model"}, {"st": 53, "ed": 55, "text": "hierarchical structure"}, {"st": 76, "ed": 79, "text": "monte carlo sampling"}, {"st": 104, "ed": 107, "text": "latent dirichlet allocation"}, {"st": 125, "ed": 127, "text": "empirical performance"}]
[{"st": 0, "ed": 3, "text": "efficient market hypothesis"}, {"st": 79, "ed": 81, "text": "classification models"}, {"st": 129, "ed": 131, "text": "encouraging results"}, {"st": 142, "ed": 144, "text": "prediction model"}]
[{"st": 34, "ed": 36, "text": "world knowledge"}, {"st": 39, "ed": 41, "text": "world knowledge"}, {"st": 55, "ed": 57, "text": "key challenges"}, {"st": 62, "ed": 64, "text": "world knowledge"}, {"st": 82, "ed": 84, "text": "world knowledge"}, {"st": 96, "ed": 98, "text": "world knowledge"}, {"st": 115, "ed": 117, "text": "world knowledge"}, {"st": 126, "ed": 128, "text": "clustering algorithm"}, {"st": 194, "ed": 196, "text": "benchmark datasets"}, {"st": 202, "ed": 204, "text": "world knowledge"}, {"st": 208, "ed": 210, "text": "significantly outperform"}, {"st": 215, "ed": 217, "text": "clustering algorithms"}, {"st": 220, "ed": 222, "text": "clustering algorithms"}, {"st": 224, "ed": 226, "text": "world knowledge"}]
[{"st": 8, "ed": 10, "text": "natural language"}, {"st": 21, "ed": 23, "text": "neural network"}, {"st": 27, "ed": 29, "text": "recent advances"}, {"st": 37, "ed": 39, "text": "character level"}]
[{"st": 3, "ed": 5, "text": "social media"}, {"st": 34, "ed": 36, "text": "social media"}, {"st": 40, "ed": 42, "text": "machine learning"}, {"st": 53, "ed": 55, "text": "labeled data"}, {"st": 65, "ed": 67, "text": "machine learning"}, {"st": 74, "ed": 76, "text": "classification methods"}, {"st": 81, "ed": 83, "text": "labeled data"}, {"st": 94, "ed": 96, "text": "feature engineering"}, {"st": 105, "ed": 107, "text": "neural network"}, {"st": 108, "ed": 110, "text": "classification methods"}, {"st": 113, "ed": 115, "text": "multi class"}, {"st": 121, "ed": 123, "text": "neural network"}, {"st": 129, "ed": 131, "text": "feature engineering"}, {"st": 149, "ed": 151, "text": "labeled data"}, {"st": 154, "ed": 156, "text": "proposed method"}]
[{"st": 12, "ed": 14, "text": "word embeddings"}, {"st": 15, "ed": 17, "text": "natural language"}, {"st": 29, "ed": 31, "text": "related tasks"}, {"st": 47, "ed": 49, "text": "word embeddings"}, {"st": 104, "ed": 106, "text": "word embeddings"}]
[{"st": 6, "ed": 8, "text": "recently proposed"}, {"st": 12, "ed": 14, "text": "dnn hmm"}, {"st": 14, "ed": 17, "text": "deep neural network"}, {"st": 17, "ed": 20, "text": "hidden markov model"}, {"st": 32, "ed": 34, "text": "finite set"}, {"st": 50, "ed": 52, "text": "context dependent"}, {"st": 68, "ed": 70, "text": "classification error"}, {"st": 97, "ed": 99, "text": "dnn hmm"}, {"st": 103, "ed": 105, "text": "front end"}, {"st": 111, "ed": 113, "text": "recognition accuracy"}, {"st": 115, "ed": 117, "text": "dnn hmm"}, {"st": 126, "ed": 128, "text": "random sampling"}, {"st": 139, "ed": 142, "text": "word error rate"}]
[{"st": 23, "ed": 25, "text": "current approaches"}, {"st": 40, "ed": 42, "text": "multi relational"}, {"st": 68, "ed": 70, "text": "multi layered"}, {"st": 91, "ed": 93, "text": "co occurrence"}, {"st": 142, "ed": 144, "text": "network topology"}]
[{"st": 5, "ed": 7, "text": "widely applied"}, {"st": 28, "ed": 32, "text": "latent dirichlet allocation lda"}, {"st": 35, "ed": 37, "text": "social media"}, {"st": 48, "ed": 50, "text": "natural language"}, {"st": 55, "ed": 57, "text": "lda based"}, {"st": 70, "ed": 72, "text": "labeled data"}, {"st": 90, "ed": 92, "text": "lda based"}, {"st": 97, "ed": 99, "text": "topic model"}, {"st": 154, "ed": 156, "text": "prior information"}, {"st": 158, "ed": 160, "text": "topic model"}, {"st": 180, "ed": 182, "text": "conduct experiments"}, {"st": 192, "ed": 194, "text": "improved performance"}, {"st": 198, "ed": 200, "text": "quantitative evaluations"}]
[{"st": 13, "ed": 15, "text": "challenging task"}, {"st": 17, "ed": 19, "text": "content analysis"}, {"st": 31, "ed": 33, "text": "existing methods"}, {"st": 36, "ed": 39, "text": "latent semantic analysis"}, {"st": 41, "ed": 44, "text": "latent dirichlet allocation"}, {"st": 56, "ed": 59, "text": "word co occurrence"}, {"st": 88, "ed": 90, "text": "recent results"}, {"st": 91, "ed": 93, "text": "word embeddings"}, {"st": 111, "ed": 113, "text": "topic model"}, {"st": 116, "ed": 118, "text": "latent topics"}, {"st": 130, "ed": 133, "text": "word co occurrence"}, {"st": 146, "ed": 149, "text": "markov random field"}, {"st": 168, "ed": 170, "text": "real world"}]
[{"st": 72, "ed": 74, "text": "artificial intelligence"}, {"st": 84, "ed": 87, "text": "latent dirichlet allocation"}, {"st": 90, "ed": 92, "text": "recently proposed"}, {"st": 92, "ed": 94, "text": "method called"}, {"st": 94, "ed": 96, "text": "hierarchical latent"}, {"st": 103, "ed": 105, "text": "topic model"}, {"st": 126, "ed": 128, "text": "topic model"}, {"st": 143, "ed": 145, "text": "fine grained"}]
[{"st": 4, "ed": 6, "text": "neural network"}, {"st": 9, "ed": 11, "text": "speech recognition"}, {"st": 20, "ed": 22, "text": "low power"}, {"st": 28, "ed": 31, "text": "recurrent neural networks"}, {"st": 48, "ed": 50, "text": "character level"}, {"st": 60, "ed": 62, "text": "word level"}, {"st": 74, "ed": 76, "text": "character level"}, {"st": 79, "ed": 81, "text": "word level"}, {"st": 90, "ed": 92, "text": "search algorithm"}, {"st": 95, "ed": 99, "text": "hidden markov model hmm"}, {"st": 107, "ed": 109, "text": "parallel processing"}, {"st": 112, "ed": 114, "text": "low latency"}, {"st": 138, "ed": 140, "text": "proposed algorithm"}]
[{"st": 16, "ed": 18, "text": "decision making"}, {"st": 19, "ed": 21, "text": "social media"}, {"st": 43, "ed": 45, "text": "natural language"}, {"st": 64, "ed": 67, "text": "deep neural network"}, {"st": 91, "ed": 93, "text": "distributed representation"}, {"st": 102, "ed": 104, "text": "higher level"}, {"st": 114, "ed": 116, "text": "online algorithm"}, {"st": 118, "ed": 121, "text": "stochastic gradient descent"}, {"st": 126, "ed": 128, "text": "online fashion"}, {"st": 139, "ed": 141, "text": "real world"}]
[{"st": 17, "ed": 19, "text": "recommendation systems"}, {"st": 22, "ed": 24, "text": "dialogue systems"}, {"st": 33, "ed": 35, "text": "deep learning"}, {"st": 82, "ed": 85, "text": "tens of thousands"}, {"st": 91, "ed": 93, "text": "hand written"}, {"st": 124, "ed": 126, "text": "hand written"}]
[{"st": 6, "ed": 8, "text": "network embedding"}, {"st": 12, "ed": 14, "text": "low dimensional"}, {"st": 22, "ed": 24, "text": "network embedding"}, {"st": 29, "ed": 31, "text": "network structure"}, {"st": 41, "ed": 43, "text": "content information"}, {"st": 50, "ed": 52, "text": "real world"}, {"st": 69, "ed": 71, "text": "network embedding"}, {"st": 79, "ed": 81, "text": "network structure"}, {"st": 100, "ed": 102, "text": "content information"}, {"st": 111, "ed": 113, "text": "real world"}, {"st": 118, "ed": 120, "text": "node classification"}, {"st": 127, "ed": 129, "text": "network embedding"}, {"st": 134, "ed": 136, "text": "content information"}]
[{"st": 30, "ed": 32, "text": "text classification"}, {"st": 60, "ed": 62, "text": "previous studies"}, {"st": 76, "ed": 78, "text": "sentiment analysis"}, {"st": 106, "ed": 108, "text": "empirical study"}, {"st": 163, "ed": 166, "text": "support vector machine"}]
[{"st": 27, "ed": 29, "text": "natural language"}, {"st": 60, "ed": 62, "text": "natural language"}, {"st": 89, "ed": 91, "text": "scene classification"}, {"st": 159, "ed": 161, "text": "low dimensional"}, {"st": 176, "ed": 178, "text": "natural language"}]
[{"st": 50, "ed": 53, "text": "naive bayes classifier"}]
[{"st": 7, "ed": 9, "text": "national security"}, {"st": 19, "ed": 22, "text": "a long standing"}, {"st": 52, "ed": 54, "text": "machine learning"}, {"st": 60, "ed": 62, "text": "state department"}, {"st": 90, "ed": 92, "text": "incomplete data"}, {"st": 124, "ed": 126, "text": "analysis reveals"}, {"st": 157, "ed": 159, "text": "recommender systems"}]
[{"st": 1, "ed": 3, "text": "recommender systems"}, {"st": 10, "ed": 12, "text": "collaborative filtering"}, {"st": 14, "ed": 16, "text": "content based"}, {"st": 87, "ed": 89, "text": "latent embedding"}, {"st": 95, "ed": 97, "text": "textual description"}]
[{"st": 10, "ed": 12, "text": "provable guarantees"}, {"st": 17, "ed": 19, "text": "topic models"}, {"st": 147, "ed": 149, "text": "feature representation"}, {"st": 201, "ed": 203, "text": "sample complexity"}, {"st": 218, "ed": 220, "text": "multi view"}]
[{"st": 24, "ed": 26, "text": "important information"}, {"st": 45, "ed": 47, "text": "multi modal"}, {"st": 47, "ed": 49, "text": "speech recognition"}, {"st": 75, "ed": 78, "text": "end to end"}, {"st": 84, "ed": 87, "text": "recurrent neural networks"}, {"st": 122, "ed": 124, "text": "audio visual"}, {"st": 134, "ed": 136, "text": "large vocabulary"}, {"st": 141, "ed": 143, "text": "previously published"}]
[{"st": 1, "ed": 3, "text": "topic models"}, {"st": 8, "ed": 10, "text": "large scale"}, {"st": 28, "ed": 30, "text": "social media"}, {"st": 79, "ed": 81, "text": "topic models"}, {"st": 85, "ed": 88, "text": "number of topics"}, {"st": 131, "ed": 133, "text": "random walk"}, {"st": 157, "ed": 159, "text": "random walk"}, {"st": 166, "ed": 168, "text": "topic models"}, {"st": 172, "ed": 174, "text": "topic models"}, {"st": 188, "ed": 190, "text": "inference procedures"}, {"st": 193, "ed": 195, "text": "topic models"}, {"st": 197, "ed": 200, "text": "simple and efficient"}, {"st": 210, "ed": 212, "text": "topic models"}, {"st": 225, "ed": 227, "text": "prior knowledge"}]
[{"st": 0, "ed": 3, "text": "latent dirichlet allocation"}, {"st": 14, "ed": 16, "text": "posterior probabilities"}, {"st": 41, "ed": 43, "text": "most probable"}, {"st": 63, "ed": 65, "text": "mutual information"}, {"st": 137, "ed": 139, "text": "higher quality"}, {"st": 170, "ed": 172, "text": "domain specific"}, {"st": 219, "ed": 221, "text": "substantial improvement"}]
[{"st": 9, "ed": 11, "text": "sentiment classification"}, {"st": 17, "ed": 19, "text": "challenging task"}, {"st": 23, "ed": 25, "text": "training data"}, {"st": 33, "ed": 35, "text": "previously proposed"}, {"st": 38, "ed": 40, "text": "typically require"}, {"st": 67, "ed": 69, "text": "weakly supervised"}, {"st": 76, "ed": 78, "text": "multi layer"}, {"st": 78, "ed": 80, "text": "convolutional network"}, {"st": 86, "ed": 88, "text": "pre training"}, {"st": 165, "ed": 167, "text": "generalization properties"}]
[{"st": 0, "ed": 2, "text": "deep learning"}, {"st": 19, "ed": 21, "text": "time consuming"}, {"st": 82, "ed": 84, "text": "speech recognition"}, {"st": 86, "ed": 89, "text": "stochastic gradient descent"}, {"st": 102, "ed": 105, "text": "stochastic gradient descent"}, {"st": 112, "ed": 114, "text": "feed forward"}, {"st": 114, "ed": 117, "text": "deep neural networks"}, {"st": 121, "ed": 124, "text": "short term memory"}]
[{"st": 11, "ed": 13, "text": "gated recurrent"}, {"st": 13, "ed": 15, "text": "neural networks"}, {"st": 24, "ed": 26, "text": "highly correlated"}]
[{"st": 1, "ed": 4, "text": "text to speech"}, {"st": 14, "ed": 16, "text": "text analysis"}, {"st": 31, "ed": 33, "text": "domain expertise"}, {"st": 46, "ed": 49, "text": "end to end"}, {"st": 50, "ed": 53, "text": "text to speech"}, {"st": 123, "ed": 125, "text": "frame level"}]
[{"st": 1, "ed": 3, "text": "deep learning"}, {"st": 12, "ed": 14, "text": "recommender systems"}, {"st": 15, "ed": 17, "text": "traditional methods"}, {"st": 30, "ed": 32, "text": "neural nets"}, {"st": 35, "ed": 37, "text": "latent representation"}, {"st": 51, "ed": 53, "text": "latent representation"}, {"st": 67, "ed": 69, "text": "latent representations"}, {"st": 164, "ed": 166, "text": "latent representation"}]
[{"st": 10, "ed": 12, "text": "posterior probabilities"}, {"st": 19, "ed": 21, "text": "posterior probabilities"}, {"st": 28, "ed": 30, "text": "posterior probabilities"}, {"st": 63, "ed": 65, "text": "posterior probabilities"}, {"st": 68, "ed": 70, "text": "posterior probabilities"}, {"st": 85, "ed": 87, "text": "training data"}, {"st": 90, "ed": 92, "text": "speech data"}, {"st": 100, "ed": 103, "text": "source and target"}, {"st": 128, "ed": 130, "text": "joint training"}, {"st": 143, "ed": 145, "text": "speech recognition"}, {"st": 147, "ed": 149, "text": "posterior probabilities"}, {"st": 151, "ed": 153, "text": "speech synthesis"}, {"st": 159, "ed": 161, "text": "proposed method"}, {"st": 177, "ed": 179, "text": "approach outperforms"}]
[{"st": 1, "ed": 3, "text": "open domain"}, {"st": 3, "ed": 6, "text": "question answering qa"}, {"st": 26, "ed": 28, "text": "complex questions"}, {"st": 62, "ed": 64, "text": "unsupervised learning"}, {"st": 96, "ed": 99, "text": "approach significantly outperforms"}, {"st": 104, "ed": 106, "text": "qa systems"}]
[{"st": 11, "ed": 13, "text": "modified version"}, {"st": 56, "ed": 58, "text": "significantly reduces"}, {"st": 113, "ed": 115, "text": "generation process"}]
[{"st": 2, "ed": 4, "text": "distributed stochastic"}, {"st": 62, "ed": 65, "text": "neural machine translation"}, {"st": 67, "ed": 69, "text": "image classification"}, {"st": 79, "ed": 81, "text": "convergence rate"}]
[{"st": 34, "ed": 37, "text": "convolutional neural network"}, {"st": 45, "ed": 47, "text": "attention based"}, {"st": 47, "ed": 50, "text": "recurrent neural network"}, {"st": 91, "ed": 93, "text": "analysis shows"}]
[{"st": 7, "ed": 10, "text": "deep neural networks"}, {"st": 11, "ed": 13, "text": "computer vision"}, {"st": 69, "ed": 71, "text": "weak supervision"}, {"st": 104, "ed": 106, "text": "weak supervision"}, {"st": 113, "ed": 116, "text": "simple yet effective"}, {"st": 132, "ed": 134, "text": "point wise"}, {"st": 135, "ed": 137, "text": "pair wise"}, {"st": 155, "ed": 157, "text": "word embedding"}, {"st": 168, "ed": 170, "text": "training instances"}, {"st": 185, "ed": 187, "text": "large scale"}, {"st": 197, "ed": 199, "text": "objective functions"}, {"st": 210, "ed": 212, "text": "weakly supervised"}, {"st": 215, "ed": 217, "text": "impressive performance"}, {"st": 248, "ed": 250, "text": "pre training"}, {"st": 254, "ed": 256, "text": "weakly labeled"}]
[{"st": 10, "ed": 12, "text": "supervised learning"}, {"st": 13, "ed": 15, "text": "annotated data"}, {"st": 23, "ed": 25, "text": "classification task"}, {"st": 28, "ed": 30, "text": "active learning"}, {"st": 56, "ed": 58, "text": "training set"}, {"st": 87, "ed": 89, "text": "stack overflow"}, {"st": 99, "ed": 101, "text": "training examples"}]
[{"st": 1, "ed": 4, "text": "deep neural networks"}, {"st": 17, "ed": 19, "text": "learned features"}, {"st": 59, "ed": 62, "text": "deep neural network"}]
[{"st": 11, "ed": 13, "text": "machine learning"}, {"st": 14, "ed": 16, "text": "text mining"}, {"st": 23, "ed": 25, "text": "social media"}, {"st": 34, "ed": 36, "text": "ground truth"}, {"st": 42, "ed": 44, "text": "large scale"}, {"st": 51, "ed": 53, "text": "feature learning"}, {"st": 63, "ed": 65, "text": "social media"}, {"st": 73, "ed": 75, "text": "multi view"}, {"st": 75, "ed": 78, "text": "unsupervised feature learning"}, {"st": 120, "ed": 122, "text": "significantly outperformed"}, {"st": 135, "ed": 137, "text": "social media"}]
[{"st": 5, "ed": 7, "text": "non stationary"}, {"st": 7, "ed": 9, "text": "environmental noise"}, {"st": 10, "ed": 13, "text": "a long standing"}, {"st": 16, "ed": 19, "text": "automatic speech recognition"}, {"st": 27, "ed": 29, "text": "supervised approaches"}, {"st": 34, "ed": 37, "text": "deep neural networks"}, {"st": 57, "ed": 59, "text": "unsupervised methods"}, {"st": 61, "ed": 63, "text": "real life"}, {"st": 70, "ed": 72, "text": "recently developed"}, {"st": 73, "ed": 75, "text": "deep learning"}, {"st": 78, "ed": 80, "text": "non stationary"}, {"st": 101, "ed": 103, "text": "speech recognition"}, {"st": 109, "ed": 111, "text": "multi channel"}, {"st": 115, "ed": 117, "text": "front end"}, {"st": 121, "ed": 123, "text": "speech recognition"}, {"st": 128, "ed": 130, "text": "front end"}]
[{"st": 0, "ed": 2, "text": "task specific"}, {"st": 17, "ed": 19, "text": "existing approaches"}, {"st": 63, "ed": 65, "text": "class label"}, {"st": 85, "ed": 87, "text": "task specific"}, {"st": 87, "ed": 90, "text": "words and phrases"}, {"st": 96, "ed": 98, "text": "class labels"}, {"st": 109, "ed": 111, "text": "task specific"}, {"st": 121, "ed": 124, "text": "convolutional neural network"}, {"st": 137, "ed": 139, "text": "task specific"}, {"st": 151, "ed": 154, "text": "approach significantly outperforms"}, {"st": 160, "ed": 162, "text": "case studies"}, {"st": 170, "ed": 172, "text": "case study"}, {"st": 191, "ed": 193, "text": "case study"}]
[{"st": 12, "ed": 14, "text": "examples include"}, {"st": 32, "ed": 34, "text": "matrix factorization"}, {"st": 34, "ed": 36, "text": "based methods"}, {"st": 47, "ed": 49, "text": "latent factors"}, {"st": 101, "ed": 104, "text": "problem and propose"}, {"st": 109, "ed": 111, "text": "text embedding"}, {"st": 121, "ed": 123, "text": "latent feature"}, {"st": 125, "ed": 127, "text": "text embedding"}, {"st": 131, "ed": 134, "text": "end to end"}, {"st": 136, "ed": 138, "text": "user interactions"}, {"st": 151, "ed": 153, "text": "text data"}, {"st": 157, "ed": 159, "text": "user interactions"}, {"st": 164, "ed": 166, "text": "text embedding"}, {"st": 170, "ed": 172, "text": "text embedding"}, {"st": 183, "ed": 185, "text": "significantly improve"}, {"st": 188, "ed": 190, "text": "recommendation systems"}, {"st": 191, "ed": 193, "text": "real world"}]
[{"st": 13, "ed": 15, "text": "feature based"}, {"st": 16, "ed": 18, "text": "kernel based"}, {"st": 33, "ed": 35, "text": "deep learning"}, {"st": 49, "ed": 52, "text": "convolutional neural network"}, {"st": 106, "ed": 109, "text": "compares favorably to"}, {"st": 118, "ed": 120, "text": "kernel based"}, {"st": 126, "ed": 128, "text": "relative improvement"}, {"st": 149, "ed": 151, "text": "kernel based"}]
[{"st": 0, "ed": 4, "text": "deep convolutional neural networks"}, {"st": 17, "ed": 19, "text": "applications including"}, {"st": 19, "ed": 21, "text": "speech recognition"}, {"st": 22, "ed": 24, "text": "event detection"}, {"st": 55, "ed": 59, "text": "deep convolutional neural network"}, {"st": 66, "ed": 68, "text": "promising results"}, {"st": 86, "ed": 88, "text": "input features"}, {"st": 127, "ed": 130, "text": "deep neural networks"}, {"st": 135, "ed": 137, "text": "network architecture"}, {"st": 188, "ed": 191, "text": "deep neural networks"}]
[{"st": 10, "ed": 13, "text": "recurrent neural network"}, {"st": 57, "ed": 59, "text": "features extracted"}, {"st": 61, "ed": 63, "text": "speech signal"}, {"st": 96, "ed": 98, "text": "low level"}, {"st": 103, "ed": 105, "text": "fine grained"}, {"st": 126, "ed": 128, "text": "attention mechanism"}, {"st": 150, "ed": 152, "text": "hierarchical clustering"}]
[{"st": 0, "ed": 2, "text": "word embedding"}, {"st": 5, "ed": 7, "text": "skip gram"}, {"st": 29, "ed": 31, "text": "latent topic"}, {"st": 73, "ed": 75, "text": "previous works"}, {"st": 80, "ed": 82, "text": "word embeddings"}, {"st": 87, "ed": 89, "text": "latent topics"}, {"st": 93, "ed": 95, "text": "latent topics"}, {"st": 97, "ed": 99, "text": "word embeddings"}, {"st": 123, "ed": 125, "text": "word embeddings"}, {"st": 126, "ed": 128, "text": "latent topics"}, {"st": 137, "ed": 139, "text": "word embeddings"}, {"st": 180, "ed": 182, "text": "word embeddings"}, {"st": 184, "ed": 186, "text": "latent topics"}, {"st": 188, "ed": 191, "text": "effective and efficient"}]
[{"st": 6, "ed": 9, "text": "semi supervised learning"}, {"st": 20, "ed": 22, "text": "text documents"}, {"st": 33, "ed": 36, "text": "divide and conquer"}, {"st": 40, "ed": 42, "text": "k means"}, {"st": 46, "ed": 49, "text": "labeled and unlabeled"}, {"st": 52, "ed": 54, "text": "k means"}, {"st": 98, "ed": 100, "text": "nearest neighbor"}]
[{"st": 12, "ed": 14, "text": "encoder decoder"}, {"st": 54, "ed": 56, "text": "multi layered"}, {"st": 56, "ed": 58, "text": "bi directional"}, {"st": 58, "ed": 62, "text": "gated recurrent unit gru"}, {"st": 66, "ed": 68, "text": "multi layered"}, {"st": 89, "ed": 92, "text": "fully connected layers"}, {"st": 98, "ed": 100, "text": "proposed method"}, {"st": 122, "ed": 124, "text": "machine translation"}, {"st": 125, "ed": 127, "text": "image captioning"}, {"st": 134, "ed": 136, "text": "proposed method"}]
[{"st": 38, "ed": 40, "text": "machine learning"}, {"st": 43, "ed": 45, "text": "large corpora"}, {"st": 47, "ed": 49, "text": "manually annotated"}, {"st": 131, "ed": 133, "text": "case study"}]
[{"st": 1, "ed": 3, "text": "sentiment analysis"}, {"st": 11, "ed": 13, "text": "fine grained"}, {"st": 25, "ed": 27, "text": "classification tasks"}, {"st": 38, "ed": 41, "text": "recurrent neural network"}, {"st": 44, "ed": 46, "text": "jointly learning"}, {"st": 70, "ed": 72, "text": "fine grained"}, {"st": 72, "ed": 74, "text": "sentiment classification"}]
[{"st": 4, "ed": 6, "text": "large scale"}, {"st": 15, "ed": 17, "text": "natural language"}, {"st": 79, "ed": 81, "text": "open domain"}, {"st": 114, "ed": 116, "text": "question answering"}, {"st": 210, "ed": 214, "text": "available at https github.com"}]
[{"st": 5, "ed": 7, "text": "speech perception"}, {"st": 11, "ed": 14, "text": "automatic speech recognition"}, {"st": 16, "ed": 19, "text": "text to speech"}, {"st": 101, "ed": 104, "text": "labeled and unlabeled"}, {"st": 153, "ed": 155, "text": "deep learning"}, {"st": 159, "ed": 161, "text": "speech perception"}, {"st": 170, "ed": 172, "text": "proposed approach"}, {"st": 172, "ed": 174, "text": "significantly improved"}]
[{"st": 7, "ed": 10, "text": "automatic speech recognition"}, {"st": 12, "ed": 14, "text": "performance degradation"}, {"st": 27, "ed": 30, "text": "propose and evaluate"}, {"st": 59, "ed": 61, "text": "front end"}, {"st": 67, "ed": 70, "text": "mean square error"}, {"st": 81, "ed": 83, "text": "cross entropy"}, {"st": 133, "ed": 135, "text": "deep learning"}, {"st": 139, "ed": 141, "text": "speech separation"}, {"st": 176, "ed": 180, "text": "word error rate wer"}, {"st": 193, "ed": 195, "text": "speech recognition"}, {"st": 225, "ed": 227, "text": "speech recognition"}, {"st": 233, "ed": 235, "text": "large vocabulary"}, {"st": 235, "ed": 237, "text": "continuous speech"}]
[{"st": 5, "ed": 8, "text": "text to speech"}, {"st": 15, "ed": 18, "text": "text to speech"}, {"st": 48, "ed": 50, "text": "network architecture"}, {"st": 56, "ed": 58, "text": "existing literature"}, {"st": 94, "ed": 96, "text": "context free"}, {"st": 96, "ed": 98, "text": "lookup table"}, {"st": 170, "ed": 172, "text": "source code"}]
[{"st": 11, "ed": 13, "text": "feature extraction"}, {"st": 47, "ed": 49, "text": "optimization method"}, {"st": 86, "ed": 89, "text": "recurrent neural network"}, {"st": 105, "ed": 107, "text": "proposed method"}, {"st": 114, "ed": 116, "text": "prediction accuracy"}]
[{"st": 0, "ed": 2, "text": "segmental models"}, {"st": 31, "ed": 33, "text": "segmental models"}, {"st": 34, "ed": 36, "text": "segmental models"}, {"st": 38, "ed": 40, "text": "neural network"}, {"st": 44, "ed": 46, "text": "segmental models"}, {"st": 48, "ed": 50, "text": "competitive results"}, {"st": 51, "ed": 53, "text": "speech recognition"}, {"st": 55, "ed": 58, "text": "end to end"}, {"st": 71, "ed": 73, "text": "segmental models"}, {"st": 81, "ed": 83, "text": "neural network"}, {"st": 88, "ed": 90, "text": "finite state"}, {"st": 94, "ed": 97, "text": "end to end"}, {"st": 97, "ed": 99, "text": "segmental models"}, {"st": 107, "ed": 109, "text": "frame level"}, {"st": 122, "ed": 124, "text": "search space"}, {"st": 135, "ed": 137, "text": "loss functions"}, {"st": 138, "ed": 141, "text": "end to end"}, {"st": 148, "ed": 150, "text": "multi stage"}, {"st": 151, "ed": 154, "text": "end to end"}, {"st": 162, "ed": 164, "text": "frame level"}]
[{"st": 0, "ed": 2, "text": "neural networks"}, {"st": 11, "ed": 13, "text": "natural language"}, {"st": 21, "ed": 23, "text": "outperform traditional"}, {"st": 23, "ed": 25, "text": "machine learning"}, {"st": 39, "ed": 41, "text": "deep learning"}, {"st": 44, "ed": 47, "text": "difficult to train"}, {"st": 70, "ed": 72, "text": "deep learning"}, {"st": 74, "ed": 76, "text": "achieves comparable"}, {"st": 79, "ed": 81, "text": "deep learning"}, {"st": 86, "ed": 88, "text": "sentiment analysis"}]
[{"st": 4, "ed": 6, "text": "text summarization"}, {"st": 12, "ed": 14, "text": "deep learning"}, {"st": 21, "ed": 23, "text": "feature extraction"}, {"st": 60, "ed": 63, "text": "restricted boltzmann machine"}]
[{"st": 7, "ed": 10, "text": "latent dirichlet allocation"}, {"st": 15, "ed": 17, "text": "topic model"}, {"st": 32, "ed": 34, "text": "topic models"}, {"st": 67, "ed": 69, "text": "topic models"}, {"st": 84, "ed": 86, "text": "noisy data"}]
[{"st": 12, "ed": 14, "text": "open domain"}, {"st": 45, "ed": 47, "text": "speech recognition"}, {"st": 98, "ed": 101, "text": "training and test"}, {"st": 114, "ed": 116, "text": "neural network"}, {"st": 130, "ed": 132, "text": "post processing"}]
[{"st": 22, "ed": 24, "text": "extracting information"}, {"st": 67, "ed": 69, "text": "automatically identify"}, {"st": 88, "ed": 90, "text": "key contribution"}, {"st": 106, "ed": 108, "text": "machine learning"}, {"st": 110, "ed": 112, "text": "deep learning"}, {"st": 168, "ed": 170, "text": "machine learning"}, {"st": 176, "ed": 178, "text": "content based"}, {"st": 178, "ed": 180, "text": "question answering"}]
[{"st": 13, "ed": 17, "text": "hidden markov model hmm"}, {"st": 18, "ed": 21, "text": "automatic speech recognition"}, {"st": 101, "ed": 103, "text": "hmm based"}, {"st": 121, "ed": 123, "text": "hidden state"}, {"st": 150, "ed": 152, "text": "conditional independence"}, {"st": 157, "ed": 159, "text": "mutual information"}, {"st": 164, "ed": 167, "text": "deep neural network"}, {"st": 168, "ed": 170, "text": "posterior probabilities"}, {"st": 186, "ed": 188, "text": "hmm based"}, {"st": 196, "ed": 198, "text": "gaussian mixture"}, {"st": 206, "ed": 208, "text": "hidden layers"}, {"st": 220, "ed": 223, "text": "sheds light on"}, {"st": 226, "ed": 228, "text": "low dimensional"}, {"st": 238, "ed": 240, "text": "hmm based"}]
[{"st": 22, "ed": 24, "text": "sequence prediction"}, {"st": 25, "ed": 27, "text": "segmental models"}, {"st": 47, "ed": 49, "text": "segmental models"}, {"st": 57, "ed": 59, "text": "computationally expensive"}, {"st": 81, "ed": 83, "text": "higher order"}, {"st": 115, "ed": 117, "text": "segmental models"}, {"st": 146, "ed": 148, "text": "segmental models"}, {"st": 166, "ed": 168, "text": "time consuming"}, {"st": 172, "ed": 175, "text": "end to end"}, {"st": 177, "ed": 179, "text": "segmental models"}, {"st": 181, "ed": 183, "text": "loss functions"}, {"st": 186, "ed": 189, "text": "end to end"}, {"st": 192, "ed": 194, "text": "log loss"}, {"st": 209, "ed": 211, "text": "log loss"}, {"st": 214, "ed": 217, "text": "end to end"}, {"st": 218, "ed": 220, "text": "approach called"}, {"st": 230, "ed": 233, "text": "end to end"}, {"st": 239, "ed": 242, "text": "hidden markov models"}, {"st": 254, "ed": 256, "text": "segmental models"}, {"st": 257, "ed": 259, "text": "large vocabulary"}, {"st": 259, "ed": 261, "text": "sequence prediction"}]
[{"st": 1, "ed": 4, "text": "automatic speech recognition"}, {"st": 7, "ed": 9, "text": "multi level"}, {"st": 9, "ed": 11, "text": "pattern recognition"}, {"st": 46, "ed": 48, "text": "deep learning"}, {"st": 58, "ed": 61, "text": "end to end"}, {"st": 75, "ed": 77, "text": "hand engineered"}, {"st": 83, "ed": 86, "text": "end to end"}, {"st": 106, "ed": 109, "text": "deep neural network"}, {"st": 109, "ed": 112, "text": "hidden markov model"}, {"st": 112, "ed": 114, "text": "dnn hmm"}, {"st": 122, "ed": 125, "text": "end to end"}, {"st": 125, "ed": 127, "text": "attention based"}, {"st": 127, "ed": 129, "text": "encoder decoder"}, {"st": 146, "ed": 148, "text": "attention based"}, {"st": 153, "ed": 155, "text": "training process"}, {"st": 157, "ed": 160, "text": "end to end"}, {"st": 176, "ed": 178, "text": "attention based"}, {"st": 194, "ed": 196, "text": "encoder decoder"}, {"st": 200, "ed": 202, "text": "front end"}]
[{"st": 1, "ed": 3, "text": "topic models"}, {"st": 29, "ed": 31, "text": "topic model"}, {"st": 33, "ed": 35, "text": "inference algorithm"}, {"st": 71, "ed": 73, "text": "qualitative analysis"}, {"st": 74, "ed": 76, "text": "comparable results"}, {"st": 90, "ed": 92, "text": "quantitative analysis"}, {"st": 100, "ed": 102, "text": "classification accuracy"}, {"st": 104, "ed": 106, "text": "computational performance"}]
[{"st": 6, "ed": 8, "text": "generative model"}, {"st": 33, "ed": 35, "text": "generative model"}, {"st": 37, "ed": 41, "text": "generative adversarial network gan"}, {"st": 54, "ed": 56, "text": "generative network"}, {"st": 72, "ed": 76, "text": "recurrent neural network rnn"}, {"st": 83, "ed": 85, "text": "generative network"}, {"st": 105, "ed": 107, "text": "large scale"}, {"st": 144, "ed": 146, "text": "generative network"}]
[{"st": 27, "ed": 29, "text": "neural network"}, {"st": 90, "ed": 92, "text": "predictive power"}, {"st": 97, "ed": 100, "text": "text and image"}, {"st": 129, "ed": 133, "text": "mean squared error mse"}, {"st": 172, "ed": 174, "text": "image representations"}, {"st": 179, "ed": 181, "text": "performance improvement"}, {"st": 198, "ed": 200, "text": "object detection"}]
[{"st": 14, "ed": 17, "text": "deep neural network"}, {"st": 35, "ed": 37, "text": "hidden layer"}, {"st": 43, "ed": 45, "text": "hidden layer"}, {"st": 57, "ed": 59, "text": "classification accuracy"}, {"st": 73, "ed": 75, "text": "hidden layer"}, {"st": 89, "ed": 91, "text": "dimensional representation"}, {"st": 108, "ed": 110, "text": "hidden layer"}]
[{"st": 9, "ed": 13, "text": "automatic speech recognition asr"}, {"st": 28, "ed": 31, "text": "training and inference"}, {"st": 55, "ed": 57, "text": "speech signal"}, {"st": 59, "ed": 61, "text": "ground truth"}, {"st": 70, "ed": 72, "text": "remains unclear"}, {"st": 77, "ed": 79, "text": "real world"}, {"st": 111, "ed": 113, "text": "training data"}, {"st": 115, "ed": 117, "text": "error rate"}, {"st": 117, "ed": 119, "text": "evaluation metrics"}, {"st": 142, "ed": 144, "text": "reinforcement learning"}, {"st": 149, "ed": 151, "text": "training scheme"}, {"st": 152, "ed": 154, "text": "maximum likelihood"}, {"st": 156, "ed": 158, "text": "proposed approach"}, {"st": 160, "ed": 162, "text": "policy gradient"}, {"st": 178, "ed": 180, "text": "training process"}, {"st": 182, "ed": 184, "text": "directly optimize"}, {"st": 188, "ed": 190, "text": "levenshtein distance"}, {"st": 198, "ed": 200, "text": "significantly improved"}, {"st": 209, "ed": 211, "text": "maximum likelihood"}]
[{"st": 2, "ed": 4, "text": "machine learning"}, {"st": 5, "ed": 7, "text": "natural language"}, {"st": 66, "ed": 69, "text": "supervised machine learning"}, {"st": 85, "ed": 87, "text": "training data"}, {"st": 140, "ed": 142, "text": "text processing"}, {"st": 148, "ed": 150, "text": "training data"}, {"st": 169, "ed": 171, "text": "text processing"}, {"st": 172, "ed": 175, "text": "named entity recognition"}, {"st": 175, "ed": 177, "text": "sentiment analysis"}, {"st": 203, "ed": 206, "text": "supervised machine learning"}, {"st": 300, "ed": 302, "text": "big data"}]
[{"st": 13, "ed": 15, "text": "social media"}, {"st": 69, "ed": 71, "text": "topic modeling"}, {"st": 73, "ed": 75, "text": "sentiment analysis"}, {"st": 82, "ed": 84, "text": "natural language"}, {"st": 86, "ed": 88, "text": "text mining"}, {"st": 98, "ed": 100, "text": "topic modeling"}, {"st": 102, "ed": 106, "text": "restricted boltzmann machine rbm"}, {"st": 111, "ed": 113, "text": "neural networks"}, {"st": 133, "ed": 135, "text": "text data"}, {"st": 145, "ed": 147, "text": "topic modeling"}, {"st": 152, "ed": 154, "text": "proposed method"}, {"st": 160, "ed": 162, "text": "contrastive divergence"}, {"st": 184, "ed": 186, "text": "text data"}, {"st": 186, "ed": 188, "text": "sentiment classification"}, {"st": 199, "ed": 201, "text": "existing models"}, {"st": 209, "ed": 211, "text": "generative model"}, {"st": 211, "ed": 213, "text": "sentiment classification"}]
[{"st": 0, "ed": 2, "text": "kernel methods"}, {"st": 17, "ed": 19, "text": "natural language"}, {"st": 31, "ed": 33, "text": "natural language"}, {"st": 35, "ed": 37, "text": "recently proposed"}, {"st": 39, "ed": 42, "text": "locality sensitive hashing"}, {"st": 44, "ed": 46, "text": "significantly reduce"}, {"st": 47, "ed": 49, "text": "computational cost"}, {"st": 75, "ed": 77, "text": "natural language"}, {"st": 80, "ed": 82, "text": "general classification"}, {"st": 94, "ed": 96, "text": "classification problems"}, {"st": 99, "ed": 102, "text": "variational lower bound"}, {"st": 104, "ed": 106, "text": "mutual information"}, {"st": 110, "ed": 112, "text": "feature vectors"}, {"st": 119, "ed": 121, "text": "proposed approach"}, {"st": 124, "ed": 126, "text": "information extraction"}, {"st": 135, "ed": 137, "text": "significant speedup"}]
[{"st": 1, "ed": 3, "text": "topic modeling"}, {"st": 9, "ed": 12, "text": "word co occurrence"}, {"st": 88, "ed": 90, "text": "prior information"}, {"st": 141, "ed": 143, "text": "gibbs sampling"}]
[{"st": 7, "ed": 9, "text": "keyword spotting"}, {"st": 24, "ed": 26, "text": "challenging task"}, {"st": 41, "ed": 44, "text": "deep neural networks"}, {"st": 47, "ed": 49, "text": "feature representations"}, {"st": 60, "ed": 62, "text": "class imbalance"}, {"st": 64, "ed": 66, "text": "power consumption"}, {"st": 78, "ed": 80, "text": "false negative"}, {"st": 86, "ed": 89, "text": "false positive rate"}]
[{"st": 5, "ed": 7, "text": "fine grained"}, {"st": 7, "ed": 9, "text": "sentiment analysis"}, {"st": 13, "ed": 17, "text": "multiple instance learning mil"}, {"st": 50, "ed": 52, "text": "attention based"}, {"st": 57, "ed": 60, "text": "positive and negative"}]
[{"st": 1, "ed": 3, "text": "deep learning"}, {"st": 9, "ed": 11, "text": "network architectures"}, {"st": 18, "ed": 20, "text": "network structure"}, {"st": 40, "ed": 43, "text": "static and dynamic"}, {"st": 86, "ed": 88, "text": "network structure"}, {"st": 98, "ed": 100, "text": "instance specific"}, {"st": 135, "ed": 137, "text": "optimization techniques"}, {"st": 138, "ed": 140, "text": "pre defined"}, {"st": 202, "ed": 204, "text": "memory management"}]
[{"st": 2, "ed": 5, "text": "probabilistic topic models"}, {"st": 32, "ed": 34, "text": "hierarchical latent"}, {"st": 72, "ed": 74, "text": "co occurrences"}, {"st": 75, "ed": 77, "text": "co occurrences"}, {"st": 82, "ed": 84, "text": "multiple levels"}, {"st": 85, "ed": 87, "text": "latent variables"}, {"st": 109, "ed": 111, "text": "binary data"}, {"st": 132, "ed": 134, "text": "real valued"}, {"st": 148, "ed": 150, "text": "generation process"}, {"st": 167, "ed": 169, "text": "empirical results"}, {"st": 173, "ed": 175, "text": "significantly outperforms"}, {"st": 178, "ed": 180, "text": "lda based"}]
[{"st": 70, "ed": 72, "text": "machine learning"}, {"st": 87, "ed": 89, "text": "previous results"}, {"st": 93, "ed": 95, "text": "multimodal data"}, {"st": 108, "ed": 110, "text": "machine learning"}, {"st": 162, "ed": 164, "text": "artificial intelligence"}, {"st": 187, "ed": 189, "text": "elderly care"}]
[{"st": 6, "ed": 8, "text": "machine learning"}, {"st": 11, "ed": 13, "text": "training data"}, {"st": 18, "ed": 21, "text": "real world data"}, {"st": 75, "ed": 77, "text": "fine tuning"}, {"st": 108, "ed": 110, "text": "significantly improves"}, {"st": 184, "ed": 186, "text": "differential privacy"}, {"st": 187, "ed": 189, "text": "distributed training"}]
[{"st": 21, "ed": 23, "text": "speech recognition"}, {"st": 100, "ed": 102, "text": "features extracted"}, {"st": 110, "ed": 113, "text": "gaussian mixture model"}, {"st": 117, "ed": 120, "text": "deep neural network"}, {"st": 140, "ed": 142, "text": "proposed approach"}, {"st": 149, "ed": 151, "text": "performance improvements"}, {"st": 155, "ed": 157, "text": "a 10"}, {"st": 164, "ed": 166, "text": "dnn based"}]
[{"st": 5, "ed": 7, "text": "lifelong learning"}, {"st": 17, "ed": 19, "text": "learning process"}, {"st": 47, "ed": 49, "text": "sentiment classification"}, {"st": 57, "ed": 59, "text": "bayesian optimization"}, {"st": 71, "ed": 73, "text": "proposed method"}, {"st": 74, "ed": 76, "text": "baseline methods"}, {"st": 80, "ed": 82, "text": "lifelong learning"}]
[{"st": 8, "ed": 10, "text": "adversarial samples"}, {"st": 11, "ed": 13, "text": "white box"}, {"st": 55, "ed": 57, "text": "deep learning"}, {"st": 86, "ed": 88, "text": "character level"}, {"st": 111, "ed": 113, "text": "real world"}, {"st": 131, "ed": 133, "text": "classification accuracy"}, {"st": 162, "ed": 164, "text": "deep learning"}]
[{"st": 0, "ed": 2, "text": "text mining"}, {"st": 8, "ed": 10, "text": "extracting information"}, {"st": 25, "ed": 27, "text": "pre processing"}, {"st": 65, "ed": 68, "text": "curse of dimensionality"}, {"st": 71, "ed": 73, "text": "machine learning"}, {"st": 87, "ed": 89, "text": "feature extraction"}, {"st": 138, "ed": 140, "text": "classification accuracy"}, {"st": 144, "ed": 146, "text": "benchmark datasets"}]
[{"st": 0, "ed": 2, "text": "adversarial samples"}, {"st": 6, "ed": 8, "text": "extensively studied"}, {"st": 17, "ed": 19, "text": "gradient based"}, {"st": 49, "ed": 51, "text": "generating adversarial"}, {"st": 58, "ed": 60, "text": "input space"}, {"st": 98, "ed": 100, "text": "embedding space"}, {"st": 130, "ed": 132, "text": "extensive experiments"}, {"st": 209, "ed": 211, "text": "closely related"}]
[{"st": 0, "ed": 2, "text": "neural network"}, {"st": 4, "ed": 7, "text": "achieved great success"}, {"st": 102, "ed": 104, "text": "neural networks"}]
[{"st": 10, "ed": 12, "text": "open domain"}, {"st": 12, "ed": 14, "text": "text generation"}, {"st": 23, "ed": 25, "text": "text generation"}, {"st": 55, "ed": 57, "text": "text generation"}, {"st": 62, "ed": 64, "text": "fine tuned"}, {"st": 64, "ed": 66, "text": "open source"}, {"st": 85, "ed": 87, "text": "future research"}]
[{"st": 6, "ed": 8, "text": "neural network"}, {"st": 31, "ed": 33, "text": "convolutional layers"}, {"st": 35, "ed": 37, "text": "high level"}, {"st": 54, "ed": 56, "text": "layer wise"}, {"st": 56, "ed": 58, "text": "learning rate"}, {"st": 60, "ed": 62, "text": "batch normalization"}, {"st": 64, "ed": 66, "text": "highly competitive"}]
[{"st": 11, "ed": 13, "text": "neural network"}, {"st": 14, "ed": 16, "text": "speech synthesis"}, {"st": 62, "ed": 64, "text": "fine tuning"}, {"st": 67, "ed": 69, "text": "generative model"}]
[{"st": 5, "ed": 7, "text": "speech signals"}, {"st": 64, "ed": 66, "text": "carefully designed"}, {"st": 66, "ed": 69, "text": "deep neural network"}, {"st": 88, "ed": 90, "text": "proposed framework"}, {"st": 119, "ed": 121, "text": "speech signals"}, {"st": 146, "ed": 148, "text": "speech processing"}]
[{"st": 35, "ed": 37, "text": "cross domain"}, {"st": 67, "ed": 69, "text": "content based"}, {"st": 69, "ed": 71, "text": "cross domain"}, {"st": 74, "ed": 76, "text": "cold start"}, {"st": 90, "ed": 92, "text": "multi class"}, {"st": 111, "ed": 113, "text": "domain adaptation"}, {"st": 121, "ed": 123, "text": "source domain"}, {"st": 134, "ed": 136, "text": "neural network"}, {"st": 141, "ed": 143, "text": "domain adaptation"}, {"st": 148, "ed": 150, "text": "denoising autoencoder"}, {"st": 175, "ed": 177, "text": "yahoo japan"}, {"st": 181, "ed": 183, "text": "approach outperforms"}, {"st": 184, "ed": 186, "text": "baseline methods"}, {"st": 188, "ed": 190, "text": "cross domain"}, {"st": 190, "ed": 192, "text": "collaborative filtering"}]
[{"st": 6, "ed": 8, "text": "programming language"}, {"st": 61, "ed": 63, "text": "automatically learn"}, {"st": 63, "ed": 65, "text": "cross language"}, {"st": 81, "ed": 83, "text": "key idea"}, {"st": 102, "ed": 104, "text": "cross language"}, {"st": 116, "ed": 118, "text": "neural network"}, {"st": 122, "ed": 124, "text": "word embeddings"}, {"st": 137, "ed": 139, "text": "higher levels"}, {"st": 186, "ed": 188, "text": "automatically learn"}, {"st": 200, "ed": 202, "text": "cross language"}, {"st": 236, "ed": 238, "text": "https github.com"}, {"st": 240, "ed": 242, "text": "programming language"}, {"st": 250, "ed": 252, "text": "cross language"}, {"st": 256, "ed": 258, "text": "structural information"}]
[{"st": 2, "ed": 4, "text": "machine learning"}, {"st": 34, "ed": 36, "text": "textual data"}, {"st": 39, "ed": 41, "text": "text classification"}, {"st": 59, "ed": 61, "text": "empirical results"}, {"st": 65, "ed": 67, "text": "multi class"}, {"st": 67, "ed": 69, "text": "sentiment analysis"}]
[{"st": 9, "ed": 11, "text": "open source"}, {"st": 35, "ed": 37, "text": "web services"}, {"st": 49, "ed": 51, "text": "distributed systems"}]
[{"st": 22, "ed": 24, "text": "neural networks"}, {"st": 41, "ed": 43, "text": "low power"}, {"st": 71, "ed": 73, "text": "low power"}, {"st": 73, "ed": 75, "text": "deep learning"}, {"st": 87, "ed": 89, "text": "image sensor"}, {"st": 127, "ed": 129, "text": "receptive field"}, {"st": 130, "ed": 132, "text": "weight parameters"}, {"st": 155, "ed": 157, "text": "deep learning"}]
[{"st": 1, "ed": 3, "text": "cross entropy"}]
[{"st": 4, "ed": 6, "text": "recently introduced"}, {"st": 6, "ed": 8, "text": "highly efficient"}, {"st": 61, "ed": 63, "text": "recently proposed"}, {"st": 100, "ed": 102, "text": "speech recognition"}]
[{"st": 40, "ed": 42, "text": "neural network"}, {"st": 58, "ed": 60, "text": "training procedure"}, {"st": 80, "ed": 82, "text": "experiments confirm"}]
[{"st": 5, "ed": 7, "text": "neural network"}, {"st": 45, "ed": 47, "text": "differential geometry"}, {"st": 53, "ed": 55, "text": "natural gradient"}, {"st": 57, "ed": 59, "text": "fisher information"}]
[{"st": 4, "ed": 6, "text": "unlabeled data"}, {"st": 8, "ed": 11, "text": "unsupervised feature learning"}, {"st": 14, "ed": 16, "text": "important role"}, {"st": 26, "ed": 28, "text": "statistical mechanics"}, {"st": 31, "ed": 33, "text": "unsupervised learning"}, {"st": 35, "ed": 38, "text": "restricted boltzmann machine"}, {"st": 42, "ed": 44, "text": "message passing"}, {"st": 61, "ed": 63, "text": "statistical analysis"}, {"st": 85, "ed": 87, "text": "message passing"}, {"st": 91, "ed": 93, "text": "phase transition"}, {"st": 103, "ed": 105, "text": "phase transition"}, {"st": 118, "ed": 120, "text": "mean field"}, {"st": 132, "ed": 134, "text": "message passing"}, {"st": 155, "ed": 157, "text": "phase transition"}, {"st": 169, "ed": 171, "text": "hyper parameter"}, {"st": 196, "ed": 199, "text": "restricted boltzmann machine"}]
[{"st": 5, "ed": 7, "text": "hierarchical models"}, {"st": 13, "ed": 15, "text": "hierarchical models"}, {"st": 30, "ed": 32, "text": "hidden variables"}, {"st": 50, "ed": 52, "text": "linear subspaces"}, {"st": 55, "ed": 58, "text": "feedforward neural networks"}, {"st": 93, "ed": 96, "text": "restricted boltzmann machine"}, {"st": 101, "ed": 103, "text": "v 1"}, {"st": 103, "ed": 105, "text": "v 1"}, {"st": 106, "ed": 108, "text": "v 1"}, {"st": 125, "ed": 127, "text": "v 1"}]
[{"st": 22, "ed": 25, "text": "unsupervised feature learning"}, {"st": 27, "ed": 29, "text": "noisy data"}, {"st": 52, "ed": 54, "text": "receptive field"}, {"st": 62, "ed": 64, "text": "feature learning"}, {"st": 66, "ed": 68, "text": "statistical mechanics"}, {"st": 93, "ed": 95, "text": "receptive field"}, {"st": 105, "ed": 107, "text": "receptive field"}, {"st": 130, "ed": 132, "text": "handwritten digits"}, {"st": 138, "ed": 140, "text": "neural activity"}]
[{"st": 34, "ed": 37, "text": "deep neural network"}, {"st": 46, "ed": 48, "text": "predictive accuracy"}, {"st": 63, "ed": 65, "text": "feature selection"}, {"st": 72, "ed": 74, "text": "functional analysis"}, {"st": 112, "ed": 114, "text": "case study"}, {"st": 153, "ed": 155, "text": "predictive accuracy"}, {"st": 175, "ed": 177, "text": "feature selection"}]
[{"st": 6, "ed": 9, "text": "end to end"}, {"st": 11, "ed": 15, "text": "convolutional neural network cnn"}, {"st": 21, "ed": 23, "text": "machine learning"}, {"st": 56, "ed": 58, "text": "building blocks"}, {"st": 63, "ed": 65, "text": "image classification"}, {"st": 71, "ed": 73, "text": "multi level"}, {"st": 94, "ed": 96, "text": "significant improvements"}, {"st": 108, "ed": 110, "text": "comparable results"}]
[{"st": 1, "ed": 3, "text": "speech processing"}, {"st": 12, "ed": 14, "text": "deep learning"}, {"st": 17, "ed": 19, "text": "significant progress"}, {"st": 25, "ed": 27, "text": "remains challenging"}, {"st": 110, "ed": 112, "text": "encoder decoder"}, {"st": 116, "ed": 118, "text": "source separation"}, {"st": 159, "ed": 161, "text": "speech separation"}, {"st": 164, "ed": 166, "text": "computational cost"}, {"st": 167, "ed": 169, "text": "speech separation"}, {"st": 170, "ed": 172, "text": "significantly reduces"}, {"st": 186, "ed": 188, "text": "low power"}]
[{"st": 22, "ed": 24, "text": "neutron stars"}, {"st": 52, "ed": 54, "text": "deep learning"}, {"st": 55, "ed": 58, "text": "convolutional neural networks"}, {"st": 60, "ed": 62, "text": "time series"}, {"st": 69, "ed": 71, "text": "gravitational wave"}, {"st": 94, "ed": 96, "text": "real data"}, {"st": 102, "ed": 104, "text": "parameter estimation"}, {"st": 113, "ed": 115, "text": "continuous data"}, {"st": 127, "ed": 129, "text": "machine learning"}, {"st": 162, "ed": 164, "text": "computationally efficient"}, {"st": 175, "ed": 177, "text": "time series"}, {"st": 179, "ed": 181, "text": "non stationary"}, {"st": 182, "ed": 184, "text": "gaussian noise"}, {"st": 196, "ed": 198, "text": "gravitational wave"}, {"st": 208, "ed": 210, "text": "unified framework"}]
[{"st": 22, "ed": 24, "text": "neutron stars"}, {"st": 50, "ed": 54, "text": "deep convolutional neural networks"}, {"st": 60, "ed": 62, "text": "gravitational wave"}, {"st": 88, "ed": 90, "text": "real data"}, {"st": 101, "ed": 103, "text": "parameter estimation"}, {"st": 112, "ed": 114, "text": "continuous data"}, {"st": 126, "ed": 128, "text": "machine learning"}, {"st": 152, "ed": 154, "text": "computationally efficient"}, {"st": 170, "ed": 172, "text": "time series"}, {"st": 174, "ed": 176, "text": "non stationary"}, {"st": 177, "ed": 179, "text": "gaussian noise"}, {"st": 191, "ed": 193, "text": "gravitational wave"}]
[{"st": 0, "ed": 2, "text": "gravitational wave"}, {"st": 20, "ed": 22, "text": "gravitational wave"}, {"st": 42, "ed": 44, "text": "non stationary"}, {"st": 59, "ed": 62, "text": "principal component analysis"}, {"st": 73, "ed": 75, "text": "gaussian noise"}, {"st": 78, "ed": 82, "text": "signal to noise ratio"}, {"st": 82, "ed": 84, "text": "gravitational wave"}, {"st": 89, "ed": 91, "text": "computationally expensive"}, {"st": 104, "ed": 106, "text": "signal processing"}, {"st": 113, "ed": 115, "text": "deep learning"}, {"st": 117, "ed": 119, "text": "gravitational wave"}, {"st": 123, "ed": 125, "text": "gaussian noise"}, {"st": 137, "ed": 139, "text": "denoising autoencoder"}, {"st": 144, "ed": 146, "text": "bi directional"}, {"st": 147, "ed": 150, "text": "short term memory"}, {"st": 161, "ed": 163, "text": "deep learning"}, {"st": 172, "ed": 174, "text": "gaussian noise"}, {"st": 180, "ed": 182, "text": "gravitational wave"}]
[{"st": 31, "ed": 33, "text": "entorhinal cortex"}, {"st": 173, "ed": 175, "text": "significantly improve"}]
[{"st": 4, "ed": 6, "text": "residual networks"}, {"st": 7, "ed": 9, "text": "mean field"}, {"st": 17, "ed": 20, "text": "feedforward neural networks"}, {"st": 48, "ed": 50, "text": "input space"}, {"st": 69, "ed": 71, "text": "skip connections"}, {"st": 123, "ed": 125, "text": "residual networks"}, {"st": 140, "ed": 142, "text": "input space"}, {"st": 152, "ed": 154, "text": "activation function"}, {"st": 159, "ed": 161, "text": "residual networks"}, {"st": 175, "ed": 177, "text": "accurately predict"}, {"st": 227, "ed": 229, "text": "residual networks"}, {"st": 263, "ed": 265, "text": "bessel function"}]
[]
[{"st": 42, "ed": 44, "text": "logistic regression"}, {"st": 68, "ed": 70, "text": "rasch model"}, {"st": 93, "ed": 95, "text": "dimensionality reduction"}, {"st": 137, "ed": 139, "text": "proposed method"}, {"st": 146, "ed": 149, "text": "principal component analysis"}]
[{"st": 1, "ed": 3, "text": "nonparametric bayesian"}, {"st": 5, "ed": 7, "text": "factor analysis"}, {"st": 11, "ed": 13, "text": "observed data"}, {"st": 31, "ed": 33, "text": "mathbf x"}, {"st": 56, "ed": 58, "text": "latent features"}, {"st": 73, "ed": 75, "text": "randomly generated"}]
[{"st": 41, "ed": 43, "text": "statistical analysis"}, {"st": 59, "ed": 61, "text": "statistical analysis"}, {"st": 95, "ed": 99, "text": "markov chain monte carlo"}, {"st": 142, "ed": 144, "text": "artificial intelligence"}, {"st": 149, "ed": 151, "text": "statistical analysis"}, {"st": 162, "ed": 164, "text": "dynamic programming"}, {"st": 165, "ed": 167, "text": "control theory"}, {"st": 182, "ed": 184, "text": "statistical analysis"}, {"st": 195, "ed": 197, "text": "control theory"}, {"st": 198, "ed": 200, "text": "machine learning"}]
[{"st": 6, "ed": 8, "text": "learning problems"}, {"st": 11, "ed": 13, "text": "feature space"}, {"st": 16, "ed": 18, "text": "structured sparsity"}, {"st": 22, "ed": 24, "text": "prior knowledge"}, {"st": 39, "ed": 41, "text": "optimization algorithms"}, {"st": 62, "ed": 64, "text": "sparsity inducing"}, {"st": 64, "ed": 66, "text": "regularization terms"}, {"st": 68, "ed": 70, "text": "group lasso"}, {"st": 73, "ed": 75, "text": "l 2"}, {"st": 86, "ed": 88, "text": "unified framework"}, {"st": 112, "ed": 114, "text": "building block"}, {"st": 140, "ed": 142, "text": "frac 1"}, {"st": 174, "ed": 176, "text": "real world"}]
[{"st": 42, "ed": 44, "text": "unlike previous"}, {"st": 62, "ed": 64, "text": "exponential loss"}, {"st": 68, "ed": 70, "text": "result shows"}, {"st": 75, "ed": 77, "text": "exponential loss"}, {"st": 96, "ed": 98, "text": "ell 1"}, {"st": 149, "ed": 151, "text": "exponential loss"}]
[{"st": 0, "ed": 2, "text": "kernel methods"}, {"st": 19, "ed": 21, "text": "central role"}, {"st": 59, "ed": 61, "text": "gaussian processes"}, {"st": 63, "ed": 65, "text": "kernel function"}, {"st": 73, "ed": 75, "text": "kernel methods"}, {"st": 79, "ed": 81, "text": "supervised learning"}, {"st": 135, "ed": 137, "text": "kernel functions"}]
[{"st": 24, "ed": 26, "text": "temporal information"}, {"st": 29, "ed": 31, "text": "existing techniques"}, {"st": 59, "ed": 62, "text": "statistical relational learning"}, {"st": 75, "ed": 77, "text": "pattern mining"}, {"st": 120, "ed": 122, "text": "ensemble methods"}, {"st": 132, "ed": 134, "text": "outperform traditional"}]
[{"st": 14, "ed": 16, "text": "dynamic systems"}, {"st": 19, "ed": 21, "text": "transition function"}, {"st": 30, "ed": 33, "text": "gaussian process gp"}, {"st": 40, "ed": 42, "text": "signal processing"}, {"st": 42, "ed": 44, "text": "machine learning"}, {"st": 53, "ed": 55, "text": "posterior probability"}, {"st": 88, "ed": 90, "text": "dynamic systems"}, {"st": 106, "ed": 108, "text": "proposed approach"}]
[{"st": 3, "ed": 5, "text": "graphical model"}, {"st": 13, "ed": 15, "text": "generative model"}, {"st": 24, "ed": 26, "text": "joint distribution"}, {"st": 43, "ed": 45, "text": "graphical model"}, {"st": 64, "ed": 66, "text": "granger causality"}, {"st": 145, "ed": 147, "text": "structure learning"}, {"st": 149, "ed": 152, "text": "undirected graphical models"}, {"st": 153, "ed": 155, "text": "mutual information"}, {"st": 165, "ed": 167, "text": "sample complexity"}, {"st": 192, "ed": 194, "text": "confidence intervals"}, {"st": 218, "ed": 220, "text": "synthetic data"}, {"st": 221, "ed": 223, "text": "real data"}]
[{"st": 1, "ed": 4, "text": "electronic health records"}, {"st": 42, "ed": 44, "text": "decision support"}, {"st": 59, "ed": 61, "text": "valuable information"}, {"st": 75, "ed": 77, "text": "case study"}, {"st": 91, "ed": 94, "text": "electronic health record"}, {"st": 185, "ed": 187, "text": "odds ratio"}, {"st": 231, "ed": 233, "text": "decision support"}, {"st": 235, "ed": 237, "text": "real world"}]
[{"st": 4, "ed": 6, "text": "chain graphs"}, {"st": 19, "ed": 21, "text": "constraint based"}, {"st": 47, "ed": 49, "text": "chain graphs"}, {"st": 62, "ed": 64, "text": "learning algorithms"}]
[{"st": 1, "ed": 4, "text": "undirected graphical model"}, {"st": 6, "ed": 9, "text": "joint probability distribution"}, {"st": 12, "ed": 14, "text": "undirected graph"}, {"st": 25, "ed": 27, "text": "random variables"}, {"st": 31, "ed": 33, "text": "conditional independence"}, {"st": 38, "ed": 41, "text": "undirected graphical model"}, {"st": 83, "ed": 85, "text": "junction tree"}, {"st": 149, "ed": 151, "text": "junction tree"}, {"st": 178, "ed": 180, "text": "junction tree"}]
[{"st": 10, "ed": 12, "text": "convex function"}, {"st": 15, "ed": 18, "text": "block coordinate descent"}, {"st": 39, "ed": 41, "text": "existing algorithms"}, {"st": 75, "ed": 78, "text": "block coordinate descent"}, {"st": 95, "ed": 97, "text": "theoretical guarantees"}]
[]
[{"st": 14, "ed": 16, "text": "undirected graph"}, {"st": 25, "ed": 27, "text": "random variables"}, {"st": 90, "ed": 92, "text": "probability distributions"}]
[{"st": 5, "ed": 7, "text": "learning algorithms"}]
[{"st": 125, "ed": 127, "text": "insurance fraud"}]
[{"st": 65, "ed": 67, "text": "recent studies"}, {"st": 118, "ed": 120, "text": "matrix factorization"}]
[{"st": 57, "ed": 59, "text": "causal structure"}, {"st": 65, "ed": 67, "text": "observed variables"}, {"st": 96, "ed": 98, "text": "causal relationships"}]
[{"st": 8, "ed": 10, "text": "decision making"}, {"st": 53, "ed": 55, "text": "randomly generated"}, {"st": 71, "ed": 73, "text": "np hard"}]
[{"st": 16, "ed": 18, "text": "chain graphs"}, {"st": 100, "ed": 102, "text": "chain graphs"}, {"st": 117, "ed": 119, "text": "bayesian networks"}, {"st": 131, "ed": 133, "text": "learning algorithms"}]
[{"st": 1, "ed": 3, "text": "free energy"}, {"st": 10, "ed": 12, "text": "variational principle"}, {"st": 15, "ed": 17, "text": "decision making"}, {"st": 40, "ed": 42, "text": "free energy"}, {"st": 45, "ed": 47, "text": "decision trees"}, {"st": 78, "ed": 80, "text": "decision rules"}, {"st": 90, "ed": 92, "text": "decision rules"}, {"st": 98, "ed": 100, "text": "free energy"}, {"st": 119, "ed": 121, "text": "computational cost"}, {"st": 142, "ed": 144, "text": "free energy"}]
[{"st": 5, "ed": 7, "text": "network analysis"}, {"st": 8, "ed": 10, "text": "data mining"}]
[{"st": 32, "ed": 34, "text": "gaussian processes"}, {"st": 39, "ed": 41, "text": "pre processing"}, {"st": 46, "ed": 48, "text": "fully automated"}, {"st": 52, "ed": 54, "text": "raw data"}, {"st": 59, "ed": 61, "text": "pre processing"}, {"st": 64, "ed": 66, "text": "hyper parameters"}, {"st": 71, "ed": 73, "text": "marginal likelihood"}, {"st": 111, "ed": 114, "text": "low computational cost"}]
[{"st": 0, "ed": 3, "text": "expectation maximization em"}, {"st": 14, "ed": 16, "text": "finite state"}, {"st": 26, "ed": 28, "text": "current methods"}, {"st": 29, "ed": 31, "text": "fixed size"}, {"st": 72, "ed": 74, "text": "framework called"}, {"st": 91, "ed": 93, "text": "variational bayesian"}, {"st": 114, "ed": 116, "text": "benchmark problems"}]
[{"st": 10, "ed": 12, "text": "lloyd shapley"}, {"st": 42, "ed": 44, "text": "nash equilibria"}, {"st": 55, "ed": 57, "text": "special cases"}, {"st": 75, "ed": 77, "text": "artificial intelligence"}, {"st": 94, "ed": 96, "text": "real world"}, {"st": 105, "ed": 107, "text": "resource allocation"}]
[{"st": 42, "ed": 44, "text": "data science"}, {"st": 66, "ed": 68, "text": "low level"}, {"st": 80, "ed": 82, "text": "predictive models"}, {"st": 88, "ed": 90, "text": "generated data"}, {"st": 109, "ed": 111, "text": "data science"}, {"st": 131, "ed": 133, "text": "predictive model"}, {"st": 140, "ed": 142, "text": "big data"}, {"st": 151, "ed": 154, "text": "massive amounts of"}, {"st": 159, "ed": 161, "text": "feature set"}, {"st": 178, "ed": 180, "text": "random forest"}, {"st": 205, "ed": 207, "text": "false positive"}, {"st": 235, "ed": 237, "text": "predictive model"}, {"st": 258, "ed": 260, "text": "off line"}]
[{"st": 25, "ed": 28, "text": "taking advantage of"}, {"st": 28, "ed": 30, "text": "recent advances"}, {"st": 40, "ed": 42, "text": "inference problems"}, {"st": 43, "ed": 47, "text": "markov random fields mrfs"}, {"st": 68, "ed": 70, "text": "probabilistic inference"}, {"st": 79, "ed": 81, "text": "recent theoretical"}, {"st": 82, "ed": 84, "text": "empirical results"}, {"st": 129, "ed": 131, "text": "variational inference"}, {"st": 150, "ed": 152, "text": "message passing"}, {"st": 157, "ed": 160, "text": "zero sum game"}, {"st": 176, "ed": 178, "text": "synthetic experiments"}, {"st": 181, "ed": 183, "text": "approximation algorithms"}, {"st": 198, "ed": 200, "text": "random variables"}, {"st": 235, "ed": 237, "text": "edge weights"}]
[{"st": 20, "ed": 22, "text": "decision making"}, {"st": 30, "ed": 32, "text": "decision makers"}, {"st": 47, "ed": 49, "text": "artificial intelligence"}, {"st": 124, "ed": 126, "text": "hyperbolic discounting"}, {"st": 134, "ed": 136, "text": "probabilistic model"}]
[{"st": 5, "ed": 7, "text": "software design"}, {"st": 14, "ed": 16, "text": "computational intelligence"}, {"st": 18, "ed": 20, "text": "multi core"}, {"st": 26, "ed": 28, "text": "functional programming"}, {"st": 34, "ed": 37, "text": "probabilistic graphical models"}, {"st": 50, "ed": 53, "text": "inference and learning"}, {"st": 58, "ed": 60, "text": "maximum likelihood"}, {"st": 61, "ed": 63, "text": "importance sampling"}, {"st": 64, "ed": 66, "text": "greedy search"}, {"st": 87, "ed": 89, "text": "parallel processing"}, {"st": 112, "ed": 114, "text": "multi core"}, {"st": 122, "ed": 124, "text": "open source"}, {"st": 127, "ed": 129, "text": "massive data"}]
[{"st": 4, "ed": 6, "text": "feature selection"}, {"st": 12, "ed": 14, "text": "input features"}, {"st": 30, "ed": 32, "text": "computationally efficient"}, {"st": 32, "ed": 34, "text": "feature selection"}, {"st": 39, "ed": 41, "text": "input features"}, {"st": 58, "ed": 60, "text": "input output"}, {"st": 69, "ed": 71, "text": "kernel functions"}, {"st": 87, "ed": 89, "text": "kernel based"}, {"st": 96, "ed": 98, "text": "globally optimal"}, {"st": 116, "ed": 118, "text": "proposed method"}, {"st": 121, "ed": 123, "text": "feature selection"}]
[]
[{"st": 16, "ed": 18, "text": "unstructured text"}, {"st": 27, "ed": 29, "text": "k theory"}, {"st": 39, "ed": 42, "text": "formal concept analysis"}, {"st": 44, "ed": 46, "text": "self organizing"}, {"st": 49, "ed": 52, "text": "hidden markov models"}, {"st": 65, "ed": 67, "text": "text mining"}, {"st": 71, "ed": 73, "text": "text mining"}, {"st": 79, "ed": 81, "text": "unstructured text"}, {"st": 101, "ed": 103, "text": "text mining"}]
[{"st": 38, "ed": 40, "text": "higher order"}, {"st": 123, "ed": 125, "text": "file format"}, {"st": 126, "ed": 128, "text": "command line"}]
[{"st": 14, "ed": 16, "text": "data mining"}, {"st": 32, "ed": 34, "text": "domain specific"}, {"st": 34, "ed": 36, "text": "data mining"}, {"st": 42, "ed": 44, "text": "data mining"}]
[{"st": 8, "ed": 10, "text": "stochastic optimization"}, {"st": 72, "ed": 74, "text": "conjugate prior"}, {"st": 81, "ed": 83, "text": "posterior distribution"}, {"st": 108, "ed": 110, "text": "convex objective"}]
[{"st": 0, "ed": 2, "text": "gaussian processes"}, {"st": 4, "ed": 6, "text": "powerful tools"}, {"st": 7, "ed": 9, "text": "probabilistic modeling"}, {"st": 16, "ed": 18, "text": "prior distributions"}, {"st": 22, "ed": 24, "text": "hierarchical bayesian"}, {"st": 68, "ed": 70, "text": "theoretical properties"}, {"st": 86, "ed": 88, "text": "gp models"}]
[{"st": 2, "ed": 5, "text": "stochastic variational inference"}, {"st": 21, "ed": 23, "text": "probabilistic models"}, {"st": 29, "ed": 32, "text": "probabilistic topic models"}, {"st": 32, "ed": 35, "text": "latent dirichlet allocation"}, {"st": 37, "ed": 40, "text": "hierarchical dirichlet process"}, {"st": 43, "ed": 46, "text": "stochastic variational inference"}, {"st": 61, "ed": 63, "text": "new york"}, {"st": 82, "ed": 84, "text": "variational inference"}, {"st": 98, "ed": 100, "text": "topic model"}, {"st": 104, "ed": 107, "text": "stochastic variational inference"}, {"st": 114, "ed": 116, "text": "massive data"}]
[{"st": 7, "ed": 10, "text": "block coordinate descent"}, {"st": 30, "ed": 32, "text": "convex function"}, {"st": 51, "ed": 54, "text": "number of iterations"}, {"st": 93, "ed": 95, "text": "worst case"}, {"st": 165, "ed": 167, "text": "problem involving"}]
[{"st": 3, "ed": 6, "text": "sequential decision making"}, {"st": 27, "ed": 29, "text": "hierarchical structure"}, {"st": 52, "ed": 55, "text": "markov decision processes"}, {"st": 99, "ed": 101, "text": "substantial improvements"}, {"st": 102, "ed": 104, "text": "convergence rates"}, {"st": 115, "ed": 117, "text": "significant computational"}, {"st": 179, "ed": 181, "text": "domains including"}, {"st": 183, "ed": 186, "text": "discrete and continuous"}]
[{"st": 5, "ed": 7, "text": "open source"}, {"st": 17, "ed": 19, "text": "probabilistic models"}, {"st": 37, "ed": 39, "text": "inference engine"}]
[{"st": 8, "ed": 10, "text": "common sense"}, {"st": 23, "ed": 25, "text": "alan turing"}, {"st": 53, "ed": 56, "text": "artificial intelligence ai"}, {"st": 105, "ed": 107, "text": "common sense"}, {"st": 109, "ed": 111, "text": "probabilistic inference"}, {"st": 113, "ed": 115, "text": "statistical model"}, {"st": 142, "ed": 144, "text": "cognitive science"}]
[{"st": 0, "ed": 2, "text": "gaussian processes"}, {"st": 20, "ed": 22, "text": "closed form"}, {"st": 28, "ed": 30, "text": "gaussian processes"}, {"st": 43, "ed": 45, "text": "spectral density"}, {"st": 66, "ed": 68, "text": "gaussian process"}]
[{"st": 4, "ed": 6, "text": "message passing"}, {"st": 9, "ed": 11, "text": "constraint satisfaction"}, {"st": 18, "ed": 20, "text": "belief propagation"}, {"st": 49, "ed": 51, "text": "inference procedures"}, {"st": 59, "ed": 61, "text": "gibbs sampler"}, {"st": 93, "ed": 95, "text": "real world"}, {"st": 124, "ed": 127, "text": "compares favorably with"}, {"st": 137, "ed": 139, "text": "computational complexity"}]
[{"st": 5, "ed": 7, "text": "prediction market"}, {"st": 26, "ed": 28, "text": "exponential family"}, {"st": 69, "ed": 71, "text": "learning agents"}, {"st": 74, "ed": 76, "text": "exponential family"}]
[{"st": 6, "ed": 8, "text": "monte carlo"}, {"st": 10, "ed": 14, "text": "markov chain monte carlo"}, {"st": 15, "ed": 17, "text": "probabilistic programming"}, {"st": 22, "ed": 24, "text": "programming language"}, {"st": 48, "ed": 50, "text": "probabilistic programming"}, {"st": 64, "ed": 66, "text": "machine code"}, {"st": 80, "ed": 82, "text": "probabilistic programming"}, {"st": 96, "ed": 98, "text": "probabilistic programming"}]
[{"st": 118, "ed": 120, "text": "domain knowledge"}, {"st": 165, "ed": 167, "text": "case study"}, {"st": 170, "ed": 172, "text": "alpine marmot"}]
[{"st": 28, "ed": 30, "text": "error rate"}, {"st": 86, "ed": 88, "text": "computational burden"}, {"st": 131, "ed": 133, "text": "low rank"}, {"st": 135, "ed": 137, "text": "low variance"}, {"st": 152, "ed": 154, "text": "matrix completion"}, {"st": 171, "ed": 173, "text": "without sacrificing"}]
[{"st": 8, "ed": 10, "text": "convergence analysis"}, {"st": 84, "ed": 86, "text": "off policy"}, {"st": 89, "ed": 92, "text": "temporal difference learning"}, {"st": 93, "ed": 95, "text": "linear function"}]
[{"st": 7, "ed": 9, "text": "optimal policy"}, {"st": 11, "ed": 14, "text": "markov decision process"}, {"st": 22, "ed": 24, "text": "primal dual"}, {"st": 63, "ed": 65, "text": "computational complexity"}, {"st": 74, "ed": 76, "text": "optimal policy"}, {"st": 80, "ed": 82, "text": "mathcal o"}, {"st": 82, "ed": 84, "text": "left frac"}, {"st": 102, "ed": 104, "text": "infinite horizon"}, {"st": 108, "ed": 110, "text": "mathcal o"}, {"st": 110, "ed": 112, "text": "left frac"}, {"st": 126, "ed": 128, "text": "finite horizon"}]
[{"st": 9, "ed": 12, "text": "a posteriori map"}, {"st": 23, "ed": 25, "text": "most probable"}, {"st": 60, "ed": 62, "text": "naive bayes"}]
[{"st": 4, "ed": 6, "text": "nonparametric bayesian"}, {"st": 8, "ed": 11, "text": "exploratory data analysis"}, {"st": 12, "ed": 14, "text": "feature construction"}, {"st": 29, "ed": 31, "text": "time series"}, {"st": 45, "ed": 47, "text": "allocation lda"}, {"st": 51, "ed": 53, "text": "hierarchical dirichlet"}, {"st": 64, "ed": 66, "text": "latent topics"}, {"st": 123, "ed": 125, "text": "supervised learning"}]
[{"st": 4, "ed": 6, "text": "chain graphs"}, {"st": 18, "ed": 20, "text": "gaussian distributions"}, {"st": 39, "ed": 41, "text": "mathbb r"}, {"st": 63, "ed": 65, "text": "mathbb r"}, {"st": 80, "ed": 82, "text": "gaussian distributions"}]
[{"st": 80, "ed": 82, "text": "message passing"}, {"st": 137, "ed": 139, "text": "character recognition"}]
[{"st": 11, "ed": 14, "text": "mixture of gaussians"}, {"st": 25, "ed": 27, "text": "input data"}, {"st": 35, "ed": 37, "text": "dimensional vector"}, {"st": 92, "ed": 94, "text": "em algorithm"}, {"st": 113, "ed": 116, "text": "blind source separation"}, {"st": 125, "ed": 127, "text": "objective function"}, {"st": 132, "ed": 134, "text": "differential entropy"}, {"st": 142, "ed": 144, "text": "differential entropy"}]
[{"st": 7, "ed": 9, "text": "statistical modeling"}, {"st": 21, "ed": 23, "text": "predictive modeling"}, {"st": 60, "ed": 62, "text": "statistical models"}, {"st": 84, "ed": 86, "text": "regularization term"}, {"st": 88, "ed": 90, "text": "learning algorithm"}, {"st": 91, "ed": 93, "text": "objective function"}, {"st": 114, "ed": 116, "text": "prior knowledge"}, {"st": 145, "ed": 147, "text": "generalization bound"}]
[{"st": 2, "ed": 4, "text": "time consuming"}, {"st": 5, "ed": 7, "text": "error prone"}, {"st": 9, "ed": 11, "text": "inference procedures"}, {"st": 16, "ed": 18, "text": "probabilistic programming"}, {"st": 33, "ed": 35, "text": "automatically generate"}, {"st": 36, "ed": 38, "text": "inference procedure"}, {"st": 63, "ed": 65, "text": "probabilistic programming"}, {"st": 91, "ed": 93, "text": "programming language"}, {"st": 128, "ed": 130, "text": "conditional independence"}]
[{"st": 29, "ed": 32, "text": "point of view"}, {"st": 67, "ed": 69, "text": "free energy"}, {"st": 71, "ed": 73, "text": "decision making"}, {"st": 86, "ed": 88, "text": "decision making"}]
[{"st": 66, "ed": 68, "text": "sufficient conditions"}, {"st": 100, "ed": 103, "text": "sufficient condition for"}]
[{"st": 5, "ed": 7, "text": "virtual machine"}, {"st": 8, "ed": 10, "text": "probabilistic programming"}, {"st": 25, "ed": 27, "text": "probabilistic models"}, {"st": 28, "ed": 30, "text": "inference problems"}, {"st": 38, "ed": 40, "text": "higher order"}, {"st": 102, "ed": 104, "text": "control flow"}, {"st": 104, "ed": 106, "text": "higher order"}, {"st": 125, "ed": 127, "text": "latent variables"}, {"st": 165, "ed": 167, "text": "inference problems"}, {"st": 197, "ed": 199, "text": "previous approaches"}, {"st": 221, "ed": 223, "text": "gibbs sampling"}, {"st": 229, "ed": 233, "text": "markov chain monte carlo"}, {"st": 234, "ed": 236, "text": "mean field"}, {"st": 236, "ed": 238, "text": "variational inference"}]
[{"st": 0, "ed": 2, "text": "bayesian probability"}, {"st": 54, "ed": 56, "text": "bayesian probability"}]
[{"st": 90, "ed": 92, "text": "graphical model"}, {"st": 109, "ed": 111, "text": "probabilistic logic"}]
[{"st": 1, "ed": 5, "text": "markov chain monte carlo"}, {"st": 15, "ed": 17, "text": "probabilistic program"}, {"st": 40, "ed": 42, "text": "sample size"}, {"st": 66, "ed": 68, "text": "probabilistic programming"}, {"st": 71, "ed": 73, "text": "empirical results"}]
[{"st": 9, "ed": 12, "text": "loopy belief propagation"}, {"st": 16, "ed": 19, "text": "markov random fields"}, {"st": 22, "ed": 24, "text": "continuous state"}, {"st": 51, "ed": 53, "text": "exponential family"}, {"st": 60, "ed": 62, "text": "expectation propagation"}, {"st": 92, "ed": 94, "text": "belief propagation"}, {"st": 106, "ed": 108, "text": "computational cost"}, {"st": 115, "ed": 117, "text": "computational complexity"}, {"st": 139, "ed": 141, "text": "computational complexity"}, {"st": 150, "ed": 152, "text": "marginal distributions"}]
[{"st": 15, "ed": 17, "text": "challenging problems"}, {"st": 19, "ed": 21, "text": "collaborative filtering"}, {"st": 33, "ed": 35, "text": "hierarchical structure"}, {"st": 62, "ed": 64, "text": "closely related"}, {"st": 73, "ed": 75, "text": "theoretical properties"}, {"st": 81, "ed": 83, "text": "sufficient conditions"}, {"st": 91, "ed": 93, "text": "cold start"}, {"st": 110, "ed": 112, "text": "widely applied"}, {"st": 112, "ed": 114, "text": "performance metrics"}]
[{"st": 11, "ed": 13, "text": "probabilistic programming"}, {"st": 36, "ed": 38, "text": "probabilistic programming"}, {"st": 50, "ed": 52, "text": "control flow"}]
[{"st": 72, "ed": 74, "text": "prediction models"}, {"st": 126, "ed": 128, "text": "prediction models"}, {"st": 202, "ed": 205, "text": "click through rate"}, {"st": 241, "ed": 243, "text": "meta analysis"}, {"st": 243, "ed": 245, "text": "based approach"}]
[{"st": 2, "ed": 4, "text": "human brain"}, {"st": 25, "ed": 27, "text": "prohibitively expensive"}, {"st": 84, "ed": 86, "text": "large scale"}]
[{"st": 4, "ed": 6, "text": "case study"}, {"st": 39, "ed": 41, "text": "machine learning"}, {"st": 70, "ed": 72, "text": "home automation"}]
[{"st": 2, "ed": 4, "text": "belief propagation"}, {"st": 8, "ed": 10, "text": "message passing"}, {"st": 15, "ed": 18, "text": "a posteriori map"}, {"st": 25, "ed": 27, "text": "graphical model"}, {"st": 41, "ed": 43, "text": "optimization problems"}, {"st": 47, "ed": 49, "text": "shortest path"}, {"st": 52, "ed": 54, "text": "vertex cover"}, {"st": 61, "ed": 63, "text": "linear programming"}, {"st": 112, "ed": 114, "text": "matching problem"}, {"st": 147, "ed": 150, "text": "o n 2"}]
[{"st": 11, "ed": 13, "text": "object categories"}, {"st": 17, "ed": 20, "text": "hierarchical dirichlet process"}, {"st": 29, "ed": 31, "text": "object categories"}, {"st": 134, "ed": 137, "text": "kullback leibler divergence"}, {"st": 166, "ed": 168, "text": "monte carlo"}, {"st": 203, "ed": 205, "text": "graphical model"}, {"st": 226, "ed": 228, "text": "greedy algorithms"}, {"st": 233, "ed": 235, "text": "theoretical justification"}, {"st": 246, "ed": 248, "text": "humanoid robot"}]
[{"st": 6, "ed": 8, "text": "coarse grained"}, {"st": 51, "ed": 53, "text": "moment matching"}, {"st": 71, "ed": 73, "text": "latent variable"}, {"st": 92, "ed": 94, "text": "individual level"}, {"st": 107, "ed": 109, "text": "case study"}, {"st": 119, "ed": 122, "text": "blind source separation"}, {"st": 133, "ed": 135, "text": "accurate predictions"}]
[{"st": 58, "ed": 60, "text": "random walks"}, {"st": 115, "ed": 117, "text": "brownian motion"}, {"st": 156, "ed": 161, "text": "simulated and real world data"}]
[{"st": 6, "ed": 8, "text": "statistical methods"}, {"st": 30, "ed": 32, "text": "method called"}, {"st": 108, "ed": 110, "text": "mixture components"}, {"st": 112, "ed": 114, "text": "parametric models"}, {"st": 150, "ed": 152, "text": "latent variables"}, {"st": 192, "ed": 194, "text": "gibbs sampling"}, {"st": 208, "ed": 210, "text": "empirical results"}, {"st": 249, "ed": 251, "text": "common sense"}, {"st": 253, "ed": 255, "text": "multiple domains"}, {"st": 257, "ed": 259, "text": "predictive accuracy"}]
[{"st": 2, "ed": 6, "text": "markov chain monte carlo"}, {"st": 9, "ed": 11, "text": "monte carlo"}, {"st": 15, "ed": 17, "text": "probabilistic programming"}, {"st": 36, "ed": 38, "text": "discriminative model"}, {"st": 40, "ed": 42, "text": "neural network"}, {"st": 78, "ed": 80, "text": "probabilistic programming"}, {"st": 89, "ed": 91, "text": "significantly improve"}]
[{"st": 0, "ed": 2, "text": "bounded rationality"}, {"st": 4, "ed": 6, "text": "decision making"}, {"st": 17, "ed": 19, "text": "open problem"}, {"st": 20, "ed": 22, "text": "artificial intelligence"}, {"st": 22, "ed": 24, "text": "reinforcement learning"}, {"st": 24, "ed": 26, "text": "computational neuroscience"}, {"st": 38, "ed": 40, "text": "bounded rationality"}, {"st": 53, "ed": 55, "text": "free energy"}, {"st": 58, "ed": 60, "text": "objective function"}, {"st": 81, "ed": 83, "text": "monte carlo"}, {"st": 92, "ed": 94, "text": "exhaustive search"}, {"st": 118, "ed": 120, "text": "decision making"}, {"st": 146, "ed": 148, "text": "decision rules"}]
[{"st": 11, "ed": 13, "text": "time series"}, {"st": 36, "ed": 38, "text": "discriminative features"}, {"st": 41, "ed": 43, "text": "mutual information"}, {"st": 54, "ed": 56, "text": "time series"}, {"st": 100, "ed": 102, "text": "method outperforms"}, {"st": 103, "ed": 105, "text": "machine learning"}]
[{"st": 0, "ed": 3, "text": "inverse reinforcement learning"}, {"st": 40, "ed": 42, "text": "large scale"}, {"st": 73, "ed": 77, "text": "partially observable markov decision"}, {"st": 95, "ed": 97, "text": "multi agent"}, {"st": 121, "ed": 123, "text": "control problem"}, {"st": 128, "ed": 130, "text": "learning scheme"}]
[{"st": 4, "ed": 6, "text": "recommender systems"}, {"st": 27, "ed": 29, "text": "low rank"}, {"st": 47, "ed": 49, "text": "nuclear norm"}, {"st": 51, "ed": 53, "text": "convex relaxation"}, {"st": 76, "ed": 78, "text": "rank minimization"}, {"st": 88, "ed": 90, "text": "empirical results"}, {"st": 111, "ed": 113, "text": "rank approximation"}, {"st": 120, "ed": 122, "text": "recommendation systems"}, {"st": 124, "ed": 126, "text": "approximation error"}, {"st": 131, "ed": 133, "text": "real data"}, {"st": 137, "ed": 139, "text": "rank approximation"}]
[{"st": 0, "ed": 2, "text": "probabilistic inference"}, {"st": 23, "ed": 25, "text": "inference procedures"}, {"st": 36, "ed": 38, "text": "inference algorithms"}, {"st": 47, "ed": 49, "text": "exact inference"}, {"st": 53, "ed": 55, "text": "probabilistic programs"}, {"st": 64, "ed": 66, "text": "inference procedures"}, {"st": 74, "ed": 76, "text": "probabilistic programming"}, {"st": 78, "ed": 80, "text": "real world"}]
[{"st": 25, "ed": 27, "text": "optimization algorithms"}]
[{"st": 3, "ed": 5, "text": "worst case"}, {"st": 6, "ed": 8, "text": "optimization problem"}, {"st": 9, "ed": 11, "text": "budget constraint"}, {"st": 17, "ed": 19, "text": "practical applications"}, {"st": 20, "ed": 22, "text": "artificial intelligence"}, {"st": 31, "ed": 33, "text": "greedy algorithms"}, {"st": 52, "ed": 54, "text": "greedy algorithms"}, {"st": 56, "ed": 58, "text": "near optimal"}, {"st": 64, "ed": 66, "text": "near optimal"}, {"st": 75, "ed": 77, "text": "cost sensitive"}, {"st": 86, "ed": 88, "text": "near optimal"}, {"st": 105, "ed": 107, "text": "theoretical results"}, {"st": 113, "ed": 115, "text": "greedy algorithms"}, {"st": 117, "ed": 119, "text": "active learning"}]
[{"st": 9, "ed": 11, "text": "social science"}, {"st": 59, "ed": 61, "text": "cluster ensemble"}, {"st": 68, "ed": 70, "text": "cluster ensemble"}, {"st": 75, "ed": 77, "text": "cluster ensemble"}, {"st": 166, "ed": 168, "text": "decision making"}]
[{"st": 2, "ed": 4, "text": "least squares"}, {"st": 11, "ed": 13, "text": "machine learning"}, {"st": 30, "ed": 32, "text": "linear model"}, {"st": 34, "ed": 36, "text": "closed form"}, {"st": 88, "ed": 90, "text": "sensitivity analysis"}, {"st": 91, "ed": 93, "text": "differential privacy"}, {"st": 172, "ed": 174, "text": "post processing"}, {"st": 186, "ed": 188, "text": "differential privacy"}, {"st": 194, "ed": 196, "text": "differential privacy"}, {"st": 220, "ed": 222, "text": "differentially private"}]
[{"st": 0, "ed": 2, "text": "probabilistic modeling"}, {"st": 34, "ed": 36, "text": "predictive accuracy"}, {"st": 38, "ed": 40, "text": "predictive accuracy"}, {"st": 96, "ed": 98, "text": "real data"}]
[{"st": 0, "ed": 4, "text": "markov chain monte carlo"}, {"st": 6, "ed": 8, "text": "belief propagation"}, {"st": 28, "ed": 30, "text": "probabilistic method"}, {"st": 71, "ed": 73, "text": "approximation error"}, {"st": 142, "ed": 144, "text": "pair wise"}, {"st": 151, "ed": 153, "text": "highly accurate"}, {"st": 175, "ed": 177, "text": "pair wise"}, {"st": 180, "ed": 182, "text": "main idea"}, {"st": 196, "ed": 198, "text": "related problems"}, {"st": 261, "ed": 263, "text": "non trivial"}]
[{"st": 0, "ed": 2, "text": "reinforcement learning"}, {"st": 56, "ed": 58, "text": "policy gradient"}, {"st": 66, "ed": 68, "text": "significantly improved"}, {"st": 82, "ed": 84, "text": "mutual information"}]
[{"st": 5, "ed": 8, "text": "0 p 1"}, {"st": 15, "ed": 17, "text": "nuclear norm"}, {"st": 31, "ed": 33, "text": "norm minimization"}, {"st": 35, "ed": 39, "text": "singular value decomposition svd"}, {"st": 55, "ed": 57, "text": "large scale"}, {"st": 104, "ed": 106, "text": "efficient algorithms"}, {"st": 129, "ed": 131, "text": "matrix completion"}, {"st": 136, "ed": 138, "text": "global convergence"}, {"st": 139, "ed": 141, "text": "performance guarantees"}, {"st": 147, "ed": 149, "text": "convergence properties"}, {"st": 155, "ed": 159, "text": "synthetic and real world"}, {"st": 176, "ed": 179, "text": "orders of magnitude"}]
[{"st": 3, "ed": 5, "text": "compact representations"}, {"st": 28, "ed": 30, "text": "powerful tool"}, {"st": 60, "ed": 62, "text": "multi step"}, {"st": 95, "ed": 97, "text": "exact bayesian"}, {"st": 109, "ed": 112, "text": "loopy belief propagation"}, {"st": 113, "ed": 115, "text": "approximate inference"}, {"st": 125, "ed": 127, "text": "scales linearly"}, {"st": 134, "ed": 137, "text": "static and dynamic"}, {"st": 184, "ed": 187, "text": "loopy belief propagation"}, {"st": 188, "ed": 190, "text": "exact inference"}, {"st": 193, "ed": 196, "text": "static and dynamic"}, {"st": 201, "ed": 203, "text": "approximate inference"}]
[{"st": 55, "ed": 57, "text": "existing results"}, {"st": 61, "ed": 64, "text": "taking into account"}, {"st": 72, "ed": 74, "text": "artificial intelligence"}]
[{"st": 1, "ed": 3, "text": "recent years"}, {"st": 4, "ed": 6, "text": "recommendation systems"}, {"st": 63, "ed": 66, "text": "cold start problem"}, {"st": 106, "ed": 108, "text": "optimization problem"}, {"st": 149, "ed": 151, "text": "low quality"}, {"st": 192, "ed": 194, "text": "optimization problem"}, {"st": 246, "ed": 248, "text": "computationally efficient"}, {"st": 256, "ed": 258, "text": "value iteration"}]
[{"st": 0, "ed": 2, "text": "unstructured data"}, {"st": 11, "ed": 13, "text": "data model"}, {"st": 19, "ed": 21, "text": "pre defined"}, {"st": 24, "ed": 26, "text": "unstructured data"}, {"st": 28, "ed": 30, "text": "text data"}, {"st": 46, "ed": 48, "text": "unstructured data"}, {"st": 110, "ed": 112, "text": "service providers"}, {"st": 128, "ed": 130, "text": "text mining"}]
[{"st": 1, "ed": 4, "text": "electronic health record"}, {"st": 26, "ed": 28, "text": "survival analysis"}, {"st": 37, "ed": 39, "text": "survival analysis"}, {"st": 49, "ed": 51, "text": "previous approaches"}, {"st": 67, "ed": 69, "text": "latent structure"}, {"st": 97, "ed": 100, "text": "continuous and discrete"}, {"st": 110, "ed": 112, "text": "survival analysis"}, {"st": 121, "ed": 124, "text": "coronary heart disease"}, {"st": 153, "ed": 155, "text": "survival analysis"}]
[{"st": 16, "ed": 18, "text": "existing methods"}, {"st": 23, "ed": 25, "text": "time series"}, {"st": 94, "ed": 96, "text": "cause effect"}, {"st": 107, "ed": 109, "text": "machine learning"}, {"st": 158, "ed": 160, "text": "highly accurate"}, {"st": 185, "ed": 187, "text": "bi directional"}]
[{"st": 0, "ed": 2, "text": "quantitative analysis"}, {"st": 9, "ed": 11, "text": "descriptive statistics"}, {"st": 55, "ed": 57, "text": "answer questions"}]
[{"st": 47, "ed": 49, "text": "assisted living"}, {"st": 99, "ed": 101, "text": "domain experts"}, {"st": 155, "ed": 157, "text": "case study"}, {"st": 158, "ed": 160, "text": "real life"}, {"st": 179, "ed": 181, "text": "automatically generated"}]
[{"st": 4, "ed": 6, "text": "dark energy"}, {"st": 29, "ed": 31, "text": "specifically designed"}, {"st": 98, "ed": 100, "text": "conditional generative"}, {"st": 113, "ed": 115, "text": "variational autoencoder"}, {"st": 124, "ed": 126, "text": "conditional generative"}]
[{"st": 29, "ed": 32, "text": "pga european tour"}, {"st": 82, "ed": 84, "text": "machine learning"}, {"st": 98, "ed": 100, "text": "machine learning"}, {"st": 106, "ed": 108, "text": "feature selection"}, {"st": 113, "ed": 115, "text": "feature selection"}]
[{"st": 106, "ed": 108, "text": "pattern mining"}, {"st": 110, "ed": 112, "text": "constraint satisfaction"}, {"st": 128, "ed": 130, "text": "pattern mining"}, {"st": 133, "ed": 135, "text": "proposed algorithm"}, {"st": 161, "ed": 163, "text": "empirically evaluate"}]
[{"st": 18, "ed": 20, "text": "fluid mechanics"}, {"st": 22, "ed": 24, "text": "plasma physics"}, {"st": 67, "ed": 70, "text": "scale to large"}, {"st": 72, "ed": 74, "text": "normalizing flows"}, {"st": 74, "ed": 76, "text": "kernel methods"}, {"st": 77, "ed": 79, "text": "variational approximations"}, {"st": 102, "ed": 104, "text": "differential geometry"}, {"st": 118, "ed": 120, "text": "normalizing flows"}, {"st": 149, "ed": 151, "text": "n sphere"}]
[{"st": 7, "ed": 9, "text": "causal direction"}, {"st": 11, "ed": 14, "text": "discrete random variables"}, {"st": 17, "ed": 19, "text": "unlike previous"}, {"st": 62, "ed": 64, "text": "main result"}, {"st": 132, "ed": 134, "text": "marginal distributions"}, {"st": 145, "ed": 147, "text": "greedy algorithm"}, {"st": 165, "ed": 167, "text": "greedy algorithm"}, {"st": 176, "ed": 178, "text": "shannon entropy"}, {"st": 183, "ed": 185, "text": "causal inference"}, {"st": 187, "ed": 189, "text": "similar performance"}, {"st": 195, "ed": 197, "text": "additive noise"}, {"st": 216, "ed": 218, "text": "random variables"}, {"st": 229, "ed": 231, "text": "causal inference"}, {"st": 239, "ed": 241, "text": "additive noise"}]
[{"st": 9, "ed": 11, "text": "machine learning"}, {"st": 12, "ed": 14, "text": "artificial intelligence"}, {"st": 23, "ed": 26, "text": "first order logic"}, {"st": 36, "ed": 39, "text": "first order logic"}, {"st": 59, "ed": 63, "text": "markov chain monte carlo"}, {"st": 114, "ed": 116, "text": "gibbs sampling"}, {"st": 156, "ed": 158, "text": "probabilistic inference"}]
[{"st": 65, "ed": 67, "text": "survival analysis"}, {"st": 71, "ed": 73, "text": "powerful tools"}, {"st": 94, "ed": 96, "text": "survival analysis"}, {"st": 170, "ed": 172, "text": "limited data"}, {"st": 188, "ed": 190, "text": "real world"}]
[{"st": 2, "ed": 4, "text": "social networking"}, {"st": 109, "ed": 111, "text": "machine learning"}, {"st": 119, "ed": 121, "text": "decision making"}, {"st": 146, "ed": 148, "text": "reinforcement learning"}, {"st": 155, "ed": 157, "text": "highly successful"}]
[{"st": 9, "ed": 12, "text": "component analysis pca"}, {"st": 32, "ed": 34, "text": "robust pca"}, {"st": 35, "ed": 37, "text": "prior information"}, {"st": 47, "ed": 49, "text": "low rank"}, {"st": 70, "ed": 72, "text": "proposed method"}, {"st": 85, "ed": 87, "text": "prior information"}, {"st": 99, "ed": 101, "text": "prior information"}, {"st": 108, "ed": 110, "text": "prior information"}, {"st": 114, "ed": 116, "text": "ell 1"}, {"st": 126, "ed": 129, "text": "singular value decomposition"}, {"st": 135, "ed": 137, "text": "low rank"}, {"st": 141, "ed": 143, "text": "theoretical bounds"}, {"st": 160, "ed": 162, "text": "low rank"}, {"st": 164, "ed": 166, "text": "numerical experiments"}, {"st": 182, "ed": 184, "text": "proposed algorithm"}, {"st": 199, "ed": 201, "text": "proposed method"}]
[{"st": 78, "ed": 80, "text": "social networks"}, {"st": 86, "ed": 88, "text": "distance metric"}, {"st": 91, "ed": 94, "text": "dynamic time warping"}, {"st": 105, "ed": 108, "text": "k nearest neighbors"}, {"st": 108, "ed": 110, "text": "k nn"}, {"st": 123, "ed": 126, "text": "directed acyclic graph"}, {"st": 151, "ed": 153, "text": "k nn"}, {"st": 155, "ed": 157, "text": "real world"}, {"st": 164, "ed": 166, "text": "social network"}]
[{"st": 1, "ed": 3, "text": "frame problem"}, {"st": 16, "ed": 20, "text": "stanford encyclopedia of philosophy"}, {"st": 69, "ed": 71, "text": "frame problem"}, {"st": 79, "ed": 81, "text": "causal model"}, {"st": 88, "ed": 90, "text": "bayes net"}, {"st": 150, "ed": 152, "text": "framework called"}, {"st": 182, "ed": 184, "text": "proposed framework"}]
[{"st": 11, "ed": 14, "text": "discrete random variables"}, {"st": 18, "ed": 20, "text": "recently proposed"}, {"st": 22, "ed": 24, "text": "framework called"}, {"st": 63, "ed": 65, "text": "marginal distributions"}, {"st": 67, "ed": 70, "text": "discrete random variables"}, {"st": 76, "ed": 78, "text": "joint distribution"}, {"st": 98, "ed": 100, "text": "convex polytope"}, {"st": 112, "ed": 114, "text": "recently shown"}, {"st": 121, "ed": 123, "text": "np hard"}, {"st": 133, "ed": 135, "text": "joint distributions"}, {"st": 156, "ed": 158, "text": "greedy algorithm"}, {"st": 161, "ed": 163, "text": "approximate solution"}, {"st": 183, "ed": 185, "text": "local minimum"}, {"st": 191, "ed": 193, "text": "approximation error"}]
[{"st": 44, "ed": 46, "text": "clustering techniques"}, {"st": 48, "ed": 50, "text": "correspondence analysis"}, {"st": 75, "ed": 77, "text": "free software"}]
[{"st": 33, "ed": 35, "text": "pattern mining"}, {"st": 103, "ed": 105, "text": "recent advances"}, {"st": 113, "ed": 116, "text": "learning to rank"}, {"st": 132, "ed": 134, "text": "sampling distribution"}, {"st": 146, "ed": 148, "text": "proposed algorithm"}, {"st": 156, "ed": 158, "text": "pattern mining"}]
[{"st": 48, "ed": 51, "text": "k nearest neighbor"}, {"st": 51, "ed": 53, "text": "k nn"}, {"st": 84, "ed": 86, "text": "k nn"}, {"st": 113, "ed": 116, "text": "bias and variance"}, {"st": 137, "ed": 140, "text": "o n 2"}, {"st": 195, "ed": 197, "text": "computationally tractable"}]
[{"st": 3, "ed": 5, "text": "machine learning"}, {"st": 6, "ed": 8, "text": "data mining"}, {"st": 29, "ed": 31, "text": "clustering techniques"}, {"st": 164, "ed": 166, "text": "cross domain"}]
[{"st": 7, "ed": 9, "text": "causal graph"}, {"st": 21, "ed": 23, "text": "causal graph"}, {"st": 33, "ed": 35, "text": "causal graph"}, {"st": 49, "ed": 51, "text": "causal graph"}, {"st": 104, "ed": 106, "text": "greedy algorithm"}, {"st": 112, "ed": 114, "text": "causal graph"}]
[{"st": 3, "ed": 5, "text": "large scale"}, {"st": 119, "ed": 121, "text": "cold start"}, {"st": 136, "ed": 138, "text": "iterative algorithm"}, {"st": 152, "ed": 155, "text": "guaranteed to converge"}, {"st": 158, "ed": 161, "text": "number of iterations"}, {"st": 178, "ed": 180, "text": "significantly outperforms"}, {"st": 181, "ed": 183, "text": "existing algorithms"}]
[{"st": 4, "ed": 6, "text": "observational data"}, {"st": 15, "ed": 17, "text": "instrumental variable"}, {"st": 20, "ed": 22, "text": "causal discovery"}, {"st": 31, "ed": 33, "text": "closed form"}, {"st": 142, "ed": 144, "text": "posterior distribution"}, {"st": 178, "ed": 180, "text": "sampling algorithm"}]
[{"st": 25, "ed": 27, "text": "random variables"}, {"st": 71, "ed": 73, "text": "random variable"}, {"st": 105, "ed": 107, "text": "compact representation"}, {"st": 161, "ed": 163, "text": "random variables"}, {"st": 194, "ed": 196, "text": "problems including"}, {"st": 253, "ed": 255, "text": "neural networks"}, {"st": 257, "ed": 259, "text": "real world"}]
[{"st": 6, "ed": 8, "text": "machine learning"}, {"st": 22, "ed": 24, "text": "prediction models"}, {"st": 104, "ed": 106, "text": "transfer learning"}, {"st": 114, "ed": 116, "text": "large datasets"}]
[{"st": 8, "ed": 10, "text": "unstructured data"}, {"st": 37, "ed": 39, "text": "recommender systems"}, {"st": 87, "ed": 89, "text": "learning strategy"}, {"st": 94, "ed": 96, "text": "reinforcement learning"}, {"st": 103, "ed": 105, "text": "human perception"}, {"st": 120, "ed": 122, "text": "method yields"}]
[{"st": 46, "ed": 48, "text": "historical data"}, {"st": 72, "ed": 74, "text": "learning process"}, {"st": 154, "ed": 156, "text": "evaluation shows"}]
[{"st": 3, "ed": 5, "text": "machine learning"}, {"st": 94, "ed": 96, "text": "artificial intelligence"}]
[{"st": 21, "ed": 23, "text": "key insight"}, {"st": 97, "ed": 99, "text": "empirically evaluated"}, {"st": 100, "ed": 102, "text": "synthetic data"}]
[{"st": 16, "ed": 18, "text": "causal discovery"}, {"st": 22, "ed": 25, "text": "received much attention"}, {"st": 37, "ed": 39, "text": "causal model"}, {"st": 71, "ed": 73, "text": "causal model"}, {"st": 79, "ed": 81, "text": "causal model"}, {"st": 104, "ed": 106, "text": "higher order"}, {"st": 130, "ed": 132, "text": "factor analysis"}, {"st": 146, "ed": 149, "text": "independent component analysis"}]
[{"st": 110, "ed": 112, "text": "non trivial"}, {"st": 141, "ed": 144, "text": "false positive rate"}]
[{"st": 14, "ed": 16, "text": "probabilistic program"}, {"st": 28, "ed": 30, "text": "probabilistic program"}, {"st": 34, "ed": 36, "text": "graphical model"}, {"st": 65, "ed": 67, "text": "source code"}, {"st": 81, "ed": 83, "text": "constraint satisfaction"}, {"st": 85, "ed": 87, "text": "performance improvements"}, {"st": 101, "ed": 103, "text": "tasks including"}]
[{"st": 137, "ed": 140, "text": "taking into account"}]
[{"st": 118, "ed": 121, "text": "directions for future"}]
[{"st": 7, "ed": 9, "text": "machine learning"}, {"st": 26, "ed": 28, "text": "user experience"}, {"st": 147, "ed": 149, "text": "non stationary"}]
[{"st": 7, "ed": 9, "text": "united states"}, {"st": 12, "ed": 14, "text": "colorectal cancer"}, {"st": 22, "ed": 25, "text": "american cancer society"}, {"st": 77, "ed": 79, "text": "false negatives"}, {"st": 79, "ed": 81, "text": "colon cancer"}, {"st": 101, "ed": 103, "text": "colorectal cancer"}, {"st": 110, "ed": 112, "text": "anal fissure"}, {"st": 117, "ed": 119, "text": "ulcerative colitis"}, {"st": 121, "ed": 123, "text": "rectal prolapse"}, {"st": 123, "ed": 125, "text": "ischemic colitis"}, {"st": 184, "ed": 186, "text": "ensemble based"}, {"st": 186, "ed": 188, "text": "classification algorithm"}, {"st": 211, "ed": 214, "text": "fold cross validation"}]
[{"st": 1, "ed": 3, "text": "recent years"}, {"st": 115, "ed": 117, "text": "autonomous driving"}, {"st": 131, "ed": 133, "text": "mathematical model"}]
[{"st": 70, "ed": 72, "text": "monte carlo"}, {"st": 77, "ed": 80, "text": "high computational cost"}, {"st": 93, "ed": 95, "text": "deep learning"}, {"st": 106, "ed": 109, "text": "deep neural network"}, {"st": 129, "ed": 132, "text": "end to end"}, {"st": 150, "ed": 152, "text": "proposed approach"}, {"st": 175, "ed": 177, "text": "numerical results"}, {"st": 182, "ed": 184, "text": "proposed approach"}, {"st": 194, "ed": 196, "text": "extremely high"}]
[{"st": 5, "ed": 7, "text": "gaussian processes"}, {"st": 10, "ed": 12, "text": "spatio temporal"}, {"st": 30, "ed": 32, "text": "vector calculus"}, {"st": 48, "ed": 51, "text": "kullback leibler divergence"}, {"st": 60, "ed": 62, "text": "spatio temporal"}, {"st": 89, "ed": 91, "text": "key feature"}, {"st": 100, "ed": 102, "text": "gaussian process"}, {"st": 115, "ed": 117, "text": "vector calculus"}, {"st": 124, "ed": 126, "text": "synthetic data"}]
[{"st": 26, "ed": 28, "text": "anti virus"}, {"st": 44, "ed": 46, "text": "anti virus"}, {"st": 85, "ed": 87, "text": "community detection"}, {"st": 107, "ed": 109, "text": "anti virus"}]
[{"st": 0, "ed": 2, "text": "reinforcement learning"}, {"st": 105, "ed": 107, "text": "virtual environment"}, {"st": 116, "ed": 119, "text": "deep reinforcement learning"}, {"st": 202, "ed": 204, "text": "user preferences"}]
[{"st": 109, "ed": 111, "text": "extracted features"}, {"st": 125, "ed": 128, "text": "support vector machines"}]
[{"st": 8, "ed": 10, "text": "reinforcement learning"}, {"st": 16, "ed": 18, "text": "recent results"}, {"st": 20, "ed": 22, "text": "reinforcement learning"}, {"st": 40, "ed": 42, "text": "learning process"}, {"st": 50, "ed": 52, "text": "wide variety"}, {"st": 100, "ed": 102, "text": "reinforcement learning"}, {"st": 115, "ed": 117, "text": "machine learning"}]
[{"st": 24, "ed": 26, "text": "common sense"}, {"st": 130, "ed": 132, "text": "training data"}, {"st": 133, "ed": 135, "text": "real world"}, {"st": 141, "ed": 143, "text": "time consuming"}, {"st": 153, "ed": 155, "text": "predictive model"}, {"st": 224, "ed": 226, "text": "significantly reduced"}, {"st": 254, "ed": 256, "text": "predictive model"}, {"st": 259, "ed": 261, "text": "objective function"}]
[{"st": 14, "ed": 16, "text": "frac 1"}, {"st": 24, "ed": 26, "text": "frac 1"}, {"st": 43, "ed": 45, "text": "statistical learning"}, {"st": 46, "ed": 48, "text": "machine learning"}, {"st": 54, "ed": 56, "text": "reinforcement learning"}, {"st": 64, "ed": 67, "text": "stochastic gradient descent"}, {"st": 67, "ed": 69, "text": "based optimization"}, {"st": 73, "ed": 75, "text": "computationally expensive"}, {"st": 84, "ed": 86, "text": "frac 1"}, {"st": 121, "ed": 123, "text": "based methods"}, {"st": 140, "ed": 143, "text": "linear convergence rate"}, {"st": 165, "ed": 167, "text": "objective function"}]
[{"st": 100, "ed": 102, "text": "artificially intelligent"}, {"st": 129, "ed": 131, "text": "machine learning"}]
[{"st": 29, "ed": 31, "text": "hidden states"}, {"st": 51, "ed": 53, "text": "hidden states"}, {"st": 71, "ed": 73, "text": "latent state"}, {"st": 95, "ed": 98, "text": "hidden markov models"}, {"st": 128, "ed": 130, "text": "modified version"}]
[{"st": 1, "ed": 3, "text": "article presents"}, {"st": 6, "ed": 8, "text": "answer set"}, {"st": 21, "ed": 23, "text": "programming paradigm"}, {"st": 29, "ed": 31, "text": "optimization problem"}, {"st": 47, "ed": 49, "text": "pattern mining"}, {"st": 50, "ed": 52, "text": "background knowledge"}, {"st": 56, "ed": 58, "text": "data mining"}, {"st": 70, "ed": 72, "text": "pattern mining"}, {"st": 96, "ed": 98, "text": "computational performance"}, {"st": 137, "ed": 139, "text": "constraint programming"}, {"st": 142, "ed": 144, "text": "declarative programming"}]
[{"st": 66, "ed": 68, "text": "classification techniques"}, {"st": 94, "ed": 96, "text": "machine learning"}, {"st": 133, "ed": 135, "text": "majority voting"}, {"st": 138, "ed": 140, "text": "classification performance"}, {"st": 174, "ed": 177, "text": "graphical user interface"}]
[{"st": 85, "ed": 87, "text": "multi channel"}, {"st": 124, "ed": 126, "text": "multi channel"}, {"st": 148, "ed": 150, "text": "analysis shows"}, {"st": 152, "ed": 154, "text": "total number"}]
[{"st": 33, "ed": 36, "text": "black box optimization"}, {"st": 98, "ed": 100, "text": "practical applications"}, {"st": 108, "ed": 110, "text": "optimization algorithm"}, {"st": 125, "ed": 127, "text": "small sample"}]
[{"st": 0, "ed": 2, "text": "network embedding"}, {"st": 6, "ed": 8, "text": "low dimensional"}, {"st": 25, "ed": 27, "text": "network embedding"}, {"st": 48, "ed": 50, "text": "scale free"}, {"st": 54, "ed": 56, "text": "scale free"}, {"st": 65, "ed": 67, "text": "heavy tailed"}, {"st": 82, "ed": 84, "text": "real world"}, {"st": 97, "ed": 99, "text": "learning representations"}, {"st": 100, "ed": 102, "text": "scale free"}, {"st": 105, "ed": 107, "text": "theoretically analyze"}, {"st": 114, "ed": 117, "text": "scale free network"}, {"st": 139, "ed": 141, "text": "scale free"}, {"st": 143, "ed": 145, "text": "network embedding"}, {"st": 167, "ed": 169, "text": "skip gram"}, {"st": 171, "ed": 173, "text": "extensive experiments"}, {"st": 186, "ed": 188, "text": "heavy tailed"}, {"st": 198, "ed": 200, "text": "embedding models"}]
[{"st": 0, "ed": 4, "text": "recurrent neural networks rnns"}, {"st": 38, "ed": 40, "text": "big data"}, {"st": 62, "ed": 64, "text": "recurrent units"}, {"st": 78, "ed": 80, "text": "recurrent units"}, {"st": 81, "ed": 84, "text": "short term memory"}, {"st": 87, "ed": 89, "text": "gated recurrent"}, {"st": 105, "ed": 109, "text": "convolutional neural networks cnns"}, {"st": 117, "ed": 119, "text": "initialization methods"}, {"st": 139, "ed": 141, "text": "gated recurrent"}, {"st": 153, "ed": 155, "text": "lstm networks"}, {"st": 166, "ed": 168, "text": "lstm architecture"}, {"st": 178, "ed": 180, "text": "false alarms"}]
[{"st": 79, "ed": 81, "text": "machine learning"}, {"st": 81, "ed": 83, "text": "based method"}, {"st": 114, "ed": 116, "text": "spectral clustering"}]
[{"st": 140, "ed": 142, "text": "computationally feasible"}, {"st": 145, "ed": 147, "text": "variational inference"}, {"st": 150, "ed": 152, "text": "posterior distribution"}, {"st": 155, "ed": 158, "text": "stochastic gradient descent"}]
[{"st": 8, "ed": 10, "text": "higher order"}, {"st": 40, "ed": 42, "text": "higher order"}, {"st": 50, "ed": 52, "text": "embedding methods"}, {"st": 57, "ed": 59, "text": "higher order"}, {"st": 76, "ed": 78, "text": "wide variety"}]
[{"st": 24, "ed": 26, "text": "inference tasks"}, {"st": 27, "ed": 29, "text": "achieves higher"}, {"st": 40, "ed": 42, "text": "optimization techniques"}, {"st": 55, "ed": 57, "text": "signal processing"}, {"st": 58, "ed": 60, "text": "machine learning"}, {"st": 83, "ed": 85, "text": "inference algorithms"}, {"st": 90, "ed": 92, "text": "application areas"}]
[{"st": 12, "ed": 14, "text": "decision making"}, {"st": 94, "ed": 96, "text": "decision making"}, {"st": 121, "ed": 123, "text": "rational basis"}, {"st": 126, "ed": 128, "text": "empirical evidence"}, {"st": 159, "ed": 161, "text": "decision making"}, {"st": 180, "ed": 182, "text": "decision making"}]
[{"st": 80, "ed": 82, "text": "message passing"}, {"st": 119, "ed": 121, "text": "previously proposed"}, {"st": 129, "ed": 131, "text": "worst case"}]
[{"st": 0, "ed": 2, "text": "random walks"}, {"st": 9, "ed": 11, "text": "network embedding"}, {"st": 24, "ed": 26, "text": "random walks"}, {"st": 64, "ed": 66, "text": "random walks"}, {"st": 73, "ed": 75, "text": "existing methods"}, {"st": 87, "ed": 89, "text": "proposed framework"}, {"st": 132, "ed": 134, "text": "proposed framework"}, {"st": 152, "ed": 154, "text": "existing methods"}]
[{"st": 20, "ed": 22, "text": "machine learning"}, {"st": 30, "ed": 32, "text": "previous approaches"}, {"st": 34, "ed": 36, "text": "multi label"}, {"st": 59, "ed": 61, "text": "gradient boosting"}, {"st": 82, "ed": 84, "text": "negative examples"}, {"st": 114, "ed": 116, "text": "significantly outperforms"}, {"st": 117, "ed": 120, "text": "k nearest neighbors"}]
[{"st": 16, "ed": 18, "text": "client side"}, {"st": 48, "ed": 50, "text": "client side"}, {"st": 51, "ed": 53, "text": "machine learning"}, {"st": 77, "ed": 79, "text": "computational efficiency"}]
[{"st": 7, "ed": 9, "text": "machine learning"}, {"st": 38, "ed": 40, "text": "predictive modeling"}, {"st": 62, "ed": 65, "text": "end to end"}, {"st": 74, "ed": 76, "text": "supervised learning"}, {"st": 88, "ed": 90, "text": "predictive models"}, {"st": 98, "ed": 100, "text": "feature engineering"}, {"st": 109, "ed": 111, "text": "input features"}, {"st": 121, "ed": 123, "text": "machine learning"}, {"st": 131, "ed": 133, "text": "predictive models"}, {"st": 180, "ed": 182, "text": "supervised learning"}, {"st": 224, "ed": 226, "text": "machine learning"}, {"st": 232, "ed": 234, "text": "predictive models"}]
[{"st": 56, "ed": 58, "text": "significant progress"}, {"st": 68, "ed": 70, "text": "current approaches"}, {"st": 82, "ed": 84, "text": "real world"}, {"st": 91, "ed": 93, "text": "significantly higher"}, {"st": 151, "ed": 153, "text": "fully connected"}, {"st": 153, "ed": 155, "text": "neural network"}]
[{"st": 62, "ed": 64, "text": "reinforcement learning"}, {"st": 112, "ed": 114, "text": "multi agent"}, {"st": 152, "ed": 155, "text": "multi agent systems"}, {"st": 159, "ed": 161, "text": "expert knowledge"}, {"st": 174, "ed": 177, "text": "multi agent systems"}]
[{"st": 53, "ed": 55, "text": "poorly understood"}, {"st": 140, "ed": 142, "text": "machine learning"}, {"st": 144, "ed": 146, "text": "artificial intelligence"}, {"st": 171, "ed": 173, "text": "artificial intelligence"}]
[{"st": 15, "ed": 17, "text": "pattern recognition"}, {"st": 27, "ed": 30, "text": "point of view"}, {"st": 35, "ed": 37, "text": "pattern recognition"}, {"st": 48, "ed": 50, "text": "pattern recognition"}, {"st": 103, "ed": 105, "text": "optimization problem"}, {"st": 139, "ed": 141, "text": "pattern recognition"}]
[{"st": 44, "ed": 46, "text": "exponential function"}]
[{"st": 38, "ed": 40, "text": "computer vision"}]
[{"st": 63, "ed": 65, "text": "probabilistic model"}]
[{"st": 34, "ed": 37, "text": "hidden markov models"}]
[{"st": 1, "ed": 4, "text": "minimum description length"}, {"st": 71, "ed": 73, "text": "low precision"}, {"st": 212, "ed": 215, "text": "degrees of freedom"}, {"st": 219, "ed": 223, "text": "feed forward neural network"}, {"st": 234, "ed": 236, "text": "hidden layer"}, {"st": 260, "ed": 262, "text": "hidden layer"}]
[{"st": 5, "ed": 7, "text": "open problem"}, {"st": 10, "ed": 12, "text": "submodular functions"}, {"st": 20, "ed": 22, "text": "submodular functions"}, {"st": 39, "ed": 41, "text": "computer science"}, {"st": 42, "ed": 44, "text": "computer vision"}, {"st": 44, "ed": 46, "text": "artificial intelligence"}, {"st": 55, "ed": 57, "text": "expressive power"}, {"st": 81, "ed": 83, "text": "submodular functions"}, {"st": 100, "ed": 102, "text": "submodular functions"}, {"st": 111, "ed": 113, "text": "submodular functions"}]
[{"st": 23, "ed": 25, "text": "face recognition"}, {"st": 26, "ed": 28, "text": "natural language"}, {"st": 86, "ed": 88, "text": "social web"}]
[{"st": 8, "ed": 10, "text": "challenging task"}]
[{"st": 0, "ed": 2, "text": "gesture recognition"}, {"st": 13, "ed": 15, "text": "main goal"}, {"st": 16, "ed": 18, "text": "gesture recognition"}, {"st": 64, "ed": 67, "text": "spatial and temporal"}, {"st": 80, "ed": 82, "text": "vision based"}, {"st": 158, "ed": 160, "text": "existing works"}, {"st": 214, "ed": 216, "text": "traditional methods"}, {"st": 224, "ed": 226, "text": "based methods"}, {"st": 227, "ed": 229, "text": "based methods"}, {"st": 230, "ed": 232, "text": "feature based"}, {"st": 243, "ed": 245, "text": "gesture recognition"}]
[{"st": 12, "ed": 14, "text": "image segmentation"}, {"st": 32, "ed": 34, "text": "based optimization"}, {"st": 65, "ed": 67, "text": "optimization problem"}, {"st": 135, "ed": 137, "text": "linear programming"}, {"st": 144, "ed": 146, "text": "global optimum"}, {"st": 165, "ed": 167, "text": "standard length"}]
[{"st": 2, "ed": 4, "text": "point cloud"}, {"st": 78, "ed": 80, "text": "point cloud"}, {"st": 98, "ed": 101, "text": "conditional random field"}, {"st": 127, "ed": 129, "text": "point cloud"}, {"st": 132, "ed": 134, "text": "wide variety"}]
[{"st": 46, "ed": 48, "text": "active learning"}, {"st": 53, "ed": 55, "text": "recent theoretical"}]
[{"st": 51, "ed": 53, "text": "input image"}, {"st": 57, "ed": 59, "text": "color space"}]
[{"st": 0, "ed": 2, "text": "scene understanding"}, {"st": 11, "ed": 13, "text": "depth estimation"}, {"st": 13, "ed": 15, "text": "object detection"}, {"st": 43, "ed": 45, "text": "raw image"}, {"st": 77, "ed": 79, "text": "classification models"}, {"st": 161, "ed": 163, "text": "significantly improves"}, {"st": 173, "ed": 175, "text": "scene understanding"}, {"st": 178, "ed": 180, "text": "depth estimation"}, {"st": 184, "ed": 186, "text": "object detection"}, {"st": 194, "ed": 196, "text": "improves performance"}]
[{"st": 0, "ed": 2, "text": "rgb d"}, {"st": 6, "ed": 8, "text": "rgb image"}, {"st": 13, "ed": 15, "text": "becoming increasingly"}, {"st": 34, "ed": 36, "text": "point cloud"}, {"st": 47, "ed": 49, "text": "graphical model"}, {"st": 76, "ed": 78, "text": "object classes"}, {"st": 103, "ed": 105, "text": "maximum margin"}, {"st": 143, "ed": 145, "text": "object classes"}, {"st": 165, "ed": 167, "text": "contextual information"}, {"st": 182, "ed": 184, "text": "mobile robot"}, {"st": 190, "ed": 192, "text": "object classes"}]
[{"st": 14, "ed": 16, "text": "becoming increasingly"}, {"st": 88, "ed": 90, "text": "machine learning"}, {"st": 96, "ed": 98, "text": "k means"}, {"st": 131, "ed": 133, "text": "query language"}]
[{"st": 74, "ed": 76, "text": "random noise"}]
[{"st": 18, "ed": 20, "text": "maximum likelihood"}]
[{"st": 94, "ed": 96, "text": "spike timing"}, {"st": 103, "ed": 105, "text": "self organizing"}, {"st": 116, "ed": 118, "text": "self organizing"}, {"st": 121, "ed": 123, "text": "spike timing"}, {"st": 125, "ed": 127, "text": "learning rule"}, {"st": 143, "ed": 145, "text": "case study"}, {"st": 157, "ed": 159, "text": "continuous speech"}]
[{"st": 6, "ed": 8, "text": "computer vision"}, {"st": 17, "ed": 19, "text": "computational efficiency"}, {"st": 141, "ed": 143, "text": "machine vision"}, {"st": 202, "ed": 204, "text": "machine vision"}, {"st": 207, "ed": 209, "text": "bayesian probability"}, {"st": 247, "ed": 249, "text": "machine vision"}]
[{"st": 39, "ed": 42, "text": "real and synthetic"}, {"st": 50, "ed": 52, "text": "fixed length"}, {"st": 52, "ed": 54, "text": "feature vector"}]
[{"st": 162, "ed": 164, "text": "large scale"}, {"st": 188, "ed": 190, "text": "commonsense reasoning"}]
[{"st": 50, "ed": 52, "text": "commonsense reasoning"}, {"st": 55, "ed": 57, "text": "large scale"}, {"st": 76, "ed": 78, "text": "ambient intelligence"}, {"st": 107, "ed": 109, "text": "open source"}, {"st": 116, "ed": 118, "text": "artificial intelligence"}, {"st": 131, "ed": 133, "text": "scene understanding"}, {"st": 154, "ed": 156, "text": "commonsense reasoning"}, {"st": 156, "ed": 159, "text": "spatial and temporal"}]
[{"st": 6, "ed": 9, "text": "computer aided diagnosis"}, {"st": 13, "ed": 16, "text": "detection and classification"}, {"st": 26, "ed": 28, "text": "x ray"}, {"st": 40, "ed": 42, "text": "moving average"}, {"st": 52, "ed": 54, "text": "least squares"}, {"st": 67, "ed": 69, "text": "statistical inference"}, {"st": 79, "ed": 81, "text": "k means"}, {"st": 92, "ed": 94, "text": "benign tumor"}]
[{"st": 60, "ed": 62, "text": "rgb d"}, {"st": 74, "ed": 77, "text": "markov random field"}, {"st": 112, "ed": 115, "text": "support vector machine"}]
[{"st": 98, "ed": 100, "text": "functional form"}]
[{"st": 20, "ed": 22, "text": "previously unseen"}, {"st": 29, "ed": 31, "text": "developmental psychology"}, {"st": 71, "ed": 74, "text": "end to end"}, {"st": 118, "ed": 120, "text": "based approach"}, {"st": 122, "ed": 124, "text": "simulated data"}, {"st": 152, "ed": 154, "text": "synthetic data"}]
[{"st": 0, "ed": 2, "text": "residual learning"}, {"st": 12, "ed": 15, "text": "deep neural networks"}, {"st": 22, "ed": 24, "text": "residual networks"}, {"st": 38, "ed": 40, "text": "recognition tasks"}, {"st": 52, "ed": 54, "text": "large scale"}, {"st": 62, "ed": 64, "text": "residual learning"}, {"st": 65, "ed": 67, "text": "deep networks"}, {"st": 73, "ed": 75, "text": "related tasks"}, {"st": 104, "ed": 106, "text": "residual learning"}, {"st": 118, "ed": 120, "text": "visual concepts"}, {"st": 129, "ed": 131, "text": "residual network"}, {"st": 139, "ed": 141, "text": "achieve competitive"}, {"st": 144, "ed": 146, "text": "detection performance"}, {"st": 148, "ed": 150, "text": "visual sentiment"}, {"st": 151, "ed": 153, "text": "detection problem"}, {"st": 164, "ed": 166, "text": "residual network"}, {"st": 169, "ed": 171, "text": "detection performance"}, {"st": 178, "ed": 180, "text": "residual network"}]
[{"st": 8, "ed": 10, "text": "visual representations"}, {"st": 10, "ed": 12, "text": "current approaches"}, {"st": 13, "ed": 15, "text": "computer vision"}, {"st": 32, "ed": 34, "text": "visual representation"}, {"st": 55, "ed": 57, "text": "visual representations"}, {"st": 59, "ed": 61, "text": "vision systems"}, {"st": 66, "ed": 69, "text": "images and videos"}, {"st": 151, "ed": 153, "text": "learned representations"}, {"st": 159, "ed": 161, "text": "nearest neighbor"}, {"st": 173, "ed": 175, "text": "image classification"}]
[{"st": 0, "ed": 2, "text": "visual features"}, {"st": 52, "ed": 54, "text": "rgb image"}, {"st": 58, "ed": 60, "text": "point cloud"}, {"st": 131, "ed": 134, "text": "support vector machine"}, {"st": 153, "ed": 155, "text": "feature vector"}, {"st": 221, "ed": 223, "text": "active learning"}]
[{"st": 0, "ed": 2, "text": "medical image"}, {"st": 45, "ed": 47, "text": "multi modal"}, {"st": 47, "ed": 49, "text": "medical image"}, {"st": 87, "ed": 89, "text": "medical image"}, {"st": 93, "ed": 95, "text": "medical image"}, {"st": 103, "ed": 105, "text": "image fusion"}, {"st": 127, "ed": 129, "text": "open ended"}]
[{"st": 11, "ed": 13, "text": "computer vision"}, {"st": 18, "ed": 20, "text": "applications including"}, {"st": 108, "ed": 110, "text": "temporal information"}, {"st": 200, "ed": 202, "text": "temporal coherence"}]
[{"st": 6, "ed": 8, "text": "recently proposed"}, {"st": 20, "ed": 22, "text": "input space"}, {"st": 56, "ed": 58, "text": "input data"}, {"st": 103, "ed": 105, "text": "escherichia coli"}, {"st": 156, "ed": 158, "text": "amino acid"}, {"st": 173, "ed": 175, "text": "context dependent"}, {"st": 188, "ed": 190, "text": "based approach"}, {"st": 191, "ed": 193, "text": "structured data"}]
[{"st": 8, "ed": 10, "text": "multi view"}, {"st": 73, "ed": 75, "text": "large scale"}, {"st": 87, "ed": 89, "text": "image data"}, {"st": 111, "ed": 113, "text": "input data"}, {"st": 163, "ed": 165, "text": "multi scale"}, {"st": 166, "ed": 168, "text": "network design"}, {"st": 181, "ed": 183, "text": "multi scale"}, {"st": 207, "ed": 209, "text": "multi scale"}, {"st": 216, "ed": 218, "text": "multi view"}]
[{"st": 38, "ed": 41, "text": "hidden markov model"}]
[{"st": 26, "ed": 28, "text": "low dimensional"}, {"st": 56, "ed": 58, "text": "high dimensional"}, {"st": 61, "ed": 63, "text": "low dimensional"}, {"st": 99, "ed": 103, "text": "singular value decomposition svd"}, {"st": 105, "ed": 107, "text": "computational burden"}, {"st": 168, "ed": 170, "text": "low dimensional"}, {"st": 181, "ed": 183, "text": "greedy algorithm"}, {"st": 190, "ed": 192, "text": "significantly faster"}, {"st": 234, "ed": 236, "text": "numerical results"}]
[{"st": 43, "ed": 46, "text": "difficult to train"}, {"st": 52, "ed": 54, "text": "training data"}, {"st": 68, "ed": 70, "text": "training examples"}, {"st": 87, "ed": 89, "text": "training procedure"}, {"st": 103, "ed": 105, "text": "conditional distribution"}, {"st": 121, "ed": 123, "text": "training data"}, {"st": 137, "ed": 139, "text": "real data"}, {"st": 155, "ed": 158, "text": "quantitatively and qualitatively"}]
[{"st": 0, "ed": 4, "text": "multiple instance learning mil"}, {"st": 8, "ed": 10, "text": "weakly supervised"}, {"st": 12, "ed": 14, "text": "training instances"}, {"st": 44, "ed": 46, "text": "weakly labeled"}, {"st": 58, "ed": 60, "text": "computer vision"}, {"st": 150, "ed": 152, "text": "instance labels"}, {"st": 178, "ed": 180, "text": "application areas"}]
[{"st": 8, "ed": 10, "text": "applied mathematics"}, {"st": 39, "ed": 41, "text": "artificial intelligence"}, {"st": 46, "ed": 48, "text": "machine vision"}, {"st": 75, "ed": 77, "text": "machine vision"}, {"st": 88, "ed": 91, "text": "high performance computing"}, {"st": 120, "ed": 123, "text": "mean squared error"}, {"st": 143, "ed": 145, "text": "machine vision"}, {"st": 156, "ed": 158, "text": "package manager"}]
[{"st": 14, "ed": 16, "text": "shannon entropy"}, {"st": 49, "ed": 51, "text": "feature vector"}, {"st": 62, "ed": 64, "text": "shannon entropy"}, {"st": 65, "ed": 67, "text": "pattern classification"}]
[{"st": 6, "ed": 8, "text": "image data"}, {"st": 9, "ed": 11, "text": "feature descriptors"}, {"st": 34, "ed": 36, "text": "hash table"}, {"st": 71, "ed": 74, "text": "k nearest neighbor"}, {"st": 87, "ed": 89, "text": "theoretical analysis"}, {"st": 103, "ed": 105, "text": "empirical results"}]
[{"st": 107, "ed": 109, "text": "visual information"}, {"st": 136, "ed": 138, "text": "cognitive neuroscience"}]
[{"st": 6, "ed": 8, "text": "object recognition"}, {"st": 30, "ed": 32, "text": "preliminary experiments"}]
[{"st": 22, "ed": 24, "text": "salient features"}, {"st": 69, "ed": 71, "text": "linear models"}, {"st": 76, "ed": 78, "text": "higher order"}, {"st": 83, "ed": 85, "text": "humanoid robot"}, {"st": 102, "ed": 104, "text": "classification performance"}]
[{"st": 3, "ed": 6, "text": "a posteriori map"}, {"st": 10, "ed": 14, "text": "markov random fields mrfs"}, {"st": 22, "ed": 24, "text": "tree based"}, {"st": 32, "ed": 34, "text": "method named"}, {"st": 37, "ed": 39, "text": "local search"}, {"st": 41, "ed": 44, "text": "takes advantage of"}, {"st": 55, "ed": 57, "text": "local search"}, {"st": 65, "ed": 67, "text": "exponentially large"}, {"st": 72, "ed": 74, "text": "limited memory"}, {"st": 90, "ed": 92, "text": "ising model"}, {"st": 94, "ed": 96, "text": "real world"}, {"st": 98, "ed": 100, "text": "computer vision"}, {"st": 120, "ed": 122, "text": "significant computational"}]
[{"st": 35, "ed": 37, "text": "turing test"}, {"st": 63, "ed": 66, "text": "optical character recognition"}, {"st": 144, "ed": 146, "text": "image recognition"}, {"st": 168, "ed": 170, "text": "natural language"}, {"st": 171, "ed": 173, "text": "multiple objects"}]
[{"st": 9, "ed": 11, "text": "real world"}, {"st": 11, "ed": 13, "text": "decision making"}, {"st": 39, "ed": 41, "text": "decision making"}, {"st": 57, "ed": 59, "text": "scene understanding"}, {"st": 63, "ed": 65, "text": "computer vision"}, {"st": 71, "ed": 73, "text": "conventional methods"}, {"st": 93, "ed": 95, "text": "binary classification"}, {"st": 136, "ed": 139, "text": "qualitative and quantitative"}, {"st": 153, "ed": 155, "text": "proposed method"}]
[{"st": 18, "ed": 20, "text": "proposed approach"}]
[{"st": 13, "ed": 15, "text": "object detection"}, {"st": 54, "ed": 56, "text": "recent progress"}, {"st": 58, "ed": 60, "text": "object detection"}, {"st": 66, "ed": 68, "text": "closely related"}, {"st": 104, "ed": 106, "text": "evaluation metrics"}, {"st": 113, "ed": 115, "text": "open problems"}, {"st": 117, "ed": 119, "text": "evaluation metrics"}]
[{"st": 147, "ed": 149, "text": "vision systems"}]
[{"st": 9, "ed": 11, "text": "temporal dynamics"}, {"st": 23, "ed": 25, "text": "face image"}, {"st": 40, "ed": 42, "text": "temporal dynamics"}, {"st": 55, "ed": 58, "text": "the paper presents"}, {"st": 72, "ed": 74, "text": "classification accuracy"}, {"st": 81, "ed": 83, "text": "standard classification"}, {"st": 93, "ed": 95, "text": "application domains"}, {"st": 122, "ed": 124, "text": "competitive performance"}]
[{"st": 37, "ed": 39, "text": "previous methods"}, {"st": 51, "ed": 53, "text": "user defined"}, {"st": 60, "ed": 62, "text": "object category"}, {"st": 87, "ed": 89, "text": "user defined"}, {"st": 176, "ed": 178, "text": "proposed approach"}]
[{"st": 6, "ed": 8, "text": "visual content"}, {"st": 8, "ed": 13, "text": "deep convolutional neural networks cnn"}, {"st": 16, "ed": 18, "text": "promising performance"}, {"st": 20, "ed": 22, "text": "vision based"}, {"st": 120, "ed": 122, "text": "visual content"}, {"st": 167, "ed": 169, "text": "proposed algorithm"}]
[{"st": 9, "ed": 11, "text": "widely studied"}, {"st": 16, "ed": 18, "text": "spatio temporal"}, {"st": 25, "ed": 27, "text": "deep learning"}, {"st": 42, "ed": 44, "text": "jointly learns"}, {"st": 57, "ed": 61, "text": "recurrent neural networks rnns"}, {"st": 63, "ed": 68, "text": "long short term memory lstm"}, {"st": 153, "ed": 155, "text": "shows significant"}]
[{"st": 5, "ed": 9, "text": "convolutional neural network cnn"}, {"st": 16, "ed": 19, "text": "deep convolutional network"}, {"st": 38, "ed": 40, "text": "fully connected"}, {"st": 48, "ed": 50, "text": "feature representation"}, {"st": 60, "ed": 62, "text": "convolutional layers"}, {"st": 66, "ed": 69, "text": "fully connected layers"}, {"st": 79, "ed": 81, "text": "layer 3"}, {"st": 100, "ed": 103, "text": "detection and classification"}]
[{"st": 40, "ed": 42, "text": "feature descriptors"}, {"st": 62, "ed": 64, "text": "feature descriptors"}, {"st": 69, "ed": 71, "text": "observed data"}, {"st": 117, "ed": 119, "text": "object recognition"}, {"st": 124, "ed": 126, "text": "optimization problem"}, {"st": 160, "ed": 162, "text": "recent advances"}, {"st": 188, "ed": 190, "text": "cost function"}, {"st": 195, "ed": 197, "text": "real world"}, {"st": 197, "ed": 199, "text": "rgb d"}]
[{"st": 6, "ed": 8, "text": "user generated"}, {"st": 28, "ed": 30, "text": "user generated"}, {"st": 60, "ed": 62, "text": "textual data"}, {"st": 65, "ed": 67, "text": "related tasks"}, {"st": 91, "ed": 93, "text": "image dataset"}, {"st": 111, "ed": 113, "text": "zero shot"}, {"st": 142, "ed": 144, "text": "multiple datasets"}]
[{"st": 19, "ed": 21, "text": "scene understanding"}, {"st": 34, "ed": 36, "text": "optical flow"}, {"st": 45, "ed": 47, "text": "thermal energy"}, {"st": 49, "ed": 51, "text": "thermal energy"}, {"st": 82, "ed": 84, "text": "clustering process"}, {"st": 104, "ed": 106, "text": "pre defined"}]
[{"st": 17, "ed": 19, "text": "scene understanding"}, {"st": 47, "ed": 49, "text": "video game"}, {"st": 80, "ed": 84, "text": "convolutional neural network cnn"}, {"st": 118, "ed": 120, "text": "raw image"}]
[{"st": 40, "ed": 42, "text": "existing approaches"}, {"st": 74, "ed": 76, "text": "fine grained"}, {"st": 94, "ed": 96, "text": "user interface"}, {"st": 140, "ed": 142, "text": "open source"}, {"st": 147, "ed": 149, "text": "image analysis"}, {"st": 178, "ed": 180, "text": "online learning"}, {"st": 219, "ed": 222, "text": "end to end"}, {"st": 235, "ed": 237, "text": "user study"}]
[{"st": 3, "ed": 5, "text": "low cost"}, {"st": 85, "ed": 87, "text": "rgb d"}, {"st": 98, "ed": 100, "text": "density function"}, {"st": 109, "ed": 111, "text": "virtual environment"}, {"st": 114, "ed": 116, "text": "density function"}, {"st": 116, "ed": 119, "text": "takes into account"}, {"st": 147, "ed": 149, "text": "rgb d"}, {"st": 158, "ed": 160, "text": "random forest"}, {"st": 175, "ed": 178, "text": "conditional random fields"}, {"st": 181, "ed": 183, "text": "evaluation shows"}, {"st": 194, "ed": 196, "text": "average precision"}, {"st": 215, "ed": 217, "text": "computationally efficient"}]
[{"st": 3, "ed": 5, "text": "deep learning"}, {"st": 9, "ed": 11, "text": "recent years"}, {"st": 15, "ed": 17, "text": "object recognition"}, {"st": 58, "ed": 60, "text": "large scale"}, {"st": 60, "ed": 62, "text": "visual concepts"}, {"st": 65, "ed": 67, "text": "active learning"}, {"st": 92, "ed": 94, "text": "central role"}]
[{"st": 3, "ed": 5, "text": "experiential learning"}, {"st": 20, "ed": 22, "text": "real world"}, {"st": 58, "ed": 61, "text": "deep neural networks"}, {"st": 90, "ed": 92, "text": "visual features"}, {"st": 104, "ed": 106, "text": "feature space"}, {"st": 126, "ed": 128, "text": "multi step"}, {"st": 147, "ed": 149, "text": "feature space"}, {"st": 164, "ed": 166, "text": "approach outperforms"}]
[{"st": 23, "ed": 25, "text": "spatio temporal"}, {"st": 42, "ed": 44, "text": "starting point"}, {"st": 147, "ed": 149, "text": "spatio temporal"}]
[{"st": 8, "ed": 10, "text": "rule based"}, {"st": 26, "ed": 28, "text": "recognition accuracy"}, {"st": 33, "ed": 35, "text": "supervised learning"}, {"st": 42, "ed": 44, "text": "recognition systems"}, {"st": 65, "ed": 67, "text": "rule based"}, {"st": 118, "ed": 120, "text": "rule based"}, {"st": 123, "ed": 125, "text": "recognition accuracy"}, {"st": 133, "ed": 135, "text": "recognition rate"}]
[]
[{"st": 49, "ed": 51, "text": "social media"}]
[{"st": 71, "ed": 73, "text": "generic framework"}, {"st": 111, "ed": 113, "text": "vision based"}]
[{"st": 9, "ed": 11, "text": "pair wise"}, {"st": 22, "ed": 24, "text": "recovery guarantees"}, {"st": 73, "ed": 75, "text": "method yields"}, {"st": 143, "ed": 145, "text": "simulated data"}]
[{"st": 3, "ed": 5, "text": "facial expression"}, {"st": 18, "ed": 20, "text": "facial expression"}, {"st": 31, "ed": 34, "text": "facial expression recognition"}, {"st": 36, "ed": 38, "text": "proposed method"}, {"st": 44, "ed": 48, "text": "convolutional neural networks cnn"}, {"st": 50, "ed": 52, "text": "face recognition"}, {"st": 66, "ed": 68, "text": "small sample"}, {"st": 71, "ed": 75, "text": "scale invariant feature transform"}, {"st": 75, "ed": 77, "text": "sift features"}, {"st": 92, "ed": 94, "text": "training data"}, {"st": 125, "ed": 127, "text": "proposed approach"}]
[{"st": 0, "ed": 2, "text": "previous studies"}]
[{"st": 166, "ed": 168, "text": "large scale"}, {"st": 179, "ed": 181, "text": "user generated"}, {"st": 191, "ed": 193, "text": "prediction accuracy"}]
[{"st": 11, "ed": 13, "text": "real world"}, {"st": 93, "ed": 95, "text": "moving objects"}, {"st": 140, "ed": 142, "text": "prior information"}]
[{"st": 161, "ed": 163, "text": "applications including"}]
[]
[{"st": 2, "ed": 4, "text": "low latency"}, {"st": 30, "ed": 32, "text": "multi agent"}, {"st": 44, "ed": 46, "text": "parameter tuning"}, {"st": 55, "ed": 58, "text": "end to end"}, {"st": 68, "ed": 70, "text": "multi agent"}, {"st": 81, "ed": 84, "text": "deep neural network"}, {"st": 121, "ed": 123, "text": "multi agent"}, {"st": 132, "ed": 135, "text": "deep neural network"}, {"st": 140, "ed": 143, "text": "simulated and real"}, {"st": 190, "ed": 192, "text": "training data"}]
[{"st": 12, "ed": 14, "text": "important issue"}, {"st": 24, "ed": 27, "text": "taking into account"}, {"st": 138, "ed": 140, "text": "previous research"}, {"st": 185, "ed": 188, "text": "and vice versa"}, {"st": 194, "ed": 196, "text": "proposed method"}]
[{"st": 60, "ed": 62, "text": "time series"}, {"st": 134, "ed": 136, "text": "recognition accuracy"}, {"st": 140, "ed": 142, "text": "lighting conditions"}]
[{"st": 9, "ed": 11, "text": "question answering"}, {"st": 16, "ed": 18, "text": "natural language"}, {"st": 46, "ed": 48, "text": "automatically generated"}, {"st": 104, "ed": 107, "text": "self paced learning"}, {"st": 134, "ed": 137, "text": "self paced learning"}]
[{"st": 72, "ed": 75, "text": "training and test"}, {"st": 92, "ed": 94, "text": "rgb d"}, {"st": 202, "ed": 204, "text": "ground truth"}, {"st": 223, "ed": 225, "text": "pose estimation"}]
[{"st": 8, "ed": 11, "text": "taking into account"}, {"st": 32, "ed": 35, "text": "constrained optimization problem"}, {"st": 67, "ed": 69, "text": "dynamic range"}, {"st": 95, "ed": 97, "text": "visual quality"}]
[{"st": 51, "ed": 53, "text": "previous research"}, {"st": 71, "ed": 73, "text": "temporal information"}, {"st": 74, "ed": 76, "text": "multimodal data"}, {"st": 96, "ed": 98, "text": "real world"}, {"st": 107, "ed": 109, "text": "empirical study"}, {"st": 138, "ed": 140, "text": "baseline methods"}]
[{"st": 22, "ed": 24, "text": "rgb d"}, {"st": 49, "ed": 51, "text": "gibbs sampling"}, {"st": 52, "ed": 54, "text": "weakly supervised"}, {"st": 86, "ed": 88, "text": "rgb d"}, {"st": 114, "ed": 116, "text": "training data"}]
[{"st": 42, "ed": 44, "text": "point wise"}, {"st": 70, "ed": 72, "text": "incomplete data"}, {"st": 130, "ed": 132, "text": "compressive sensing"}, {"st": 156, "ed": 159, "text": "2d and 3d"}, {"st": 191, "ed": 193, "text": "depth estimation"}, {"st": 198, "ed": 200, "text": "real world"}, {"st": 235, "ed": 237, "text": "extensive experimental"}, {"st": 239, "ed": 242, "text": "2d and 3d"}, {"st": 242, "ed": 244, "text": "problems including"}, {"st": 244, "ed": 246, "text": "monte carlo"}, {"st": 256, "ed": 258, "text": "empirical results"}, {"st": 261, "ed": 263, "text": "proposed approach"}]
[{"st": 27, "ed": 29, "text": "based approach"}, {"st": 79, "ed": 81, "text": "constraint based"}, {"st": 96, "ed": 98, "text": "cross view"}]
[{"st": 6, "ed": 8, "text": "transfer learning"}, {"st": 11, "ed": 14, "text": "classification and regression"}, {"st": 20, "ed": 22, "text": "pre trained"}, {"st": 26, "ed": 28, "text": "feature vector"}, {"st": 32, "ed": 34, "text": "feature maps"}, {"st": 35, "ed": 37, "text": "multiple layers"}, {"st": 72, "ed": 75, "text": "classification and regression"}, {"st": 91, "ed": 93, "text": "previous approaches"}]
[{"st": 1, "ed": 3, "text": "human brain"}, {"st": 5, "ed": 9, "text": "functional magnetic resonance imaging"}, {"st": 11, "ed": 13, "text": "gained increasing"}, {"st": 18, "ed": 20, "text": "encouraging results"}, {"st": 26, "ed": 28, "text": "classification tasks"}, {"st": 39, "ed": 41, "text": "main challenges"}, {"st": 59, "ed": 61, "text": "limited data"}, {"st": 62, "ed": 64, "text": "existing methods"}, {"st": 101, "ed": 103, "text": "latent variable"}, {"st": 107, "ed": 109, "text": "latent representation"}, {"st": 111, "ed": 113, "text": "generative model"}, {"st": 172, "ed": 174, "text": "variational bayesian"}, {"st": 178, "ed": 180, "text": "latent variables"}, {"st": 191, "ed": 193, "text": "latent representations"}, {"st": 208, "ed": 210, "text": "training set"}]
[{"st": 78, "ed": 80, "text": "proposed method"}]
[{"st": 8, "ed": 10, "text": "real world"}, {"st": 14, "ed": 16, "text": "time consuming"}, {"st": 22, "ed": 24, "text": "recent advances"}, {"st": 28, "ed": 30, "text": "deep learning"}, {"st": 39, "ed": 41, "text": "training data"}, {"st": 55, "ed": 57, "text": "unreal engine"}, {"st": 73, "ed": 75, "text": "physics engine"}, {"st": 124, "ed": 126, "text": "modular design"}, {"st": 158, "ed": 160, "text": "real world"}]
[{"st": 5, "ed": 8, "text": "epidermal growth factor"}, {"st": 19, "ed": 21, "text": "breast cancer"}, {"st": 112, "ed": 115, "text": "artificial intelligence ai"}, {"st": 176, "ed": 178, "text": "ground truth"}, {"st": 193, "ed": 196, "text": "man vs machine"}]
[{"st": 24, "ed": 26, "text": "visual information"}, {"st": 42, "ed": 44, "text": "tennis court"}, {"st": 52, "ed": 54, "text": "visual recognition"}, {"st": 58, "ed": 60, "text": "rule based"}, {"st": 122, "ed": 124, "text": "object detectors"}, {"st": 127, "ed": 129, "text": "deep learning"}, {"st": 138, "ed": 140, "text": "improve performance"}, {"st": 176, "ed": 178, "text": "improve performance"}, {"st": 179, "ed": 181, "text": "visual reasoning"}]
[{"st": 11, "ed": 14, "text": "the past decade"}, {"st": 43, "ed": 45, "text": "supervised setting"}, {"st": 79, "ed": 81, "text": "optical flow"}, {"st": 102, "ed": 104, "text": "variational autoencoder"}, {"st": 155, "ed": 157, "text": "sensor fusion"}, {"st": 173, "ed": 175, "text": "proposed approach"}, {"st": 181, "ed": 183, "text": "supervised learning"}]
[{"st": 6, "ed": 8, "text": "path planning"}, {"st": 28, "ed": 30, "text": "path planning"}, {"st": 34, "ed": 37, "text": "travelling salesman problem"}, {"st": 66, "ed": 68, "text": "performance improvement"}, {"st": 82, "ed": 84, "text": "parallel computing"}, {"st": 100, "ed": 102, "text": "significantly reduced"}, {"st": 114, "ed": 116, "text": "proposed algorithm"}]
[{"st": 2, "ed": 4, "text": "image processing"}, {"st": 5, "ed": 7, "text": "computer vision"}, {"st": 17, "ed": 19, "text": "visual features"}, {"st": 22, "ed": 24, "text": "recent works"}, {"st": 27, "ed": 29, "text": "visual features"}, {"st": 31, "ed": 33, "text": "pre trained"}, {"st": 33, "ed": 36, "text": "deep neural networks"}, {"st": 45, "ed": 47, "text": "recent works"}, {"st": 51, "ed": 53, "text": "visual features"}, {"st": 62, "ed": 64, "text": "prediction tasks"}, {"st": 76, "ed": 78, "text": "visual features"}, {"st": 171, "ed": 173, "text": "visual features"}]
[{"st": 6, "ed": 8, "text": "machine learning"}, {"st": 25, "ed": 27, "text": "image space"}, {"st": 30, "ed": 33, "text": "trained neural network"}, {"st": 43, "ed": 45, "text": "adversarial examples"}, {"st": 100, "ed": 102, "text": "adversarial examples"}, {"st": 105, "ed": 107, "text": "object detection"}, {"st": 113, "ed": 116, "text": "trained neural network"}, {"st": 159, "ed": 161, "text": "autonomous car"}, {"st": 164, "ed": 166, "text": "stop sign"}, {"st": 203, "ed": 205, "text": "internal representation"}, {"st": 218, "ed": 220, "text": "adversarial examples"}]
[{"st": 7, "ed": 9, "text": "artificial intelligence"}, {"st": 86, "ed": 88, "text": "neural network"}, {"st": 89, "ed": 91, "text": "jointly learns"}, {"st": 116, "ed": 118, "text": "internal representation"}, {"st": 132, "ed": 134, "text": "back propagation"}, {"st": 141, "ed": 143, "text": "image regions"}, {"st": 169, "ed": 171, "text": "qualitative analysis"}]
[{"st": 30, "ed": 34, "text": "deep convolutional neural network"}, {"st": 82, "ed": 84, "text": "proposed method"}, {"st": 100, "ed": 102, "text": "ground truth"}, {"st": 103, "ed": 105, "text": "bounding boxes"}, {"st": 109, "ed": 111, "text": "qualitative results"}, {"st": 139, "ed": 141, "text": "learned features"}, {"st": 146, "ed": 148, "text": "classification task"}, {"st": 156, "ed": 158, "text": "method outperforms"}, {"st": 159, "ed": 161, "text": "random initialization"}, {"st": 167, "ed": 169, "text": "source code"}]
[{"st": 3, "ed": 5, "text": "point clouds"}, {"st": 6, "ed": 8, "text": "complex environments"}, {"st": 43, "ed": 46, "text": "gaussian mixture models"}, {"st": 58, "ed": 60, "text": "probability distributions"}, {"st": 66, "ed": 68, "text": "point cloud"}, {"st": 114, "ed": 117, "text": "2d and 3d"}, {"st": 125, "ed": 127, "text": "complex environments"}]
[{"st": 9, "ed": 11, "text": "challenging problem"}, {"st": 18, "ed": 20, "text": "lighting conditions"}, {"st": 75, "ed": 77, "text": "method named"}, {"st": 103, "ed": 105, "text": "based methods"}]
[{"st": 20, "ed": 22, "text": "training samples"}, {"st": 39, "ed": 41, "text": "feature extraction"}, {"st": 47, "ed": 49, "text": "moving average"}, {"st": 61, "ed": 64, "text": "end to end"}, {"st": 86, "ed": 88, "text": "feature extraction"}, {"st": 98, "ed": 100, "text": "neural networks"}, {"st": 102, "ed": 105, "text": "end to end"}, {"st": 115, "ed": 117, "text": "residual learning"}, {"st": 123, "ed": 125, "text": "extensive experiments"}, {"st": 127, "ed": 129, "text": "benchmark datasets"}]
[{"st": 11, "ed": 14, "text": "deep neural networks"}, {"st": 15, "ed": 17, "text": "error correction"}, {"st": 21, "ed": 23, "text": "feature level"}, {"st": 40, "ed": 42, "text": "fully connected"}, {"st": 56, "ed": 58, "text": "shared representation"}, {"st": 77, "ed": 79, "text": "discriminative features"}, {"st": 95, "ed": 97, "text": "error correcting"}, {"st": 118, "ed": 120, "text": "proposed approach"}]
[{"st": 4, "ed": 6, "text": "open world"}, {"st": 89, "ed": 91, "text": "task specific"}, {"st": 104, "ed": 106, "text": "task specific"}, {"st": 131, "ed": 133, "text": "object instances"}]
[{"st": 28, "ed": 30, "text": "mid level"}, {"st": 112, "ed": 114, "text": "prediction model"}, {"st": 130, "ed": 132, "text": "source code"}]
[{"st": 24, "ed": 26, "text": "computer vision"}, {"st": 74, "ed": 76, "text": "optical flow"}, {"st": 79, "ed": 81, "text": "feature vectors"}, {"st": 86, "ed": 88, "text": "multilayer perceptron"}, {"st": 111, "ed": 114, "text": "feedforward neural network"}, {"st": 131, "ed": 133, "text": "multilayer perceptron"}]
[{"st": 8, "ed": 11, "text": "learning from demonstration"}, {"st": 45, "ed": 47, "text": "monte carlo"}, {"st": 106, "ed": 108, "text": "autonomous driving"}, {"st": 115, "ed": 118, "text": "learning from demonstration"}, {"st": 118, "ed": 120, "text": "method outperforms"}, {"st": 130, "ed": 132, "text": "real world"}]
[{"st": 0, "ed": 2, "text": "quantum information"}, {"st": 5, "ed": 7, "text": "learning systems"}, {"st": 27, "ed": 29, "text": "quantum information"}, {"st": 31, "ed": 33, "text": "machine learning"}, {"st": 35, "ed": 38, "text": "artificial intelligence ai"}, {"st": 81, "ed": 83, "text": "quantum computing"}, {"st": 115, "ed": 117, "text": "quantum computing"}, {"st": 124, "ed": 126, "text": "speed ups"}, {"st": 131, "ed": 133, "text": "big data"}, {"st": 172, "ed": 174, "text": "interactive learning"}, {"st": 258, "ed": 260, "text": "recent developments"}, {"st": 269, "ed": 271, "text": "machine learning"}, {"st": 272, "ed": 274, "text": "artificial intelligence"}]
[{"st": 4, "ed": 9, "text": "deep convolutional neural networks cnn"}, {"st": 10, "ed": 12, "text": "image classification"}, {"st": 13, "ed": 15, "text": "object detection"}, {"st": 67, "ed": 69, "text": "training data"}, {"st": 146, "ed": 148, "text": "optical flow"}, {"st": 149, "ed": 151, "text": "object tracking"}]
[{"st": 31, "ed": 33, "text": "neural network"}, {"st": 36, "ed": 38, "text": "low dimensional"}, {"st": 62, "ed": 64, "text": "prior knowledge"}, {"st": 68, "ed": 70, "text": "loss functions"}, {"st": 100, "ed": 102, "text": "quantitative evaluation"}, {"st": 107, "ed": 109, "text": "nearest neighbors"}, {"st": 146, "ed": 148, "text": "transfer learning"}, {"st": 169, "ed": 171, "text": "low dimensional"}, {"st": 176, "ed": 178, "text": "reinforcement learning"}, {"st": 188, "ed": 190, "text": "raw data"}]
[{"st": 18, "ed": 20, "text": "key contribution"}, {"st": 25, "ed": 27, "text": "low level"}, {"st": 27, "ed": 29, "text": "visual processing"}]
[{"st": 21, "ed": 23, "text": "depth estimation"}, {"st": 35, "ed": 37, "text": "higher level"}, {"st": 53, "ed": 55, "text": "low resolution"}, {"st": 61, "ed": 65, "text": "simultaneous localization and mapping"}, {"st": 82, "ed": 84, "text": "rgb d"}, {"st": 84, "ed": 86, "text": "raw data"}, {"st": 120, "ed": 123, "text": "mean square error"}, {"st": 154, "ed": 156, "text": "proposed algorithm"}, {"st": 170, "ed": 172, "text": "super resolution"}]
[{"st": 16, "ed": 18, "text": "industrial design"}, {"st": 20, "ed": 22, "text": "computer graphics"}, {"st": 31, "ed": 33, "text": "traditional methods"}, {"st": 41, "ed": 43, "text": "deep learning"}, {"st": 55, "ed": 58, "text": "end to end"}, {"st": 62, "ed": 64, "text": "deep learning"}, {"st": 75, "ed": 77, "text": "point cloud"}, {"st": 109, "ed": 111, "text": "distance function"}, {"st": 122, "ed": 125, "text": "deep learning based"}]
[{"st": 16, "ed": 18, "text": "motion capture"}, {"st": 100, "ed": 102, "text": "motion capture"}]
[{"st": 81, "ed": 83, "text": "visual perception"}, {"st": 176, "ed": 178, "text": "visual search"}]
[{"st": 0, "ed": 3, "text": "deep neural networks"}, {"st": 31, "ed": 33, "text": "computationally expensive"}, {"st": 95, "ed": 97, "text": "np hard"}, {"st": 106, "ed": 108, "text": "assignment problem"}, {"st": 151, "ed": 153, "text": "significant gains"}]
[{"st": 0, "ed": 3, "text": "deep neural networks"}, {"st": 11, "ed": 13, "text": "machine learning"}, {"st": 94, "ed": 96, "text": "deep models"}, {"st": 109, "ed": 111, "text": "deep architectures"}, {"st": 112, "ed": 114, "text": "fine tune"}, {"st": 163, "ed": 165, "text": "fine tuning"}]
[{"st": 0, "ed": 2, "text": "digital image"}, {"st": 33, "ed": 35, "text": "wide variety"}, {"st": 163, "ed": 165, "text": "recently published"}]
[{"st": 24, "ed": 26, "text": "adversarial examples"}, {"st": 70, "ed": 72, "text": "stop sign"}, {"st": 118, "ed": 120, "text": "stop sign"}, {"st": 150, "ed": 152, "text": "adversarial attack"}, {"st": 188, "ed": 190, "text": "bounding box"}]
[{"st": 98, "ed": 100, "text": "spatio temporal"}, {"st": 104, "ed": 106, "text": "logic programming"}]
[{"st": 0, "ed": 3, "text": "optical character recognition"}, {"st": 82, "ed": 84, "text": "character recognition"}]
[{"st": 6, "ed": 8, "text": "ground truth"}, {"st": 16, "ed": 18, "text": "bounding boxes"}, {"st": 31, "ed": 33, "text": "prevent overfitting"}, {"st": 87, "ed": 89, "text": "adversarial perturbations"}, {"st": 91, "ed": 93, "text": "ground truth"}, {"st": 96, "ed": 98, "text": "worst case"}, {"st": 103, "ed": 105, "text": "bounding box"}, {"st": 128, "ed": 130, "text": "nash equilibrium"}, {"st": 151, "ed": 153, "text": "significantly improve"}, {"st": 165, "ed": 167, "text": "object detection"}, {"st": 171, "ed": 173, "text": "improves performance"}]
[{"st": 71, "ed": 73, "text": "hand crafted"}, {"st": 74, "ed": 76, "text": "learned features"}, {"st": 89, "ed": 91, "text": "classification methods"}, {"st": 116, "ed": 118, "text": "pablo picasso"}, {"st": 121, "ed": 123, "text": "egon schiele"}]
[{"st": 0, "ed": 2, "text": "recent research"}, {"st": 34, "ed": 36, "text": "potential applications"}, {"st": 57, "ed": 59, "text": "current methods"}, {"st": 124, "ed": 126, "text": "generative adversarial"}, {"st": 158, "ed": 160, "text": "proposed approach"}, {"st": 185, "ed": 187, "text": "training data"}]
[{"st": 3, "ed": 5, "text": "attention models"}, {"st": 46, "ed": 48, "text": "visual attention"}, {"st": 51, "ed": 53, "text": "significant improvements"}, {"st": 54, "ed": 56, "text": "retrieval performance"}, {"st": 93, "ed": 95, "text": "improved performance"}, {"st": 102, "ed": 104, "text": "proposed approach"}, {"st": 117, "ed": 119, "text": "large margin"}, {"st": 121, "ed": 123, "text": "similar performance"}, {"st": 141, "ed": 143, "text": "source code"}]
[{"st": 15, "ed": 17, "text": "belief revision"}, {"st": 34, "ed": 36, "text": "answer set"}, {"st": 38, "ed": 40, "text": "abductive reasoning"}, {"st": 50, "ed": 52, "text": "visual processing"}, {"st": 56, "ed": 58, "text": "object tracking"}, {"st": 74, "ed": 76, "text": "answer set"}, {"st": 95, "ed": 97, "text": "vision community"}, {"st": 99, "ed": 101, "text": "recently developed"}]
[{"st": 12, "ed": 14, "text": "open question"}, {"st": 34, "ed": 38, "text": "functional magnetic resonance imaging"}, {"st": 51, "ed": 53, "text": "deep network"}, {"st": 64, "ed": 66, "text": "object category"}, {"st": 69, "ed": 71, "text": "visual cortex"}, {"st": 200, "ed": 203, "text": "end to end"}, {"st": 285, "ed": 287, "text": "structural similarity"}]
[{"st": 4, "ed": 8, "text": "deep neural network architecture"}, {"st": 29, "ed": 31, "text": "episodic memory"}, {"st": 38, "ed": 40, "text": "latent vector"}, {"st": 61, "ed": 63, "text": "future frames"}, {"st": 65, "ed": 68, "text": "end to end"}, {"st": 83, "ed": 85, "text": "latent vector"}, {"st": 103, "ed": 105, "text": "large scale"}, {"st": 115, "ed": 117, "text": "generalization capability"}, {"st": 119, "ed": 121, "text": "real world"}]
[{"st": 1, "ed": 3, "text": "recent years"}, {"st": 10, "ed": 14, "text": "functional magnetic resonance imaging"}, {"st": 22, "ed": 24, "text": "natural image"}, {"st": 33, "ed": 35, "text": "existing methods"}, {"st": 41, "ed": 43, "text": "prior information"}, {"st": 55, "ed": 57, "text": "prior information"}, {"st": 71, "ed": 73, "text": "visual cortex"}, {"st": 79, "ed": 83, "text": "convolutional neural network cnn"}, {"st": 92, "ed": 94, "text": "natural images"}, {"st": 99, "ed": 101, "text": "pre trained"}, {"st": 108, "ed": 110, "text": "image reconstruction"}, {"st": 123, "ed": 125, "text": "sparse linear"}, {"st": 161, "ed": 163, "text": "promising results"}, {"st": 168, "ed": 170, "text": "natural image"}, {"st": 178, "ed": 180, "text": "prior information"}, {"st": 195, "ed": 197, "text": "training set"}, {"st": 205, "ed": 207, "text": "reconstructed images"}, {"st": 222, "ed": 224, "text": "visual features"}, {"st": 228, "ed": 230, "text": "visual perception"}]
[]
[{"st": 8, "ed": 10, "text": "feature learning"}, {"st": 42, "ed": 44, "text": "point cloud"}, {"st": 97, "ed": 99, "text": "input features"}, {"st": 122, "ed": 124, "text": "proposed method"}, {"st": 134, "ed": 136, "text": "point cloud"}, {"st": 160, "ed": 162, "text": "benchmark datasets"}]
[{"st": 84, "ed": 86, "text": "generative adversarial"}, {"st": 86, "ed": 88, "text": "neural networks"}, {"st": 92, "ed": 94, "text": "neural networks"}]
[{"st": 2, "ed": 4, "text": "great potential"}, {"st": 81, "ed": 83, "text": "deep learning"}, {"st": 87, "ed": 89, "text": "machine learning"}, {"st": 141, "ed": 143, "text": "contextual information"}, {"st": 164, "ed": 166, "text": "multi modal"}, {"st": 199, "ed": 201, "text": "experiments confirm"}, {"st": 220, "ed": 222, "text": "proposed approach"}]
[{"st": 6, "ed": 8, "text": "deep learning"}, {"st": 9, "ed": 11, "text": "image recognition"}, {"st": 13, "ed": 15, "text": "computer vision"}, {"st": 21, "ed": 23, "text": "art history"}, {"st": 24, "ed": 26, "text": "cognitive psychology"}, {"st": 50, "ed": 52, "text": "visual perception"}, {"st": 58, "ed": 60, "text": "erwin panofsky"}, {"st": 63, "ed": 65, "text": "image analysis"}, {"st": 67, "ed": 69, "text": "eleanor rosch"}, {"st": 113, "ed": 115, "text": "machine learning"}, {"st": 168, "ed": 170, "text": "image recognition"}]
[{"st": 20, "ed": 22, "text": "low cost"}, {"st": 73, "ed": 75, "text": "continuous integration"}, {"st": 115, "ed": 117, "text": "optimization problem"}, {"st": 120, "ed": 124, "text": "deep recurrent neural networks"}, {"st": 126, "ed": 128, "text": "highly accurate"}, {"st": 169, "ed": 171, "text": "challenging task"}]
[{"st": 8, "ed": 10, "text": "machine learning"}, {"st": 13, "ed": 16, "text": "deep neural networks"}, {"st": 51, "ed": 53, "text": "image pixels"}, {"st": 180, "ed": 182, "text": "large scale"}, {"st": 185, "ed": 187, "text": "imagenet dataset"}, {"st": 197, "ed": 199, "text": "black box"}, {"st": 204, "ed": 206, "text": "box attacks"}]
[{"st": 33, "ed": 35, "text": "context information"}, {"st": 45, "ed": 47, "text": "dead reckoning"}, {"st": 75, "ed": 77, "text": "self awareness"}, {"st": 110, "ed": 112, "text": "real world"}]
[]
[{"st": 4, "ed": 6, "text": "object detection"}, {"st": 15, "ed": 17, "text": "image space"}, {"st": 47, "ed": 49, "text": "computational intelligence"}, {"st": 59, "ed": 61, "text": "object detection"}, {"st": 102, "ed": 104, "text": "higher accuracy"}]
[]
[]
[{"st": 50, "ed": 52, "text": "practical scenarios"}, {"st": 69, "ed": 71, "text": "web based"}]
[{"st": 33, "ed": 35, "text": "small change"}, {"st": 83, "ed": 85, "text": "agents learn"}, {"st": 97, "ed": 99, "text": "evolutionary pressure"}]
[{"st": 9, "ed": 11, "text": "categorical compositional"}, {"st": 13, "ed": 15, "text": "natural language"}, {"st": 66, "ed": 68, "text": "proposed framework"}, {"st": 87, "ed": 90, "text": "propose and evaluate"}, {"st": 130, "ed": 132, "text": "extensive experimental"}, {"st": 186, "ed": 188, "text": "quantum mechanics"}, {"st": 238, "ed": 240, "text": "quantum physics"}]
[{"st": 14, "ed": 16, "text": "time consuming"}, {"st": 71, "ed": 73, "text": "natural language"}, {"st": 103, "ed": 105, "text": "natural language"}, {"st": 107, "ed": 109, "text": "positive results"}, {"st": 123, "ed": 125, "text": "case study"}, {"st": 141, "ed": 143, "text": "non trivial"}]
[{"st": 43, "ed": 45, "text": "decision making"}, {"st": 80, "ed": 82, "text": "topic models"}, {"st": 229, "ed": 231, "text": "collective behavior"}, {"st": 243, "ed": 245, "text": "topic modeling"}]
[{"st": 1, "ed": 3, "text": "categorical compositional"}, {"st": 6, "ed": 8, "text": "natural language"}, {"st": 69, "ed": 71, "text": "categorical compositional"}, {"st": 73, "ed": 76, "text": "taking advantage of"}, {"st": 148, "ed": 150, "text": "quantum logic"}, {"st": 166, "ed": 168, "text": "sentence level"}]
[{"st": 10, "ed": 13, "text": "context free grammar"}, {"st": 22, "ed": 24, "text": "successfully applied"}, {"st": 53, "ed": 55, "text": "efficient algorithms"}, {"st": 58, "ed": 60, "text": "minimization problem"}, {"st": 66, "ed": 68, "text": "empirical evaluation"}, {"st": 88, "ed": 90, "text": "standard benchmarks"}]
[{"st": 61, "ed": 63, "text": "question answering"}, {"st": 109, "ed": 111, "text": "uk parliament"}]
[{"st": 5, "ed": 7, "text": "natural language"}, {"st": 48, "ed": 51, "text": "short term memory"}, {"st": 52, "ed": 54, "text": "encoder decoder"}, {"st": 56, "ed": 58, "text": "models 1"}, {"st": 59, "ed": 61, "text": "word embeddings"}, {"st": 68, "ed": 70, "text": "objective functions"}, {"st": 74, "ed": 77, "text": "cross entropy loss"}, {"st": 92, "ed": 94, "text": "open domain"}, {"st": 97, "ed": 99, "text": "encoder decoder"}]
[{"st": 0, "ed": 2, "text": "conversational agents"}, {"st": 43, "ed": 45, "text": "alexa prize"}, {"st": 60, "ed": 62, "text": "conversational agents"}, {"st": 87, "ed": 89, "text": "alexa prize"}, {"st": 176, "ed": 178, "text": "natural language"}, {"st": 183, "ed": 185, "text": "response generation"}, {"st": 196, "ed": 198, "text": "alexa prize"}, {"st": 210, "ed": 212, "text": "speech recognition"}, {"st": 217, "ed": 219, "text": "user experience"}, {"st": 240, "ed": 242, "text": "alexa prize"}]
[{"st": 4, "ed": 6, "text": "challenging problem"}, {"st": 9, "ed": 11, "text": "task oriented"}, {"st": 145, "ed": 148, "text": "tens of thousands"}, {"st": 155, "ed": 157, "text": "alexa prize"}]
[{"st": 0, "ed": 2, "text": "conversational agents"}, {"st": 15, "ed": 17, "text": "goal oriented"}, {"st": 40, "ed": 42, "text": "alexa prize"}, {"st": 54, "ed": 56, "text": "conversational agents"}, {"st": 63, "ed": 65, "text": "alexa prize"}, {"st": 100, "ed": 102, "text": "goal oriented"}, {"st": 137, "ed": 139, "text": "conversational agents"}, {"st": 182, "ed": 184, "text": "alexa prize"}, {"st": 203, "ed": 206, "text": "hundreds of thousands"}, {"st": 220, "ed": 222, "text": "automatic evaluation"}]
[{"st": 16, "ed": 18, "text": "textual data"}, {"st": 24, "ed": 26, "text": "current source"}, {"st": 36, "ed": 38, "text": "extremely large"}, {"st": 81, "ed": 83, "text": "united states"}, {"st": 108, "ed": 110, "text": "classification models"}, {"st": 121, "ed": 124, "text": "spatial and temporal"}, {"st": 131, "ed": 133, "text": "traffic congestion"}, {"st": 143, "ed": 145, "text": "traffic congestion"}, {"st": 163, "ed": 165, "text": "widely adopted"}, {"st": 279, "ed": 282, "text": "spatial and temporal"}]
[{"st": 0, "ed": 2, "text": "multi channel"}, {"st": 2, "ed": 4, "text": "speech enhancement"}, {"st": 5, "ed": 7, "text": "ad hoc"}, {"st": 45, "ed": 47, "text": "deep learning"}, {"st": 59, "ed": 61, "text": "efficient inference"}, {"st": 74, "ed": 76, "text": "deep learning"}, {"st": 98, "ed": 100, "text": "framework called"}, {"st": 131, "ed": 133, "text": "speech enhancement"}, {"st": 135, "ed": 141, "text": "experiments on synthetic and real world"}]
[{"st": 1, "ed": 3, "text": "successful applications"}, {"st": 4, "ed": 6, "text": "computer vision"}, {"st": 38, "ed": 40, "text": "fully automatic"}, {"st": 127, "ed": 129, "text": "max margin"}, {"st": 131, "ed": 133, "text": "parameter learning"}]
[{"st": 0, "ed": 2, "text": "spectral clustering"}, {"st": 20, "ed": 22, "text": "compressed sensing"}, {"st": 23, "ed": 25, "text": "matrix completion"}, {"st": 35, "ed": 37, "text": "partially observed"}, {"st": 46, "ed": 48, "text": "compressed sensing"}, {"st": 49, "ed": 51, "text": "matrix completion"}, {"st": 69, "ed": 71, "text": "affinity matrix"}, {"st": 88, "ed": 90, "text": "spectral clustering"}, {"st": 92, "ed": 94, "text": "multi class"}, {"st": 106, "ed": 108, "text": "compressed sensing"}, {"st": 109, "ed": 111, "text": "matrix completion"}, {"st": 113, "ed": 115, "text": "affinity matrix"}, {"st": 125, "ed": 127, "text": "multi class"}, {"st": 135, "ed": 137, "text": "k 1"}, {"st": 141, "ed": 143, "text": "affinity matrix"}, {"st": 154, "ed": 156, "text": "theoretical guarantees"}, {"st": 159, "ed": 161, "text": "numerical results"}]
[{"st": 13, "ed": 15, "text": "local spatial"}, {"st": 25, "ed": 27, "text": "recently developed"}, {"st": 40, "ed": 42, "text": "higher order"}, {"st": 86, "ed": 88, "text": "dimensional subspaces"}, {"st": 90, "ed": 92, "text": "l 2"}, {"st": 96, "ed": 98, "text": "natural images"}, {"st": 114, "ed": 116, "text": "hidden units"}, {"st": 124, "ed": 126, "text": "hidden units"}, {"st": 146, "ed": 148, "text": "natural images"}, {"st": 151, "ed": 153, "text": "local spatial"}]
[{"st": 8, "ed": 10, "text": "d dimensional"}, {"st": 10, "ed": 12, "text": "linear subspaces"}, {"st": 107, "ed": 109, "text": "d dimensional"}, {"st": 116, "ed": 118, "text": "minimization problems"}, {"st": 139, "ed": 142, "text": "0 p 1"}, {"st": 165, "ed": 167, "text": "generating distribution"}, {"st": 195, "ed": 198, "text": "0 p 1"}]
[{"st": 10, "ed": 12, "text": "text based"}, {"st": 31, "ed": 33, "text": "low level"}, {"st": 33, "ed": 35, "text": "visual features"}, {"st": 69, "ed": 71, "text": "transfer learning"}, {"st": 95, "ed": 97, "text": "pair wise"}, {"st": 114, "ed": 116, "text": "sparse representation"}, {"st": 127, "ed": 129, "text": "elastic net"}, {"st": 138, "ed": 140, "text": "labeled samples"}, {"st": 141, "ed": 143, "text": "unlabeled samples"}, {"st": 151, "ed": 153, "text": "extensive experiments"}, {"st": 160, "ed": 162, "text": "benchmark datasets"}]
[{"st": 91, "ed": 93, "text": "classification problem"}, {"st": 113, "ed": 115, "text": "main idea"}, {"st": 147, "ed": 149, "text": "base learners"}]
[{"st": 6, "ed": 8, "text": "hierarchical clustering"}, {"st": 66, "ed": 68, "text": "case studies"}, {"st": 84, "ed": 86, "text": "hierarchical clustering"}]
[{"st": 58, "ed": 60, "text": "existing methods"}]
[{"st": 10, "ed": 12, "text": "object classification"}, {"st": 21, "ed": 23, "text": "proposed method"}, {"st": 47, "ed": 49, "text": "mutual information"}, {"st": 70, "ed": 72, "text": "discriminative power"}, {"st": 83, "ed": 85, "text": "real datasets"}, {"st": 92, "ed": 94, "text": "image classification"}]
[{"st": 51, "ed": 53, "text": "local geometry"}, {"st": 107, "ed": 109, "text": "computer vision"}, {"st": 115, "ed": 117, "text": "extensive experiments"}]
[{"st": 11, "ed": 13, "text": "recent years"}, {"st": 14, "ed": 16, "text": "theoretical results"}, {"st": 31, "ed": 33, "text": "low rank"}, {"st": 45, "ed": 47, "text": "robust pca"}, {"st": 59, "ed": 62, "text": "image and video"}, {"st": 64, "ed": 66, "text": "face recognition"}, {"st": 153, "ed": 155, "text": "low rank"}, {"st": 182, "ed": 185, "text": "times faster than"}, {"st": 200, "ed": 202, "text": "face images"}]
[{"st": 2, "ed": 4, "text": "linear models"}, {"st": 5, "ed": 7, "text": "heavy tailed"}, {"st": 18, "ed": 20, "text": "most probable"}, {"st": 32, "ed": 34, "text": "compressed sensing"}, {"st": 40, "ed": 42, "text": "posterior distribution"}, {"st": 44, "ed": 46, "text": "latent variables"}, {"st": 61, "ed": 63, "text": "posterior distribution"}, {"st": 65, "ed": 67, "text": "sparse linear"}, {"st": 74, "ed": 76, "text": "variational bayesian"}, {"st": 102, "ed": 104, "text": "large scale"}, {"st": 109, "ed": 111, "text": "recently proposed"}, {"st": 121, "ed": 124, "text": "markov random fields"}, {"st": 186, "ed": 188, "text": "extremely large"}]
[{"st": 13, "ed": 15, "text": "finite dimensional"}, {"st": 31, "ed": 33, "text": "linear map"}, {"st": 49, "ed": 51, "text": "performance bounds"}]
[]
[{"st": 12, "ed": 15, "text": "support vector machines"}, {"st": 16, "ed": 18, "text": "increasingly popular"}, {"st": 23, "ed": 26, "text": "artificial neural network"}, {"st": 136, "ed": 138, "text": "ground truth"}, {"st": 193, "ed": 195, "text": "error rates"}]
[{"st": 36, "ed": 38, "text": "electron microscopy"}, {"st": 61, "ed": 63, "text": "recent advances"}, {"st": 64, "ed": 66, "text": "unsupervised learning"}, {"st": 67, "ed": 69, "text": "signal processing"}, {"st": 77, "ed": 79, "text": "without sacrificing"}, {"st": 92, "ed": 94, "text": "sparse linear"}, {"st": 97, "ed": 99, "text": "basis functions"}, {"st": 109, "ed": 111, "text": "compressive sensing"}]
[{"st": 2, "ed": 4, "text": "level set"}, {"st": 50, "ed": 52, "text": "level set"}, {"st": 75, "ed": 77, "text": "total variation"}, {"st": 95, "ed": 97, "text": "level set"}]
[{"st": 33, "ed": 35, "text": "point cloud"}, {"st": 40, "ed": 43, "text": "principal component analysis"}, {"st": 61, "ed": 63, "text": "arc length"}, {"st": 141, "ed": 143, "text": "arc length"}, {"st": 145, "ed": 147, "text": "power law"}, {"st": 150, "ed": 152, "text": "log normal"}, {"st": 163, "ed": 165, "text": "at large"}, {"st": 167, "ed": 169, "text": "arc length"}, {"st": 214, "ed": 216, "text": "dimensionality reduction"}, {"st": 228, "ed": 230, "text": "extragalactic astronomy"}, {"st": 252, "ed": 254, "text": "log normal"}, {"st": 256, "ed": 258, "text": "luminosity function"}]
[{"st": 5, "ed": 7, "text": "edge detection"}, {"st": 8, "ed": 11, "text": "synthetic aperture radar"}, {"st": 19, "ed": 21, "text": "edge detection"}, {"st": 32, "ed": 34, "text": "recently proposed"}, {"st": 62, "ed": 64, "text": "simulated data"}]
[{"st": 0, "ed": 2, "text": "sparse linear"}, {"st": 3, "ed": 6, "text": "generalized linear models"}, {"st": 9, "ed": 11, "text": "likelihood function"}, {"st": 37, "ed": 39, "text": "map estimation"}, {"st": 54, "ed": 56, "text": "type ii"}, {"st": 74, "ed": 76, "text": "cost functions"}, {"st": 81, "ed": 83, "text": "dual space"}, {"st": 96, "ed": 98, "text": "type ii"}, {"st": 165, "ed": 167, "text": "type ii"}, {"st": 182, "ed": 184, "text": "update rules"}, {"st": 188, "ed": 191, "text": "local and global"}, {"st": 225, "ed": 227, "text": "type ii"}, {"st": 254, "ed": 256, "text": "type ii"}]
[{"st": 12, "ed": 14, "text": "cost function"}, {"st": 21, "ed": 23, "text": "np hard"}, {"st": 28, "ed": 30, "text": "nuclear norm"}, {"st": 36, "ed": 38, "text": "penalty term"}, {"st": 47, "ed": 49, "text": "practical scenarios"}, {"st": 61, "ed": 63, "text": "low rank"}, {"st": 67, "ed": 69, "text": "special cases"}, {"st": 82, "ed": 84, "text": "variational approximation"}, {"st": 87, "ed": 89, "text": "nuclear norm"}, {"st": 121, "ed": 123, "text": "convex relaxations"}, {"st": 137, "ed": 139, "text": "low rank"}, {"st": 146, "ed": 150, "text": "robust principal component analysis"}, {"st": 157, "ed": 159, "text": "low rank"}, {"st": 164, "ed": 167, "text": "theoretical and empirical"}, {"st": 181, "ed": 183, "text": "based approaches"}]
[{"st": 4, "ed": 6, "text": "generic framework"}, {"st": 7, "ed": 9, "text": "component analysis"}, {"st": 49, "ed": 51, "text": "regularization techniques"}, {"st": 56, "ed": 58, "text": "clustering methods"}, {"st": 61, "ed": 63, "text": "semi supervised"}, {"st": 80, "ed": 82, "text": "proposed framework"}]
[{"st": 118, "ed": 120, "text": "pattern recognition"}]
[{"st": 27, "ed": 29, "text": "image patches"}, {"st": 96, "ed": 98, "text": "low dimensional"}, {"st": 111, "ed": 113, "text": "experiments demonstrate"}]
[{"st": 15, "ed": 18, "text": "kullback leibler divergence"}, {"st": 35, "ed": 37, "text": "posterior distribution"}, {"st": 43, "ed": 45, "text": "closed form"}, {"st": 58, "ed": 60, "text": "exponential family"}, {"st": 79, "ed": 82, "text": "speed and accuracy"}]
[{"st": 12, "ed": 14, "text": "complex network"}, {"st": 17, "ed": 19, "text": "graph drawing"}, {"st": 63, "ed": 65, "text": "multi dimensional"}, {"st": 72, "ed": 75, "text": "block coordinate descent"}, {"st": 89, "ed": 91, "text": "regularization term"}]
[{"st": 2, "ed": 4, "text": "visual tracking"}, {"st": 7, "ed": 9, "text": "object appearance"}, {"st": 102, "ed": 105, "text": "non euclidean geometry"}, {"st": 120, "ed": 124, "text": "markov chain monte carlo"}, {"st": 128, "ed": 130, "text": "quantitative evaluation"}, {"st": 137, "ed": 139, "text": "proposed approach"}]
[{"st": 13, "ed": 15, "text": "robust statistics"}, {"st": 40, "ed": 42, "text": "convex relaxation"}, {"st": 45, "ed": 47, "text": "principal component"}, {"st": 60, "ed": 62, "text": "per iteration"}, {"st": 69, "ed": 71, "text": "large scale"}, {"st": 88, "ed": 90, "text": "per iteration"}, {"st": 97, "ed": 99, "text": "frank wolfe"}, {"st": 108, "ed": 110, "text": "frank wolfe"}, {"st": 113, "ed": 115, "text": "low rank"}, {"st": 143, "ed": 145, "text": "proposed approach"}, {"st": 147, "ed": 149, "text": "numerical experiments"}]
[{"st": 1, "ed": 3, "text": "remote sensing"}, {"st": 11, "ed": 13, "text": "spatial resolution"}, {"st": 16, "ed": 18, "text": "spectral resolution"}, {"st": 36, "ed": 38, "text": "active research"}, {"st": 59, "ed": 61, "text": "data fusion"}, {"st": 67, "ed": 69, "text": "convex objective"}, {"st": 94, "ed": 96, "text": "additive noise"}, {"st": 102, "ed": 104, "text": "total variation"}, {"st": 166, "ed": 172, "text": "alternating direction method of multipliers admm"}, {"st": 193, "ed": 195, "text": "low dimensional"}]
[{"st": 6, "ed": 8, "text": "becoming increasingly"}, {"st": 21, "ed": 23, "text": "pattern recognition"}, {"st": 59, "ed": 61, "text": "challenging problem"}, {"st": 64, "ed": 66, "text": "computational complexity"}, {"st": 157, "ed": 159, "text": "classification accuracy"}]
[{"st": 5, "ed": 7, "text": "sparse signal"}, {"st": 9, "ed": 12, "text": "spike and slab"}, {"st": 35, "ed": 38, "text": "spike and slab"}, {"st": 50, "ed": 52, "text": "existing solutions"}, {"st": 61, "ed": 63, "text": "approach called"}, {"st": 73, "ed": 75, "text": "optimization problem"}, {"st": 90, "ed": 92, "text": "convex optimization"}, {"st": 102, "ed": 104, "text": "optimal solution"}, {"st": 125, "ed": 127, "text": "non negativity"}, {"st": 137, "ed": 139, "text": "real world"}, {"st": 146, "ed": 148, "text": "synthetic data"}, {"st": 151, "ed": 153, "text": "real world"}, {"st": 154, "ed": 156, "text": "recovery problem"}]
[{"st": 0, "ed": 4, "text": "canonical correlation analysis cca"}, {"st": 21, "ed": 23, "text": "potential applications"}, {"st": 61, "ed": 63, "text": "open source"}, {"st": 149, "ed": 151, "text": "accurately predict"}]
[{"st": 0, "ed": 3, "text": "loopy belief propagation"}, {"st": 12, "ed": 14, "text": "statistical mechanics"}, {"st": 16, "ed": 18, "text": "message passing"}, {"st": 18, "ed": 20, "text": "type inference"}, {"st": 30, "ed": 34, "text": "markov random fields mrfs"}, {"st": 41, "ed": 43, "text": "message passing"}, {"st": 54, "ed": 56, "text": "random fields"}, {"st": 71, "ed": 73, "text": "pair wise"}, {"st": 75, "ed": 77, "text": "random fields"}, {"st": 91, "ed": 93, "text": "bethe free"}, {"st": 95, "ed": 97, "text": "random fields"}, {"st": 107, "ed": 109, "text": "computational cost"}, {"st": 129, "ed": 131, "text": "proposed method"}, {"st": 133, "ed": 135, "text": "image restoration"}, {"st": 141, "ed": 143, "text": "theoretical results"}, {"st": 149, "ed": 151, "text": "numerical results"}]
[{"st": 7, "ed": 9, "text": "image reconstruction"}, {"st": 25, "ed": 27, "text": "computationally intensive"}, {"st": 54, "ed": 57, "text": "convolutional neural networks"}, {"st": 58, "ed": 60, "text": "image reconstruction"}, {"st": 63, "ed": 65, "text": "neural networks"}, {"st": 66, "ed": 68, "text": "computationally intensive"}, {"st": 87, "ed": 89, "text": "numerical experiments"}, {"st": 95, "ed": 97, "text": "computationally efficient"}]
[{"st": 0, "ed": 2, "text": "diffusion mri"}, {"st": 28, "ed": 30, "text": "angular resolution"}, {"st": 65, "ed": 67, "text": "compressed sensing"}, {"st": 207, "ed": 209, "text": "computational complexity"}, {"st": 213, "ed": 215, "text": "sparse coding"}, {"st": 217, "ed": 219, "text": "large scale"}, {"st": 229, "ed": 231, "text": "sparse coding"}, {"st": 238, "ed": 240, "text": "large scale"}, {"st": 251, "ed": 253, "text": "method achieves"}]
[{"st": 42, "ed": 46, "text": "expectation maximization em algorithm"}, {"st": 59, "ed": 61, "text": "stationary points"}, {"st": 71, "ed": 73, "text": "mean field"}, {"st": 111, "ed": 113, "text": "mean field"}, {"st": 121, "ed": 123, "text": "numerical experiments"}, {"st": 166, "ed": 168, "text": "probabilistic model"}]
[{"st": 13, "ed": 15, "text": "lipschitz continuous"}, {"st": 49, "ed": 51, "text": "low dimensional"}, {"st": 77, "ed": 79, "text": "handwritten digit"}, {"st": 82, "ed": 84, "text": "training sets"}]
[{"st": 2, "ed": 4, "text": "vision tasks"}, {"st": 9, "ed": 11, "text": "object recognition"}, {"st": 21, "ed": 23, "text": "low dimensional"}, {"st": 23, "ed": 25, "text": "linear subspaces"}, {"st": 30, "ed": 32, "text": "image space"}, {"st": 46, "ed": 48, "text": "ell 1"}, {"st": 61, "ed": 63, "text": "significantly lower"}, {"st": 63, "ed": 65, "text": "dimensional spaces"}, {"st": 90, "ed": 92, "text": "preliminary experiments"}, {"st": 96, "ed": 98, "text": "digit recognition"}]
[{"st": 34, "ed": 36, "text": "parameter estimation"}, {"st": 40, "ed": 42, "text": "kernel functions"}, {"st": 59, "ed": 61, "text": "open research"}, {"st": 72, "ed": 74, "text": "kernel function"}, {"st": 141, "ed": 143, "text": "living organisms"}]
[{"st": 64, "ed": 66, "text": "underlying structure"}, {"st": 115, "ed": 117, "text": "closely related"}]
[{"st": 1, "ed": 3, "text": "compressed sensing"}, {"st": 8, "ed": 10, "text": "sparse signal"}, {"st": 12, "ed": 14, "text": "observed data"}, {"st": 17, "ed": 19, "text": "sparse coding"}, {"st": 36, "ed": 38, "text": "sparse linear"}, {"st": 81, "ed": 83, "text": "sparse recovery"}, {"st": 107, "ed": 109, "text": "natural image"}]
[{"st": 22, "ed": 24, "text": "face perception"}, {"st": 42, "ed": 44, "text": "autism spectrum"}, {"st": 120, "ed": 122, "text": "supervised learning"}, {"st": 130, "ed": 133, "text": "support vector machine"}, {"st": 147, "ed": 149, "text": "cross validation"}, {"st": 151, "ed": 153, "text": "classification algorithm"}, {"st": 173, "ed": 175, "text": "proposed method"}, {"st": 177, "ed": 179, "text": "classification accuracies"}, {"st": 189, "ed": 191, "text": "proposed method"}]
[{"st": 1, "ed": 3, "text": "remote sensing"}, {"st": 8, "ed": 10, "text": "spectral resolution"}, {"st": 47, "ed": 49, "text": "data fusion"}, {"st": 57, "ed": 59, "text": "active research"}, {"st": 83, "ed": 85, "text": "convex objective"}, {"st": 115, "ed": 117, "text": "total variation"}, {"st": 179, "ed": 181, "text": "low dimensional"}, {"st": 198, "ed": 204, "text": "alternating direction method of multipliers admm"}, {"st": 206, "ed": 208, "text": "optimization problem"}, {"st": 256, "ed": 259, "text": "simulated and real"}]
[{"st": 11, "ed": 13, "text": "l 2"}, {"st": 14, "ed": 16, "text": "norm minimization"}, {"st": 25, "ed": 27, "text": "low rank"}, {"st": 44, "ed": 46, "text": "l 2"}, {"st": 47, "ed": 49, "text": "norm minimization"}, {"st": 64, "ed": 66, "text": "observed data"}, {"st": 68, "ed": 70, "text": "low rank"}, {"st": 90, "ed": 92, "text": "convergence rate"}, {"st": 104, "ed": 106, "text": "step size"}, {"st": 139, "ed": 141, "text": "numerical experiments"}, {"st": 142, "ed": 146, "text": "synthetic and real data"}]
[{"st": 0, "ed": 2, "text": "variational methods"}, {"st": 16, "ed": 18, "text": "highly successful"}, {"st": 26, "ed": 28, "text": "super resolution"}, {"st": 30, "ed": 32, "text": "optical flow"}, {"st": 67, "ed": 69, "text": "input data"}, {"st": 140, "ed": 142, "text": "total variation"}, {"st": 146, "ed": 148, "text": "ell 1"}, {"st": 172, "ed": 175, "text": "taking into account"}, {"st": 222, "ed": 224, "text": "recent results"}]
[{"st": 0, "ed": 2, "text": "prior distributions"}, {"st": 4, "ed": 6, "text": "natural images"}, {"st": 31, "ed": 33, "text": "nearest neighbor"}, {"st": 35, "ed": 37, "text": "nearest neighbor"}, {"st": 103, "ed": 105, "text": "mean field"}, {"st": 119, "ed": 121, "text": "mean field"}, {"st": 121, "ed": 123, "text": "method called"}, {"st": 135, "ed": 137, "text": "monte carlo"}, {"st": 159, "ed": 161, "text": "receptive fields"}]
[{"st": 3, "ed": 5, "text": "image segmentation"}, {"st": 11, "ed": 14, "text": "loopy belief propagation"}, {"st": 36, "ed": 38, "text": "proposed method"}]
[{"st": 10, "ed": 12, "text": "most probable"}, {"st": 16, "ed": 19, "text": "markov random field"}, {"st": 25, "ed": 27, "text": "energy minimization"}, {"st": 34, "ed": 36, "text": "np hard"}, {"st": 119, "ed": 121, "text": "theoretical properties"}, {"st": 123, "ed": 125, "text": "proposed approach"}]
[{"st": 7, "ed": 9, "text": "inverse problems"}, {"st": 11, "ed": 13, "text": "image processing"}, {"st": 13, "ed": 15, "text": "social networks"}, {"st": 15, "ed": 17, "text": "compressive sensing"}, {"st": 74, "ed": 76, "text": "low rank"}, {"st": 124, "ed": 126, "text": "sparse representation"}, {"st": 142, "ed": 144, "text": "penalty term"}, {"st": 154, "ed": 156, "text": "ell 1"}, {"st": 159, "ed": 161, "text": "nuclear norm"}, {"st": 180, "ed": 182, "text": "penalty term"}, {"st": 190, "ed": 192, "text": "cost function"}, {"st": 206, "ed": 208, "text": "cost function"}, {"st": 217, "ed": 219, "text": "low rank"}, {"st": 228, "ed": 230, "text": "alternating minimization"}, {"st": 230, "ed": 234, "text": "method of multipliers admm"}, {"st": 244, "ed": 246, "text": "experiments conducted"}, {"st": 248, "ed": 251, "text": "simulated and real"}]
[{"st": 6, "ed": 8, "text": "linear subspaces"}, {"st": 26, "ed": 28, "text": "gaussian noise"}, {"st": 47, "ed": 49, "text": "low rank"}, {"st": 78, "ed": 80, "text": "low noise"}, {"st": 95, "ed": 98, "text": "provide sufficient conditions"}, {"st": 103, "ed": 105, "text": "low noise"}, {"st": 116, "ed": 118, "text": "classification error"}, {"st": 162, "ed": 164, "text": "numerical results"}, {"st": 182, "ed": 184, "text": "synthetic data"}, {"st": 187, "ed": 189, "text": "motion segmentation"}, {"st": 191, "ed": 193, "text": "hand written"}, {"st": 193, "ed": 195, "text": "digit classification"}]
[{"st": 5, "ed": 7, "text": "linear models"}, {"st": 8, "ed": 10, "text": "high dimensional"}, {"st": 15, "ed": 17, "text": "computer vision"}, {"st": 29, "ed": 32, "text": "principal components analysis"}, {"st": 33, "ed": 35, "text": "low dimensional"}, {"st": 135, "ed": 137, "text": "method outperforms"}]
[{"st": 0, "ed": 4, "text": "convolutional neural networks cnn"}, {"st": 44, "ed": 46, "text": "image classification"}, {"st": 49, "ed": 53, "text": "multiple instance learning mil"}, {"st": 102, "ed": 104, "text": "pooling layers"}, {"st": 113, "ed": 116, "text": "end to end"}, {"st": 118, "ed": 120, "text": "previous methods"}]
[{"st": 19, "ed": 21, "text": "large datasets"}, {"st": 38, "ed": 40, "text": "computational cost"}, {"st": 59, "ed": 61, "text": "gaussian noise"}, {"st": 74, "ed": 76, "text": "bayesian posterior"}, {"st": 102, "ed": 104, "text": "detailed balance"}, {"st": 110, "ed": 112, "text": "recent studies"}, {"st": 117, "ed": 119, "text": "detailed balance"}, {"st": 141, "ed": 143, "text": "detailed balance"}]
[{"st": 88, "ed": 90, "text": "configuration space"}]
[{"st": 91, "ed": 93, "text": "graphical model"}, {"st": 106, "ed": 108, "text": "observed data"}, {"st": 133, "ed": 135, "text": "multi layer"}]
[{"st": 1, "ed": 3, "text": "magnetic resonance"}, {"st": 16, "ed": 18, "text": "white matter"}, {"st": 125, "ed": 127, "text": "graph matching"}, {"st": 132, "ed": 134, "text": "loss function"}, {"st": 155, "ed": 157, "text": "promising results"}, {"st": 161, "ed": 163, "text": "proposed method"}, {"st": 165, "ed": 167, "text": "simulated annealing"}]
[{"st": 24, "ed": 26, "text": "existing solutions"}, {"st": 86, "ed": 88, "text": "steady state"}, {"st": 129, "ed": 131, "text": "feature extraction"}, {"st": 131, "ed": 133, "text": "feature selection"}]
[{"st": 3, "ed": 5, "text": "image restoration"}, {"st": 16, "ed": 18, "text": "transfer function"}, {"st": 24, "ed": 26, "text": "image restoration"}, {"st": 54, "ed": 56, "text": "image restoration"}, {"st": 85, "ed": 87, "text": "image restoration"}]
[{"st": 4, "ed": 8, "text": "robust principal component analysis"}, {"st": 32, "ed": 34, "text": "level set"}, {"st": 36, "ed": 38, "text": "convex optimization"}, {"st": 41, "ed": 43, "text": "theoretical results"}, {"st": 60, "ed": 62, "text": "numerical experiments"}, {"st": 63, "ed": 67, "text": "simulated and real world"}]
[{"st": 5, "ed": 7, "text": "pattern recognition"}, {"st": 33, "ed": 35, "text": "decision tree"}, {"st": 35, "ed": 37, "text": "pattern recognition"}, {"st": 41, "ed": 43, "text": "training set"}, {"st": 56, "ed": 58, "text": "training set"}, {"st": 113, "ed": 115, "text": "probability distributions"}, {"st": 135, "ed": 137, "text": "randomly selected"}]
[{"st": 3, "ed": 5, "text": "riemannian manifold"}, {"st": 16, "ed": 18, "text": "mathbb r"}, {"st": 69, "ed": 71, "text": "ricci curvature"}, {"st": 87, "ed": 89, "text": "mathbb r"}, {"st": 102, "ed": 104, "text": "gaussian curvature"}]
[{"st": 15, "ed": 17, "text": "underlying structure"}, {"st": 19, "ed": 21, "text": "starting point"}, {"st": 99, "ed": 101, "text": "textual description"}]
[{"st": 0, "ed": 2, "text": "big data"}, {"st": 39, "ed": 41, "text": "extracting information"}, {"st": 73, "ed": 76, "text": "problem of classifying"}, {"st": 86, "ed": 89, "text": "linear discriminant analysis"}, {"st": 112, "ed": 114, "text": "proposed method"}, {"st": 124, "ed": 127, "text": "principal component analysis"}, {"st": 160, "ed": 162, "text": "big data"}]
[{"st": 1, "ed": 3, "text": "multi view"}, {"st": 46, "ed": 48, "text": "multi view"}, {"st": 57, "ed": 59, "text": "hierarchical structure"}, {"st": 60, "ed": 62, "text": "multiple views"}]
[{"st": 24, "ed": 26, "text": "image processing"}, {"st": 32, "ed": 34, "text": "ell 1"}, {"st": 50, "ed": 52, "text": "total variation"}, {"st": 58, "ed": 60, "text": "special cases"}, {"st": 114, "ed": 116, "text": "affine transformation"}, {"st": 128, "ed": 130, "text": "numerical simulations"}, {"st": 131, "ed": 133, "text": "image restoration"}]
[{"st": 11, "ed": 13, "text": "accurate predictions"}, {"st": 36, "ed": 38, "text": "total variation"}, {"st": 50, "ed": 52, "text": "local neighborhood"}, {"st": 68, "ed": 70, "text": "classification problems"}, {"st": 77, "ed": 79, "text": "total variation"}]
[{"st": 11, "ed": 13, "text": "low dimensional"}, {"st": 22, "ed": 24, "text": "sufficient conditions"}, {"st": 40, "ed": 42, "text": "low noise"}, {"st": 96, "ed": 98, "text": "low noise"}, {"st": 133, "ed": 135, "text": "low noise"}, {"st": 147, "ed": 151, "text": "synthetic and real data"}, {"st": 180, "ed": 182, "text": "low noise"}]
[{"st": 2, "ed": 4, "text": "clustering problems"}, {"st": 16, "ed": 18, "text": "point set"}, {"st": 83, "ed": 85, "text": "clustering problems"}, {"st": 103, "ed": 105, "text": "objective function"}]
[{"st": 35, "ed": 37, "text": "online algorithm"}, {"st": 93, "ed": 95, "text": "computationally expensive"}]
[{"st": 20, "ed": 22, "text": "higher order"}, {"st": 74, "ed": 76, "text": "global optimal"}, {"st": 79, "ed": 81, "text": "alternating optimization"}, {"st": 85, "ed": 87, "text": "computationally expensive"}, {"st": 101, "ed": 103, "text": "classification performance"}, {"st": 105, "ed": 107, "text": "computation cost"}]
[{"st": 97, "ed": 99, "text": "classification tasks"}]
[{"st": 13, "ed": 17, "text": "functional magnetic resonance imaging"}, {"st": 18, "ed": 20, "text": "widely adopted"}, {"st": 63, "ed": 65, "text": "neural network"}, {"st": 80, "ed": 83, "text": "deep neural network"}, {"st": 102, "ed": 105, "text": "proof of concept"}]
[{"st": 6, "ed": 10, "text": "dynamic time warping dtw"}, {"st": 13, "ed": 15, "text": "successfully applied"}, {"st": 17, "ed": 19, "text": "pattern recognition"}, {"st": 23, "ed": 25, "text": "based clustering"}, {"st": 35, "ed": 37, "text": "article presents"}, {"st": 37, "ed": 39, "text": "sufficient conditions"}, {"st": 73, "ed": 75, "text": "statistical theory"}]
[{"st": 0, "ed": 2, "text": "optical flow"}, {"st": 33, "ed": 35, "text": "optical flow"}, {"st": 38, "ed": 40, "text": "ill posed"}, {"st": 55, "ed": 57, "text": "optical flow"}, {"st": 57, "ed": 59, "text": "inference problem"}, {"st": 79, "ed": 81, "text": "optical flow"}, {"st": 85, "ed": 87, "text": "optical flow"}, {"st": 94, "ed": 96, "text": "optical flow"}, {"st": 111, "ed": 113, "text": "linear model"}, {"st": 116, "ed": 118, "text": "gaussian noise"}, {"st": 131, "ed": 133, "text": "optical flow"}]
[{"st": 4, "ed": 6, "text": "gaussian process"}, {"st": 8, "ed": 10, "text": "accurately estimate"}, {"st": 11, "ed": 13, "text": "diffusion mri"}, {"st": 91, "ed": 93, "text": "linear interpolation"}]
[{"st": 5, "ed": 8, "text": "cryo electron microscopy"}, {"st": 27, "ed": 29, "text": "transfer function"}, {"st": 43, "ed": 45, "text": "main challenges"}, {"st": 52, "ed": 56, "text": "signal to noise ratio"}, {"st": 99, "ed": 101, "text": "mahalanobis distance"}, {"st": 113, "ed": 115, "text": "similarity measure"}, {"st": 141, "ed": 143, "text": "synthetic datasets"}]
[{"st": 59, "ed": 61, "text": "multi channel"}, {"st": 78, "ed": 80, "text": "face perception"}, {"st": 110, "ed": 112, "text": "autism spectrum"}, {"st": 116, "ed": 118, "text": "high anxiety"}]
[{"st": 0, "ed": 3, "text": "blind source separation"}, {"st": 42, "ed": 44, "text": "remote sensing"}, {"st": 55, "ed": 57, "text": "intra class"}, {"st": 81, "ed": 83, "text": "intra class"}, {"st": 116, "ed": 119, "text": "blind source separation"}, {"st": 133, "ed": 135, "text": "intra class"}, {"st": 161, "ed": 163, "text": "extended versions"}, {"st": 195, "ed": 197, "text": "synthetic data"}, {"st": 230, "ed": 232, "text": "real data"}]
[]
[{"st": 30, "ed": 32, "text": "computational burden"}, {"st": 63, "ed": 65, "text": "structural properties"}, {"st": 87, "ed": 89, "text": "low rank"}, {"st": 91, "ed": 93, "text": "low variance"}, {"st": 101, "ed": 104, "text": "low rank matrix"}, {"st": 198, "ed": 200, "text": "medium sized"}, {"st": 220, "ed": 222, "text": "larger datasets"}, {"st": 262, "ed": 264, "text": "multi core"}]
[{"st": 1, "ed": 3, "text": "signal processing"}, {"st": 10, "ed": 12, "text": "multi dimensional"}, {"st": 15, "ed": 18, "text": "taking into account"}, {"st": 33, "ed": 35, "text": "dimensionality reduction"}, {"st": 60, "ed": 62, "text": "geometric structure"}, {"st": 65, "ed": 67, "text": "functional connectivity"}, {"st": 100, "ed": 102, "text": "sampling methods"}, {"st": 109, "ed": 112, "text": "principal component analysis"}, {"st": 114, "ed": 117, "text": "independent component analysis"}]
[{"st": 5, "ed": 7, "text": "multivariate regression"}, {"st": 72, "ed": 74, "text": "linear regression"}, {"st": 85, "ed": 87, "text": "optimization problem"}, {"st": 123, "ed": 125, "text": "critical point"}, {"st": 135, "ed": 138, "text": "synthetic and real"}, {"st": 140, "ed": 142, "text": "imaging data"}, {"st": 149, "ed": 151, "text": "multivariate regression"}]
[{"st": 8, "ed": 10, "text": "dynamic range"}, {"st": 15, "ed": 18, "text": "field of view"}, {"st": 19, "ed": 21, "text": "dynamic range"}, {"st": 47, "ed": 50, "text": "end to end"}, {"st": 50, "ed": 53, "text": "deep neural network"}, {"st": 58, "ed": 61, "text": "field of view"}, {"st": 66, "ed": 68, "text": "strong assumptions"}, {"st": 118, "ed": 121, "text": "deep neural network"}, {"st": 134, "ed": 137, "text": "field of view"}, {"st": 141, "ed": 143, "text": "fine tune"}, {"st": 169, "ed": 171, "text": "significantly outperform"}, {"st": 194, "ed": 196, "text": "photo realistic"}]
[{"st": 4, "ed": 6, "text": "real world"}, {"st": 30, "ed": 32, "text": "extremely high"}, {"st": 47, "ed": 49, "text": "deep learning"}, {"st": 49, "ed": 51, "text": "neural network"}, {"st": 63, "ed": 65, "text": "ms coco"}, {"st": 109, "ed": 111, "text": "computer vision"}, {"st": 120, "ed": 122, "text": "ms coco"}, {"st": 125, "ed": 127, "text": "significantly outperforms"}]
[{"st": 55, "ed": 57, "text": "big data"}, {"st": 63, "ed": 65, "text": "highly efficient"}, {"st": 65, "ed": 67, "text": "machine learning"}, {"st": 68, "ed": 70, "text": "image analysis"}, {"st": 83, "ed": 85, "text": "machine learning"}, {"st": 110, "ed": 112, "text": "computer science"}, {"st": 141, "ed": 143, "text": "main challenges"}, {"st": 150, "ed": 152, "text": "machine learning"}, {"st": 153, "ed": 155, "text": "image analysis"}]
[{"st": 32, "ed": 34, "text": "fully automated"}, {"st": 50, "ed": 52, "text": "deep learning"}, {"st": 62, "ed": 65, "text": "optical character recognition"}, {"st": 98, "ed": 100, "text": "fully automatic"}, {"st": 117, "ed": 119, "text": "fully automatic"}]
[{"st": 3, "ed": 5, "text": "matrix factorization"}, {"st": 50, "ed": 52, "text": "matrix factorization"}, {"st": 102, "ed": 104, "text": "medical imaging"}, {"st": 115, "ed": 117, "text": "deep networks"}, {"st": 151, "ed": 153, "text": "network architecture"}]
[{"st": 2, "ed": 4, "text": "unseen classes"}, {"st": 17, "ed": 19, "text": "recent works"}, {"st": 20, "ed": 22, "text": "open set"}, {"st": 118, "ed": 120, "text": "visual features"}, {"st": 136, "ed": 138, "text": "supervised learning"}, {"st": 138, "ed": 140, "text": "zero shot"}, {"st": 141, "ed": 143, "text": "open set"}, {"st": 148, "ed": 150, "text": "benchmark datasets"}]
[{"st": 41, "ed": 43, "text": "inverse problems"}, {"st": 57, "ed": 59, "text": "image patches"}, {"st": 88, "ed": 90, "text": "missing data"}, {"st": 135, "ed": 137, "text": "dynamic range"}]
[{"st": 1, "ed": 4, "text": "visual object recognition"}, {"st": 46, "ed": 48, "text": "computer vision"}, {"st": 50, "ed": 53, "text": "deep neural networks"}, {"st": 58, "ed": 60, "text": "classification performance"}, {"st": 61, "ed": 63, "text": "object recognition"}, {"st": 159, "ed": 161, "text": "additive noise"}, {"st": 171, "ed": 173, "text": "classification error"}, {"st": 225, "ed": 227, "text": "computer vision"}]
[{"st": 37, "ed": 39, "text": "feature extraction"}, {"st": 47, "ed": 49, "text": "dynamic range"}, {"st": 64, "ed": 66, "text": "energy efficient"}]
[{"st": 6, "ed": 8, "text": "sensitivity analysis"}, {"st": 16, "ed": 18, "text": "convex functions"}, {"st": 21, "ed": 23, "text": "geometric structure"}, {"st": 68, "ed": 70, "text": "optimization problems"}, {"st": 88, "ed": 90, "text": "image processing"}, {"st": 90, "ed": 92, "text": "machine learning"}, {"st": 114, "ed": 116, "text": "optimization problems"}, {"st": 117, "ed": 119, "text": "small perturbations"}, {"st": 131, "ed": 133, "text": "existing results"}, {"st": 136, "ed": 138, "text": "typically assume"}, {"st": 145, "ed": 147, "text": "active set"}, {"st": 154, "ed": 156, "text": "small perturbations"}, {"st": 180, "ed": 182, "text": "active set"}, {"st": 209, "ed": 211, "text": "ill posed"}, {"st": 211, "ed": 213, "text": "inverse problems"}, {"st": 227, "ed": 229, "text": "theoretical results"}, {"st": 231, "ed": 233, "text": "numerical simulations"}, {"st": 249, "ed": 251, "text": "low dimensional"}]
[{"st": 56, "ed": 58, "text": "deep learning"}, {"st": 65, "ed": 68, "text": "end to end"}, {"st": 73, "ed": 75, "text": "hand crafted"}, {"st": 121, "ed": 125, "text": "signal to noise ratio"}, {"st": 147, "ed": 149, "text": "convolutional networks"}, {"st": 169, "ed": 171, "text": "network architecture"}, {"st": 186, "ed": 188, "text": "spatio temporal"}, {"st": 200, "ed": 202, "text": "classification performance"}, {"st": 268, "ed": 270, "text": "classification performance"}]
[{"st": 16, "ed": 19, "text": "mild cognitive impairment"}, {"st": 34, "ed": 36, "text": "spherical harmonics"}, {"st": 49, "ed": 51, "text": "magnetic resonance"}, {"st": 55, "ed": 57, "text": "fully automatic"}, {"st": 74, "ed": 77, "text": "support vector machines"}, {"st": 81, "ed": 83, "text": "relevant features"}, {"st": 112, "ed": 114, "text": "standard deviation"}, {"st": 224, "ed": 226, "text": "recently published"}, {"st": 230, "ed": 232, "text": "classification methods"}]
[{"st": 1, "ed": 4, "text": "the past decade"}, {"st": 6, "ed": 8, "text": "significant progress"}, {"st": 9, "ed": 11, "text": "computational power"}, {"st": 18, "ed": 20, "text": "deep learning"}, {"st": 22, "ed": 24, "text": "increasingly popular"}, {"st": 27, "ed": 29, "text": "excellent performance"}, {"st": 30, "ed": 32, "text": "computer vision"}, {"st": 37, "ed": 40, "text": "protein data bank"}, {"st": 62, "ed": 64, "text": "amino acid"}, {"st": 65, "ed": 67, "text": "amino acid"}, {"st": 94, "ed": 97, "text": "convolutional neural networks"}, {"st": 142, "ed": 145, "text": "protein data bank"}, {"st": 165, "ed": 169, "text": "available at https github.com"}]
[{"st": 10, "ed": 12, "text": "lip reading"}, {"st": 63, "ed": 66, "text": "hidden markov models"}]
[{"st": 1, "ed": 3, "text": "main goal"}, {"st": 21, "ed": 23, "text": "connectivity patterns"}, {"st": 35, "ed": 37, "text": "deep architecture"}, {"st": 44, "ed": 46, "text": "connectivity patterns"}, {"st": 47, "ed": 49, "text": "human brain"}, {"st": 70, "ed": 72, "text": "multi resolution"}, {"st": 142, "ed": 144, "text": "deep learning"}, {"st": 150, "ed": 152, "text": "auto encoder"}, {"st": 179, "ed": 181, "text": "learned representations"}, {"st": 247, "ed": 249, "text": "cluster centers"}]
[{"st": 13, "ed": 15, "text": "large scale"}, {"st": 42, "ed": 45, "text": "deep neural network"}, {"st": 48, "ed": 51, "text": "generative adversarial network"}, {"st": 66, "ed": 69, "text": "2d and 3d"}, {"st": 72, "ed": 74, "text": "key feature"}, {"st": 83, "ed": 85, "text": "low dimensional"}, {"st": 99, "ed": 103, "text": "markov chain monte carlo"}, {"st": 108, "ed": 111, "text": "2d and 3d"}, {"st": 153, "ed": 155, "text": "case studies"}, {"st": 157, "ed": 159, "text": "steady state"}]
[{"st": 31, "ed": 33, "text": "spike timing"}, {"st": 37, "ed": 39, "text": "lateral inhibition"}, {"st": 84, "ed": 86, "text": "continual learning"}, {"st": 132, "ed": 134, "text": "biological neural"}, {"st": 139, "ed": 142, "text": "biological neural network"}, {"st": 144, "ed": 147, "text": "orders of magnitude"}, {"st": 154, "ed": 156, "text": "biological neural"}, {"st": 159, "ed": 161, "text": "neural networks"}, {"st": 221, "ed": 223, "text": "starting point"}, {"st": 232, "ed": 234, "text": "machine learning"}]
[{"st": 1, "ed": 3, "text": "recent years"}, {"st": 22, "ed": 24, "text": "machine learning"}, {"st": 25, "ed": 27, "text": "feature extraction"}, {"st": 74, "ed": 76, "text": "pre processing"}, {"st": 76, "ed": 78, "text": "performance metrics"}, {"st": 79, "ed": 81, "text": "cross validation"}, {"st": 133, "ed": 135, "text": "feature extraction"}, {"st": 137, "ed": 139, "text": "feature extraction"}, {"st": 162, "ed": 164, "text": "feature extraction"}, {"st": 180, "ed": 182, "text": "mr images"}, {"st": 198, "ed": 200, "text": "feature extraction"}, {"st": 204, "ed": 206, "text": "classification techniques"}, {"st": 212, "ed": 215, "text": "multiple kernel learning"}]
[{"st": 78, "ed": 81, "text": "spatial and temporal"}, {"st": 111, "ed": 114, "text": "expectation maximization algorithm"}, {"st": 170, "ed": 172, "text": "accurate prediction"}]
[{"st": 77, "ed": 79, "text": "edge detection"}, {"st": 112, "ed": 114, "text": "raw image"}, {"st": 123, "ed": 125, "text": "edge detection"}]
[{"st": 8, "ed": 10, "text": "machine learning"}, {"st": 52, "ed": 54, "text": "extracted features"}, {"st": 86, "ed": 88, "text": "feature extraction"}, {"st": 102, "ed": 104, "text": "error correcting"}, {"st": 154, "ed": 156, "text": "proposed approach"}]
[{"st": 1, "ed": 3, "text": "image reconstruction"}, {"st": 12, "ed": 14, "text": "inverse problem"}, {"st": 21, "ed": 24, "text": "deep neural networks"}, {"st": 31, "ed": 33, "text": "computer vision"}, {"st": 47, "ed": 49, "text": "deep residual"}, {"st": 49, "ed": 52, "text": "convolutional neural network"}, {"st": 55, "ed": 57, "text": "image quality"}, {"st": 69, "ed": 71, "text": "proposed method"}, {"st": 76, "ed": 78, "text": "neural network"}, {"st": 80, "ed": 82, "text": "iterative reconstruction"}, {"st": 84, "ed": 86, "text": "image representation"}, {"st": 92, "ed": 94, "text": "post processing"}, {"st": 98, "ed": 100, "text": "objective function"}, {"st": 103, "ed": 105, "text": "optimization problem"}, {"st": 110, "ed": 116, "text": "alternating direction method of multipliers admm"}, {"st": 122, "ed": 124, "text": "real data"}, {"st": 138, "ed": 140, "text": "neural network"}, {"st": 144, "ed": 146, "text": "neural network"}, {"st": 150, "ed": 152, "text": "maximum likelihood"}]
[{"st": 39, "ed": 41, "text": "previous methods"}, {"st": 59, "ed": 63, "text": "singular value decomposition svd"}, {"st": 64, "ed": 67, "text": "stochastic gradient descent"}, {"st": 77, "ed": 79, "text": "large datasets"}, {"st": 85, "ed": 87, "text": "training data"}, {"st": 108, "ed": 110, "text": "method achieves"}]
[{"st": 136, "ed": 138, "text": "visual attention"}]
[{"st": 31, "ed": 33, "text": "main goal"}, {"st": 41, "ed": 43, "text": "classification algorithm"}, {"st": 45, "ed": 47, "text": "sinus rhythm"}, {"st": 77, "ed": 80, "text": "convolutional neural networks"}, {"st": 83, "ed": 87, "text": "convolutional neural network cnn"}, {"st": 108, "ed": 110, "text": "training data"}, {"st": 119, "ed": 121, "text": "low quality"}, {"st": 154, "ed": 156, "text": "feature based"}, {"st": 156, "ed": 158, "text": "post processing"}]
[{"st": 6, "ed": 8, "text": "image denoising"}, {"st": 12, "ed": 15, "text": "markov random field"}, {"st": 29, "ed": 31, "text": "image denoising"}, {"st": 31, "ed": 33, "text": "problems including"}, {"st": 78, "ed": 80, "text": "numerical experiments"}]
[{"st": 44, "ed": 46, "text": "fully convolutional"}, {"st": 55, "ed": 58, "text": "field of view"}, {"st": 93, "ed": 95, "text": "source code"}]
[{"st": 0, "ed": 5, "text": "deep convolutional neural networks cnns"}, {"st": 7, "ed": 9, "text": "impressive performance"}, {"st": 10, "ed": 12, "text": "visual object"}, {"st": 63, "ed": 65, "text": "highly complex"}, {"st": 108, "ed": 110, "text": "convolutional filters"}, {"st": 133, "ed": 135, "text": "receptive field"}]
[{"st": 37, "ed": 39, "text": "parametric models"}, {"st": 67, "ed": 69, "text": "computer aided"}, {"st": 80, "ed": 82, "text": "smart grid"}, {"st": 86, "ed": 88, "text": "deep learning"}, {"st": 97, "ed": 100, "text": "deep neural network"}, {"st": 113, "ed": 115, "text": "west bengal"}, {"st": 119, "ed": 121, "text": "prediction performance"}]
[{"st": 10, "ed": 12, "text": "riemannian geometry"}, {"st": 16, "ed": 18, "text": "computational complexity"}, {"st": 35, "ed": 37, "text": "closed form"}, {"st": 55, "ed": 57, "text": "recently introduced"}, {"st": 59, "ed": 61, "text": "parallel transport"}, {"st": 65, "ed": 67, "text": "riemannian manifold"}, {"st": 68, "ed": 70, "text": "finite dimensional"}, {"st": 76, "ed": 79, "text": "qualitative and quantitative"}]
[{"st": 8, "ed": 10, "text": "dimension reduction"}, {"st": 16, "ed": 18, "text": "subspace learning"}, {"st": 31, "ed": 35, "text": "singular value decomposition svd"}, {"st": 39, "ed": 41, "text": "subspace learning"}, {"st": 50, "ed": 52, "text": "robust subspace"}, {"st": 54, "ed": 56, "text": "robust pca"}, {"st": 69, "ed": 71, "text": "lower dimensional"}, {"st": 100, "ed": 102, "text": "low dimensional"}, {"st": 126, "ed": 128, "text": "robust subspace"}, {"st": 143, "ed": 145, "text": "robust subspace"}, {"st": 161, "ed": 164, "text": "low rank matrix"}, {"st": 172, "ed": 174, "text": "robust subspace"}]
[{"st": 7, "ed": 9, "text": "clustering techniques"}, {"st": 153, "ed": 156, "text": "k means clustering"}, {"st": 161, "ed": 163, "text": "clustering techniques"}, {"st": 172, "ed": 174, "text": "north carolina"}, {"st": 205, "ed": 207, "text": "empirical study"}]
[{"st": 5, "ed": 7, "text": "convex relaxations"}, {"st": 9, "ed": 11, "text": "optimisation problems"}, {"st": 16, "ed": 18, "text": "semidefinite programming"}, {"st": 61, "ed": 63, "text": "convex relaxation"}, {"st": 93, "ed": 95, "text": "problems including"}]
[{"st": 2, "ed": 4, "text": "image reconstruction"}, {"st": 25, "ed": 27, "text": "image patches"}, {"st": 28, "ed": 30, "text": "neural networks"}, {"st": 35, "ed": 37, "text": "prior information"}, {"st": 49, "ed": 51, "text": "variational autoencoders"}, {"st": 58, "ed": 60, "text": "image patches"}, {"st": 80, "ed": 82, "text": "proposed algorithm"}, {"st": 111, "ed": 113, "text": "white matter"}, {"st": 192, "ed": 194, "text": "white matter"}, {"st": 227, "ed": 230, "text": "method compares favorably"}, {"st": 244, "ed": 246, "text": "prior probability"}, {"st": 246, "ed": 248, "text": "map estimation"}, {"st": 248, "ed": 250, "text": "machine learning"}, {"st": 250, "ed": 252, "text": "variational inference"}, {"st": 252, "ed": 254, "text": "deep learning"}]
[{"st": 1, "ed": 4, "text": "image to image"}, {"st": 11, "ed": 13, "text": "input image"}, {"st": 34, "ed": 36, "text": "conditional generative"}, {"st": 47, "ed": 50, "text": "low dimensional latent"}, {"st": 70, "ed": 72, "text": "latent code"}, {"st": 84, "ed": 86, "text": "latent code"}, {"st": 99, "ed": 101, "text": "latent code"}, {"st": 112, "ed": 114, "text": "mode collapse"}, {"st": 131, "ed": 133, "text": "network architectures"}, {"st": 141, "ed": 143, "text": "proposed method"}]
[{"st": 1, "ed": 3, "text": "image reconstruction"}, {"st": 44, "ed": 46, "text": "image reconstruction"}, {"st": 48, "ed": 50, "text": "generative adversarial"}, {"st": 50, "ed": 52, "text": "neural networks"}, {"st": 59, "ed": 61, "text": "unsupervised learning"}, {"st": 79, "ed": 81, "text": "fully convolutional"}, {"st": 81, "ed": 83, "text": "neural network"}, {"st": 99, "ed": 101, "text": "image generation"}, {"st": 145, "ed": 148, "text": "fast and accurate"}, {"st": 152, "ed": 154, "text": "image dataset"}, {"st": 168, "ed": 170, "text": "generative network"}]
[]
[{"st": 3, "ed": 5, "text": "robust subspace"}, {"st": 14, "ed": 16, "text": "provable guarantees"}, {"st": 32, "ed": 34, "text": "low dimensional"}, {"st": 62, "ed": 64, "text": "robust pca"}, {"st": 73, "ed": 75, "text": "online algorithm"}, {"st": 97, "ed": 99, "text": "robust pca"}, {"st": 127, "ed": 129, "text": "low rank"}, {"st": 138, "ed": 140, "text": "near optimal"}, {"st": 220, "ed": 223, "text": "theoretically and empirically"}]
[{"st": 6, "ed": 8, "text": "functional imaging"}, {"st": 17, "ed": 19, "text": "quantitative results"}, {"st": 39, "ed": 41, "text": "magnetic resonance"}, {"st": 41, "ed": 43, "text": "mr images"}, {"st": 55, "ed": 58, "text": "deep neural network"}, {"st": 75, "ed": 77, "text": "mr images"}, {"st": 83, "ed": 85, "text": "u net"}, {"st": 92, "ed": 94, "text": "patient data"}, {"st": 119, "ed": 121, "text": "mr images"}, {"st": 123, "ed": 125, "text": "u net"}, {"st": 132, "ed": 134, "text": "network structure"}, {"st": 154, "ed": 156, "text": "quantitative analysis"}, {"st": 160, "ed": 162, "text": "patient data"}, {"st": 178, "ed": 180, "text": "network structure"}, {"st": 190, "ed": 192, "text": "u net"}]
[{"st": 9, "ed": 11, "text": "lighting conditions"}, {"st": 17, "ed": 19, "text": "machine learning"}, {"st": 21, "ed": 24, "text": "unsupervised domain adaptation"}, {"st": 29, "ed": 31, "text": "current approaches"}, {"st": 78, "ed": 80, "text": "domain adaptation"}, {"st": 100, "ed": 102, "text": "empirically demonstrate"}, {"st": 138, "ed": 140, "text": "source domain"}, {"st": 142, "ed": 145, "text": "generative adversarial network"}, {"st": 162, "ed": 164, "text": "training data"}]
[{"st": 13, "ed": 15, "text": "multiple objects"}, {"st": 29, "ed": 31, "text": "image analysis"}, {"st": 48, "ed": 50, "text": "coarse grained"}, {"st": 50, "ed": 52, "text": "group level"}, {"st": 62, "ed": 66, "text": "convolutional neural networks cnns"}, {"st": 84, "ed": 86, "text": "raw image"}, {"st": 98, "ed": 100, "text": "u net"}, {"st": 102, "ed": 104, "text": "loss function"}, {"st": 133, "ed": 135, "text": "network size"}, {"st": 141, "ed": 143, "text": "u net"}, {"st": 153, "ed": 155, "text": "problem specific"}, {"st": 155, "ed": 157, "text": "image data"}, {"st": 159, "ed": 161, "text": "labeled examples"}, {"st": 239, "ed": 241, "text": "object tracking"}, {"st": 253, "ed": 255, "text": "time series"}, {"st": 255, "ed": 257, "text": "image data"}]
[{"st": 15, "ed": 17, "text": "important role"}, {"st": 25, "ed": 27, "text": "spatial information"}, {"st": 48, "ed": 50, "text": "spatial information"}, {"st": 67, "ed": 69, "text": "deep learning"}, {"st": 85, "ed": 87, "text": "spatial information"}, {"st": 114, "ed": 116, "text": "false alarms"}, {"st": 125, "ed": 127, "text": "false alarms"}, {"st": 178, "ed": 180, "text": "false alarm"}]
[{"st": 0, "ed": 2, "text": "feature extraction"}, {"st": 32, "ed": 34, "text": "signal processing"}, {"st": 34, "ed": 36, "text": "applications including"}, {"st": 92, "ed": 94, "text": "challenging task"}, {"st": 112, "ed": 114, "text": "based approach"}, {"st": 143, "ed": 145, "text": "error rate"}, {"st": 168, "ed": 170, "text": "feature extraction"}]
[{"st": 0, "ed": 3, "text": "deep neural networks"}, {"st": 9, "ed": 13, "text": "vulnerable to adversarial examples"}, {"st": 22, "ed": 24, "text": "adversarial examples"}, {"st": 40, "ed": 42, "text": "adversarial examples"}, {"st": 66, "ed": 68, "text": "adversarial examples"}, {"st": 69, "ed": 73, "text": "generative adversarial networks gans"}, {"st": 93, "ed": 95, "text": "adversarial perturbations"}, {"st": 104, "ed": 106, "text": "adversarial training"}, {"st": 143, "ed": 145, "text": "white box"}, {"st": 147, "ed": 149, "text": "black box"}, {"st": 166, "ed": 168, "text": "adversarial examples"}, {"st": 178, "ed": 180, "text": "success rate"}]
[{"st": 0, "ed": 2, "text": "recent studies"}, {"st": 6, "ed": 9, "text": "deep neural networks"}, {"st": 25, "ed": 27, "text": "adversarial examples"}, {"st": 65, "ed": 67, "text": "active research"}, {"st": 108, "ed": 110, "text": "distance measures"}, {"st": 112, "ed": 114, "text": "extensive experiments"}, {"st": 119, "ed": 121, "text": "adversarial examples"}, {"st": 168, "ed": 170, "text": "adversarial examples"}, {"st": 180, "ed": 182, "text": "deep networks"}, {"st": 186, "ed": 188, "text": "adversarial examples"}]
[{"st": 33, "ed": 35, "text": "automatic segmentation"}, {"st": 108, "ed": 110, "text": "detection method"}, {"st": 134, "ed": 136, "text": "feature extraction"}, {"st": 137, "ed": 139, "text": "matrix decomposition"}, {"st": 144, "ed": 146, "text": "method produces"}]
[{"st": 18, "ed": 22, "text": "convolutional neural networks cnn"}, {"st": 74, "ed": 76, "text": "classification models"}, {"st": 78, "ed": 80, "text": "neural networks"}, {"st": 136, "ed": 138, "text": "significantly reduce"}]
[{"st": 24, "ed": 26, "text": "log likelihood"}, {"st": 31, "ed": 34, "text": "gaussian mixture model"}, {"st": 39, "ed": 41, "text": "image patches"}, {"st": 56, "ed": 59, "text": "gaussian mixture model"}, {"st": 95, "ed": 97, "text": "computationally intensive"}, {"st": 120, "ed": 122, "text": "efficiently solved"}, {"st": 165, "ed": 168, "text": "tens of thousands"}, {"st": 171, "ed": 173, "text": "main contribution"}, {"st": 209, "ed": 211, "text": "image denoising"}]
[{"st": 0, "ed": 2, "text": "computed tomography"}, {"st": 26, "ed": 28, "text": "remains challenging"}, {"st": 55, "ed": 57, "text": "deep learning"}, {"st": 57, "ed": 61, "text": "convolutional neural networks cnn"}, {"st": 75, "ed": 77, "text": "lidc idri"}, {"st": 128, "ed": 130, "text": "highly accurate"}]
[{"st": 5, "ed": 7, "text": "loss function"}, {"st": 8, "ed": 10, "text": "training scheme"}, {"st": 28, "ed": 30, "text": "neural network"}, {"st": 57, "ed": 59, "text": "network outputs"}, {"st": 72, "ed": 75, "text": "false positive rate"}, {"st": 76, "ed": 78, "text": "significantly higher"}]
[{"st": 91, "ed": 94, "text": "convolutional neural network"}, {"st": 96, "ed": 99, "text": "fully convolutional network"}, {"st": 100, "ed": 103, "text": "encoder decoder architecture"}, {"st": 129, "ed": 131, "text": "significantly improved"}, {"st": 146, "ed": 148, "text": "generalization ability"}]
[{"st": 18, "ed": 21, "text": "generative adversarial networks"}, {"st": 47, "ed": 49, "text": "neural networks"}, {"st": 109, "ed": 111, "text": "ct images"}]
[{"st": 26, "ed": 28, "text": "noisy data"}, {"st": 32, "ed": 34, "text": "existing methods"}, {"st": 65, "ed": 67, "text": "method called"}, {"st": 85, "ed": 87, "text": "neural network"}, {"st": 91, "ed": 93, "text": "sparse linear"}, {"st": 116, "ed": 118, "text": "elastic net"}, {"st": 121, "ed": 123, "text": "promising results"}, {"st": 124, "ed": 127, "text": "simulated and real"}]
[{"st": 0, "ed": 2, "text": "autonomous driving"}, {"st": 17, "ed": 19, "text": "current methods"}, {"st": 41, "ed": 43, "text": "point cloud"}, {"st": 78, "ed": 82, "text": "convolutional neural network cnn"}, {"st": 129, "ed": 131, "text": "detection algorithms"}]
[{"st": 15, "ed": 17, "text": "similarity measure"}, {"st": 30, "ed": 32, "text": "systems theory"}, {"st": 33, "ed": 35, "text": "machine learning"}, {"st": 134, "ed": 136, "text": "input output"}, {"st": 173, "ed": 175, "text": "input output"}, {"st": 190, "ed": 192, "text": "machine learning"}, {"st": 195, "ed": 197, "text": "time series"}]
[{"st": 5, "ed": 7, "text": "classification models"}, {"st": 10, "ed": 12, "text": "online dating"}, {"st": 26, "ed": 29, "text": "takes advantage of"}, {"st": 66, "ed": 68, "text": "online dating"}, {"st": 72, "ed": 74, "text": "online dating"}, {"st": 76, "ed": 78, "text": "feature set"}, {"st": 111, "ed": 113, "text": "logistic regression"}]
[{"st": 17, "ed": 19, "text": "important role"}, {"st": 24, "ed": 26, "text": "multiple objects"}, {"st": 161, "ed": 163, "text": "experimentally demonstrate"}]
[{"st": 0, "ed": 3, "text": "cryo electron microscopy"}, {"st": 44, "ed": 46, "text": "based approaches"}, {"st": 56, "ed": 58, "text": "positive unlabeled"}, {"st": 58, "ed": 60, "text": "classification problem"}, {"st": 67, "ed": 71, "text": "convolutional neural network cnn"}, {"st": 97, "ed": 99, "text": "labeled data"}, {"st": 103, "ed": 105, "text": "machine learning"}, {"st": 113, "ed": 115, "text": "objective function"}, {"st": 137, "ed": 139, "text": "unlabeled data"}, {"st": 142, "ed": 145, "text": "stochastic gradient descent"}, {"st": 179, "ed": 181, "text": "training examples"}, {"st": 189, "ed": 191, "text": "large margin"}, {"st": 194, "ed": 196, "text": "labeled training"}, {"st": 209, "ed": 211, "text": "labeled data"}, {"st": 223, "ed": 225, "text": "positive unlabeled"}, {"st": 261, "ed": 263, "text": "objective function"}, {"st": 268, "ed": 270, "text": "positive unlabeled"}]
[{"st": 0, "ed": 2, "text": "big data"}, {"st": 16, "ed": 18, "text": "recent years"}, {"st": 76, "ed": 78, "text": "pre processing"}, {"st": 159, "ed": 161, "text": "learning process"}, {"st": 176, "ed": 178, "text": "distributed computing"}, {"st": 187, "ed": 189, "text": "decision tree"}, {"st": 189, "ed": 191, "text": "learning algorithm"}, {"st": 214, "ed": 216, "text": "data stream"}]
[{"st": 11, "ed": 13, "text": "topic models"}, {"st": 37, "ed": 39, "text": "latent topics"}, {"st": 50, "ed": 53, "text": "efficient and effective"}, {"st": 63, "ed": 65, "text": "topic models"}, {"st": 84, "ed": 86, "text": "unstructured text"}, {"st": 120, "ed": 122, "text": "case studies"}, {"st": 126, "ed": 128, "text": "basic research"}]
[{"st": 4, "ed": 6, "text": "large scale"}, {"st": 9, "ed": 11, "text": "spanish language"}, {"st": 57, "ed": 59, "text": "machine learning"}]
[{"st": 1, "ed": 3, "text": "generative models"}, {"st": 5, "ed": 8, "text": "latent dirichlet allocation"}, {"st": 13, "ed": 15, "text": "topic modeling"}, {"st": 35, "ed": 37, "text": "generative models"}, {"st": 50, "ed": 52, "text": "topic modeling"}, {"st": 58, "ed": 60, "text": "generative model"}, {"st": 78, "ed": 80, "text": "semi supervised"}, {"st": 88, "ed": 90, "text": "word level"}, {"st": 90, "ed": 92, "text": "domain knowledge"}, {"st": 136, "ed": 140, "text": "unsupervised and semi supervised"}]
[{"st": 0, "ed": 2, "text": "social media"}, {"st": 16, "ed": 18, "text": "public health"}, {"st": 31, "ed": 33, "text": "textual data"}, {"st": 91, "ed": 93, "text": "proposed framework"}, {"st": 95, "ed": 97, "text": "text mining"}, {"st": 98, "ed": 100, "text": "sentiment analysis"}, {"st": 101, "ed": 103, "text": "topic modeling"}, {"st": 163, "ed": 165, "text": "public health"}]
[{"st": 0, "ed": 2, "text": "social media"}, {"st": 15, "ed": 17, "text": "public health"}, {"st": 19, "ed": 21, "text": "social media"}, {"st": 46, "ed": 48, "text": "large scale"}, {"st": 49, "ed": 51, "text": "public health"}, {"st": 55, "ed": 57, "text": "challenging research"}, {"st": 177, "ed": 179, "text": "blood pressure"}, {"st": 194, "ed": 196, "text": "weight loss"}, {"st": 198, "ed": 200, "text": "mental health"}, {"st": 225, "ed": 227, "text": "social media"}, {"st": 244, "ed": 246, "text": "public health"}]
[{"st": 0, "ed": 2, "text": "text based"}, {"st": 49, "ed": 51, "text": "method called"}, {"st": 62, "ed": 64, "text": "text based"}, {"st": 74, "ed": 76, "text": "machine translation"}, {"st": 77, "ed": 80, "text": "generative adversarial networks"}, {"st": 85, "ed": 87, "text": "machine translation"}, {"st": 107, "ed": 110, "text": "propose and evaluate"}, {"st": 165, "ed": 167, "text": "proposed method"}]
[{"st": 33, "ed": 35, "text": "recent years"}, {"st": 35, "ed": 37, "text": "social media"}, {"st": 49, "ed": 51, "text": "social media"}, {"st": 61, "ed": 63, "text": "social media"}, {"st": 81, "ed": 83, "text": "social media"}, {"st": 90, "ed": 92, "text": "text mining"}, {"st": 104, "ed": 106, "text": "text mining"}, {"st": 107, "ed": 109, "text": "sentiment analysis"}, {"st": 113, "ed": 115, "text": "proposed approach"}]
[{"st": 0, "ed": 2, "text": "decision theory"}, {"st": 16, "ed": 18, "text": "prior probability"}, {"st": 84, "ed": 86, "text": "supervised learning"}]
[{"st": 107, "ed": 109, "text": "coding theory"}, {"st": 130, "ed": 132, "text": "random noise"}, {"st": 178, "ed": 180, "text": "natural extension"}, {"st": 190, "ed": 192, "text": "statistical properties"}, {"st": 241, "ed": 243, "text": "natural extension"}]
[{"st": 19, "ed": 21, "text": "past observations"}, {"st": 29, "ed": 31, "text": "bayes rule"}, {"st": 33, "ed": 35, "text": "true distribution"}, {"st": 79, "ed": 81, "text": "weighted sum"}, {"st": 90, "ed": 92, "text": "countable set"}, {"st": 166, "ed": 168, "text": "true distribution"}]
[{"st": 18, "ed": 20, "text": "main idea"}, {"st": 68, "ed": 71, "text": "taking into account"}]
[{"st": 1, "ed": 3, "text": "evolutionary algorithms"}, {"st": 27, "ed": 29, "text": "selection pressure"}, {"st": 51, "ed": 53, "text": "local optima"}, {"st": 71, "ed": 73, "text": "selection pressure"}]
[{"st": 0, "ed": 2, "text": "reinforcement learning"}, {"st": 19, "ed": 21, "text": "stochastic optimization"}]
[{"st": 20, "ed": 22, "text": "k 1"}, {"st": 81, "ed": 83, "text": "prediction errors"}, {"st": 136, "ed": 138, "text": "loss functions"}]
[{"st": 74, "ed": 76, "text": "sequential decision"}, {"st": 88, "ed": 90, "text": "expected reward"}, {"st": 96, "ed": 98, "text": "reinforcement learning"}, {"st": 112, "ed": 114, "text": "mixture distribution"}, {"st": 117, "ed": 119, "text": "weighted sum"}, {"st": 141, "ed": 143, "text": "bayes optimal"}, {"st": 176, "ed": 178, "text": "bayes optimal"}, {"st": 219, "ed": 222, "text": "markov decision processes"}]
[{"st": 47, "ed": 49, "text": "domain specific"}, {"st": 118, "ed": 120, "text": "tasks involving"}]
[{"st": 10, "ed": 12, "text": "past observations"}, {"st": 20, "ed": 22, "text": "bayes rule"}, {"st": 25, "ed": 27, "text": "generating distribution"}, {"st": 62, "ed": 64, "text": "weighted sum"}, {"st": 164, "ed": 166, "text": "bayes optimal"}, {"st": 176, "ed": 178, "text": "bayes optimal"}]
[{"st": 2, "ed": 4, "text": "artificial intelligence"}, {"st": 49, "ed": 51, "text": "problem solving"}, {"st": 51, "ed": 53, "text": "decision making"}, {"st": 54, "ed": 56, "text": "reinforcement learning"}, {"st": 85, "ed": 87, "text": "computer science"}]
[{"st": 16, "ed": 18, "text": "sequential decision"}]
[{"st": 8, "ed": 10, "text": "random variables"}, {"st": 32, "ed": 34, "text": "prior distribution"}, {"st": 39, "ed": 41, "text": "posterior distribution"}, {"st": 48, "ed": 50, "text": "incomplete data"}, {"st": 113, "ed": 115, "text": "posterior distribution"}, {"st": 142, "ed": 144, "text": "incremental learning"}, {"st": 145, "ed": 147, "text": "naive bayes"}, {"st": 156, "ed": 158, "text": "mutual information"}, {"st": 168, "ed": 170, "text": "mutual information"}, {"st": 175, "ed": 177, "text": "real data"}]
[{"st": 33, "ed": 35, "text": "past observations"}, {"st": 44, "ed": 46, "text": "chain rule"}, {"st": 49, "ed": 51, "text": "generating distribution"}, {"st": 91, "ed": 93, "text": "weighted sum"}, {"st": 103, "ed": 105, "text": "expected loss"}, {"st": 107, "ed": 109, "text": "bayes optimal"}, {"st": 125, "ed": 127, "text": "bayes optimal"}, {"st": 150, "ed": 152, "text": "significantly smaller"}, {"st": 156, "ed": 158, "text": "performance measures"}]
[{"st": 6, "ed": 9, "text": "minimum description length"}, {"st": 51, "ed": 54, "text": "universal turing machine"}, {"st": 90, "ed": 92, "text": "convergence speed"}, {"st": 137, "ed": 139, "text": "sufficient conditions"}]
[{"st": 8, "ed": 10, "text": "challenging problem"}, {"st": 191, "ed": 193, "text": "random variables"}]
[{"st": 25, "ed": 27, "text": "true distribution"}, {"st": 55, "ed": 57, "text": "prediction performance"}]
[{"st": 14, "ed": 16, "text": "dependency structure"}, {"st": 18, "ed": 20, "text": "unknown distribution"}, {"st": 33, "ed": 35, "text": "random variables"}, {"st": 71, "ed": 73, "text": "mutual information"}, {"st": 171, "ed": 173, "text": "mutual information"}]
[{"st": 4, "ed": 6, "text": "collaborative filtering"}, {"st": 44, "ed": 47, "text": "low rank matrix"}, {"st": 62, "ed": 65, "text": "low rank matrix"}, {"st": 69, "ed": 71, "text": "special case"}, {"st": 79, "ed": 82, "text": "row and column"}, {"st": 90, "ed": 92, "text": "matrix completion"}, {"st": 117, "ed": 119, "text": "matrix completion"}, {"st": 123, "ed": 125, "text": "matrix completion"}, {"st": 138, "ed": 140, "text": "multi task"}]
[{"st": 36, "ed": 38, "text": "decision boundaries"}, {"st": 41, "ed": 43, "text": "classification accuracy"}]
[{"st": 25, "ed": 27, "text": "true distribution"}, {"st": 57, "ed": 59, "text": "prediction performance"}]
[{"st": 0, "ed": 2, "text": "reinforcement learning"}, {"st": 48, "ed": 50, "text": "reinforcement learning"}]
[{"st": 11, "ed": 13, "text": "partially observable"}, {"st": 20, "ed": 22, "text": "dynamic programming"}, {"st": 53, "ed": 56, "text": "hidden markov models"}, {"st": 57, "ed": 60, "text": "input and output"}, {"st": 80, "ed": 82, "text": "optimization method"}]
[{"st": 18, "ed": 21, "text": "sequential decision making"}, {"st": 43, "ed": 45, "text": "decision maker"}, {"st": 62, "ed": 64, "text": "relevant information"}, {"st": 66, "ed": 68, "text": "decision maker"}, {"st": 89, "ed": 91, "text": "decision maker"}, {"st": 140, "ed": 142, "text": "decision makers"}]
[{"st": 9, "ed": 12, "text": "principal component analysis"}, {"st": 16, "ed": 18, "text": "feature selection"}, {"st": 19, "ed": 21, "text": "sparse pca"}, {"st": 25, "ed": 27, "text": "linear combinations"}, {"st": 56, "ed": 58, "text": "clustering technique"}, {"st": 85, "ed": 87, "text": "sparse pca"}, {"st": 111, "ed": 113, "text": "feature selection"}]
[{"st": 31, "ed": 33, "text": "cognitive neuroscience"}, {"st": 72, "ed": 74, "text": "tabu search"}, {"st": 96, "ed": 98, "text": "local search"}]
[{"st": 56, "ed": 58, "text": "max a"}, {"st": 185, "ed": 187, "text": "b 2"}]
[{"st": 34, "ed": 36, "text": "knowledge engineering"}, {"st": 97, "ed": 99, "text": "knowledge management"}, {"st": 99, "ed": 101, "text": "artificial intelligence"}, {"st": 123, "ed": 125, "text": "knowledge management"}, {"st": 133, "ed": 135, "text": "expert systems"}, {"st": 153, "ed": 155, "text": "knowledge management"}, {"st": 156, "ed": 158, "text": "at large"}]
[{"st": 8, "ed": 10, "text": "reinforcement learning"}, {"st": 19, "ed": 21, "text": "policy iteration"}, {"st": 38, "ed": 40, "text": "supervised learning"}, {"st": 48, "ed": 50, "text": "policy iteration"}, {"st": 65, "ed": 68, "text": "multi armed bandit"}, {"st": 73, "ed": 75, "text": "comparable performance"}, {"st": 97, "ed": 99, "text": "reinforcement learning"}, {"st": 100, "ed": 102, "text": "inverted pendulum"}]
[{"st": 51, "ed": 53, "text": "exploration exploitation"}, {"st": 59, "ed": 61, "text": "bandit problem"}, {"st": 61, "ed": 64, "text": "with expert advice"}, {"st": 86, "ed": 88, "text": "optimal regret"}, {"st": 105, "ed": 107, "text": "bandit problem"}, {"st": 108, "ed": 110, "text": "partial information"}, {"st": 142, "ed": 144, "text": "preliminary experiments"}]
[{"st": 17, "ed": 20, "text": "hidden markov models"}, {"st": 28, "ed": 30, "text": "learning algorithm"}, {"st": 33, "ed": 38, "text": "non negative matrix factorization nmf"}, {"st": 39, "ed": 41, "text": "higher order"}, {"st": 72, "ed": 77, "text": "non negative matrix factorization nmf"}, {"st": 84, "ed": 86, "text": "numerical examples"}]
[]
[{"st": 17, "ed": 19, "text": "reinforcement learning"}, {"st": 68, "ed": 70, "text": "learning process"}]
[{"st": 3, "ed": 5, "text": "multi agent"}, {"st": 26, "ed": 28, "text": "multi agent"}, {"st": 90, "ed": 92, "text": "learning agents"}, {"st": 115, "ed": 118, "text": "ordinary differential equations"}, {"st": 151, "ed": 153, "text": "steady state"}]
[{"st": 2, "ed": 4, "text": "machine learning"}, {"st": 38, "ed": 41, "text": "empirical risk minimization"}, {"st": 50, "ed": 52, "text": "differential privacy"}, {"st": 86, "ed": 88, "text": "machine learning"}, {"st": 95, "ed": 97, "text": "objective function"}, {"st": 114, "ed": 116, "text": "theoretical results"}, {"st": 124, "ed": 126, "text": "generalization bounds"}, {"st": 144, "ed": 146, "text": "machine learning"}, {"st": 149, "ed": 152, "text": "end to end"}, {"st": 169, "ed": 171, "text": "logistic regression"}, {"st": 177, "ed": 179, "text": "encouraging results"}, {"st": 195, "ed": 198, "text": "theoretically and empirically"}]
[{"st": 0, "ed": 2, "text": "adaptive control"}, {"st": 4, "ed": 6, "text": "notoriously difficult"}, {"st": 26, "ed": 28, "text": "optimal policy"}, {"st": 32, "ed": 34, "text": "adaptive control"}, {"st": 95, "ed": 98, "text": "markov decision processes"}, {"st": 99, "ed": 101, "text": "finite state"}, {"st": 114, "ed": 116, "text": "conjugate prior"}, {"st": 133, "ed": 135, "text": "gibbs sampler"}, {"st": 142, "ed": 144, "text": "preliminary results"}]
[{"st": 8, "ed": 10, "text": "random variables"}, {"st": 28, "ed": 30, "text": "learning algorithm"}, {"st": 41, "ed": 44, "text": "minimum description length"}, {"st": 75, "ed": 77, "text": "random variables"}]
[{"st": 5, "ed": 7, "text": "neural networks"}, {"st": 9, "ed": 11, "text": "probabilistic models"}, {"st": 19, "ed": 21, "text": "additional assumptions"}, {"st": 26, "ed": 28, "text": "probabilistic models"}, {"st": 34, "ed": 36, "text": "neural networks"}, {"st": 58, "ed": 60, "text": "random variables"}, {"st": 63, "ed": 65, "text": "multiple sources"}, {"st": 80, "ed": 82, "text": "neural networks"}]
[{"st": 21, "ed": 23, "text": "data mining"}, {"st": 29, "ed": 31, "text": "labeled data"}, {"st": 34, "ed": 37, "text": "supervised machine learning"}, {"st": 86, "ed": 88, "text": "new york"}, {"st": 97, "ed": 99, "text": "generative model"}, {"st": 112, "ed": 115, "text": "conditional random field"}, {"st": 146, "ed": 148, "text": "training data"}]
[{"st": 11, "ed": 13, "text": "domains including"}, {"st": 49, "ed": 51, "text": "affinity propagation"}, {"st": 69, "ed": 71, "text": "challenging task"}, {"st": 91, "ed": 93, "text": "real world"}, {"st": 93, "ed": 95, "text": "social media"}, {"st": 113, "ed": 115, "text": "empirical results"}, {"st": 118, "ed": 120, "text": "proposed approach"}, {"st": 136, "ed": 138, "text": "affinity propagation"}, {"st": 141, "ed": 143, "text": "approach yields"}]
[{"st": 13, "ed": 15, "text": "labeled examples"}, {"st": 33, "ed": 35, "text": "np hard"}, {"st": 56, "ed": 58, "text": "learning theory"}, {"st": 91, "ed": 93, "text": "previous results"}, {"st": 140, "ed": 143, "text": "positive and negative"}, {"st": 173, "ed": 175, "text": "ell 2"}, {"st": 250, "ed": 252, "text": "unlike previous"}]
[]
[]
[{"st": 3, "ed": 5, "text": "important issue"}, {"st": 16, "ed": 18, "text": "reinforcement learning"}, {"st": 34, "ed": 36, "text": "answer set"}, {"st": 44, "ed": 46, "text": "reinforcement learning"}, {"st": 55, "ed": 57, "text": "domain specific"}, {"st": 81, "ed": 83, "text": "reinforcement learning"}, {"st": 97, "ed": 99, "text": "reinforcement learning"}]
[{"st": 72, "ed": 74, "text": "approach called"}, {"st": 122, "ed": 124, "text": "learning framework"}, {"st": 141, "ed": 143, "text": "ridge regression"}, {"st": 154, "ed": 156, "text": "optimal solution"}, {"st": 166, "ed": 168, "text": "baseline methods"}, {"st": 173, "ed": 175, "text": "cross validation"}, {"st": 227, "ed": 229, "text": "recent studies"}]
[{"st": 6, "ed": 8, "text": "web search"}, {"st": 25, "ed": 27, "text": "non stationary"}, {"st": 46, "ed": 48, "text": "breaking news"}, {"st": 137, "ed": 139, "text": "empirical comparison"}, {"st": 144, "ed": 146, "text": "real life"}, {"st": 157, "ed": 159, "text": "applications including"}]
[{"st": 109, "ed": 111, "text": "reinforcement learning"}, {"st": 142, "ed": 144, "text": "case study"}]
[{"st": 2, "ed": 4, "text": "kernel based"}, {"st": 9, "ed": 11, "text": "reinforcement learning"}, {"st": 19, "ed": 21, "text": "key challenges"}, {"st": 47, "ed": 49, "text": "learning agents"}, {"st": 69, "ed": 71, "text": "efficient online"}, {"st": 82, "ed": 84, "text": "policy iteration"}, {"st": 85, "ed": 87, "text": "least squares"}]
[{"st": 1, "ed": 3, "text": "turing test"}, {"st": 6, "ed": 8, "text": "human intelligence"}, {"st": 150, "ed": 152, "text": "human intelligence"}, {"st": 177, "ed": 179, "text": "social intelligence"}, {"st": 196, "ed": 198, "text": "social learning"}, {"st": 202, "ed": 204, "text": "artificial intelligence"}]
[{"st": 1, "ed": 3, "text": "real world"}, {"st": 8, "ed": 10, "text": "optimisation problems"}, {"st": 58, "ed": 60, "text": "objective functions"}]
[{"st": 1, "ed": 3, "text": "policy search"}, {"st": 5, "ed": 7, "text": "look ahead"}, {"st": 23, "ed": 26, "text": "sequential decision making"}, {"st": 69, "ed": 71, "text": "look ahead"}, {"st": 88, "ed": 90, "text": "learning scheme"}, {"st": 110, "ed": 112, "text": "look ahead"}, {"st": 121, "ed": 123, "text": "scoring function"}, {"st": 154, "ed": 156, "text": "significantly reduce"}, {"st": 160, "ed": 162, "text": "look ahead"}]
[{"st": 10, "ed": 12, "text": "propositional logic"}, {"st": 14, "ed": 16, "text": "background knowledge"}, {"st": 75, "ed": 77, "text": "decision problem"}, {"st": 102, "ed": 104, "text": "background knowledge"}, {"st": 136, "ed": 138, "text": "background knowledge"}, {"st": 151, "ed": 153, "text": "background knowledge"}]
[{"st": 1, "ed": 3, "text": "real world"}, {"st": 14, "ed": 16, "text": "edge weights"}, {"st": 23, "ed": 25, "text": "random field"}, {"st": 29, "ed": 31, "text": "finite dimensional"}, {"st": 31, "ed": 33, "text": "gaussian process"}, {"st": 61, "ed": 63, "text": "active learning"}, {"st": 63, "ed": 65, "text": "classification problems"}, {"st": 70, "ed": 72, "text": "worst case"}, {"st": 98, "ed": 100, "text": "selection algorithm"}, {"st": 142, "ed": 144, "text": "mutual information"}]
[{"st": 6, "ed": 8, "text": "partially observable"}, {"st": 20, "ed": 22, "text": "widely studied"}, {"st": 24, "ed": 26, "text": "control theory"}, {"st": 34, "ed": 36, "text": "posterior distribution"}, {"st": 67, "ed": 69, "text": "approximation algorithm"}, {"st": 79, "ed": 83, "text": "markov chain monte carlo"}, {"st": 92, "ed": 94, "text": "proposal distribution"}, {"st": 159, "ed": 161, "text": "approximation algorithms"}]
[{"st": 1, "ed": 3, "text": "recommender systems"}, {"st": 34, "ed": 36, "text": "sequential decision"}, {"st": 40, "ed": 43, "text": "markov decision processes"}, {"st": 116, "ed": 118, "text": "n gram"}, {"st": 118, "ed": 120, "text": "predictive model"}, {"st": 126, "ed": 128, "text": "n gram"}, {"st": 138, "ed": 140, "text": "predictive accuracy"}, {"st": 151, "ed": 153, "text": "predictive model"}]
[{"st": 2, "ed": 4, "text": "collaborative filtering"}, {"st": 7, "ed": 9, "text": "time series"}, {"st": 9, "ed": 11, "text": "estimation problem"}, {"st": 63, "ed": 65, "text": "predictive accuracy"}]
[{"st": 18, "ed": 20, "text": "main idea"}, {"st": 27, "ed": 29, "text": "local search"}]
[{"st": 5, "ed": 7, "text": "sampling based"}, {"st": 8, "ed": 10, "text": "learning algorithms"}, {"st": 11, "ed": 14, "text": "dynamic bayesian networks"}, {"st": 46, "ed": 48, "text": "monte carlo"}, {"st": 97, "ed": 99, "text": "kalman filter"}, {"st": 101, "ed": 103, "text": "junction tree"}, {"st": 107, "ed": 109, "text": "finite dimensional"}, {"st": 122, "ed": 124, "text": "accurate estimates"}, {"st": 134, "ed": 136, "text": "non stationary"}, {"st": 139, "ed": 142, "text": "radial basis function"}, {"st": 154, "ed": 156, "text": "application areas"}, {"st": 161, "ed": 163, "text": "finite dimensional"}]
[{"st": 3, "ed": 5, "text": "graphical model"}, {"st": 46, "ed": 48, "text": "conditional distributions"}, {"st": 66, "ed": 68, "text": "computationally efficient"}, {"st": 86, "ed": 88, "text": "probabilistic inference"}, {"st": 88, "ed": 90, "text": "collaborative filtering"}]
[{"st": 67, "ed": 69, "text": "most probable"}, {"st": 77, "ed": 81, "text": "dynamic time warping dtw"}, {"st": 87, "ed": 89, "text": "correlation analysis"}, {"st": 101, "ed": 103, "text": "real applications"}]
[{"st": 47, "ed": 49, "text": "case studies"}, {"st": 52, "ed": 54, "text": "machine learning"}]
[{"st": 27, "ed": 30, "text": "ability to learn"}]
[{"st": 0, "ed": 2, "text": "probabilistic logic"}, {"st": 21, "ed": 24, "text": "inference and learning"}, {"st": 28, "ed": 30, "text": "graphical model"}, {"st": 35, "ed": 37, "text": "probabilistic logic"}, {"st": 59, "ed": 61, "text": "probabilistic logic"}, {"st": 73, "ed": 75, "text": "efficient algorithms"}, {"st": 104, "ed": 106, "text": "inference tasks"}, {"st": 128, "ed": 130, "text": "graphical model"}, {"st": 141, "ed": 143, "text": "parameter estimation"}, {"st": 152, "ed": 154, "text": "expectation maximization"}, {"st": 165, "ed": 167, "text": "proposed approach"}, {"st": 175, "ed": 177, "text": "inference algorithms"}, {"st": 185, "ed": 187, "text": "probabilistic logic"}, {"st": 200, "ed": 202, "text": "probabilistic logic"}]
[{"st": 1, "ed": 3, "text": "policy search"}, {"st": 6, "ed": 8, "text": "reinforcement learning"}, {"st": 59, "ed": 61, "text": "local optimum"}, {"st": 78, "ed": 80, "text": "local optimum"}, {"st": 84, "ed": 86, "text": "performance guarantee"}, {"st": 100, "ed": 102, "text": "policy iteration"}, {"st": 103, "ed": 106, "text": "approximate dynamic programming"}, {"st": 117, "ed": 119, "text": "approximation error"}, {"st": 121, "ed": 123, "text": "policy search"}, {"st": 128, "ed": 130, "text": "local search"}]
[{"st": 8, "ed": 10, "text": "wide variety"}, {"st": 13, "ed": 15, "text": "machine learning"}, {"st": 28, "ed": 30, "text": "large scale"}, {"st": 30, "ed": 32, "text": "problem instances"}, {"st": 34, "ed": 36, "text": "fast approximate"}, {"st": 81, "ed": 83, "text": "approximation error"}, {"st": 112, "ed": 115, "text": "easy to implement"}, {"st": 131, "ed": 133, "text": "error bounds"}]
[{"st": 84, "ed": 86, "text": "score function"}, {"st": 88, "ed": 91, "text": "markov random field"}, {"st": 125, "ed": 127, "text": "maximum margin"}]
[]
[{"st": 1, "ed": 3, "text": "exploration exploitation"}, {"st": 49, "ed": 51, "text": "proposed algorithm"}, {"st": 53, "ed": 55, "text": "near optimal"}]
[{"st": 4, "ed": 6, "text": "based methods"}, {"st": 41, "ed": 43, "text": "shannon entropy"}, {"st": 86, "ed": 88, "text": "genetic algorithms"}]
[{"st": 4, "ed": 6, "text": "machine learning"}, {"st": 24, "ed": 26, "text": "reinforcement learning"}, {"st": 35, "ed": 37, "text": "reinforcement learning"}, {"st": 81, "ed": 83, "text": "quantum superposition"}, {"st": 98, "ed": 100, "text": "quantum state"}, {"st": 118, "ed": 120, "text": "probability amplitude"}, {"st": 139, "ed": 142, "text": "exploration and exploitation"}, {"st": 155, "ed": 158, "text": "exploration and exploitation"}, {"st": 160, "ed": 162, "text": "probability amplitude"}]
[{"st": 6, "ed": 8, "text": "random walk"}, {"st": 13, "ed": 15, "text": "clustering algorithms"}, {"st": 21, "ed": 24, "text": "each data point"}]
[{"st": 5, "ed": 7, "text": "reinforcement learning"}, {"st": 19, "ed": 21, "text": "past observations"}, {"st": 62, "ed": 64, "text": "sufficient conditions"}, {"st": 103, "ed": 105, "text": "reinforcement learning"}, {"st": 110, "ed": 113, "text": "markov decision processes"}]
[{"st": 4, "ed": 6, "text": "principled approach"}, {"st": 13, "ed": 15, "text": "reinforcement learning"}, {"st": 32, "ed": 34, "text": "reinforcement learning"}, {"st": 56, "ed": 58, "text": "open question"}, {"st": 65, "ed": 67, "text": "computationally feasible"}, {"st": 80, "ed": 82, "text": "monte carlo"}, {"st": 83, "ed": 85, "text": "search algorithm"}, {"st": 103, "ed": 105, "text": "encouraging results"}, {"st": 111, "ed": 113, "text": "partially observable"}, {"st": 121, "ed": 124, "text": "directions for future"}]
[{"st": 6, "ed": 8, "text": "hinge loss"}, {"st": 17, "ed": 19, "text": "machine learning"}, {"st": 29, "ed": 31, "text": "closely related"}, {"st": 45, "ed": 47, "text": "recently shown"}, {"st": 71, "ed": 73, "text": "hadamard matrix"}, {"st": 77, "ed": 79, "text": "risk minimization"}, {"st": 99, "ed": 101, "text": "objective function"}, {"st": 108, "ed": 110, "text": "hinge loss"}]
[{"st": 129, "ed": 131, "text": "hidden variables"}, {"st": 140, "ed": 142, "text": "david hume"}]
[{"st": 5, "ed": 7, "text": "text classification"}, {"st": 10, "ed": 12, "text": "sequential decision"}, {"st": 17, "ed": 19, "text": "agent learns"}, {"st": 44, "ed": 46, "text": "proposed algorithm"}, {"st": 52, "ed": 54, "text": "text classification"}, {"st": 56, "ed": 59, "text": "markov decision process"}, {"st": 76, "ed": 78, "text": "proposed approach"}, {"st": 78, "ed": 80, "text": "performs comparably"}, {"st": 85, "ed": 88, "text": "large training sets"}]
[{"st": 38, "ed": 40, "text": "answer set"}, {"st": 47, "ed": 50, "text": "inductive logic programming"}, {"st": 77, "ed": 79, "text": "desired properties"}, {"st": 103, "ed": 105, "text": "learning process"}, {"st": 145, "ed": 147, "text": "semi automatic"}]
[{"st": 6, "ed": 8, "text": "approximate inference"}, {"st": 102, "ed": 104, "text": "approximate inference"}, {"st": 120, "ed": 122, "text": "approximate inference"}, {"st": 145, "ed": 147, "text": "approximate inference"}, {"st": 166, "ed": 168, "text": "approximate inference"}, {"st": 173, "ed": 175, "text": "graphical models"}, {"st": 176, "ed": 178, "text": "real world"}]
[{"st": 63, "ed": 66, "text": "goodness of fit"}, {"st": 86, "ed": 89, "text": "goodness of fit"}]
[{"st": 3, "ed": 5, "text": "learning agents"}, {"st": 20, "ed": 22, "text": "reinforcement learning"}, {"st": 27, "ed": 29, "text": "finite state"}, {"st": 29, "ed": 32, "text": "markov decision processes"}, {"st": 83, "ed": 85, "text": "main contribution"}, {"st": 108, "ed": 111, "text": "dynamic bayesian networks"}]
[{"st": 1, "ed": 4, "text": "markov decision processes"}, {"st": 9, "ed": 11, "text": "learning agents"}, {"st": 27, "ed": 30, "text": "dynamic bayesian networks"}, {"st": 34, "ed": 36, "text": "large scale"}, {"st": 36, "ed": 38, "text": "real world"}, {"st": 63, "ed": 65, "text": "relevant features"}, {"st": 77, "ed": 79, "text": "building blocks"}]
[{"st": 7, "ed": 9, "text": "feature selection"}, {"st": 28, "ed": 30, "text": "sparse linear"}, {"st": 40, "ed": 42, "text": "computational efficiency"}, {"st": 45, "ed": 47, "text": "detection performance"}, {"st": 63, "ed": 65, "text": "sparse linear"}]
[{"st": 5, "ed": 7, "text": "reinforcement learning"}, {"st": 20, "ed": 22, "text": "control problems"}, {"st": 49, "ed": 51, "text": "state space"}]
[{"st": 3, "ed": 5, "text": "learning agents"}, {"st": 25, "ed": 27, "text": "reinforcement learning"}, {"st": 32, "ed": 34, "text": "finite state"}, {"st": 34, "ed": 37, "text": "markov decision processes"}, {"st": 92, "ed": 94, "text": "reinforcement learning"}, {"st": 119, "ed": 121, "text": "main contribution"}, {"st": 144, "ed": 147, "text": "dynamic bayesian networks"}]
[{"st": 1, "ed": 3, "text": "intelligent systems"}, {"st": 12, "ed": 14, "text": "face recognition"}, {"st": 33, "ed": 35, "text": "artificial intelligence"}, {"st": 65, "ed": 67, "text": "significant progress"}, {"st": 156, "ed": 158, "text": "open problems"}]
[{"st": 7, "ed": 9, "text": "active learning"}, {"st": 40, "ed": 42, "text": "noise free"}, {"st": 44, "ed": 46, "text": "greedy algorithm"}, {"st": 78, "ed": 80, "text": "active learning"}, {"st": 89, "ed": 91, "text": "optimal policy"}, {"st": 99, "ed": 101, "text": "active learning"}, {"st": 111, "ed": 113, "text": "diminishing returns"}, {"st": 162, "ed": 164, "text": "problem involving"}]
[{"st": 11, "ed": 13, "text": "multi agent"}, {"st": 70, "ed": 72, "text": "multi agent"}, {"st": 78, "ed": 80, "text": "multi agent"}, {"st": 129, "ed": 131, "text": "multi agent"}, {"st": 161, "ed": 164, "text": "first order logic"}, {"st": 171, "ed": 173, "text": "learning algorithm"}, {"st": 212, "ed": 215, "text": "case based reasoning"}]
[{"st": 2, "ed": 5, "text": "role playing game"}, {"st": 46, "ed": 48, "text": "decision making"}, {"st": 62, "ed": 64, "text": "search algorithms"}, {"st": 87, "ed": 89, "text": "search space"}, {"st": 106, "ed": 109, "text": "sequential decision making"}, {"st": 167, "ed": 169, "text": "reinforcement learning"}]
[{"st": 0, "ed": 2, "text": "recent research"}, {"st": 20, "ed": 22, "text": "gaussian process"}, {"st": 27, "ed": 29, "text": "exploration strategies"}, {"st": 72, "ed": 74, "text": "computational burden"}, {"st": 79, "ed": 81, "text": "based approach"}, {"st": 85, "ed": 87, "text": "path planning"}, {"st": 104, "ed": 106, "text": "path planning"}, {"st": 140, "ed": 143, "text": "provide theoretical guarantees"}, {"st": 169, "ed": 171, "text": "performance degradation"}, {"st": 178, "ed": 180, "text": "empirical evaluation"}, {"st": 181, "ed": 183, "text": "real world"}, {"st": 223, "ed": 225, "text": "significant computational"}]
[{"st": 4, "ed": 6, "text": "systems biology"}, {"st": 43, "ed": 45, "text": "boolean logic"}, {"st": 53, "ed": 55, "text": "recently introduced"}, {"st": 74, "ed": 76, "text": "answer set"}, {"st": 80, "ed": 82, "text": "problem solving"}, {"st": 105, "ed": 107, "text": "significant improvements"}, {"st": 113, "ed": 116, "text": "efficiency and scalability"}, {"st": 118, "ed": 120, "text": "global optimality"}]
[{"st": 0, "ed": 2, "text": "social media"}, {"st": 42, "ed": 44, "text": "social media"}, {"st": 60, "ed": 62, "text": "large scale"}, {"st": 143, "ed": 145, "text": "statistical modeling"}, {"st": 159, "ed": 161, "text": "automatic segmentation"}, {"st": 200, "ed": 202, "text": "efficient inference"}, {"st": 220, "ed": 222, "text": "user study"}, {"st": 244, "ed": 246, "text": "higher quality"}]
[{"st": 31, "ed": 33, "text": "data model"}, {"st": 37, "ed": 39, "text": "logic programming"}, {"st": 48, "ed": 50, "text": "knowledge engineering"}, {"st": 59, "ed": 61, "text": "machine learning"}, {"st": 68, "ed": 71, "text": "inductive logic programming"}, {"st": 77, "ed": 79, "text": "machine learning"}, {"st": 80, "ed": 82, "text": "logic programming"}, {"st": 117, "ed": 119, "text": "learning rule"}]
[{"st": 2, "ed": 5, "text": "real time strategy"}, {"st": 82, "ed": 84, "text": "bayes net"}, {"st": 95, "ed": 97, "text": "generative model"}]
[{"st": 6, "ed": 8, "text": "user preferences"}, {"st": 43, "ed": 45, "text": "cost function"}, {"st": 92, "ed": 94, "text": "noisy labels"}, {"st": 118, "ed": 120, "text": "extensive experiments"}, {"st": 124, "ed": 126, "text": "cost function"}]
[{"st": 19, "ed": 21, "text": "early detection"}, {"st": 60, "ed": 63, "text": "spatial and temporal"}, {"st": 91, "ed": 93, "text": "multivariate data"}, {"st": 99, "ed": 101, "text": "statistically significant"}, {"st": 125, "ed": 127, "text": "small scale"}, {"st": 143, "ed": 145, "text": "approach called"}, {"st": 222, "ed": 224, "text": "false alarm"}]
[{"st": 2, "ed": 4, "text": "hierarchical bayesian"}, {"st": 24, "ed": 26, "text": "riemannian manifold"}, {"st": 27, "ed": 29, "text": "monte carlo"}, {"st": 54, "ed": 56, "text": "monte carlo"}, {"st": 103, "ed": 105, "text": "gibbs sampling"}]
[{"st": 0, "ed": 2, "text": "latent variable"}, {"st": 3, "ed": 5, "text": "models including"}, {"st": 7, "ed": 10, "text": "conditional random fields"}, {"st": 12, "ed": 14, "text": "special case"}, {"st": 19, "ed": 21, "text": "natural language"}, {"st": 27, "ed": 29, "text": "computational complexity"}, {"st": 36, "ed": 39, "text": "conditional random fields"}, {"st": 49, "ed": 51, "text": "computational complexity"}, {"st": 65, "ed": 67, "text": "np hard"}, {"st": 95, "ed": 97, "text": "exact inference"}, {"st": 99, "ed": 101, "text": "exact inference"}]
[{"st": 5, "ed": 7, "text": "latent semantic"}, {"st": 11, "ed": 13, "text": "high level"}, {"st": 19, "ed": 21, "text": "large vocabulary"}, {"st": 23, "ed": 25, "text": "mid level"}, {"st": 31, "ed": 33, "text": "sparse representation"}, {"st": 43, "ed": 45, "text": "challenging task"}, {"st": 64, "ed": 66, "text": "latent semantic"}, {"st": 97, "ed": 99, "text": "sparse representation"}, {"st": 105, "ed": 107, "text": "sparse coding"}, {"st": 109, "ed": 111, "text": "structured sparsity"}, {"st": 114, "ed": 116, "text": "l1 norm"}, {"st": 119, "ed": 121, "text": "mid level"}, {"st": 125, "ed": 127, "text": "embedding space"}, {"st": 134, "ed": 136, "text": "mid level"}, {"st": 164, "ed": 167, "text": "latent semantic analysis"}, {"st": 169, "ed": 171, "text": "topic models"}, {"st": 172, "ed": 174, "text": "latent semantic"}, {"st": 182, "ed": 184, "text": "mid level"}, {"st": 199, "ed": 201, "text": "high level"}]
[{"st": 28, "ed": 30, "text": "kernel based"}, {"st": 42, "ed": 44, "text": "learning problems"}, {"st": 59, "ed": 61, "text": "logic programming"}, {"st": 116, "ed": 118, "text": "background knowledge"}, {"st": 128, "ed": 131, "text": "inductive logic programming"}, {"st": 148, "ed": 151, "text": "statistical relational learning"}, {"st": 154, "ed": 156, "text": "classification regression"}, {"st": 156, "ed": 158, "text": "multitask learning"}]
[{"st": 38, "ed": 40, "text": "search engine"}, {"st": 103, "ed": 105, "text": "efficient learning"}, {"st": 110, "ed": 112, "text": "frac 1"}, {"st": 112, "ed": 114, "text": "sqrt t"}, {"st": 119, "ed": 121, "text": "learning algorithm"}, {"st": 139, "ed": 141, "text": "learning algorithms"}]
[{"st": 15, "ed": 18, "text": "transmission control protocol"}, {"st": 34, "ed": 36, "text": "soft margin"}, {"st": 36, "ed": 39, "text": "support vector machine"}]
[{"st": 20, "ed": 22, "text": "constraint programming"}]
[{"st": 4, "ed": 6, "text": "least squares"}, {"st": 20, "ed": 22, "text": "machine learning"}, {"st": 41, "ed": 43, "text": "large scale"}, {"st": 57, "ed": 59, "text": "multi output"}, {"st": 62, "ed": 64, "text": "multi label"}, {"st": 78, "ed": 81, "text": "takes advantage of"}, {"st": 86, "ed": 88, "text": "least squares"}, {"st": 130, "ed": 132, "text": "https github.com"}]
[{"st": 27, "ed": 29, "text": "active learning"}, {"st": 40, "ed": 42, "text": "related problems"}, {"st": 47, "ed": 49, "text": "highly dependent"}, {"st": 51, "ed": 53, "text": "surrogate model"}, {"st": 108, "ed": 110, "text": "bayesian optimization"}, {"st": 129, "ed": 131, "text": "operating systems"}]
[{"st": 31, "ed": 34, "text": "ability to capture"}, {"st": 72, "ed": 74, "text": "temporal logic"}, {"st": 136, "ed": 138, "text": "recently proposed"}, {"st": 143, "ed": 145, "text": "temporal logic"}]
[{"st": 7, "ed": 9, "text": "active learning"}, {"st": 99, "ed": 101, "text": "worst case"}, {"st": 107, "ed": 109, "text": "prior distribution"}, {"st": 120, "ed": 122, "text": "decision tree"}]
[{"st": 8, "ed": 10, "text": "reinforcement learning"}, {"st": 15, "ed": 17, "text": "decision making"}, {"st": 161, "ed": 164, "text": "temporal difference learning"}, {"st": 249, "ed": 251, "text": "machine learning"}, {"st": 303, "ed": 305, "text": "decision making"}]
[{"st": 25, "ed": 27, "text": "weighted sum"}, {"st": 68, "ed": 70, "text": "relative error"}, {"st": 72, "ed": 74, "text": "total variation"}, {"st": 111, "ed": 113, "text": "previously proposed"}]
[{"st": 9, "ed": 11, "text": "linear classification"}, {"st": 15, "ed": 18, "text": "a reproducing kernel"}, {"st": 25, "ed": 28, "text": "point of view"}, {"st": 38, "ed": 41, "text": "point of view"}, {"st": 60, "ed": 62, "text": "von neumann"}, {"st": 94, "ed": 96, "text": "dot product"}, {"st": 116, "ed": 118, "text": "convergence rate"}, {"st": 176, "ed": 178, "text": "primal dual"}]
[{"st": 0, "ed": 2, "text": "distributed computing"}, {"st": 5, "ed": 7, "text": "large scale"}, {"st": 67, "ed": 69, "text": "theoretical guarantees"}, {"st": 71, "ed": 73, "text": "highly efficient"}, {"st": 78, "ed": 80, "text": "highly efficient"}, {"st": 87, "ed": 89, "text": "promising results"}, {"st": 100, "ed": 102, "text": "proposed algorithm"}, {"st": 113, "ed": 115, "text": "machine learning"}]
[{"st": 3, "ed": 5, "text": "text mining"}, {"st": 8, "ed": 10, "text": "k means"}, {"st": 10, "ed": 12, "text": "naive bayes"}, {"st": 22, "ed": 24, "text": "relevant information"}, {"st": 58, "ed": 60, "text": "text documents"}, {"st": 109, "ed": 111, "text": "clustering algorithms"}, {"st": 150, "ed": 152, "text": "improved results"}]
[{"st": 30, "ed": 32, "text": "online learning"}, {"st": 88, "ed": 90, "text": "bayes optimal"}]
[{"st": 16, "ed": 18, "text": "probabilistic modeling"}, {"st": 30, "ed": 32, "text": "mathematical logic"}, {"st": 34, "ed": 36, "text": "powerful tool"}, {"st": 134, "ed": 136, "text": "peano arithmetic"}]
[{"st": 4, "ed": 6, "text": "significant progress"}, {"st": 10, "ed": 12, "text": "deep learning"}, {"st": 14, "ed": 16, "text": "feature representations"}, {"st": 21, "ed": 23, "text": "examples include"}, {"st": 27, "ed": 29, "text": "atari games"}, {"st": 56, "ed": 58, "text": "continuous control"}, {"st": 76, "ed": 79, "text": "continuous control tasks"}, {"st": 123, "ed": 125, "text": "reinforcement learning"}, {"st": 135, "ed": 137, "text": "https github.com"}]
[{"st": 86, "ed": 88, "text": "based approach"}, {"st": 117, "ed": 119, "text": "submodular function"}, {"st": 132, "ed": 134, "text": "cost sensitive"}, {"st": 157, "ed": 159, "text": "mobile robot"}]
[{"st": 7, "ed": 9, "text": "hierarchical clustering"}, {"st": 27, "ed": 29, "text": "source code"}, {"st": 33, "ed": 35, "text": "software engineer"}, {"st": 57, "ed": 59, "text": "source code"}, {"st": 127, "ed": 129, "text": "object oriented"}, {"st": 196, "ed": 198, "text": "open source"}]
[{"st": 54, "ed": 57, "text": "trial and error"}, {"st": 71, "ed": 73, "text": "existing solutions"}, {"st": 77, "ed": 79, "text": "local information"}, {"st": 125, "ed": 128, "text": "provide sufficient conditions"}, {"st": 155, "ed": 157, "text": "numerical results"}, {"st": 168, "ed": 171, "text": "trial and error"}, {"st": 171, "ed": 173, "text": "nash equilibrium"}]
[{"st": 7, "ed": 9, "text": "optimal control"}, {"st": 19, "ed": 21, "text": "online setting"}, {"st": 37, "ed": 39, "text": "explicitly model"}, {"st": 39, "ed": 41, "text": "based approach"}, {"st": 46, "ed": 48, "text": "linear function"}, {"st": 58, "ed": 60, "text": "world model"}, {"st": 65, "ed": 67, "text": "reinforcement learning"}, {"st": 74, "ed": 76, "text": "main results"}, {"st": 92, "ed": 94, "text": "generating distribution"}, {"st": 99, "ed": 101, "text": "policy evaluation"}, {"st": 106, "ed": 108, "text": "limit point"}, {"st": 110, "ed": 112, "text": "least squares"}, {"st": 129, "ed": 131, "text": "linear approximation"}]
[{"st": 3, "ed": 5, "text": "user preferences"}, {"st": 39, "ed": 41, "text": "active learning"}, {"st": 42, "ed": 44, "text": "user preferences"}, {"st": 59, "ed": 61, "text": "active learning"}, {"st": 72, "ed": 75, "text": "real world data"}, {"st": 83, "ed": 85, "text": "active learning"}]
[{"st": 0, "ed": 2, "text": "precision recall"}, {"st": 14, "ed": 16, "text": "machine learning"}, {"st": 94, "ed": 96, "text": "empirical evaluation"}]
[{"st": 6, "ed": 8, "text": "parameter learning"}, {"st": 28, "ed": 30, "text": "discrete variables"}, {"st": 56, "ed": 58, "text": "maximum likelihood"}, {"st": 71, "ed": 73, "text": "maximum likelihood"}, {"st": 118, "ed": 121, "text": "simulated and real"}, {"st": 136, "ed": 139, "text": "maximum likelihood estimator"}]
[{"st": 59, "ed": 61, "text": "equivalence classes"}, {"st": 82, "ed": 84, "text": "prior knowledge"}, {"st": 110, "ed": 112, "text": "case study"}]
[{"st": 4, "ed": 6, "text": "parameter learning"}, {"st": 7, "ed": 9, "text": "incomplete data"}, {"st": 15, "ed": 17, "text": "likelihood based"}, {"st": 97, "ed": 99, "text": "parameter space"}, {"st": 110, "ed": 112, "text": "learning parameters"}, {"st": 123, "ed": 126, "text": "takes into account"}, {"st": 137, "ed": 139, "text": "likelihood based"}, {"st": 164, "ed": 166, "text": "provide evidence"}, {"st": 168, "ed": 170, "text": "em algorithm"}]
[{"st": 69, "ed": 71, "text": "recommendation systems"}, {"st": 105, "ed": 107, "text": "relational model"}, {"st": 124, "ed": 126, "text": "infinite dimensional"}, {"st": 126, "ed": 128, "text": "latent variable"}, {"st": 148, "ed": 150, "text": "gibbs sampler"}, {"st": 208, "ed": 210, "text": "multi relational"}, {"st": 227, "ed": 229, "text": "significantly improved"}]
[{"st": 46, "ed": 48, "text": "dynamic programming"}, {"st": 56, "ed": 58, "text": "monte carlo"}, {"st": 85, "ed": 87, "text": "reinforcement learning"}, {"st": 87, "ed": 89, "text": "policy evaluation"}, {"st": 100, "ed": 102, "text": "maximum likelihood"}, {"st": 104, "ed": 106, "text": "policy evaluation"}, {"st": 118, "ed": 120, "text": "policy evaluation"}, {"st": 126, "ed": 128, "text": "policy evaluation"}, {"st": 131, "ed": 133, "text": "importance sampling"}]
[{"st": 4, "ed": 6, "text": "machine learning"}, {"st": 35, "ed": 37, "text": "higher order"}, {"st": 54, "ed": 56, "text": "machine learning"}]
[{"st": 18, "ed": 20, "text": "current methods"}, {"st": 36, "ed": 38, "text": "contingency plan"}, {"st": 140, "ed": 142, "text": "local search"}, {"st": 142, "ed": 144, "text": "policy gradient"}, {"st": 166, "ed": 168, "text": "rgb d"}]
[{"st": 47, "ed": 49, "text": "path planning"}, {"st": 53, "ed": 55, "text": "mutual information"}, {"st": 106, "ed": 108, "text": "gaussian process"}, {"st": 117, "ed": 119, "text": "near optimal"}, {"st": 149, "ed": 152, "text": "provide theoretical guarantees"}, {"st": 187, "ed": 189, "text": "empirical evaluation"}, {"st": 190, "ed": 192, "text": "real world"}, {"st": 219, "ed": 222, "text": "orders of magnitude"}]
[{"st": 0, "ed": 2, "text": "recent research"}, {"st": 27, "ed": 29, "text": "sequential decision"}, {"st": 33, "ed": 35, "text": "framework called"}, {"st": 53, "ed": 55, "text": "large scale"}, {"st": 78, "ed": 80, "text": "path planning"}, {"st": 126, "ed": 128, "text": "maximum entropy"}, {"st": 142, "ed": 144, "text": "theoretical bounds"}, {"st": 182, "ed": 184, "text": "gaussian process"}, {"st": 192, "ed": 194, "text": "gaussian process"}, {"st": 201, "ed": 204, "text": "provide sufficient conditions"}]
[{"st": 0, "ed": 2, "text": "demand response"}, {"st": 32, "ed": 34, "text": "fully automated"}, {"st": 66, "ed": 68, "text": "fully automated"}, {"st": 74, "ed": 76, "text": "reinforcement learning"}, {"st": 105, "ed": 107, "text": "explicitly modeling"}, {"st": 135, "ed": 137, "text": "computational complexity"}]
[{"st": 96, "ed": 98, "text": "basal ganglia"}, {"st": 114, "ed": 116, "text": "modified version"}]
[{"st": 3, "ed": 5, "text": "automated reasoning"}, {"st": 34, "ed": 36, "text": "related problems"}, {"st": 56, "ed": 58, "text": "large margin"}]
[{"st": 123, "ed": 126, "text": "automated theorem proving"}]
[{"st": 44, "ed": 46, "text": "prior knowledge"}, {"st": 57, "ed": 59, "text": "based method"}, {"st": 141, "ed": 143, "text": "alternating optimization"}, {"st": 174, "ed": 176, "text": "stem cell"}, {"st": 178, "ed": 180, "text": "ovarian cancer"}, {"st": 184, "ed": 186, "text": "breast cancer"}, {"st": 231, "ed": 233, "text": "ovarian cancer"}, {"st": 233, "ed": 235, "text": "breast cancer"}]
[{"st": 15, "ed": 17, "text": "multi agent"}, {"st": 17, "ed": 20, "text": "inverse reinforcement learning"}, {"st": 27, "ed": 30, "text": "inverse reinforcement learning"}, {"st": 45, "ed": 48, "text": "markov decision process"}, {"st": 121, "ed": 123, "text": "theoretical foundation"}, {"st": 132, "ed": 135, "text": "bayesian optimization algorithm"}, {"st": 153, "ed": 155, "text": "generative model"}, {"st": 203, "ed": 205, "text": "prior information"}]
[{"st": 28, "ed": 30, "text": "learning process"}, {"st": 54, "ed": 56, "text": "decision boundary"}, {"st": 91, "ed": 93, "text": "learning process"}, {"st": 145, "ed": 147, "text": "empirical risk"}, {"st": 149, "ed": 151, "text": "learning process"}, {"st": 153, "ed": 155, "text": "loss functions"}, {"st": 219, "ed": 221, "text": "learning theory"}]
[{"st": 31, "ed": 33, "text": "takes place"}, {"st": 54, "ed": 56, "text": "unsupervised learning"}, {"st": 69, "ed": 71, "text": "reward function"}, {"st": 82, "ed": 85, "text": "inverse reinforcement learning"}, {"st": 98, "ed": 101, "text": "markov decision process"}, {"st": 108, "ed": 110, "text": "partially observable"}, {"st": 134, "ed": 136, "text": "training set"}, {"st": 182, "ed": 185, "text": "proof of concept"}]
[{"st": 0, "ed": 2, "text": "exact bayesian"}, {"st": 13, "ed": 15, "text": "dynamic programming"}, {"st": 24, "ed": 26, "text": "posterior probabilities"}, {"st": 72, "ed": 74, "text": "parallel algorithm"}, {"st": 79, "ed": 81, "text": "posterior probabilities"}]
[{"st": 55, "ed": 57, "text": "simple machine"}, {"st": 94, "ed": 96, "text": "electrical load"}, {"st": 180, "ed": 182, "text": "statistically significant"}, {"st": 184, "ed": 186, "text": "task performance"}]
[{"st": 1, "ed": 3, "text": "wide variety"}, {"st": 4, "ed": 8, "text": "problems in machine learning"}, {"st": 11, "ed": 13, "text": "document summarization"}, {"st": 27, "ed": 29, "text": "submodular optimization"}, {"st": 55, "ed": 57, "text": "constant factor"}, {"st": 57, "ed": 59, "text": "worst case"}]
[{"st": 1, "ed": 3, "text": "source separation"}, {"st": 7, "ed": 9, "text": "real world"}, {"st": 43, "ed": 47, "text": "deep recurrent neural networks"}, {"st": 49, "ed": 51, "text": "source separation"}, {"st": 51, "ed": 53, "text": "tasks including"}, {"st": 54, "ed": 56, "text": "speech separation"}, {"st": 68, "ed": 72, "text": "deep recurrent neural networks"}, {"st": 89, "ed": 91, "text": "neural networks"}, {"st": 111, "ed": 113, "text": "speech separation"}, {"st": 135, "ed": 137, "text": "speech separation"}, {"st": 151, "ed": 153, "text": "existing models"}]
[{"st": 0, "ed": 2, "text": "belief propagation"}, {"st": 10, "ed": 12, "text": "probabilistic inference"}, {"st": 17, "ed": 21, "text": "markov random fields mrfs"}, {"st": 29, "ed": 31, "text": "convergence guarantees"}, {"st": 53, "ed": 55, "text": "recent works"}, {"st": 105, "ed": 107, "text": "linear equation"}, {"st": 112, "ed": 114, "text": "convergence guarantees"}]
[{"st": 4, "ed": 7, "text": "labeled training data"}, {"st": 83, "ed": 85, "text": "approval voting"}, {"st": 113, "ed": 115, "text": "theoretical guarantees"}, {"st": 130, "ed": 132, "text": "empirical studies"}, {"st": 133, "ed": 136, "text": "amazon mechanical turk"}]
[{"st": 3, "ed": 5, "text": "self organizing"}, {"st": 18, "ed": 20, "text": "problem solving"}, {"st": 25, "ed": 27, "text": "prior information"}, {"st": 80, "ed": 82, "text": "internal representation"}, {"st": 97, "ed": 99, "text": "equivalence class"}, {"st": 124, "ed": 126, "text": "arbitrary precision"}]
[{"st": 4, "ed": 6, "text": "big data"}, {"st": 72, "ed": 74, "text": "big data"}, {"st": 84, "ed": 86, "text": "machine learning"}, {"st": 95, "ed": 97, "text": "increasingly popular"}, {"st": 98, "ed": 100, "text": "data mining"}, {"st": 166, "ed": 168, "text": "big data"}]
[{"st": 7, "ed": 9, "text": "speech processing"}, {"st": 13, "ed": 16, "text": "automatic speech recognition"}, {"st": 63, "ed": 65, "text": "non stationary"}, {"st": 93, "ed": 95, "text": "recognition performance"}, {"st": 126, "ed": 128, "text": "proposed method"}, {"st": 142, "ed": 144, "text": "previous methods"}, {"st": 151, "ed": 153, "text": "feature spaces"}, {"st": 167, "ed": 169, "text": "feature spaces"}]
[{"st": 6, "ed": 8, "text": "feature extraction"}, {"st": 29, "ed": 31, "text": "amino acid"}, {"st": 48, "ed": 50, "text": "deep learning"}, {"st": 95, "ed": 98, "text": "artificial neural network"}, {"st": 139, "ed": 141, "text": "classification accuracy"}, {"st": 194, "ed": 197, "text": "support vector machine"}, {"st": 209, "ed": 212, "text": "protein data bank"}]
[{"st": 19, "ed": 21, "text": "worst case"}, {"st": 27, "ed": 29, "text": "simplex algorithm"}, {"st": 33, "ed": 35, "text": "practical applications"}, {"st": 38, "ed": 40, "text": "worst case"}, {"st": 50, "ed": 52, "text": "theoretical bounds"}, {"st": 96, "ed": 98, "text": "machine learning"}, {"st": 118, "ed": 120, "text": "theoretical results"}, {"st": 152, "ed": 155, "text": "theoretical and empirical"}]
[{"st": 95, "ed": 97, "text": "examples include"}, {"st": 105, "ed": 107, "text": "virtual machine"}, {"st": 117, "ed": 119, "text": "existing solutions"}, {"st": 123, "ed": 125, "text": "pre defined"}, {"st": 201, "ed": 203, "text": "bandit problem"}, {"st": 208, "ed": 211, "text": "upper confidence bound"}]
[{"st": 0, "ed": 4, "text": "deep reinforcement learning rl"}, {"st": 9, "ed": 11, "text": "low level"}, {"st": 16, "ed": 18, "text": "real world"}, {"st": 32, "ed": 34, "text": "deep networks"}, {"st": 43, "ed": 46, "text": "massive amounts of"}, {"st": 46, "ed": 48, "text": "labeled data"}, {"st": 86, "ed": 88, "text": "reward function"}, {"st": 95, "ed": 97, "text": "real world"}, {"st": 128, "ed": 130, "text": "reward functions"}, {"st": 181, "ed": 183, "text": "reinforcement learning"}, {"st": 185, "ed": 187, "text": "reward function"}, {"st": 226, "ed": 228, "text": "proposed method"}, {"st": 267, "ed": 269, "text": "challenging tasks"}, {"st": 287, "ed": 290, "text": "deep neural network"}, {"st": 297, "ed": 299, "text": "reward function"}, {"st": 306, "ed": 308, "text": "method outperforms"}, {"st": 309, "ed": 311, "text": "supervised learning"}]
[{"st": 13, "ed": 15, "text": "machine learning"}, {"st": 17, "ed": 19, "text": "random forests"}]
[{"st": 154, "ed": 156, "text": "fuzzy logic"}, {"st": 157, "ed": 159, "text": "neural networks"}]
[{"st": 26, "ed": 28, "text": "learning process"}, {"st": 37, "ed": 39, "text": "learning process"}, {"st": 44, "ed": 47, "text": "unsupervised feature learning"}, {"st": 49, "ed": 51, "text": "imitation learning"}, {"st": 74, "ed": 77, "text": "deep convolutional networks"}, {"st": 95, "ed": 97, "text": "racing game"}, {"st": 159, "ed": 161, "text": "reinforcement learning"}]
[{"st": 50, "ed": 52, "text": "partially observable"}, {"st": 86, "ed": 88, "text": "robotic arm"}]
[{"st": 18, "ed": 20, "text": "deep generative"}, {"st": 20, "ed": 22, "text": "machine learning"}, {"st": 68, "ed": 70, "text": "evaluation demonstrates"}, {"st": 83, "ed": 85, "text": "false positive"}, {"st": 86, "ed": 88, "text": "false negative"}]
[{"st": 15, "ed": 18, "text": "recurrent neural networks"}, {"st": 63, "ed": 65, "text": "selection method"}, {"st": 90, "ed": 92, "text": "deep autoencoder"}, {"st": 110, "ed": 112, "text": "generative model"}, {"st": 115, "ed": 117, "text": "deep structured"}, {"st": 142, "ed": 144, "text": "generative model"}]
[{"st": 0, "ed": 2, "text": "recent advances"}, {"st": 7, "ed": 9, "text": "fully convolutional"}, {"st": 9, "ed": 11, "text": "neural networks"}, {"st": 14, "ed": 16, "text": "cost functions"}, {"st": 17, "ed": 19, "text": "motion planning"}, {"st": 29, "ed": 31, "text": "demonstration data"}, {"st": 43, "ed": 46, "text": "inverse reinforcement learning"}, {"st": 79, "ed": 81, "text": "cost function"}, {"st": 86, "ed": 88, "text": "maximum entropy"}, {"st": 94, "ed": 96, "text": "prior knowledge"}, {"st": 112, "ed": 115, "text": "ability to capture"}, {"st": 148, "ed": 150, "text": "demonstration data"}]
[{"st": 80, "ed": 82, "text": "reinforcement learning"}, {"st": 87, "ed": 89, "text": "feature based"}, {"st": 89, "ed": 92, "text": "deep reinforcement learning"}, {"st": 97, "ed": 99, "text": "transfer knowledge"}, {"st": 138, "ed": 141, "text": "simulated and real"}, {"st": 153, "ed": 155, "text": "baseline methods"}]
[{"st": 14, "ed": 16, "text": "collaborative filtering"}, {"st": 23, "ed": 25, "text": "positive feedback"}, {"st": 36, "ed": 38, "text": "kernel based"}, {"st": 38, "ed": 40, "text": "collaborative filtering"}, {"st": 52, "ed": 54, "text": "efficient implementation"}, {"st": 69, "ed": 71, "text": "dot product"}, {"st": 91, "ed": 93, "text": "analysis shows"}]
[{"st": 90, "ed": 92, "text": "training set"}, {"st": 110, "ed": 112, "text": "training instances"}]
[{"st": 0, "ed": 2, "text": "deep learning"}, {"st": 14, "ed": 16, "text": "visual perception"}, {"st": 16, "ed": 18, "text": "object detection"}, {"st": 19, "ed": 21, "text": "speech recognition"}, {"st": 34, "ed": 36, "text": "learning representations"}, {"st": 42, "ed": 44, "text": "deep convolutional"}, {"st": 50, "ed": 54, "text": "applications of artificial intelligence"}, {"st": 67, "ed": 69, "text": "challenging problems"}, {"st": 78, "ed": 80, "text": "deep learning"}, {"st": 94, "ed": 96, "text": "deep learning"}, {"st": 104, "ed": 106, "text": "image processing"}, {"st": 127, "ed": 129, "text": "deep learning"}, {"st": 164, "ed": 166, "text": "deep learning"}]
[{"st": 53, "ed": 55, "text": "existing methods"}]
[{"st": 1, "ed": 3, "text": "web services"}, {"st": 27, "ed": 29, "text": "recent advances"}, {"st": 31, "ed": 33, "text": "remains challenging"}, {"st": 39, "ed": 41, "text": "web service"}, {"st": 51, "ed": 53, "text": "collaborative filtering"}, {"st": 60, "ed": 62, "text": "web services"}, {"st": 88, "ed": 90, "text": "contextual bandit"}, {"st": 92, "ed": 94, "text": "principled approach"}, {"st": 97, "ed": 99, "text": "learning algorithm"}, {"st": 107, "ed": 109, "text": "contextual information"}, {"st": 145, "ed": 147, "text": "contextual bandit"}, {"st": 150, "ed": 152, "text": "computationally efficient"}, {"st": 163, "ed": 165, "text": "bandit algorithm"}, {"st": 182, "ed": 184, "text": "successfully applied"}, {"st": 210, "ed": 212, "text": "context free"}, {"st": 212, "ed": 214, "text": "bandit algorithm"}]
[{"st": 123, "ed": 126, "text": "inductive logic programming"}, {"st": 126, "ed": 128, "text": "relational databases"}, {"st": 141, "ed": 144, "text": "theory and practice"}, {"st": 145, "ed": 147, "text": "logic programming"}]
[{"st": 80, "ed": 82, "text": "sensory input"}, {"st": 153, "ed": 155, "text": "computational efficiency"}, {"st": 214, "ed": 216, "text": "learning problems"}]
[{"st": 1, "ed": 3, "text": "stochastic optimization"}, {"st": 5, "ed": 7, "text": "partial observability"}, {"st": 21, "ed": 23, "text": "notoriously difficult"}, {"st": 53, "ed": 55, "text": "greedy algorithm"}, {"st": 68, "ed": 70, "text": "performance guarantees"}, {"st": 86, "ed": 88, "text": "greedy algorithm"}, {"st": 110, "ed": 112, "text": "applications including"}, {"st": 114, "ed": 116, "text": "viral marketing"}, {"st": 129, "ed": 131, "text": "existing results"}, {"st": 135, "ed": 137, "text": "special cases"}]
[{"st": 14, "ed": 16, "text": "expectation maximization"}, {"st": 23, "ed": 27, "text": "high level programming language"}, {"st": 41, "ed": 43, "text": "probabilistic logic"}, {"st": 61, "ed": 63, "text": "rapid prototyping"}, {"st": 65, "ed": 67, "text": "statistical models"}, {"st": 83, "ed": 85, "text": "probabilistic inference"}, {"st": 85, "ed": 87, "text": "tasks including"}, {"st": 102, "ed": 104, "text": "operational semantics"}, {"st": 144, "ed": 146, "text": "probabilistic logic"}]
[{"st": 52, "ed": 54, "text": "previous results"}, {"st": 115, "ed": 117, "text": "recent works"}, {"st": 130, "ed": 132, "text": "main result"}]
[{"st": 19, "ed": 21, "text": "text data"}, {"st": 39, "ed": 41, "text": "conditional probability"}, {"st": 113, "ed": 115, "text": "prediction error"}]
[{"st": 8, "ed": 10, "text": "active learning"}, {"st": 52, "ed": 54, "text": "soft computing"}, {"st": 91, "ed": 93, "text": "low latency"}]
[{"st": 10, "ed": 12, "text": "social interaction"}, {"st": 30, "ed": 32, "text": "intrinsic motivation"}, {"st": 49, "ed": 51, "text": "social learning"}, {"st": 52, "ed": 54, "text": "intrinsic motivation"}, {"st": 75, "ed": 77, "text": "social learning"}, {"st": 78, "ed": 80, "text": "intrinsic motivation"}]
[{"st": 78, "ed": 80, "text": "most probable"}, {"st": 88, "ed": 92, "text": "dynamic time warping dtw"}, {"st": 98, "ed": 100, "text": "statistical analysis"}, {"st": 131, "ed": 133, "text": "text processing"}]
[{"st": 174, "ed": 176, "text": "differential operator"}, {"st": 291, "ed": 293, "text": "significant progress"}]
[{"st": 18, "ed": 20, "text": "machine learning"}, {"st": 21, "ed": 23, "text": "selection methods"}, {"st": 86, "ed": 88, "text": "push button"}, {"st": 135, "ed": 137, "text": "higher order"}]
[{"st": 0, "ed": 2, "text": "time series"}, {"st": 14, "ed": 16, "text": "time series"}, {"st": 28, "ed": 30, "text": "time series"}, {"st": 35, "ed": 37, "text": "time series"}, {"st": 74, "ed": 76, "text": "local patterns"}, {"st": 78, "ed": 80, "text": "time series"}, {"st": 115, "ed": 117, "text": "local patterns"}, {"st": 138, "ed": 140, "text": "time series"}, {"st": 160, "ed": 162, "text": "local patterns"}, {"st": 184, "ed": 186, "text": "classification accuracies"}, {"st": 192, "ed": 194, "text": "statistically significant"}]
[{"st": 3, "ed": 5, "text": "machine learning"}, {"st": 49, "ed": 51, "text": "conflict resolution"}, {"st": 58, "ed": 60, "text": "concurrency control"}, {"st": 65, "ed": 67, "text": "large scale"}, {"st": 67, "ed": 69, "text": "machine learning"}, {"st": 84, "ed": 86, "text": "feature learning"}, {"st": 95, "ed": 97, "text": "large scale"}]
[]
[]
[{"st": 5, "ed": 7, "text": "machine learning"}, {"st": 14, "ed": 16, "text": "logic programming"}, {"st": 18, "ed": 20, "text": "answer set"}, {"st": 35, "ed": 37, "text": "problem solving"}, {"st": 43, "ed": 46, "text": "inductive logic programming"}, {"st": 48, "ed": 50, "text": "machine learning"}, {"st": 51, "ed": 53, "text": "logic programming"}, {"st": 174, "ed": 176, "text": "answer set"}, {"st": 184, "ed": 186, "text": "answer set"}, {"st": 194, "ed": 196, "text": "dynamical systems"}]
[{"st": 1, "ed": 3, "text": "machine learning"}, {"st": 25, "ed": 27, "text": "low variance"}, {"st": 32, "ed": 34, "text": "efficient algorithms"}, {"st": 52, "ed": 54, "text": "optimal regret"}, {"st": 57, "ed": 59, "text": "linear optimization"}]
[{"st": 10, "ed": 12, "text": "reinforcement learning"}, {"st": 38, "ed": 40, "text": "off policy"}, {"st": 67, "ed": 69, "text": "linear complexity"}, {"st": 79, "ed": 82, "text": "mean square error"}, {"st": 89, "ed": 91, "text": "step size"}, {"st": 121, "ed": 124, "text": "bias and variance"}, {"st": 126, "ed": 128, "text": "prediction error"}, {"st": 138, "ed": 140, "text": "optimal solution"}]
[{"st": 12, "ed": 14, "text": "streaming data"}, {"st": 34, "ed": 36, "text": "open source"}, {"st": 59, "ed": 61, "text": "inference algorithm"}, {"st": 62, "ed": 64, "text": "parameter learning"}, {"st": 68, "ed": 70, "text": "learning algorithm"}, {"st": 78, "ed": 80, "text": "learning algorithm"}, {"st": 83, "ed": 85, "text": "scoring functions"}, {"st": 87, "ed": 89, "text": "log likelihood"}, {"st": 93, "ed": 95, "text": "log likelihood"}, {"st": 105, "ed": 108, "text": "expectation maximization algorithm"}, {"st": 126, "ed": 128, "text": "command line"}]
[{"st": 25, "ed": 27, "text": "artificial intelligence"}, {"st": 48, "ed": 50, "text": "relational databases"}, {"st": 105, "ed": 107, "text": "originally designed"}, {"st": 125, "ed": 127, "text": "hierarchical structure"}, {"st": 153, "ed": 155, "text": "main goal"}]
[{"st": 56, "ed": 58, "text": "collaborative filtering"}, {"st": 79, "ed": 82, "text": "real world data"}, {"st": 118, "ed": 121, "text": "easy to implement"}]
[{"st": 61, "ed": 63, "text": "theoretical analysis"}]
[{"st": 23, "ed": 25, "text": "machine learning"}, {"st": 114, "ed": 117, "text": "strengths and weaknesses"}, {"st": 121, "ed": 123, "text": "empirically evaluate"}]
[{"st": 6, "ed": 8, "text": "real valued"}, {"st": 36, "ed": 38, "text": "existing approaches"}, {"st": 42, "ed": 44, "text": "task specific"}, {"st": 49, "ed": 51, "text": "ranking loss"}, {"st": 66, "ed": 69, "text": "high computational cost"}, {"st": 82, "ed": 84, "text": "highly efficient"}, {"st": 95, "ed": 97, "text": "computational complexity"}, {"st": 112, "ed": 114, "text": "generalization error"}, {"st": 123, "ed": 125, "text": "empirical study"}, {"st": 128, "ed": 130, "text": "proposed approach"}, {"st": 131, "ed": 133, "text": "highly competitive"}]
[{"st": 1, "ed": 3, "text": "large scale"}, {"st": 3, "ed": 5, "text": "machine learning"}, {"st": 10, "ed": 12, "text": "kernel machines"}, {"st": 43, "ed": 45, "text": "submodular optimization"}, {"st": 57, "ed": 59, "text": "large scale"}, {"st": 68, "ed": 70, "text": "submodular function"}, {"st": 92, "ed": 94, "text": "theoretically analyze"}, {"st": 150, "ed": 152, "text": "extensive experiments"}, {"st": 161, "ed": 163, "text": "applications including"}, {"st": 164, "ed": 166, "text": "gaussian process"}, {"st": 169, "ed": 171, "text": "based clustering"}]
[{"st": 41, "ed": 43, "text": "machine learning"}, {"st": 90, "ed": 92, "text": "raw data"}, {"st": 117, "ed": 119, "text": "low level"}, {"st": 119, "ed": 121, "text": "input variables"}, {"st": 124, "ed": 126, "text": "input variables"}, {"st": 132, "ed": 134, "text": "learning algorithm"}, {"st": 199, "ed": 201, "text": "learning algorithms"}, {"st": 213, "ed": 215, "text": "statistically significant"}, {"st": 218, "ed": 220, "text": "real world"}, {"st": 226, "ed": 228, "text": "central role"}]
[{"st": 7, "ed": 9, "text": "time series"}, {"st": 44, "ed": 46, "text": "time series"}, {"st": 51, "ed": 53, "text": "time series"}, {"st": 87, "ed": 89, "text": "training set"}, {"st": 157, "ed": 159, "text": "time series"}, {"st": 169, "ed": 171, "text": "signal processing"}]
[{"st": 1, "ed": 3, "text": "recent years"}, {"st": 4, "ed": 6, "text": "learning systems"}, {"st": 49, "ed": 51, "text": "fuzzy logic"}]
[{"st": 21, "ed": 23, "text": "decision making"}, {"st": 71, "ed": 73, "text": "rigid body"}]
[{"st": 3, "ed": 5, "text": "machine learning"}, {"st": 9, "ed": 11, "text": "low level"}, {"st": 21, "ed": 23, "text": "deep learning"}, {"st": 50, "ed": 52, "text": "reinforcement learning"}, {"st": 102, "ed": 104, "text": "deep model"}, {"st": 114, "ed": 116, "text": "hierarchical representations"}, {"st": 128, "ed": 130, "text": "hierarchical representations"}]
[{"st": 75, "ed": 77, "text": "structured prediction"}, {"st": 81, "ed": 83, "text": "deep learning"}, {"st": 100, "ed": 102, "text": "point clouds"}]
[{"st": 15, "ed": 17, "text": "real world"}, {"st": 41, "ed": 43, "text": "finite state"}, {"st": 91, "ed": 93, "text": "genetic programming"}, {"st": 117, "ed": 119, "text": "proposed framework"}, {"st": 120, "ed": 122, "text": "experiments conducted"}, {"st": 124, "ed": 126, "text": "open source"}]
[{"st": 11, "ed": 13, "text": "nash equilibrium"}, {"st": 41, "ed": 43, "text": "nash equilibrium"}]
[{"st": 72, "ed": 74, "text": "prior knowledge"}, {"st": 87, "ed": 89, "text": "gaussian process"}, {"st": 136, "ed": 138, "text": "submodular function"}, {"st": 141, "ed": 143, "text": "diminishing returns"}, {"st": 170, "ed": 172, "text": "without resorting"}, {"st": 189, "ed": 191, "text": "real world"}, {"st": 191, "ed": 193, "text": "case studies"}]
[{"st": 43, "ed": 45, "text": "application areas"}, {"st": 81, "ed": 83, "text": "reinforcement learning"}, {"st": 110, "ed": 112, "text": "reinforcement learning"}]
[{"st": 50, "ed": 54, "text": "recurrent neural networks rnns"}, {"st": 67, "ed": 69, "text": "previous methods"}, {"st": 80, "ed": 82, "text": "domain knowledge"}, {"st": 92, "ed": 94, "text": "neural networks"}, {"st": 96, "ed": 98, "text": "substantial improvements"}, {"st": 99, "ed": 101, "text": "prediction performance"}]
[{"st": 7, "ed": 9, "text": "learning algorithms"}, {"st": 16, "ed": 18, "text": "faster convergence"}, {"st": 71, "ed": 73, "text": "worst case"}]
[{"st": 23, "ed": 26, "text": "multi view learning"}, {"st": 31, "ed": 33, "text": "multi view"}, {"st": 33, "ed": 35, "text": "maximum entropy"}, {"st": 48, "ed": 50, "text": "multiple classifiers"}, {"st": 56, "ed": 58, "text": "simultaneously learns"}, {"st": 66, "ed": 68, "text": "proposed method"}, {"st": 70, "ed": 72, "text": "improved performance"}, {"st": 74, "ed": 76, "text": "multi view"}]
[{"st": 12, "ed": 14, "text": "visual recognition"}, {"st": 37, "ed": 39, "text": "neural network"}, {"st": 59, "ed": 61, "text": "higher level"}, {"st": 78, "ed": 80, "text": "cognitive tasks"}, {"st": 81, "ed": 83, "text": "visual object"}, {"st": 105, "ed": 107, "text": "spatio temporal"}, {"st": 128, "ed": 130, "text": "higher level"}]
[{"st": 1, "ed": 3, "text": "wide variety"}, {"st": 4, "ed": 8, "text": "problems in machine learning"}, {"st": 11, "ed": 13, "text": "document summarization"}, {"st": 59, "ed": 61, "text": "existing algorithms"}, {"st": 70, "ed": 72, "text": "near optimal"}]
[]
[]
[{"st": 10, "ed": 12, "text": "learning agents"}, {"st": 52, "ed": 54, "text": "internal structure"}, {"st": 65, "ed": 67, "text": "internal structure"}, {"st": 121, "ed": 123, "text": "learning agents"}]
[{"st": 6, "ed": 8, "text": "relational databases"}, {"st": 22, "ed": 24, "text": "learning algorithms"}, {"st": 66, "ed": 68, "text": "learning algorithms"}, {"st": 108, "ed": 110, "text": "learning algorithms"}, {"st": 114, "ed": 117, "text": "theoretical and empirical"}, {"st": 119, "ed": 121, "text": "existing algorithms"}, {"st": 133, "ed": 135, "text": "sample based"}, {"st": 135, "ed": 137, "text": "learning algorithms"}, {"st": 142, "ed": 144, "text": "labeled examples"}, {"st": 161, "ed": 163, "text": "learning algorithms"}, {"st": 171, "ed": 173, "text": "learning algorithms"}, {"st": 188, "ed": 190, "text": "sample based"}, {"st": 191, "ed": 193, "text": "learning algorithm"}, {"st": 204, "ed": 206, "text": "theoretical results"}, {"st": 208, "ed": 210, "text": "empirical study"}, {"st": 223, "ed": 225, "text": "real world"}]
[{"st": 77, "ed": 79, "text": "machine learning"}]
[]
[{"st": 0, "ed": 2, "text": "successful applications"}, {"st": 3, "ed": 5, "text": "reinforcement learning"}, {"st": 6, "ed": 8, "text": "real world"}, {"st": 13, "ed": 15, "text": "partially observable"}, {"st": 26, "ed": 28, "text": "hidden states"}, {"st": 51, "ed": 53, "text": "deep learning"}, {"st": 61, "ed": 63, "text": "partially observable"}, {"st": 66, "ed": 68, "text": "prior knowledge"}, {"st": 87, "ed": 89, "text": "supervised learning"}, {"st": 91, "ed": 93, "text": "reinforcement learning"}, {"st": 105, "ed": 108, "text": "recurrent neural networks"}, {"st": 112, "ed": 115, "text": "short term memory"}, {"st": 151, "ed": 155, "text": "deep q network dqn"}, {"st": 166, "ed": 168, "text": "extensive experiments"}, {"st": 181, "ed": 183, "text": "proposed approach"}]
[{"st": 3, "ed": 5, "text": "challenging problem"}, {"st": 18, "ed": 20, "text": "multi stage"}]
[{"st": 15, "ed": 17, "text": "machine learning"}, {"st": 33, "ed": 35, "text": "recent developments"}, {"st": 59, "ed": 61, "text": "theoretical foundation"}, {"st": 67, "ed": 69, "text": "answer questions"}, {"st": 127, "ed": 129, "text": "prediction accuracy"}, {"st": 146, "ed": 148, "text": "main contributions"}, {"st": 169, "ed": 171, "text": "bounded rationality"}, {"st": 219, "ed": 221, "text": "conduct experiments"}, {"st": 222, "ed": 225, "text": "real world data"}, {"st": 227, "ed": 229, "text": "national park"}]
[{"st": 6, "ed": 8, "text": "machine learning"}, {"st": 12, "ed": 14, "text": "classification methods"}, {"st": 39, "ed": 41, "text": "closed set"}, {"st": 50, "ed": 52, "text": "real world"}, {"st": 73, "ed": 75, "text": "open set"}, {"st": 80, "ed": 82, "text": "closed set"}, {"st": 104, "ed": 106, "text": "multi class"}, {"st": 109, "ed": 111, "text": "open set"}, {"st": 118, "ed": 120, "text": "training set"}, {"st": 165, "ed": 167, "text": "open set"}, {"st": 177, "ed": 179, "text": "benchmark datasets"}]
[]
[{"st": 17, "ed": 19, "text": "sex workers"}, {"st": 26, "ed": 28, "text": "public health"}, {"st": 76, "ed": 78, "text": "total number"}, {"st": 106, "ed": 108, "text": "network structure"}, {"st": 137, "ed": 139, "text": "time series"}, {"st": 149, "ed": 151, "text": "stochastic optimization"}]
[{"st": 20, "ed": 22, "text": "network architecture"}, {"st": 37, "ed": 40, "text": "deep q networks"}, {"st": 63, "ed": 65, "text": "agents learn"}, {"st": 106, "ed": 109, "text": "deep q networks"}, {"st": 124, "ed": 126, "text": "highly complex"}]
[{"st": 0, "ed": 2, "text": "reinforcement learning"}, {"st": 18, "ed": 20, "text": "optimal control"}, {"st": 37, "ed": 39, "text": "multiple tasks"}, {"st": 48, "ed": 50, "text": "prohibitively expensive"}, {"st": 69, "ed": 71, "text": "policy gradient"}, {"st": 72, "ed": 74, "text": "lifelong learning"}, {"st": 79, "ed": 81, "text": "multi task"}, {"st": 88, "ed": 90, "text": "efficiently learn"}, {"st": 114, "ed": 116, "text": "policy gradient"}, {"st": 151, "ed": 153, "text": "empirical analysis"}]
[{"st": 32, "ed": 34, "text": "world model"}, {"st": 121, "ed": 123, "text": "previously proposed"}, {"st": 152, "ed": 154, "text": "map inference"}, {"st": 168, "ed": 170, "text": "computational performance"}]
[{"st": 1, "ed": 3, "text": "matrix completion"}, {"st": 9, "ed": 12, "text": "low rank matrix"}, {"st": 25, "ed": 27, "text": "matrix completion"}, {"st": 44, "ed": 46, "text": "matrix completion"}, {"st": 96, "ed": 98, "text": "parameter tuning"}, {"st": 104, "ed": 106, "text": "latent factors"}, {"st": 109, "ed": 112, "text": "synthetic and real"}, {"st": 114, "ed": 116, "text": "encouraging results"}, {"st": 120, "ed": 122, "text": "collaborative filtering"}]
[{"st": 1, "ed": 3, "text": "real world"}, {"st": 51, "ed": 53, "text": "important applications"}, {"st": 55, "ed": 57, "text": "recommendation systems"}, {"st": 63, "ed": 65, "text": "reinforcement learning"}, {"st": 69, "ed": 72, "text": "markov decision processes"}, {"st": 125, "ed": 127, "text": "recommendation systems"}, {"st": 141, "ed": 143, "text": "feature representations"}, {"st": 156, "ed": 158, "text": "unlike existing"}, {"st": 200, "ed": 202, "text": "real world"}, {"st": 208, "ed": 210, "text": "deterministic policy"}]
[{"st": 2, "ed": 5, "text": "sequential decision making"}, {"st": 16, "ed": 19, "text": "taking into account"}, {"st": 42, "ed": 44, "text": "reinforcement learning"}, {"st": 48, "ed": 51, "text": "markov decision processes"}, {"st": 107, "ed": 109, "text": "policy gradient"}, {"st": 130, "ed": 132, "text": "lagrange multiplier"}]
[{"st": 30, "ed": 32, "text": "computational models"}, {"st": 34, "ed": 36, "text": "automatically generate"}, {"st": 39, "ed": 41, "text": "existing approaches"}, {"st": 54, "ed": 56, "text": "real world"}, {"st": 57, "ed": 59, "text": "current approaches"}, {"st": 88, "ed": 90, "text": "previous works"}, {"st": 108, "ed": 110, "text": "topic model"}, {"st": 121, "ed": 123, "text": "nonparametric bayesian"}]
[{"st": 54, "ed": 56, "text": "online learning"}, {"st": 91, "ed": 93, "text": "training data"}, {"st": 130, "ed": 132, "text": "regret bounds"}, {"st": 180, "ed": 182, "text": "grocery store"}, {"st": 235, "ed": 237, "text": "regret bounds"}]
[{"st": 75, "ed": 77, "text": "structured prediction"}, {"st": 89, "ed": 91, "text": "point cloud"}, {"st": 91, "ed": 93, "text": "natural language"}, {"st": 100, "ed": 102, "text": "embedding space"}, {"st": 111, "ed": 113, "text": "semantically meaningful"}, {"st": 122, "ed": 124, "text": "pre training"}, {"st": 129, "ed": 131, "text": "feature embedding"}, {"st": 135, "ed": 137, "text": "fine tuning"}, {"st": 138, "ed": 140, "text": "embedding space"}]
[{"st": 0, "ed": 2, "text": "link prediction"}, {"st": 11, "ed": 13, "text": "knowledge graph"}, {"st": 28, "ed": 30, "text": "link prediction"}, {"st": 37, "ed": 39, "text": "knowledge graph"}, {"st": 46, "ed": 48, "text": "link prediction"}, {"st": 54, "ed": 56, "text": "knowledge graph"}, {"st": 65, "ed": 67, "text": "latent feature"}, {"st": 69, "ed": 71, "text": "link prediction"}, {"st": 75, "ed": 77, "text": "prediction task"}, {"st": 91, "ed": 93, "text": "ranking based"}, {"st": 98, "ed": 100, "text": "large scale"}, {"st": 108, "ed": 110, "text": "link prediction"}, {"st": 110, "ed": 112, "text": "approach achieves"}, {"st": 134, "ed": 136, "text": "knowledge graph"}, {"st": 147, "ed": 149, "text": "link prediction"}]
[{"st": 9, "ed": 11, "text": "reinforcement learning"}, {"st": 11, "ed": 13, "text": "rl agent"}, {"st": 62, "ed": 64, "text": "takes place"}, {"st": 76, "ed": 78, "text": "update rule"}]
[{"st": 79, "ed": 81, "text": "machine learning"}]
[{"st": 5, "ed": 8, "text": "best arm identification"}, {"st": 24, "ed": 26, "text": "access network"}, {"st": 65, "ed": 67, "text": "core network"}, {"st": 91, "ed": 93, "text": "near optimal"}, {"st": 180, "ed": 182, "text": "proposed algorithm"}, {"st": 182, "ed": 184, "text": "shows significant"}]
[{"st": 0, "ed": 2, "text": "concept drift"}, {"st": 5, "ed": 7, "text": "smart grid"}, {"st": 37, "ed": 39, "text": "decision tree"}, {"st": 45, "ed": 47, "text": "random forest"}, {"st": 53, "ed": 55, "text": "majority voting"}, {"st": 70, "ed": 72, "text": "base learner"}, {"st": 83, "ed": 85, "text": "base learners"}, {"st": 112, "ed": 114, "text": "empirical comparison"}, {"st": 119, "ed": 121, "text": "random forest"}]
[{"st": 6, "ed": 8, "text": "recommender systems"}, {"st": 35, "ed": 37, "text": "principled approach"}, {"st": 62, "ed": 64, "text": "matrix factorization"}, {"st": 69, "ed": 71, "text": "prediction performance"}, {"st": 76, "ed": 79, "text": "theoretically and empirically"}]
[{"st": 0, "ed": 2, "text": "complex network"}, {"st": 4, "ed": 6, "text": "hyperbolic geometry"}, {"st": 16, "ed": 18, "text": "challenging problems"}, {"st": 20, "ed": 22, "text": "complex network"}, {"st": 65, "ed": 67, "text": "complex network"}, {"st": 80, "ed": 82, "text": "hyperbolic space"}, {"st": 104, "ed": 106, "text": "real applications"}, {"st": 110, "ed": 112, "text": "link prediction"}, {"st": 126, "ed": 128, "text": "machine learning"}, {"st": 134, "ed": 136, "text": "dimensionality reduction"}, {"st": 152, "ed": 154, "text": "dimensional space"}, {"st": 180, "ed": 183, "text": "fast and accurate"}, {"st": 189, "ed": 191, "text": "hyperbolic space"}]
[{"st": 16, "ed": 18, "text": "learning process"}, {"st": 34, "ed": 36, "text": "probabilistic model"}, {"st": 63, "ed": 65, "text": "collaborative filtering"}, {"st": 66, "ed": 68, "text": "recommender systems"}, {"st": 96, "ed": 98, "text": "maximum likelihood"}, {"st": 110, "ed": 112, "text": "empirical evaluation"}, {"st": 113, "ed": 115, "text": "large scale"}]
[{"st": 91, "ed": 93, "text": "theoretical framework"}, {"st": 150, "ed": 152, "text": "extensive simulations"}]
[{"st": 0, "ed": 2, "text": "reinforcement learning"}, {"st": 13, "ed": 15, "text": "cost function"}, {"st": 33, "ed": 35, "text": "optimal control"}, {"st": 58, "ed": 60, "text": "key challenges"}, {"st": 62, "ed": 64, "text": "optimal control"}, {"st": 86, "ed": 88, "text": "cost function"}, {"st": 110, "ed": 112, "text": "cost functions"}, {"st": 114, "ed": 116, "text": "neural networks"}, {"st": 129, "ed": 131, "text": "sample based"}, {"st": 146, "ed": 148, "text": "real world"}, {"st": 152, "ed": 154, "text": "substantial improvement"}, {"st": 155, "ed": 157, "text": "prior methods"}]
[{"st": 2, "ed": 4, "text": "reinforcement learning"}, {"st": 6, "ed": 8, "text": "successfully applied"}, {"st": 12, "ed": 14, "text": "challenging problems"}, {"st": 22, "ed": 24, "text": "neural network"}, {"st": 30, "ed": 32, "text": "sample complexity"}, {"st": 62, "ed": 64, "text": "sample complexity"}, {"st": 65, "ed": 68, "text": "deep reinforcement learning"}, {"st": 93, "ed": 95, "text": "learning algorithm"}, {"st": 110, "ed": 112, "text": "policy gradient"}, {"st": 125, "ed": 127, "text": "experience replay"}, {"st": 131, "ed": 133, "text": "substantially improves"}, {"st": 155, "ed": 157, "text": "learned models"}, {"st": 169, "ed": 171, "text": "linear models"}, {"st": 179, "ed": 181, "text": "faster learning"}]
[{"st": 1, "ed": 3, "text": "real world"}, {"st": 8, "ed": 10, "text": "large scale"}, {"st": 26, "ed": 28, "text": "nash equilibria"}, {"st": 43, "ed": 46, "text": "end to end"}, {"st": 50, "ed": 52, "text": "nash equilibria"}, {"st": 78, "ed": 80, "text": "nash equilibrium"}, {"st": 82, "ed": 84, "text": "reinforcement learning"}, {"st": 94, "ed": 96, "text": "real world"}]
[{"st": 26, "ed": 28, "text": "decision making"}, {"st": 33, "ed": 35, "text": "renewable energy"}, {"st": 66, "ed": 68, "text": "decision making"}, {"st": 73, "ed": 75, "text": "reinforcement learning"}, {"st": 82, "ed": 84, "text": "a level"}, {"st": 102, "ed": 104, "text": "policy improvement"}]
[{"st": 5, "ed": 7, "text": "optimization problem"}, {"st": 25, "ed": 28, "text": "trial and error"}, {"st": 34, "ed": 36, "text": "sample efficient"}, {"st": 36, "ed": 38, "text": "optimization algorithm"}, {"st": 155, "ed": 157, "text": "sample efficient"}, {"st": 181, "ed": 183, "text": "decision process"}, {"st": 252, "ed": 254, "text": "method achieves"}]
[{"st": 109, "ed": 111, "text": "decision maker"}, {"st": 115, "ed": 117, "text": "linear programming"}, {"st": 122, "ed": 124, "text": "piecewise linear"}, {"st": 142, "ed": 144, "text": "piecewise linear"}, {"st": 153, "ed": 155, "text": "semidefinite programming"}]
[{"st": 11, "ed": 13, "text": "linear classifier"}, {"st": 34, "ed": 36, "text": "recently gained"}, {"st": 45, "ed": 47, "text": "reverse engineering"}, {"st": 78, "ed": 80, "text": "reverse engineering"}, {"st": 84, "ed": 86, "text": "general framework"}, {"st": 94, "ed": 96, "text": "d dimensional"}, {"st": 101, "ed": 103, "text": "low dimensional"}, {"st": 119, "ed": 121, "text": "linear classifier"}, {"st": 125, "ed": 127, "text": "theoretical guarantee"}, {"st": 134, "ed": 136, "text": "proposed method"}, {"st": 149, "ed": 152, "text": "curse of dimensionality"}, {"st": 153, "ed": 155, "text": "computational complexity"}, {"st": 155, "ed": 157, "text": "scales linearly"}, {"st": 170, "ed": 172, "text": "near optimal"}, {"st": 174, "ed": 176, "text": "constant factor"}, {"st": 185, "ed": 187, "text": "theoretical analysis"}, {"st": 201, "ed": 203, "text": "consistently outperforms"}, {"st": 214, "ed": 217, "text": "orders of magnitude"}]
[{"st": 23, "ed": 26, "text": "learning from demonstration"}, {"st": 55, "ed": 57, "text": "time consuming"}, {"st": 89, "ed": 91, "text": "challenging problem"}, {"st": 109, "ed": 112, "text": "deep neural network"}, {"st": 119, "ed": 122, "text": "short term memory"}, {"st": 123, "ed": 126, "text": "recurrent neural network"}, {"st": 130, "ed": 132, "text": "training process"}, {"st": 155, "ed": 157, "text": "virtual environment"}, {"st": 187, "ed": 189, "text": "inverse kinematics"}, {"st": 244, "ed": 246, "text": "feedforward networks"}, {"st": 247, "ed": 250, "text": "mean squared error"}, {"st": 260, "ed": 262, "text": "training set"}]
[{"st": 1, "ed": 3, "text": "collaborative filtering"}, {"st": 30, "ed": 32, "text": "natural language"}, {"st": 38, "ed": 40, "text": "latent representation"}, {"st": 49, "ed": 51, "text": "skip gram"}]
[{"st": 123, "ed": 127, "text": "recurrent neural networks rnns"}, {"st": 188, "ed": 190, "text": "rnn model"}]
[{"st": 2, "ed": 4, "text": "belief propagation"}, {"st": 15, "ed": 17, "text": "belief propagation"}, {"st": 58, "ed": 60, "text": "computational complexity"}, {"st": 85, "ed": 87, "text": "belief propagation"}, {"st": 114, "ed": 116, "text": "per iteration"}, {"st": 214, "ed": 216, "text": "fixed point"}, {"st": 226, "ed": 229, "text": "mean square error"}]
