{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import subprocess\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOPHRASE_PATH = '/Users/rishimasand/Documents/school/college/research/text_mining/AutoPhrase'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model2segmented_text_path(model_name):\n",
    "    return os.path.join(AUTOPHRASE_PATH, model_name, 'segmentation.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_autophrase(text_to_seg, model):\n",
    "    if os.path.exists(model2segmented_text_path(model)):\n",
    "        return\n",
    "\n",
    "    os.environ['RAW_TRAIN'] =  os.path.abspath(text_to_seg)\n",
    "    os.environ['MODEL'] = model\n",
    "    mycwd = os.getcwd()\n",
    "    print(os.getcwd())\n",
    "    os.chdir(AUTOPHRASE_PATH)\n",
    "    print(os.getcwd())\n",
    "    proc = subprocess.Popen('bash auto_phrase.sh'.split(), stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n",
    "    for line in proc.stdout:\n",
    "        print(line)\n",
    "    os.chdir(mycwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment(text_to_seg, model):\n",
    "    os.environ['TEXT_TO_SEG'] = os.path.abspath(text_to_seg)   # tell autophrase with abspath\n",
    "    os.environ['MODEL'] = model\n",
    "    mycwd = os.getcwd()\n",
    "    os.chdir(AUTOPHRASE_PATH)\n",
    "    proc = subprocess.Popen('bash phrasal_segmentation.sh'.split(), stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n",
    "    for line in proc.stdout:\n",
    "        print(line)\n",
    "    os.chdir(mycwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_marker(text):\n",
    "    return re.sub('</?phrase>', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_line_count(inFile):\n",
    "    count = -1\n",
    "    for count, line in enumerate(open(inFile, 'r')):\n",
    "        pass\n",
    "    count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def condenseSpace(s):\n",
    "    return re.sub('([\\s])+', '\\g<1>', s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_nps(nps, words_original):\n",
    "    validated_nps = []\n",
    "    for np in sorted(nps, key=lambda x:x['st']):\n",
    "        st = np['st']\n",
    "        ed = np['ed']\n",
    "        token_span = words_original[st:ed]\n",
    "        # 'A polynomial time algorithm for the Lambek calculus with brackets of  bounded order'\n",
    "        if ' '.join(token_span).strip() != np['text'].strip():\n",
    "            print(' '.join(token_span))\n",
    "            print(np)\n",
    "            return validated_nps\n",
    "        validated_nps.append(np)\n",
    "    return nps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ap_tokens = []\n",
    "all_nps = []\n",
    "def write_to_json(inFile, outFile, originalFile):\n",
    "    with open(inFile, 'r') as fin, open(outFile, 'w') as fout, open(originalFile, 'r') as fOriginal:\n",
    "        total = get_line_count(inFile)\n",
    "\n",
    "        cnt = 0\n",
    "        data = []\n",
    "        for i, (line, line_original) in tqdm(enumerate(zip(fin, fOriginal)), total=total):\n",
    "            text = line.strip()\n",
    "\n",
    "            tokens = text.split(' ')\n",
    "            original_tokens = line_original.split()\n",
    "            all_ap_tokens.extend(original_tokens)\n",
    "            clean_tokens = condenseSpace(remove_marker(text)).split(' ')\n",
    "            nps = []\n",
    "            for idx, token in enumerate(tokens):\n",
    "                if '<phrase>' in token:\n",
    "                    if token.startswith('<phrase>'):\n",
    "                        span = {'st': idx}\n",
    "                    else:\n",
    "                        span = {}\n",
    "                elif '</phrase>' in token:\n",
    "                    if token.endswith('</phrase>'):\n",
    "                        try:\n",
    "                            if span:\n",
    "                                span['ed'] = idx + 1\n",
    "                                span['text'] = ' '.join(clean_tokens[span['st']:span['ed']])\n",
    "                                nps.append(span)\n",
    "                            span = {}\n",
    "                        except Exception as e:\n",
    "                            ipdb.set_trace()\n",
    "                            print(e)\n",
    "                    else:\n",
    "                        span = {}\n",
    "            if nps:\n",
    "                nps_v = validate_nps(nps, original_tokens)\n",
    "                if nps_v != nps:\n",
    "                    ipdb.set_trace()\n",
    "                nps = nps_v\n",
    "            fout.write(json.dumps(nps))\n",
    "            all_nps.extend(nps)\n",
    "            fout.write('\\n')\n",
    "    with open('data/arxiv_abstracts_10000_autophrase_raw.txt', 'w') as f:\n",
    "        f.write('\\n'.join(list(set(all_ap_tokens))))\n",
    "        f.close()\n",
    "    with open('data/arxiv_abstracts_10000_autophrase_nps.txt', 'w') as f:\n",
    "        f.write('\\n'.join(list(set([x['text'] for x in all_nps]))))\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\x1b[32m===Compilation===\\x1b[m\\n'\n",
      "b'\\x1b[32m===Tokenization===\\x1b[m\\n'\n",
      "b'Current step: Tokenizing input file...\\x1b[0K\\r\\n'\n",
      "b'real\\t0m2.143s\\n'\n",
      "b'user\\t0m12.244s\\n'\n",
      "b'sys\\t0m0.467s\\n'\n",
      "b'Detected Language: EN\\x1b[0K\\n'\n",
      "b'\\x1b[32m===Part-Of-Speech Tagging===\\x1b[m\\n'\n",
      "b'Current step: Splitting files...\\x1b[0K\\rCurrent step: Tagging...\\x1b[0K\\rCurrent step: Merging...\\x1b[0K\\r\\n'\n",
      "b'\\x1b[32m===Phrasal Segmentation===\\x1b[m\\n'\n",
      "b'=== Current Settings ===\\n'\n",
      "b'Segmentation Model Path = arxiv_abstracts_10000/segmentation.model\\n'\n",
      "b'After the phrasal segmentation, only following phrases will be highlighted with <phrase> and </phrase>\\n'\n",
      "b'\\tQ(multi-word phrases) >= 0.500000\\n'\n",
      "b'\\tQ(single-word phrases) >= 0.800000\\n'\n",
      "b'=======\\n'\n",
      "b'POS guided model loaded.\\n'\n",
      "b'# of loaded patterns = 19370\\n'\n",
      "b'# of loaded truth patterns = 47857\\n'\n",
      "b'POS transition matrix loaded\\n'\n",
      "b'Phrasal segmentation finished.\\n'\n",
      "b'   # of total highlighted quality phrases = 204754\\n'\n",
      "b'   # of total processed sentences = 68231\\n'\n",
      "b'   avg highlights per sentence = 3.00089\\n'\n",
      "b'\\n'\n",
      "b'real\\t0m2.767s\\n'\n",
      "b'user\\t0m2.669s\\n'\n",
      "b'sys\\t0m0.072s\\n'\n",
      "b'\\x1b[32m===Generating Output===\\x1b[m\\n'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 321/10000 [00:00<00:03, 3204.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\n'\n",
      "b'real\\t0m1.836s\\n'\n",
      "b'user\\t0m3.542s\\n'\n",
      "b'sys\\t0m0.223s\\n'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:02<00:00, 3349.27it/s]\n"
     ]
    }
   ],
   "source": [
    "data_dir = 'data'\n",
    "input_file_name = 'arxiv_abstracts_10000.txt'\n",
    "extensionless_input_file_name = input_file_name.split('.')[0]\n",
    "input_file_path = f'{data_dir}/{input_file_name}'\n",
    "tokenized_text_autophrase_file_path = f'{data_dir}/{extensionless_input_file_name}_autophrase.json'\n",
    "\n",
    "autophrase_model_name = extensionless_input_file_name\n",
    "\n",
    "train_autophrase(input_file_path, autophrase_model_name)\n",
    "segment(input_file_path, autophrase_model_name)\n",
    "\n",
    "write_to_json(model2segmented_text_path(autophrase_model_name), tokenized_text_autophrase_file_path, input_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "se",
   "language": "python",
   "name": "se"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
